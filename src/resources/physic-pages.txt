
<doc id="110194" url="https://it.wikipedia.org/wiki?curid=110194" title="Indice di rifrazione">
Indice di rifrazione
In fisica, l'indice di rifrazione di un materiale è una grandezza adimensionale che quantifica la diminuzione della velocità di propagazione della radiazione elettromagnetica quando attraversa un materiale. La diminuzione della velocità di propagazione viene accompagnata dalla variazione della sua direzione, secondo il fenomeno della rifrazione.
Si tratta di una grandezza utilizzata in svariati ambiti della scienza, e la sua misura può essere usata per identificare la natura del materiale in cui si propaga la radiazione. Ad esempio, in chimica vengono comunemente effettuate misure dell'indice di rifrazione con lo scopo di trarne indicazioni analitiche. In funzione dei parametri solvente, lunghezza d'onda incidente e temperatura, si effettua la misura del parametro utilizzando un rifrattometro. Questa metodica analitica viene utilizzata in vari campi: in campo medico per analisi del sangue e delle urine, in ambito industriale nell'analisi dei materiali, per determinare la concentrazione zuccherina in succhi di frutta o il grado alcolico di bevande, per certificare il livello qualitativo o evidenziare sofisticazioni di alimenti quali l'olio, il latte e il burro.
La radiazione viaggia alla massima velocità formula_1, detta velocità della luce, quando si trova nel vuoto. L'indice di rifrazione è il rapporto tra formula_1 e la velocità formula_3 della radiazione nel mezzo:
Dipende in generale dalla frequenza della radiazione, e pertanto si tratta di un numero complesso strettamente legato alla permittività elettrica.
Si consideri un'onda elettromagnetica monocromatica, che scritta in funzione del campo elettrico formula_5 ha la forma:
dove formula_7 è l'ampiezza e formula_8 è la frequenza angolare dell'onda. Il vettore d'onda è dato da formula_9, con formula_10 la direzione di propagazione e formula_11 il numero d'onda:
in cui il numero:
è la lunghezza d'onda della radiazione quando si propaga nel vuoto. La lunghezza d'onda nel materiale è data da:
e l'indice di rifrazione (in assenza di assorbimento) è:
dove formula_16 è la velocità di fase, cioè la velocità alla quale si propagano le creste dell'onda.
Le equazioni di Maxwell in un materiale possono essere scritte come:
assieme alle equazioni costitutive:
che descrivono la reazione nel mezzo alla presenza di un campo elettromagnetico.
Per risolvere queste equazioni è necessario formulare delle ipotesi (che rappresentano inevitabilmente un'approssimazione) sulla dipendenza di formula_20 ed formula_21 da formula_5 e da formula_23. Assumendo formula_24, un'approssimazione al primo ordine è che la polarizzazione del mezzo sia lineare con il campo elettrico:
dove formula_26 è la suscettività elettrica. Questa approssimazione è valida a meno che non si considerino campi estremamente intensi, come quelli che si possono ottenere con un laser: quando non è più valida si entra nel regime dell'ottica non lineare. Si assume inoltre che non ci siano cariche libere, ovvero che formula_27 e formula_28:
derivando rispetto al tempo la quarta equazione e facendo il rotore della seconda si ottiene:
Uguagliando allora il rotore della derivata nel tempo di formula_23 (dalla prima relazione) con la derivata nel tempo del rotore di formula_23 (primo termine della seconda relazione) si ha:
La prima equazione implica che la divergenza del campo elettrico è nulla. Dall'analisi differenziale è noto che per un generico vettore formula_36 si ha:
Da queste segue che il rotore del rotore del campo elettrico è pari all'opposto del laplaciano del campo stesso:
Ricordandosi che la velocità della luce può essere scritta come:
questa diventa:
ossia l'equazione di un'onda che si propaga, non a velocità formula_1 ma ad una velocità di fase inferiore pari a:
Il fattore formula_43 è l'indice di rifrazione, e può essere riscritto in funzione della costante dielettrica e della permeabilità magnetica del mezzo come:
Nel caso in cui sia formula_45 che formula_46 siano negativi la soluzione corretta delle equazioni di Maxwell impone che si debba scegliere come indice di rifrazione la radice negativa, e quindi formula_47. Questa condizione non viene mai raggiunta nei materiali reali ma è stata dimostrata la possibilità di usare dei metamateriali per ottenerla.
La legge di Snell descrive quanto la direzione di propagazione della luce è deviata nel passare da un mezzo ad un altro. Essa afferma che se il raggio proviene da una regione con indice di rifrazione formula_48 ed entra in un mezzo ad indice formula_49, gli angoli di incidenza formula_50 e di rifrazione formula_51 sono legati dall'espressione:
dove formula_53 e formula_54 sono le velocità nei mezzi, e la velocità della radiazione deve cambiare da formula_55 a formula_56. Se non vi è nessun angolo formula_51 che soddisfa la relazione, ovvero:
la luce non viene trasmessa nel secondo mezzo e si verifica il fenomeno di riflessione interna totale.
A partire dalle equazioni di Maxwell è possibile dimostrare, sfruttando il fatto che il campo elettrostatico è conservativo, che passando da un mezzo ad un altro la componente del campo elettrico tangente all'interfaccia è continua. Questo si relaziona al fatto che, dal momento che l'intensità del vettore d'onda formula_59 è proporzionale all'energia del fotone incidente, la sua componente trasversale si deve conservare. Dato che la componente trasversale del vettore d'onda resta uguale, si ha che formula_60 e quindi:
da cui formula_62.
Quando un'onda elettromagnetica incide sul materiale, parte di essa viene riflessa. La quantità di luce che viene riflessa dipende dalla riflettanza della superficie. Tale grandezza può essere calcolata a partire dall'indice di rifrazione e dall'angolo di incidenza per mezzo dell'equazione di Fresnel, secondo la quale la componente normale della riflessione viene ridotta di:
Per il vetro immerso in aria formula_64 e formula_65, che significa che il 4% della potenza viene riflessa.
Vi è un angolo formula_66, detto angolo di Brewster, per cui la radiazione polarizzata lungo il piano di incidenza viene totalmente trasmessa:
e non vi è dunque riflessione.
In tutti i sistemi reali l'indice di rifrazione varia con la frequenza dell'onda incidente, e per la legge di Snell a frequenze diverse corrispondono angoli di rifrazione diversi. Un esempio ben noto di questo fenomeno è il fatto che la luce bianca (contenente tutte le componenti spettrali) viene scomposta da un prisma.
Quando un materiale presenta assorbimento non è più possibile descrivere l'indice di rifrazione tramite un numero reale ma bisogna definire un indice di rifrazione complesso:
dove formula_69 definisce la velocità di fase con cui si propaga l'onda e formula_70 è proporzionale al coefficiente di assorbimento del sistema. L'assorbimento dell'energia della radiazione da parte del materiale è strettamente legato al fenomeno della dispersione, e le quantità formula_69 e formula_70 sono legate dalla relazione di Kramers-Kronig.
Per mostrare che formula_70 quantifica l'assorbimento dell'energia del campo è sufficiente inserire formula_69 nell'espressione del campo elettrico in un'onda piana che si propaga in direzione "z":
Considerando il vettore d'onda come numero complesso formula_76, la cui parte reale è formula_77, si ha:
Si nota che formula_70 fornisce un decadimento esponenziale, come previsto dalla Legge di Lambert-Beer. Dato che l'intensità dell'onda è proporzionale al quadrato dell'intensità del campo elettrico, il coefficiente di assorbimento diventa formula_80.
In alcune condizioni particolari (ad esempio vicino a delle risonanze dell'assorbimento) è possibile che formula_81 sia minore di 1. In questi casi la velocità di fase può essere superiore alla velocità della luce. Questo però non viola la relatività ristretta perché la velocità del segnale è la velocità di gruppo la quale rimane sempre inferiore a "c".
L'assorbimento di un materiale è la sua capacità di assorbire l'energia associata alla radiazione elettromagnetica che si propaga all'interno di esso. Si tratta dell'energia dei fotoni che viene ceduta agli elettroni, atomi e molecole del materiale: l'energia del campo elettromagnetico si trasforma in questo modo in energia interna del materiale, come ad esempio la sua energia termica. Solitamente l'intensità dell'onda elettromagnetica non influisce sull'assorbimento (in caso contrario si parla di "assorbimento non lineare"), e la sua riduzione è anche detta attenuazione.
L'assorbimento dipende sia dalla natura del materiale, sia dalla frequenza della radiazione, e può essere quantificato attraverso la permittività elettrica: si tratta di una funzione complessa della frequenza dell'onda, attraverso la quale è possibile trattare la propagazione del campo elettromagnetico in mezzi dissipativi. Normalmente il valore della permittività elettrica viene scritto come il prodotto formula_82 della permittività relativa formula_83 e della permettività del vuoto formula_84, detta anche costante dielettrica del vuoto. Poiché varia a seconda della direzione del campo elettrico rispetto al mezzo, essa è rappresentata attraverso un tensore, e solo nel caso di un dielettrico perfetto tutte le componenti del tensore hanno lo stesso valore, chiamato impropriamente "costante dielettrica".
La permittività e l'indice di rifrazione sono legati dalla relazione:
dove formula_86 è la permeabilità magnetica relativa e formula_83 la permittività elettrica relativa, un numero complesso:
In un mezzo con formula_89 (approssimazione valida per la maggior parte dei materiali), la permittività elettrica è dunque il quadrato dell'indice di rifrazione complesso. Valgono le seguenti relazioni:
Quando si analizza la permittività dal punto di vista della frequenza del campo si nota che essa può presentare un comportamento anomalo in corrispondenza di certe lunghezze d'onda. Infatti, la parte immaginaria della permittività elettrica segue un andamento risonante in corrispondenza dei suoi poli, dove presenta uno o più picchi. In corrispondenza di questi picchi l'assorbimento da parte del materiale dell'energia posseduta dal campo è massimo.
Nei materiali anisotropi la polarizzazione formula_92 non dipende solo dall'intensità del campo elettrico ma anche dalla sua polarizzazione, ovvero la costante dielettrica non è uguale sui tre assi del sistema di riferimento. Di conseguenza la costante dielettrica non può più essere descritta tramite uno scalare ma deve essere rappresentato tramite una matrice (o, più formalmente, tramite un tensore). In questo caso si ha il fenomeno detto di birifrangenza dove fasci di luce con polarizzazione diversa ed incidenti ad angoli diversi "vedono" un indice di rifrazione diverso e quindi vengono rifratti in direzioni diverse. Storicamente questo fenomeno è stato osservato per la prima volta nella calcite.
La birifrangenza è molto sfruttata sia nell'ottica non lineare che per la realizzazione di dispositivi elettro-ottici, per esempio lamine che funzionano da ritardatore di fase (lamine a mezz'onda o quarto d'onda), o dispositivi per la generazione di seconda armonica in un laser.
</doc>

<doc id="111557" url="https://it.wikipedia.org/wiki?curid=111557" title="Onda d'urto (fluidodinamica)">
Onda d'urto (fluidodinamica)
In fluidodinamica ed aerodinamica con il termine onda d'urto si indica un sottile strato di forte variazione dei campi di pressione, temperatura, densità e velocità del fluido. Tale sottile spessore, dell'ordine di 10 nm, viene modellato matematicamente come una discontinuità.
Un'onda d'urto può essere normale oppure obliqua alla direzione della velocità relativa tra onda e corrente, e può altresì essere stazionaria oppure spostarsi rispetto ad un corpo che la genera. Le onde sonore, essendo identificabili come "piccoli disturbi" di pressione e di velocità, in quanto queste ultime grandezze sono legate nelle equazioni che governano il fenomeno, rappresentano delle onde d'urto che, per la loro bassa intensità, possono essere considerate isoentropiche, cioè onde che non modificano sensibilmente l'entropia del flusso che le attraversa o che attraversano (sono anche dette onde di Mach). Il meccanismo delle onde d'urto oblique è in grado di deviare un flusso supersonico.
Di particolare interesse sono anche le onde d'urto adiabatiche, cioè quelle che si possono verificare in una corrente di fluido animata da moto omoenergetico.
 Si consideri la figura a destra. Si immagini un serbatoio a monte del condotto di figura che per qualche motivo si svuoti generando un flusso di fluido (che considereremo gas perfetto) all'interno del condotto. Dette 1 e 2 le due sezioni di controllo, detta "T" la temperatura totale nel serbatoio, e "p" la pressione totale, detto "τ" il volume di controllo, e siano le variazioni di sezione fra 1 e 2 trascurabili, individuando con formula_1 la normale alla sezione 1 e con formula_2 alla sezione 2, si immagini che, a causa delle condizioni di pressione a valle del condotto, o delle condizioni di raccordo del condotto stesso, il fluido sia costretto a cambiare repentinamente le sue proprietà di pressione, velocità e temperatura all'interno di un piccolo volume (indicato appunto con "τ").
Chiameremo questa zona di discontinuità onda d'urto normale.
Supponendo il flusso stazionario, e cioè nulle le derivate delle quantità rispetto al tempo, facciamo il bilancio della massa e della quantità di moto. Ipotizzando un flusso all'ingresso del volume di controllo supersonico unidimensionale, indicheremo con "ρ" la densità del fluido, con u la velocità e con A la sezione.
Bilancio di massa:
Coincidendo formula_4 con formula_5 il bilancio diviene
dove G è una costante invariante a monte e a valle del volume di controllo.
Bilancio della quantità di moto:
Abbiamo indicato con formula_8 la risultante delle azioni del condotto sul fluido, con M la massa di fluido, e con formula_9 l'accelerazione di gravità.
Trascuriamo ora il peso del fluido e l'azione del condotto sul fluido stesso, agendo essa sull'area laterale del volume, di ordine inferiore rispetto alle aree frontali. Dunque poiché formula_10 e formula_11 allora il bilancio della quantità di moto diviene semplicemente formula_12 invariante a monte e a valle del volume di controllo.
Facciamo ora il bilancio dell'energia:
Abbiamo dunque tre invarianti: G, I, e formula_14. Ricordiamo la definizione di velocità del suono critica formula_19:
Si è indicata con formula_21 la velocità del suono ad entalpia totale e formula_22.
Inoltre formula_23 e dunque giungiamo all'equazione che regola le onde d'urto normali:
Chiamiamo formula_25 e formula_26 le due soluzioni dell'equazione (reali e distinte oppure reali e coincidenti), poiché per la nota proprietà delle equazioni di secondo grado formula_27, allora in un urto normale è formula_28, dove con formula_29 abbiamo indicato il numero di Mach critico, definito come formula_30. Da questa relazione notiamo subito che un flusso attraverso un'onda d'urto normale passa da supersonico a subsonico o viceversa (ma quest'ultima alternativa è impossibile perché viola il 2º principio della termodinamica). 
La relazione che lega i numeri di Mach "veri" è la seguente:
Osservando tale relazione si nota che per formula_32 allora anche formula_33 (in questo caso avremo una zona di debole discontinuità, fenomeno quasi isoentropico chiamato "onda di Mach"). Se invece formula_34 allora formula_35.
Per quanto riguarda le velocità:
La velocità dunque attraverso un urto normale diminuisce.
Per le pressioni:
La pressione aumenta, dunque, attraverso l'onda.
Dalle leggi di Poisson si ricava poi:
Se formula_39 allora anche formula_40 e viceversa se formula_41. Indicando con formula_42 l'entropia, poiché formula_43 e che formula_44 per il secondo principio della termodinamica, allora è che formula_45 e dunque formula_39. Sono dunque possibili onde d'urto normali solo con flusso in ingresso supersonico. 
Per quanto riguarda la temperatura:
Da cui formula_48 perché il primo membro dell'equazione detta è negativo. Dunque la temperatura aumenta attraverso l'onda.
 Le onde d'urto oblique sono zone di discontinuità del campo fluidodinamico poste con un angolo diverso da 90° rispetto al flusso. Considerando la figura a destra, si chiami v la velocità di un sistema di riferimento che trasli senza accelerare rispetto ad un'onda d'urto normale. Chiamo formula_25 la velocità del fluido in ingresso rispetto ad un riferimento fermo, mentre formula_50 la velocità vista secondo il sistema di riferimento traslante. L'osservatore solidale con il sistema di riferimento traslante vede entrare un flusso con angolo formula_51 rispetto all'onda, e lo vede uscire deviato di un angolo formula_52. Rispetto alla trattazione fatta nel paragrafo precedente, cambieranno le quantità relative alle velocità, ma non quelle relative all'entalpia o all'entropia. Chiamo formula_53 la nuova entalpia totale, sempre invariante, mentre individuo in formula_54 l'entalpia totale relativa alla parte normale della velocità del fluido. Poiché energeticamente non è cambiato nulla rispetto alla situazione precedente, il salto di entropia sarà lo stesso.
Dunque la relazione che lega il numero di Mach d'entrata e uscita nel sistema di riferimento mobile sarà:
Il salto di densità è dato da:
La pressione varia secondo la relazione:
 La relazione tra formula_61 e formula_62, il cui grafico troviamo a sinistra, è la seguente:
Fissato un certo Mach in ingresso, come si vede dal grafico data la svolta formula_61 esistono due possibili soluzioni: una con il flusso in uscita supersonico ed una con flusso in uscita subsonico (una con formula_62 maggiore, ed una con formula_62 minore). Inoltre si individua un angolo di svolta massimo, indicato nel grafico come formula_67. Il significato fisico di questo angolo massimo è molto importante e si intuisce immediatamente che un flusso supersonico deviato da un'onda obliqua non potrà effettuare svolte superiori al formula_67 indicato in figura.
</doc>

<doc id="115062" url="https://it.wikipedia.org/wiki?curid=115062" title="Conduttore elettrico">
Conduttore elettrico
Il conduttore elettrico è un materiale in grado di far scorrere corrente elettrica al suo interno. I materiali conduttori sono caratterizzati dalla presenza di elettroni liberi nella banda di valenza degli atomi del reticolo cristallino (conduttori di prima specie) o contengono specie ioniche che si fanno carico di trasportare la corrente elettrica (conduttori di seconda specie).
La conducibilità elettrica di un conduttore di prima specie può essere interpretata mediante il modello delle bande. La carica netta su un conduttore si distribuisce sulla sua superficie, poiché in questo modo le singole cariche (che si respingono) massimizzano la loro distanza reciproca raggiungendo una configurazione che minimizza l'energia.
I materiali metallici (metalli e loro leghe) sono in genere buoni conduttori; i migliori in ordine decrescente sono:
D'altra parte possono condurre facilmente l'elettricità anche:
All'interno dei conduttori sono presenti cariche elettriche libere di muoversi, pertanto una volta raggiunto l'equilibrio elettrostatico, necessariamente il campo elettrico all'interno del conduttore è pari a zero (se così non fosse le cariche sarebbero accelerate e non vi sarebbe equilibrio). Tenendo conto di questo e grazie al teorema del flusso si ha che le cariche elettriche (o meglio gli eccessi di carica) si dispongono sulle superfici esterne dei conduttori.
Essendo
il campo nullo significa che lo spazio entro il conduttore è equipotenziale. È possibile dimostrare che, fissate le condizioni esterne, la distribuzione di carica superficiale del conduttore è unica (a meno di un coefficiente costante che dipende dal potenziale) e dipende dalla geometria del conduttore. 
Si deve notare che questa è una "definizione media macroscopica". Nelle immediate vicinanze dei nuclei atomici ci sono campi elettrici molto intensi, che tengono legati gli elettroni non liberi. 
All'esterno della superficie e in prossimità di essa per il teorema di Coulomb:
dove formula_3 è la densità di carica superficiale. Inoltre, poiché il campo è nullo entro il conduttore, il vettore campo elettrico ha direzione normale in ogni punto alla superficie:
Si nota che il valore del campo elettrostatico è maggiore dove formula_3 è maggiore, ed è possibile dimostrare che la densità superficiale di carica è maggiore dove il raggio di curvatura della superficie è minore. In altre parole il campo elettrostatico è più intenso nelle zone di una superficie a forma di punta, per via del cosiddetto fenomeno del potere disperdente delle punte. Da questo effetto hanno origine molti fenomeni come la formazione di scintille tra elettrodi di forma appuntita. Il discorso fatto finora non vale soltanto per singoli conduttori, ma anche per sistemi di più corpi conduttori posti a contatto, ad esempio, tramite un filo conduttore.
Nel caso il conduttore presenti una o più cavità al suo interno i risultati non cambiano. In effetti se si vuole calcolare il flusso di Gauss entro la cavità esso è nullo poiché non contiene cariche. Anche dentro una cavità il campo elettrostatico è nullo. Il potenziale elettrico rimane costante.
Un altro fenomeno di particolare rilevanza è l'induzione elettrostatica; tale fenomeno porta un conduttore a dividere le sue cariche se messo in prossimità di un altro corpo carico. Un esempio di induzione elettrostatica si ha nel caso di due conduttori di cui uno cavo che contiene un altro conduttore (per esempio positivo). Se i due conduttori non sono posti a contatto, la parete interna del conduttore cavo si carica negativamente poiché le cariche negative vengono attratte dal conduttore interno e quelle positive vengono respinte dallo stesso. Così sulla superficie esterna del conduttore cavo si ha una carica positiva uguale a quella del conduttore interno, in modo da mantenere equipotenziale lo spazio occupato dai due conduttori.
Si parla di induzione completa quando due conduttori sono disposti in maniera tale che tutte le linee di flusso partono da un conduttore e arrivano sull'altro. Due conduttori tra cui ci sia induzione completa formano un condensatore. Caratteristica quantitativa dei conduttori e dei condensatori è la capacità elettrica, che rappresenta appunto la capacità di un conduttore o condensatore di immagazzinare energia.
Quando un conduttore carico viene collegato "a massa" (per esempio la Terra), dopo un breve istante la differenza di potenziale tra i due conduttori si annulla, poiché la carica presente sul conduttore si trasferisce tutta alla massa, lasciando il conduttore neutro.
</doc>

<doc id="132097" url="https://it.wikipedia.org/wiki?curid=132097" title="Accelerazione di gravità">
Accelerazione di gravità
L'accelerazione di gravità, o "accelerazione gravitazionale" è l'accelerazione che un corpo subisce quando è lasciato libero di muoversi in caduta libera in un campo gravitazionale. 
Di solito si studia il valore che ha questa accelerazione sul suolo terrestre, che costituisce uno standard per le applicazioni tecniche più diffuse. Si può misurare in moltissimi modi, e si può derivare dai valori dei parametri che compaiono in leggi fisiche più generali, come la legge di gravitazione universale.
Per avere un'idea della grandezza o meno di un valore di accelerazione, si usa confrontarla con il valore di accelerazione che il campo gravitazionale terrestre provoca sugli oggetti che si trovano superficie terrestre. Infatti tutti gli uomini, gli animali, le piante e gli oggetti sulla Terra sono sottoposti a questa accelerazione, chi consapevolmente e chi inconsapevolmente.
Per questo parametro è stato fissato un valore convenzionale, che nelle unità di misura del Sistema Internazionale risulta pari a:
Notevolmente, questa scelta sul valore della costante è rimasta invariata dalla terza CGPM nel 1901.
Il valore standard di cui sopra è indicato con formula_1 o formula_2, talvolta anche con formula_3, e viene spesso impropriamente riportato tra le costanti "fisiche", per quanto sia più propriamente una costante tecnica, o costante definita (in inglese: "defined constant").
Il simbolo deve essere scritto con formula_4 minuscolo per distinguerlo dalla costante gravitazionale formula_5 che compare nella equazione di Newton.
Quando si valuta per esempio l'effetto di accelerazioni importanti su persone e strutture, per esempio nei terremoti, è una ottima consuetudine rapportare il valore ottenuto con questo valore standard esatto.
Si tratta di un valore medio, che approssima il valore dell'accelerazione di gravità presente al livello del mare a una latitudine di 45,5°. Il valore dell'accelerazione di gravità sulla superficie terrestre (formula_4) in effetti varia molto leggermente attorno al valore formula_1 a seconda del luogo. In particolare è influenzato dalla latitudine e dall'altitudine, ma viene influenzato per esempio anche dal tipo di rocce sottostanti.
L'accelerazione di gravità è misurabile semplicemente guardando un corpo in caduta libera, trascurando la resistenza dell'aria.
Il vettore dell'accelerazione di gravità terrestre ha sempre la direzione verticale ed è orientato verso il centro della Terra.
L'effettiva accelerazione che la Terra produce su un corpo in caduta varia al variare del luogo in cui questa è misurata.
Il valore dell'accelerazione aumenta con la latitudine per due ragioni:
La combinazione di questi due effetti rende il valore di formula_4 misurato ai poli circa lo 0,5% più grande di quello misurato all'equatore.
Il valore di formula_4 cui è sottoposto un corpo che si trova in aria ad altezza formula_11 sul livello del mare è calcolabile con la "formula tecnica" (che contiene cioè delle implicite unità di misura):
dove:
L'ultimo termine, h è una correzione dovuta all'altezza.
Se il corpo è sulla verticale della terraferma, viene aggiunta un'ulteriore correzione dovuta alla maggiore massa di un volume di terra rispetto all'acqua; tale maggiore massa può essere approssimata con una superficie orizzontale infinita dando luogo a un fattore di correzione (la "correzione di Bouguer", si veda Anomalia di Bouguer) pari a formula_18 volte la massa per unità di area, ovvero m·s·kg.
La gravità al di sotto della superficie terrestre viene invece calcolata sottraendo dalla massa totale della Terra la massa del guscio esterno al punto di misurazione. La forza di gravità diminuisce progressivamente all'aumentare della profondità e al centro della Terra è zero perché l'intera massa del pianeta attira il corpo in tutte le direzioni attorno a esso.
Anche variazioni locali nella composizione delle rocce e delle superfici possono alterare localmente l'accelerazione di gravità; queste anomalie sono generalmente misurate e mappate.
La costante di accelerazione gravitazionale terrestre trova inoltre grande impiego dal punto di vista fisico nello studio dei comportamenti dei corpi sottoposti a certe condizioni.
L'accelerazione standard formula_1 è spesso usata come unità di misura tecnica. Non è in effetti una unità di misura accettata dal Sistema internazionale, ma è molto comoda nella quotidianità per dare un'idea pratica della grandezza di una accelerazione. 
In questo contesto il parametro tecnico "formula_1" viene indicato semplicemente con la lettera "g", e utilizzata come unità di misura tecnica. Veniva in passato chiamata dai tecnici anche con l'espressione impropria di "forza g". Viene impiegata così anche in campo aerospaziale, per esprimere le accelerazioni alle quali sono sottoposti i velivoli, i veicoli spaziali e gli eventuali passeggeri.
Con l'espressione colloquiale forza g utilizzata in aeronautica ci si riferisce invece al fattore di carico lungo l'asse verticale di un aeromobile, unità di misura delle accelerazioni a cui astronauti e piloti sono soggetti, moltiplicato per l'accelerazione di gravità terrestre, con simbolo appunto "g".
La forza g non va quindi confusa con l'accelerazione di gravità sulla superficie terrestre. Nonostante il nome, non è una forza, ma una accelerazione; sebbene, nei casi di cui sopra sia una accelerazione relativa al riferimento considerato, determina una forza fittizia (es. la forza centrifuga). 
In questo contesto, quando ci si riferisce a "1 g" si indica un'accelerazione pari all'accelerazione di gravità media misurata sulla Terra, che vale 9,80665 m·s.
Una persona normale mediamente riesce a sopportare accelerazioni verticali di circa 5 g positivi e 2÷3 g negativi. 
Per g positivo si intende una accelerazione che produce lo stesso effetto soggettivo causato dalla gravità terrestre su un soggetto in posizione eretta; questo effetto è prodotto da una accelerazione nel senso che va dai piedi verso la testa, quindi di senso contrario alla forza di gravità cui si è soggetti stando in piedi. Per g negativi si intendono accelerazioni ed effetti soggettivi di senso inverso. I g positivi, quindi, causano il deflusso del sangue dalla testa verso i piedi, i negativi viceversa. Si calcola che un'accelerazione di 5 "g", se prolungata per vari secondi, provochi perdita di conoscenza e valori superiori possono danneggiare il corpo umano anche mortalmente, se non adeguatamente protetto.. A valori critici di g negativi infatti uno dei primi effetti è che il campo visivo diventa rosso, a causa del maggiore apporto di sangue nei capillari nei globi oculari dovuta all'aumento della pressione sanguigna. 
Con la combinazione di speciali tute anti-g e di forze applicate ai muscoli per tenerli in tensione, entrambi con lo scopo di ridurre il deflusso sanguigno dal cervello, i piloti moderni possono sopportare oltre 10 "g positivi" (100 m·s). La resistenza a "g negativi" rimane invece molto inferiore, e comunque nell'intervallo fra i 2 e i 3 "g".
L'accelerazione standard g è usata anche in campo automobilistico. In particolare, si usa per esprimere le accelerazioni che agiscono sul veicolo in curva, accelerazione, frenata, e per l'analisi delle collisioni.
L'accelerazione di gravità sulla Terra si può anche misurare in modo indiretto servendosi di un pendolo preciso, a patto che ci si limiti a misurare delle piccole oscillazioni. Per le piccole oscillazioni infatti vale la formula del pendolo "matematico" (il più semplice):
formula_21
dove:
In fisica il valore di g è deducibile come un caso particolare dalla legge della gravitazione universale.
Il valore dell'accelerazione corrisponde infatti al prodotto di alcuni dei termini nell'equazione di gravitazione:
Inserendo i valori della costante di gravitazione universale formula_5, della massa, e del raggio della Terra si ottiene infatti:
Questa è una buona approssimazione del valore medio dell'accelerazione di gravità formula_4, ma si vede subito che non è il valore che si è scelto come standard. Le differenze del valore appena calcolato col valore standard sono dovute a diversi fattori, tra cui:
</doc>

<doc id="147608" url="https://it.wikipedia.org/wiki?curid=147608" title="Riflessione (fisica)">
Riflessione (fisica)
In fisica la riflessione è il fenomeno per cui un'onda, che si propaga lungo l'interfaccia tra differenti mezzi, cambia di direzione a causa di un impatto con un materiale riflettente.
In acustica la riflessione causa gli echi ed è utilizzata nel sonar. In geologia è importante nello studio delle onde sismiche.
Assorbimento, riflessione e trasmissione sono i fenomeni che avvengono quando la luce interagisce con la materia: quando l'energia radiante incide su un corpo, una parte viene assorbita, una parte viene riflessa e una parte trasmessa e per la legge di conservazione dell'energia, la somma delle quantità di energia rispettivamente assorbita, riflessa e trasmessa è uguale alla quantità di energia incidente.
Per indicare il tipo di riflessione di cui si tratta si usano gli aggettivi:
La riflessione può avvenire:
La riflettanza ("reflectance") è il rapporto tra flusso riflesso e flusso incidente valutato per ogni lunghezza d'onda. Essendo definita come rapporto di grandezze omogenee, la riflettanza è una grandezza adimensionale e viene espressa in percentuale (0-100%) o come fattore (0.0-1.0). Inoltre riguarda il flusso e quindi la totalità della radiazione riflessa nella emisfera. Il materiale artificiale con minor rifflettanza è il Vantablack.
La riflettanza non è solo funzione della lunghezza d'onda ma anche dell'illuminazione, della geometria di irradiamento e della geometria di visione (cioè della geometria con cui si illumina il corpo e della geometria con cui si misura la quantità riflessa), per cui è necessario definire una grandezza più generale della riflettanza, cioè il fattore di riflessione.
Si fa riferimento al "diffusore riflettente ideale". Si tratta di un corpo (ideale, cioè teorico) che non assorbe e non trasmette, ma riflette diffusamente la radiazione ricevuta con radianza o luminanza uguale per ogni angolo di riflessione e indipendentemente dalla direzione della radiazione incidente. Come prima applicazione del concetto di diffusore riflettente ideale si definisce il fattore di radianza ("radiance factor") o il fattore di luminanza ("luminance factor") come il rapporto tra la radianza di un'area e quella del diffusore ideale riflettente irradiato nello stesso modo.
Con riferimento a questo corpo ideale, il fattore di riflessione ("reflectance factor" o "reflection factor") di un corpo è il rapporto tra il flusso riflesso dal corpo in un dato cono il cui vertice è sul corpo considerato e il flusso riflesso dal diffusore riflettente ideale.
Il fattore di riflessione è dunque una grandezza generica che corrisponde:
Un tipico spettrofotometro è in grado di misurare il fattore di riflessione spettrale ad intervalli di 10 nm nell'intervallo da 380 a 730 nm.
La riflessione di onde elettromagnetiche è regolata da due leggi fondamentali, ricavabili dal principio di Fermat e dal principio di Huygens-Fresnel:
Un'onda elettromagnetica riflessa può subire uno sfasamento. Questo dipende dagli indici di rifrazione del mezzo nel quale viaggia la luce (formula_1) e del mezzo oltre la superficie riflettente (formula_2):
</doc>

<doc id="1637157" url="https://it.wikipedia.org/wiki?curid=1637157" title="Campo magnetico">
Campo magnetico
In fisica, in particolare nel magnetismo, il campo magnetico è un campo vettoriale solenoidale generato nello spazio dal moto di una carica elettrica o da un campo elettrico variabile nel tempo. Insieme al campo elettrico esso costituisce il campo elettromagnetico, responsabile dell'interazione elettromagnetica.
In realtà, le equazioni relative al campo elettrico e quelle relative al campo magnetico sono solo in apparenza completamente separate: le stesse cariche elettriche, quando sono in movimento, danno luogo a una densità di corrente e divengono dunque sorgente di un campo magnetico. 
Tuttavia, poiché il fatto che le cariche siano ferme o si muovano è un fatto relativo (cioè dipendente dal sistema di riferimento scelto per descrivere il fenomeno), diviene ugualmente relativo il fatto che si abbia a che fare con un campo elettrico o con un campo magnetico. Appare dunque naturale interpretare il campo elettrico e il campo magnetico come manifestazioni diverse di un'unica entità fisica, detta campo elettromagnetico.
La scoperta della produzione di campi magnetici da parte di conduttori percorsi da corrente elettrica si deve a Ørsted nel 1820: sperimentalmente si verifica che la direzione del campo è la direzione indicata dalla posizione d'equilibrio dell'ago di una bussola immersa nel campo; lo strumento per la misura del campo magnetico è il magnetometro.
Il campo magnetico agisce su un oggetto elettricamente carico con la forza di Lorentz (nel caso di una carica elettrica in movimento) oppure tramite il momento torcente che agisce su un dipolo magnetico. L'evoluzione spaziale e temporale del campo magnetico è governata dalle equazioni di Maxwell, un sistema di quattro equazioni differenziali alle derivate parziali lineari che sta alla base della descrizione formale dell'interazione elettromagnetica.
In fisica il campo induzione magnetica (anche detto impropriamente campo magnetico) in un punto di un mezzo è individuato dal vettore formula_1 composto da una prima componente indicata con formula_2 e una seconda componente indicata con formula_3 dovuta a fenomeni microscopici che avvengono nel mezzo come tipicamente un determinato allineamento degli spin atomici. formula_1 si misura in tesla (T) o in Wb/m² ed è anche detto densità di flusso magnetico o induzione magnetica; formula_5 è detto "campo magnetizzante" e si misura in A/m (o anche in Oe); formula_6 è il "vettore di magnetizzazione", anch'esso in A/m; formula_7 è la permeabilità magnetica del vuoto pari a formula_8. In definitiva: formula_9.
formula_6 tiene conto del fatto che i momenti magnetici intrinseci (spin) degli elettroni legati si allineano mediamente in una certa direzione, spesso quella del campo applicato esternamente, e inoltre compiono dei moti medi di precessione attorno a tale direzione in senso orario o antiorario a seconda del segno della loro carica elettrica. Si tratta di moti rotatori nello stesso senso e con la stessa direzione perpendicolare, che forniscono un contributo alla corrente elettrica macroscopica soltanto sulla superficie del materiale: al suo interno i moti delle cariche affiancate tra di loro si compensano a vicenda in quanto ruotano tutte nello stesso senso e da ciò deriva il fatto che le correnti delle cariche legate agli atomi sono esprimibili come il rotore della magnetizzazione. Il legame tra formula_6 e formula_5 è generalmente spiegabile con delle trattazioni quantistiche della materia, che caratterizzano le proprietà magnetiche dei materiali come il paramagnetismo, il diamagnetismo, il ferromagnetismo, l'antiferromagnetismo, il ferrimagnetismo e il superparamagnetismo.
formula_5 è un campo magnetico che ha quattro possibili contributi: la corrente dovuta a cariche libere nel materiale, un campo magnetico applicato esternamente, la variazione nel tempo del campo elettrico e il campo demagnetizzante formula_14 che è sempre opposto in verso alla magnetizzazione infatti esso nasce qualora la magnetizzazione abbia dei punti di non uniformità lungo la propria direzione, ovvero quando formula_6 ha divergenza non nulla. L'esempio più caratteristico di necessità del campo demagnetizzante in assenza di campi magnetici applicati esternamente, di correnti elettriche libere e variazioni del campo elettrico è il fatto che in un ferromagnete formula_6 può essere comunque presente ma essendo nulla fuori dal materiale ha una discontinuità al bordo che la rende non solenoidale quindi se formula_5 fosse nullo anche formula_1 sarebbe non solenoidale e ciò contraddirebbe la seconda equazione di Maxwell :formula_19.
In ambito ingegneristico viene spesso utilizzata una convenzione diversa: le quantità fondamentali (campo elettrico e campo magnetico) sono rappresentate dalla coppia duale formula_20, mentre le induzioni corrispondenti, ovvero la coppia duale formula_21, vengono considerate la risposta del mezzo all'eccitazione elettromagnetica. Grazie a questa convenzione esiste una dualità sia a livello di unità di misura (ampere è duale di volt, weber è duale di coulomb), sia a livello di notazione. Difatti, introducendo le quantità fittizie "densità di carica magnetica" formula_22 e "densità di corrente magnetica" formula_23, è possibile scrivere delle equazioni di Maxwell perfettamente simmetriche, e ciò consente di enunciare il teorema di dualità elettromagnetica.
Sia data una carica elettrica puntiforme formula_24 in moto con velocità istantanea formula_25 in una regione caratterizzata dalla presenza di un campo elettrico formula_26 e un campo magnetico formula_1. La forza di Lorentz è la forza formula_28 esercitata dal campo elettromagnetico sulla carica, ed è proporzionale a formula_24 e al prodotto vettoriale tra formula_25 e formula_1 secondo la relazione:
dove formula_33 è la posizione della carica, formula_34 la sua velocità e formula_35 è il tempo.
Una carica positiva viene accelerata nella direzione di formula_26 e viene curvata nella direzione perpendicolare al piano formato da formula_25 e formula_1.
Si consideri il caso in cui sia presente il solo campo magnetico. La formula può essere applicata al caso di un circuito filiforme di lunghezza formula_39 percorso dalla corrente elettrica formula_40:
e sapendo che per definizione:
con formula_43 la densità di corrente, si può estendere al caso più generale di un volume formula_44 percorso da una corrente descritta dalla densità di corrente, per il quale si ha:
Dal momento che la forza di Lorentz è legata al campo magnetico tramite il prodotto vettoriale, la forza e il campo non hanno la stessa direzione, essendo perpendicolari. Come conseguenza di ciò, la forza di Lorentz non compie lavoro, infatti:
L'ultimo integrando è nullo perché è il prodotto misto di tre vettori, di cui due paralleli.
Una serie di evidenze sperimentali, tra le quali l'esperimento di Oersted del 1820, ha portato a concludere che il campo magnetico nel generico punto formula_47 generato nel vuoto da un elemento infinitesimo formula_48 di un circuito percorso da una corrente formula_40 è dato da:
dove formula_51 è la distanza tra la posizione formula_33 dell'elemento infinitesimo formula_48 del circuito e il punto formula_47 in cui è calcolato il campo, e formula_7 è la permeabilità magnetica nel vuoto.
L'integrazione su tutto il circuito della precedente espressione produce la Legge di Biot-Savart:
che rappresenta il campo magnetico totale generato dal circuito in formula_47. Nel caso più generale, in cui l'approssimazione di circuito filiforme non viene applicata, si ricorre alla densità formula_43 della corrente che attraversa una sezione di conduttore. L'espressione del campo diventa:
dove formula_60 è il volume infinitesimo, di lunghezza formula_61 e sezione formula_62, del conduttore nel punto formula_33.
Calcolando la divergenza del campo generato da un circuito si dimostra che essa è sempre nulla:
Questa proprietà costituisce la seconda equazione di Maxwell:
Il fatto che si tratti di un campo a divergenza nulla implica che il campo magnetico sia un campo solenoidale. Da questo fatto segue che, applicando il teorema del flusso di Gauss, il flusso formula_66 di formula_1 attraverso qualsiasi superficie chiusa formula_68 che contiene al suo interno il circuito è nullo:
dove formula_44 è il volume racchiuso dalla frontiera formula_68. Inoltre, il campo magnetostatico non è conservativo e quindi non è irrotazionale, cioè il suo rotore non è nullo ovunque. Partendo dalla più generale formulazione del campo magnetico, nella quale si sfrutta la densità di corrente, si dimostra che:
dove formula_43 indica il vettore densità di corrente. Questa espressione costituisce la quarta equazione di Maxwell nel caso stazionario. Applicando alla precedente espressione il teorema del rotore si ottiene la Legge di Ampère:
ovvero, la circuitazione lungo una linea chiusa del campo magnetostatico è pari alla somma algebrica delle correnti concatenate con essa.
Il potenziale vettore del campo magnetico, indicato solitamente con formula_75, è un campo vettoriale tale che formula_1 sia uguale al rotore di formula_75:
La definizione non è tuttavia univoca, dal momento che formula_5 resta invariato se ad formula_75 si somma il gradiente di una qualsiasi funzione scalare:
Il potenziale vettore definito in questo modo risulta soddisfare automaticamente le equazioni di Maxwell nel caso statico.
Nel caso elettrodinamico bisogna modificare le definizioni dei potenziali in modo da ottenere che due equazioni di Maxwell risultino immediatamente soddisfatte. Per quanto riguarda formula_75, si verifica ancora che è definito in modo che il suo rotore sia formula_1, mentre formula_44 è definito in modo che:
L'elettrostatica e la magnetostatica rappresentano due casi particolari di una teoria più generale, l'elettrodinamica, dal momento che trattano i casi in cui i campi elettrico e magnetico non variano nel tempo. In condizioni stazionarie i campi possono essere infatti trattati indipendentemente l'uno dall'altro, tuttavia in condizioni non stazionarie appaiono come le manifestazioni di una stessa entità fisica: il campo elettromagnetico.
Più precisamente, le leggi fisiche che correlano i fenomeni elettrici con quelli magnetici sono la legge di Ampere-Maxwell e la sua simmetrica legge di Faraday.
La legge di Faraday afferma che la forza elettromotrice indotta in un circuito chiuso da un campo magnetico è pari all'opposto della variazione del flusso magnetico del campo concatenato con il circuito nell'unità di tempo, ovvero:
Per la definizione di forza elettromotrice si ha, esplicitando la definizione integrale di flusso:
applicando il teorema di Stokes al primo membro:
e per quanto detto si giunge a:
Uguagliando gli integrandi segue la terza equazione di Maxwell:
Si noti che nel caso non stazionario la circuitazione del campo elettrico non è nulla, dal momento che si genera una forza elettromotrice che si oppone alla variazione del flusso del campo magnetico concatenato col circuito.
L'estensione della legge di Ampère al caso non stazionario mostra come un campo elettrico variabile nel tempo sia sorgente di un campo magnetico. Ponendo di essere nel vuoto, la forma locale della legge di Ampère costituisce la quarta equazione di Maxwell nel caso stazionario:
Tale relazione vale solamente nel caso stazionario poiché implica che la divergenza della densità di corrente sia nulla, contraddicendo in questo modo l'equazione di continuità per la corrente elettrica:
Per estendere la legge di Ampère al caso non stazionario è necessario inserire la prima legge di Maxwell nell'equazione di continuità:
Il termine
è detto corrente di spostamento, e deve essere aggiunto alla densità di corrente nel caso non stazionario.
Inserendo la densità di corrente generalizzata così ottenuta nella legge di Ampère:
si ottiene la quarta equazione di Maxwell nel vuoto. Tale espressione mostra come la variazione temporale di un campo elettrico sia sorgente di un campo magnetico.
Per descrivere il comportamento del campo magnetico nella materia è sufficiente introdurre nelle equazioni di Maxwell un termine aggiuntivo formula_96, che rappresenta la densità di corrente associata alla magnetizzazione del materiale:
Tuttavia, tale termine non è in generale noto: questo ha portato all'introduzione del vettore "intensità di magnetizzazione", anche detto vettore di "polarizzazione magnetica" e indicato con formula_6, una grandezza vettoriale macroscopica che descrive il comportamento globale del materiale soggetto alla presenza del campo magnetico. Il vettore rappresenta il momento di dipolo magnetico per unità di volume posseduto dal materiale. Definito come la media del valore medio del momento magnetico proprio formula_99 di "N" particelle contenute in un volume infinitesimo formula_100, è espresso dalla relazione:
Nel Sistema internazionale di unità di misura il vettore di polarizzazione magnetica si misura in Ampere su metro (A/m), e nella definizione il limite vale per un volume che contenga un numero significativo di atomi tale da poterne calcolare una proprietà media.
Nel caso in cui la polarizzazione atomica all'interno del materiale sia uniforme, le correnti di magnetizzazione sono descritte dalla "corrente di magnetizzazione superficiale" formula_102, data da:
ovvero la corrente di magnetizzazione è pari al flusso del vettore "densità di corrente di magnetizzazione superficiale" formula_104 attraverso una superficie formula_105. Nel caso in cui la polarizzazione atomica all'interno del materiale non sia uniforme, invece, si introduce la "corrente di magnetizzazione volumica" formula_102, data da:
ovvero la corrente di magnetizzazione volumica è pari al flusso del vettore "densità di corrente di magnetizzazione volumica" formula_108 attraverso una superficie formula_105. Le relazioni che legano la densità di corrente di magnetizzazione con il vettore di magnetizzazione sono:
dove nella prima equazione formula_111 è il versore che identifica la direzione normale alla superficie del materiale.
La presenza di materia costringe a tenere conto delle correnti amperiane nelle equazioni di Maxwell per il campo magnetico:
e porta a definire il vettore campo magnetico formula_113 nella materia come:
L'equazione di Maxwell può essere riscritta in modo equivalente:
La densità di corrente formula_43 presente nella precedente equazione si riferisce esclusivamente alle correnti elettriche, date dal moto dei soli elettroni liberi, e non alle correnti atomiche di magnetizzazione. Nel caso non stazionario, inoltre, la quarta equazione ha l'espressione:
La permeabilità magnetica è una grandezza fisica che esprime l'attitudine di una sostanza a polarizzarsi in seguito all'applicazione di un campo magnetico e si misura in henry al metro (H/m), equivalente a newton all'ampere quadrato (N/A). Nel caso in cui il materiale sia omogeneo e isotropo e la sua risposta sia lineare, i vettori formula_118 e formula_113 sono paralleli, e questo implica che la relazione tra di essi è di semplice proporzionalità:
dove formula_121 è la permeabilità magnetica del materiale considerato.
Dal momento che non tutti i materiali hanno una reazione lineare tra formula_1 e formula_5, i materiali magnetici si distinguono in tre categorie:
L'energia magnetica è l'energia associata al campo magnetico, e nel caso di materiali in cui la relazione tra formula_1 e formula_5 sia lineare l'energia magnetica contenuta in un volume formula_131 è data da:
dove il prodotto scalare:
è la densità di energia magnetica.
Per un circuito percorso da corrente la densità di energia magnetica può essere definita a partire dal potenziale vettore formula_75 del campo magnetico e il vettore densità di corrente formula_43:
Il campo elettromagnetico è dato dalla combinazione del campo elettrico formula_26 e del campo magnetico formula_1, solitamente descritti con vettori in uno spazio a tre dimensioni. Il campo elettromagnetico interagisce nello spazio con cariche elettriche e può manifestarsi anche in assenza di esse, trattandosi di un'entità fisica che può essere definita indipendentemente dalle sorgenti che l'hanno generata. In assenza di sorgenti il campo elettromagnetico è detto onda elettromagnetica, essendo un fenomeno ondulatorio che non richiede di alcun supporto materiale per diffondersi nello spazio e che nel vuoto viaggia alla velocità della luce. Secondo il modello standard, il quanto della radiazione elettromagnetica è il fotone, mediatore dell'interazione elettromagnetica.
La variazione temporale di uno dei due campi determina il manifestarsi dell'altro: campo elettrico e campo magnetico sono caratterizzati da una stretta connessione, stabilita dalle quattro equazioni di Maxwell. Le equazioni di Maxwell, insieme alla forza di Lorentz, definiscono formalmente il campo elettromagnetico e ne caratterizzano l'interazione con oggetti carichi. Le prime due equazioni di Maxwell sono omogenee e valgono sia nel vuoto sia nei mezzi materiali, e rappresentano in forma differenziale la Legge di Faraday e la legge di Gauss per il campo magnetico. Le altre due equazioni descrivono il modo in cui il materiale, nel quale avviene la propagazione, interagisce polarizzandosi con il campo elettrico e magnetico, che nella materia sono denotati con formula_139 e formula_5. Esse mostrano in forma locale la Legge di Gauss elettrica e la Legge di Ampère-Maxwell.
Le equazioni di Maxwell sono formulate anche in elettrodinamica quantistica, dove il campo elettromagnetico viene quantizzato. Nell'ambito della meccanica relativistica, i campi sono descritti dalla teoria dell'elettrodinamica classica in forma covariante, cioè invariante sotto trasformazione di Lorentz. Nell'ambito della teoria della Relatività il campo elettromagnetico è rappresentato dal tensore elettromagnetico, un tensore a due indici di cui i vettori campo elettrico e magnetico sono particolari componenti.
</doc>

<doc id="1768865" url="https://it.wikipedia.org/wiki?curid=1768865" title="Principi della dinamica">
Principi della dinamica
I principi della dinamica sono i principi alla base della omonima branca della meccanica classica, che descrive le relazioni tra il moto di un corpo e gli enti che lo modificano. Sono validi in sistemi di riferimento inerziali e descrivono bene la fisica dei corpi che si muovono a velocità molto minore di quella della luce.
Sono anche detti "Principi di Newton" perché enunciati da Isaac Newton nel suo testo fondamentale "Philosophiae Naturalis Principia Mathematica", anche se sono il frutto di una lunga evoluzione da parte di numerosi scienziati che ne ha preceduto e seguito la pubblicazione. All'interno della formalizzazione logico-matematica della meccanica newtoniana svolgono il ruolo di assiomi.
Aristotele nella sua "Fisica" del IV secolo a.C. asseriva che lo stato naturale dei corpi fosse la quiete, ossia l'assenza di moto, e che qualsiasi oggetto in movimento tende a rallentare fino a fermarsi, a meno che non venga spinto a continuare il suo movimento.
Nel Medioevo, Guglielmo di Ockham e gli occamisti, e poi, nel Quattrocento, Nicola Cusano, nell'opera "Il gioco della palla", e Leonardo da Vinci ripensarono la meccanica aristotelica: cominciarono a sviluppare una diversa dinamica, fondata su diversi principi fisici e presupposti filosofici.
Il principio di inerzia non è di banale osservazione sulla Terra, dominata dagli attriti, anzi, è letteralmente impossibile. Infatti, considerando, per esempio, una biglia che rotola su una superficie piana orizzontale molto estesa, l'esperienza comune riporta che con il passare del tempo la biglia rallenta fino a fermarsi. Questo è dovuto al fatto che essa interagisce con il piano e con l'aria. Si può osservare, comunque, che facendo diminuire progressivamente questi attriti, ad esempio rarefacendo l'aria e lisciando il piano per diverse volte, la biglia percorre uno spazio sempre maggiore prima di fermarsi. Generalizzando, l'idea che sta alla base del primo principio è che, teoricamente, diminuendo gli attriti fino a renderli nulli, il corpo non rallenti più e quindi non si fermi mai, cioè persista nel suo stato di moto rettilineo uniforme. Riferendosi invece alla tendenza di ogni corpo a mantenere lo stato di quiete o di moto si usa parlare di "inerzia" e questo concetto può esser visto come una diretta conseguenza del principio di relatività galileiana.
Ciò viene dettagliatamente descritto da Galileo in due sue opere, rispettivamente, nel 1632 e nel 1638: il "Dialogo sopra i due massimi sistemi del mondo" e "Discorsi e dimostrazioni matematiche intorno a due nuove scienze attenenti alla meccanica e i movimenti locali". Scrive Galileo:
Bisogna aggiungere, ad onor del vero, che, com’è noto, Galileo riteneva che un moto inerziale avrebbe assunto una direzione circolare e non rettilinea, come, invece, dedusse Newton. Infatti, secondo Galilei, i pianeti si muovono di moto circolare uniforme attorno al Sole, senza subire alcun tipo di effetto, gravitazionale o di altro tipo. Tuttavia, la sua prima enunciazione formale è nei "Principia" di Isaac Newton, che pur ne riconosce (impropriamente, come visto) la paternità galileiana. Newton chiarisce inoltre il concetto nella terza definizione:
La formula esplicita dell'uguaglianza fra la forza e il prodotto della massa inerziale per l'accelerazione apparve per la prima volta negli scritti di Eulero nel 1752.
I principi furono presentati tutti assieme da Newton nel 1687 nell'opera "Philosophiae Naturalis Principia Mathematica" ("I principi matematici della filosofia naturale"). Newton stesso chiamò i suoi principi "Axiomata, sive leges moti" ("Assiomi o leggi del moto"), a rimarcare che questi rappresentano la base fondante della meccanica, come gli assiomi di Euclide lo sono per la geometria, la cui validità può essere testata solo con esperimenti e a partire dai quali è possibile ricavare ogni altra legge sui moti dei corpi.
Il primo principio, detto d'inerzia, ha tradizionalmente origine con gli studi sulle orbite dei corpi celesti e sul moto dei corpi in caduta libera di Galileo. Il principio di inerzia si contrappone alla teoria fisica di Aristotele, il quale riteneva che lo stato naturale di tutti i corpi fosse quello di quiete e un agente esterno fosse necessario ad indurre il moto. Galileo ideò una serie di esperimenti, anche mentali, volti a dimostrare la non correttezza di questa assunzione. A simili conclusioni giunse anche Cartesio, nei suoi scritti riguardo alla fisica.
Il secondo principio della dinamica si deve a Newton, e introduce il concetto di forza come origine e causa del cambiamento dello stato di moto dei corpi. Nei secoli si sono susseguite numerose discussioni su come e su cosa di preciso Newton intendesse con "forza" e "cambio dello stato di moto", in relazione in particolare alla formulazione odierna del secondo principio della dinamica.
Il terzo principio esprime una importante proprietà delle forze e fu usato da Newton per dimostrare la conservazione della quantità di moto. Secondo il premio nobel Richard Feynman, il terzo principio ha una importante rilevanza nello sviluppo della meccanica:
I principi di Newton nella sua originaria formulazione sono validi per i corpi puntiformi, in quanto non considerano gli effetti che possono derivare dalla dimensione finita degli oggetti, come in particolare le rotazioni. I principi furono poi estesi ai corpi rigidi e ai corpi deformabili da Eulero nel 1750.
Nei "Principia" l'enunciato della "Lex I" è il seguente:
Questo principio, noto anche come "principio d'inerzia" o "principio di Galileo", afferma che se un corpo è fermo o si muove di moto rettilineo uniforme, allora la somma vettoriale delle forze che agiscono su di esso è nulla. Quindi, se la risultante delle forze agenti su un corpo è nulla, allora esso mantiene il proprio stato di moto. Nella realtà di tutti i giorni, si osserva che un corpo in moto tende lentamente a rallentare fino a fermarsi. Questo tuttavia non è in contraddizione con il primo principio, in quanto la forza di attrito, per esempio con l'aria o il terreno, sta agendo sul corpo modificando il proprio stato di moto. Se fosse possibile fare un esperimento in cui tutti gli attriti e le interazioni vengano annullate, ad esempio nello spazio vuoto lontano dalle galassie, allora si osserverebbe che il corpo continuerebbe a muoversi indefinitamente a velocità costante lungo una linea retta.
Gli esempi portati da Newton a proposito del cerchio in rotazione e del moto dei pianeti sono in realtà esempi di conservazione del momento angolare e rappresentano l'integrazione del principio di inerzia nel principio della conservazione della quantità di moto.
Il principio di inerzia rappresenta un punto di rottura con la fisica aristotelica in quanto l’assenza di forze è messa in relazione non solo con la quiete, ma anche con il moto rettilineo uniforme. Poiché la particolarità del moto rettilineo uniforme è che la velocità è vettorialmente costante, cioè in modulo, direzione e verso, si desume che la presenza di forze sia collegata alle variazioni di velocità. Ciò porta al secondo principio della dinamica.
Nei "Principia" l'enunciato della "Lex II" è il seguente:
Pertanto, il secondo principio, detto anche "principio di proporzionalità" o "principio di conservazione", afferma che:
Sia la forza che l'accelerazione sono dei vettori e sono indicati in grassetto nella formula. Nel testo, Newton prosegue affermando:La forza netta, o forza risultante, agente su un corpo è la somma vettoriale di tutte le forze applicate ad esso. L'accelerazione causata quindi dalle forze avrà come effetto una modifica del vettore velocità nel tempo. Questa modifica si può manifestare come un cambio della direzione della velocità, oppure come un aumento o diminuzione del suo modulo.
La massa che compare nel secondo principio della dinamica è chiamata massa inerziale, cioè misura quantitativamente la resistenza di un corpo ad essere accelerato. Infatti la stessa forza agente su un corpo di piccola massa, come ad esempio una spinta data ad un tavolo, produce un'accelerazione molto maggiore che su un corpo di grande massa, come un'automobile che con la stessa spinta cambierebbe la propria velocità di poco.
Se la massa inerziale del corpo non è costante, allora la seconda legge della dinamica può essere generalizzata con l'introduzione della quantità di moto. Ovvero, un punto materiale, cioè un corpo di dimensioni trascurabili rispetto al sistema di riferimento in esame e contemporaneamente dotato di massa, al quale sia applicata una forza, varia la quantità di moto in misura proporzionale alla forza e lungo la direzione della stessa. In altre parole, secondo una formulazione analoga a quella di Eulero: il tasso di aumento della quantità di moto è uguale e parallelo alla forza impressa:
cioè in base alla definizione di quantità di moto e di accelerazione e alla regola di Leibniz:
Per un sistema chiuso quindi il rapporto fra i moduli della forza applicata e dell'accelerazione è costante e pari alla massa inerziale:
Il secondo principio della dinamica fornisce una spiegazione per il fatto che tutti i corpi cadono con una velocità, che è indipendente dalla loro massa. Simile risultato fu raggiunto, secondo Newton, da Galileo Galilei con lo studio del piano inclinato e l'esperimento della caduta dei gravi. Tuttavia, ogni conoscitore delle opere galileiane sa che Galileo non giunse mai alla distinzione del concetto di massa da quello di peso. D’altra parte, ciò è comprensibile se si considera l’avversione galileiana nei confronti di ogni riferimento ad un'azione “a distanza” tra i corpi, come quella, per esempio, teorizzata da Keplero.
Nei "Principia" l'enunciato della "Lex III" è il seguente:Il terzo principio, detto anche "principio di azione e reazione", dove il termine "azione" deve essere inteso nell'accezione generale di forza o momento "reali", può essere riformulato come:
In termini matematici il terzo principio può essere riassunto come:
Nel proseguire del testo, Newton porta i seguenti esempi:Il terzo principio della dinamica in termini moderni implica che tutte le forze hanno origine dall'interazione di diversi corpi, in base al terzo principio se solo un corpo singolo si trovasse nello spazio, questo non potrebbe subire alcuna forza perché non vi sarebbe alcun corpo su cui la corrispondente reazione possa essere esercitata.
Un esempio chiaro è l'applicazione al sistema Terra-Luna, di cui sono sottosistemi la Terra e la Luna. La forza totale esercitata dalla Terra sulla Luna deve essere uguale, ma di senso opposto alla forza totale esercitata dalla Luna sulla Terra, in accordo con la legge di gravitazione universale.
Un esempio tipico che si può fare di applicazione controintuitiva del principio, è quello della semplice camminata: nella situazione noi imprimiamo forza al suolo all'indietro tramite il piede, il suolo reagisce con una forza uguale e contraria che poi è quella che ci spinge in avanti. Ma il suolo invece sembra non subire alcuna forza, poiché non accelera: la contraddizione si risolve considerando che la massa inerziale della Terra è enorme in confronto a quella dell'individuo, e perciò la forza si traduce in un'accelerazione piccola al punto da essere inosservabile.
Per un sistema fisico di "n" punti materiali (o corpi), il terzo principio della dinamica assieme al secondo implica la conservazione della quantità di moto e quindi la simmetria delle leggi fisiche rispetto a traslazioni spaziali. Considerando, ad esempio, due corpi isolati che interagiscono, allora in base al secondo principio della dinamica il terzo può essere riscritto come:
dove formula_11 e formula_12 sono rispettivamente le quantità di moto del corpo formula_5 e formula_6. Dato che gli incrementi possono essere sommati allora si ha:
da cui si ricava che è costante nel tempo la grandezza formula_16, che equivale alla quantità di moto totale del sistema formato dai corpi formula_5 e formula_6 considerati assieme. Questo ragionamento può essere esteso ad un numero arbitrario di corpi.
Nel caso del singolo punto materiale, la conservazione della quantità di moto deriva direttamente dal secondo principio della dinamica
Infatti è sufficiente che sul punto materiale non agisca alcuna forza esterna perché si conservi la quantità di moto. Si pensi, ad esempio, ad un razzo in volo nel vuoto spaziale. Consumando combustibile, questo riduce la sua massa e di conseguenza la sua velocità cresce di modo che il prodotto formula_20 sia costante, istante per istante.
Il testo "La fisica di Berkeley" riporta come principi fondanti la meccanica classica le seguenti (cit.):
Citando sempre dallo stesso libro, le 3 leggi di Newton sono così formulate:
Da quest'ultimo principio, integrando rispetto al tempo, discende il principio della conservazione della quantità di moto e viceversa.
"La fisica di Feynman" ha una impostazione "sui generis" che non consente di estrarre agevolmente un "corpus" di principi della dinamica espressi in maniera formale, poiché ha l'intento di costruire una visione unitaria della fisica, "filtrandola" col criterio della validità nella moderna teoria dei campi per non introdurre come invece si fa solitamente con l'approccio storico dei concetti che risultano in una teoria più ampia falsificati o particolari. Tuttavia riportiamo alcuni brani che a nostro avviso sono quanto più si avvicina ad una formulazione di tali principi. Citiamo quindi:
Per quanto riguarda il terzo principio della dinamica, Feynman lo considera, al pari della legge di gravitazione universale, una delle due sole cose sulla natura delle forze che Newton disse:
Secondo Feynman, Newton caratterizzò il concetto di forza tramite l'enunciazione di un principio generale, il terzo principio della dinamica appunto, e tramite la formulazione di una legge di forza particolare, ovvero quella gravitazionale.
I principi della dinamica non valgono in sistemi di riferimento non inerziali. Per studiare anche questi ultimi, infatti, è necessaria l'introduzione delle interazioni apparenti, ovvero forze e momenti dovuti alle accelerazioni del sistema di riferimento. Le forze apparenti, quali la forza centrifuga e la forza di Coriolis, non hanno alcuna reazione corrispondente, in altre parole il terzo principio della dinamica smette di essere vero nei sistemi di riferimento non inerziali.
La meccanica classica può essere vista come l'approssimazione a basse velocità rispetto a quella della luce della teoria della relatività ristretta. Il secondo principio della dinamica ad esempio non è più in grado di descrivere correttamente gli eventi che occorrono quando invece le velocità dei corpi sono vicine a quella della luce, dato che permette sempre di incrementare la velocità di un corpo con l'azione di una forza senza alcun limite. Inoltre, il terzo principio della dinamica richiede che l'azione e la reazione siano sempre opposte in ogni momento, generando un vincolo istantaneo fra punti lontani al di fuori dei rispettivi coni luce.
Per estendere la validità dei principi della dinamica, allargandoli ai sistemi "non inerziali" , il concetto di "azione" viene ristretto soltanto a forze e momenti, in meccanica razionale si parla di forze generalizzate, "reali" per cui vale questo principio, cioè che implicano la "reazione". Infine, per la simmetria tra i due concetti che scaturisce da questo principio si preferisce oggi parlare di interazione: ""l'interazione tra i corpi è reciproca, e unica sorgente di forza reale e momento meccanico reale. Una forza generalizzata applicata su un corpo formula_31 è "reale", se dovuta all'influenza di un qualsiasi altro corpo formula_37, e solo allora si manifesta su formula_32 con orientazione antiparallela"." Ricordando che un sistema inerziale è definito proprio in base a questo principio come sistema di riferimento in cui si manifestano solo interazioni tra i corpi, ovvero interazioni reali, e le interazioni apparenti sono appunto quelle che non provenendo dai corpi in quanto non reciproche, vengono imputate al sistema di riferimento, e non sono "reali" solo nel senso che non sono "assolute", e non nel senso di "ininfluenti" sui corpi quando presenti.
Nel 1981 Mordehai Milgrom propose una sua modifica volta a spiegare il problema delle curve di rotazione delle galassie a spirale in modo alternativo all'introduzione della materia oscura, denominata MOND dall'acronimo inglese per "Dinamica Newtoniana Modificata" che teneva conto dello strappo, che però gode di scarso consenso presso la comunità scientifica attuale, anche se le si può riconoscere di essere, popperianamente parlando, falsificabile al pari delle teorie a base di materia ed energia oscura.
</doc>

<doc id="1706832" url="https://it.wikipedia.org/wiki?curid=1706832" title="Energia">
Energia
L<nowiki>'</nowiki>energia è la grandezza fisica che misura la capacità di un corpo o di un sistema fisico di compiere lavoro, a prescindere dal fatto che tale lavoro sia o possa essere effettivamente svolto.
Il termine "energia" deriva dal tardo latino "energīa", a sua volta tratto dal greco ἐνέργεια ("enérgeia"), derivato di ἐνεργής (o l'equivalente ἐνεργός), 'attivo', composto dalla particella intensiva "en" e ἔργον ("ergon", 'lavoro', 'opera'). Il termine è stato introdotto da Aristotele in ambito filosofico per distinguere la δύναμις ("dýnamis"), la possibilità, la "potenza" propria della materia informe, dalla reale capacità (ἐνέργεια) di far assumere in atto realtà formale alle cose.
La parola italiana "energia" non è direttamente derivata dal latino, ma è ripresa nel XV secolo dal francese "énergie". «In Francia "énergie" è usato dal XV secolo nel senso di "forza in azione", con vocabolo direttamente derivato dal latino, mai con significato fisico. In Inghilterra nel 1599 "energy" è sinonimo di "forza o vigore di espressione". Thomas Young è il primo a usare, nel 1807, il termine "energy" in senso moderno»
Il concetto di energia può emergere intuitivamente dall'osservazione sperimentale che la capacità di un sistema fisico di compiere lavoro diminuisce a mano a mano che questo viene prodotto. In questo senso l'energia può essere definita come una proprietà posseduta dal sistema che può essere scambiata fra i corpi attraverso il lavoro (vedi Trasferimento di energia).
Il termine "energia" fu usato per la prima volta per indicare una grandezza fisica da Keplero nel suo "Harmonices Mundi" del 1619, tuttavia il termine "energia" fu introdotto sistematicamente nella letteratura scientifica in termini moderni solo a partire dalla fine del XIX secolo. Prima di allora si alternarono a seconda del contesto e dell'autore anche i termini "vis viva", "forza" o "lavoro". Il primo si conserva come tradizione storica ancora oggi nel nome di alcuni teoremi, mentre gli ultimi due termini hanno acquisito nella fisica moderna un significato completamente differente da quello dell'energia.
Storicamente, la prima grandezza simile a quella oggi indicata come energia cinetica apparve negli studi di Gottfried Leibniz nel 1686, chiamata con il nome di "vis viva" ("forza viva") in contrapposizione alla "vis mortua" ("forza morta") usata per designare l'inerzia. Il dibattito principale nella fisica del XVII e XVIII secolo era incentrato concettualmente non su un principio di conservazione, piuttosto sulla ricerca di una grandezza fisica che fosse in grado di misurare gli effetti dell'azione di una forza sui corpi, o in termini moderni di una interazione fra questi. Una forza che agisce su un corpo avrà l'effetto di modificare la sua velocità, così facendo cambieranno sia l'energia cinetica sia la quantità di moto formula_1 definita come:
A partire da queste due diverse possibilità nacque lo scontro fra Leibniz, che riteneva più adeguata come misura di una forza la "vis viva", e i sostenitori della teoria cartesiana, che utilizzavano invece la quantità di moto. Nella formulazione odierna della meccanica classica, entrambe le grandezze hanno la stessa importanza: come fu chiaro a partire da d'Alembert, il problema era unicamente legato all'uso di due punti di vista differenti. Infatti è possibile considerare gli effetti di una forza sommati rispetto a intervalli di tempo formula_3, da cui si ricava la variazione della quantità di moto direttamente in base al primo principio della dinamica:
Oppure è possibile considerare gli effetti di una forza sommati rispetto allo spazio, avendo in mente come esempio la compressione di una molla che frena un corpo in moto. Il risultato che si ottiene è che il lavoro formula_5 di una forza compiuto su un corpo è uguale al cambiamento dell'energia cinetica del corpo stesso:
In questo senso la differenza di energia cinetica o della quantità di moto finale e iniziale sono solo due misure diverse degli effetti dell'azione di una forza.
L'energia è una grandezza fisica estensiva (l'energia di due corpi è semplicemente la somma delle energie dei corpi presi singolarmente), che ha una importanza centrale nella formulazione di molte teorie, dalla meccanica classica alla termodinamica, dalla teoria della relatività alla meccanica quantistica.
Una precisa definizione di energia non è semplice da fornire, l'energia non ha alcuna realtà materiale ma è piuttosto un concetto matematico astratto che esprime un vincolo rispetto ai processi possibili e una simmetria temporale delle leggi fisiche. Non esiste quindi nessuna sostanza o fluido corrispondente all'energia pura. Come scrisse Feynman:
Un corpo può incrementare o diminuire la sua energia in seguito a una interazione con altri corpi: la variazione di energia riflette quindi i cambiamenti occorsi nelle sue proprietà microscopiche. Esistono numerose possibili interazioni; dal punto di vista qualitativo si possono distinguere la meccanica, con ad esempio urti fra corpi rigidi o forze fra particelle puntiformi, dalla termodinamica, dove si considerano ad esempio le reazioni fra gas a temperature differenti. Dal punto di vista del tipo di interazione, esistono in natura diversi tipi di forze, come quella gravitazionale, quella nucleare o quella elettrica. Tuttavia, tutti questi possibili processi lasciano invariata la quantità totale di energia, che quindi diviene la grandezza fisica costante per sistemi chiusi o isolati.
In ambito tecnologico l'energia permette, tramite il suo sfruttamento a livello industriale, la trasformazione di materie prime in prodotti o beni finali o direttamente la fornitura di servizi utili all'uomo e alla società.
La società moderna è estremamente dipendente dall'energia (in particolare nelle sue forme di energia meccanica, energia elettrica, energia chimica ed energia termica) in tutti i suoi processi produttivi e gestionali (ad esempio autotrazione, trasporto marittimo e aereo, riscaldamento, illuminazione, funzionamento di apparecchiature elettriche e processi industriali). Grande interesse e preoccupazione riveste dunque il problema energetico globale riguardo l'esaurimento nel tempo delle fonti fossili, la principale fonte di energia primaria, il cui utilizzo intensivo ha permesso il notevole sviluppo economico dalla prima rivoluzione industriale fino ai giorni nostri.
L'unità di misura derivata del Sistema Internazionale per l'energia è il joule (simbolo: J); in termini di unità fondamentali del SI, 1 J è pari a 1 kg·m·s. Nel CGS l'unità di misura per l'energia è l'erg, equivalente a 1 dyne·centimetro e in termini di unità base CGS a 1 g·cm·s (corrisponde a 10 J).
A seconda dell'ambito, altre unità di misura sono adottate per misurare l'energia:
L'energia meccanica è la somma di energia cinetica ed energia potenziale attinenti allo stesso sistema, da distinguere dall'energia totale del sistema E in cui rientra anche l'energia interna.
L'energia cinetica è l'energia che dipende unicamente dallo stato di moto del sistema preso in considerazione e da quello delle sue relative componenti. Per un corpo puntiforme l'energia cinetica formula_7 è uguale alla metà del prodotto della massa del corpo per il quadrato della sua velocità:
L'energia cinetica è una grandezza che può assumere solo valori positivi. Considerando corpi rigidi estesi non puntiformi, l'energia cinetica dipenderà anche dalla velocità angolare attraverso un termine aggiuntivo chiamato energia rotazionale.
La variazione dell'energia cinetica a seguito dell'azione di una forza è legata al lavoro, cioè al prodotto scalare della forza per la distanza dello spostamento effettuato. Il lavoro formula_5 di una forza compiuto su un corpo è infatti uguale al cambiamento dell'energia cinetica del corpo stesso:
in base al teorema energia-lavoro o teorema delle forze vive.
L'energia potenziale è un tipo di energia che dipende unicamente dalla configurazione o dalla posizione dei corpi e delle particelle in interazione.
A seconda del tipo di interazione e di forza considerata esistono numerosi tipi di energia potenziale. L'esempio più semplice di energia potenziale è quella posseduta da un corpo di massa formula_11 posto a un'altezza formula_12 nel campo gravitazione terrestre, uguale a:
dove formula_14 è l'accelerazione di gravità. Questo tipo di energia dipende solo dalla posizione di un corpo e quando questo viene lasciato cadere l'energia potenziale cambia durante il tempo la propria forma diventando cinetica. L'energia potenziale è definita a meno di una costante additiva, in questo esempio a meno della possibile scelta del punto rispetto a cui misurare l'altezza formula_12.
Il calore e il lavoro non possono essere definiti come "forme di energia", sebbene abbiano le sue stesse unità di misura, dato che non sono proprietà di un singolo corpo ma piuttosto sono proprietà della trasformazione termodinamica presa in considerazione. In altre parole, il calore e il lavoro non sono posseduti da un sistema e non sono quindi una variabile di stato, ma sono invece "energia in transito", la manifestazione sperimentale dello scambio di energia che avviene attraverso due sistemi. Il calore e il lavoro possono tuttavia essere misurati e utilizzati nella pratica per prevedere la differenza di energia posseduta da un corpo fra la fine e l'inizio del processo o della trasformazione.
In termodinamica il principio di conservazione dell'energia è contenuto nel primo principio della termodinamica, secondo il quale la variazione di energia di un sistema formula_16 è uguale alla somma del calore formula_17 e del lavoro formula_5 rispettivamente ceduto e compiuto dall'ambiente esterno al sistema:
Non tutta l'energia di un sistema è in grado di produrre lavoro in una trasformazione termodinamica, per via del secondo principio della termodinamica. La quantità di energia di un sistema disponibile per produrre lavoro può essere infatti molto minore di quella totale del sistema. Il rapporto tra l'energia utilizzabile e l'energia fornita da una macchina viene chiamato rendimento.
L'invarianza della quantità totale dell'energia è espressa dal principio di conservazione dell'energia, secondo il quale la variazione di energia in una regione di spazio è uguale al flusso netto di energia che fluisce verso lo spazio esterno. Sebbene l'espressione esatta dell'energia possa variare a seconda dei casi considerati, finora non è stato scoperto nessun processo in grado di incrementare o diminuire globalmente l'energia, questa può solo cambiare forma trasformandosi.
Il principio di conservazione ha guidato la scoperta di nuove forme di energia e ha permesso di scoprire nuovi tipi di processi fisici e perfino nuove particelle. Agli inizi del XX secolo furono scoperti alcuni decadimenti nucleari con emissione di elettroni che non sembravano soddisfare il principio di conservazione dell'energia. Per risolvere il problema nel 1924 Niels Bohr avanzò l'idea che a livello atomico l'energia non fosse strettamente conservata, proponendo una teoria che si rivelò errata. Wolfgang Pauli nel 1930 ed Enrico Fermi nel 1934, ritenendo fondamentale e tenendo ferma la conservazione dell'energia, postularono invece l'esistenza di nuove interazioni e di una nuova particella mai osservata prima che fosse in grado di trasportare l'energia che risultava mancante negli esperimenti. In questo modo, guidati dal principio di conservazione dell'energia, riuscirono a scoprire il neutrino, una particella priva di carica elettrica, effettivamente osservata sperimentalmente nel 1959.
Il principio di conservazione dell'energia riflette la simmetria temporale delle leggi fisiche rispetto a traslazioni temporali, il fatto cioè che queste non cambiano con lo scorrere del tempo. Un esperimento condotto a un tempo formula_20 fornirà lo stesso risultato dello stesso esperimento fatto nelle stesse medesime condizioni ma al tempo formula_21. Nella teoria della relatività, la conservazione dell'energia e la conservazione della quantità di moto sono riuniti in un'unica legge che corrisponde globalmente alla simmetria delle traslazioni nello spaziotempo quadridimensionale.
Il principio, nato nell'ambito dell'energia meccanica, può essere esteso anche a tutte le altre forme di energia a partire dal calore, dal momento che questo si ottiene per dissipazione di energia meccanica a livello macroscopico e trattasi di energia cinetica a livello molecolare, mentre tutte le altre forme di energia degradano inevitabilmente in calore.
Nella fisica classica, l'energia è una proprietà scalare continua immagazzinata da un sistema.
Nella meccanica quantistica invece per i sistemi legati (cioè i sistemi in cui l'energia della particella non supera le barriere di potenziale) è "quantizzata", cioè può assumere un numero discreto di valori (o "livelli energetici").
La celebre equazione di Einstein E=mc², diretta derivazione della teoria della relatività ristretta, mostra come massa ed energia siano due "facce della stessa medaglia" di un sistema fisico. Da questa semplice equazione si evince infatti che l'energia contribuisce all'inerzia di un corpo come la massa, cioè anche l'energia contribuisce alla resistenza di corpo a essere accelerato.
Le leggi quantistiche hanno mostrato che la massa può essere trasformata in energia e viceversa, nei processi nucleari ad esempio il decadimento dei metalli pesanti come l'uranio in elementi più leggeri comporta un difetto di massa corrispondente alla liberazione di energia sotto forma di radiazione.
Rispetto quindi alla meccanica classica, dove la massa e l'energia sono separatamente conservate, in relatività ristretta i due principi fisici possono essere fusi in un principio unico sotto la denominazione di principio di conservazione della massa/energia.
L'energia esiste in varie forme, ognuna delle quali ha una propria espressione in termini dei dettagli del sistema considerato, come la velocità o la distanza relativa fra particelle. Le principali forme di energia sono:
L'energia potenziale è quella posseduta da un materiale elastico sottoposto a deformazione. L'energia luminosa o radiante è l'energia trasportata dei fotoni che compongono la luce, quindi l'energia della radiazione elettromagnetica.
Le principali fonti di energia attraverso le quali è possibile produrre energia elettrica, energia termica o direttamente energia meccanica sono:
Con il termine "energie rinnovabili" si intendono quelle fonti di energia che non si esauriscono o si esauriscono in tempi che vanno oltre la scala dei tempi "umani" (ad esempio: energia solare, eolica, geotermica, mareomotrice, fusione nucleare), altrimenti si parla di "energie non rinnovabili" (ad esempio petrolio e carbone), mentre con il termine "energie alternative" si intendono le fonti di energia che possono essere impiegate in sostituzione dell'energia chimica prodotta dai classici combustibili o fonti fossili.
Si parla di "conversione" quando si passa da una forma di energia a un'altra, mentre si parla di "trasformazione" quando la forma di energia resta la stessa, ma se ne modificano alcuni parametri caratteristici.
Ad esempio una pila permette di convertire l'energia chimica in energia elettrica, mentre un trasformatore permette di trasformare l'energia elettrica variandone la tensione e l'intensità di corrente.
Ogni volta che avviene una conversione, una parte di energia (più o meno consistente) viene inevitabilmente convertita in energia termica; si parla in questo caso di "effetti dissipativi".
Nell'ambito della chimica degli alimenti, si parla di "valore energetico" per riferirsi all'energia che l'organismo umano può ricevere attraverso il consumo di un alimento.
Siccome parte dell'energia contenuta in un alimento può essere persa durante i processi digestivi e metabolici, il valore energetico può risultare minore rispetto al valore sperimentale ottenuto bruciando l'alimento in un calorimetro a bomba. Per tale motivo, sono stati messi a punto dei metodi sperimentali che tengono in conto tale perdite energetiche. Uno di questi metodi è l'utilizzo dei cosiddetti fattori di Atwater, grazie ai quali il valore energetico di un alimento viene calcolato a partire dal valore energetico associato ad alcuni dei suoi macronutrienti più importanti dal punto di vista energetico, in particolare: grassi, alcoli, proteine e carboidrati.
In Europa, il valore energetico è riportato per legge nell'etichetta nutrizionale dei prodotti alimentari, dove viene indicato in kcal o kJ per quantità di prodotto.
</doc>

<doc id="418362" url="https://it.wikipedia.org/wiki?curid=418362" title="Sistema di riferimento non inerziale">
Sistema di riferimento non inerziale
Un sistema di riferimento non inerziale è un sistema di riferimento nel quale la descrizione della dinamica dei corpi non vede verificato il principio di inerzia. Tutti e soli i sistemi di riferimento che si muovono di moto accelerato rispetto ad un qualsiasi sistema di riferimento inerziale presentano questa particolarità e possono essere quindi definiti non inerziali.
Un sistema di riferimento non inerziale è un sistema di riferimento in cui un corpo soggetto a una risultante delle forze esterne nulla si muove comunque di moto non uniforme, ovvero accelerato.
La descrizione di un evento di un sistema fisico può risultare differente, se operata da sistemi di riferimento differenti.
Le trasformazioni di Galileo stabiliscono le equazioni che permettono di passare dalla descrizione di un evento formula_1 da un sistema di riferimento inerziale formula_2 ad un altro inerziale formula_3: le grandezze che cambiano, da un sistema all'altro, sono la posizione e la velocità dei singoli corpi, ma l'eventuale accelerazione di un corpo risulta essere un'invariante per tutti i sistemi di riferimento inerziali.
Se si considera il punto di vista di un osservatore solidale con formula_4, ogni corpo solidale con un sistema di riferimento inerziale appare dotato di un'accelerazione pari a formula_5. Questa descrizione solidale con formula_4 non è simmetrica a quelle solidali con formula_7 e formula_2, perché l'osservatore "agganciato" a formula_4 non è in grado di individuare alcuna forza che sia responsabile dell'accelerazione dei corpi suddetti: questo osservatore è dunque costretto a "rinunciare" al principio di inerzia, e a constatare che oggetti e persone, se descritti dal proprio sistema di riferimento, possono subire variazioni della propria velocità senza che vi sia un'azione esterna a causarla.
Per "reintegrare" i principi della dinamica, l'osservatore nel sistema di riferimento non inerziale può fare appello alle cosiddette interazioni apparenti, postulare cioè ad hoc l'esistenza di forze e momenti legati ai corpi accelerati dal sistema di riferimento stesso.
È un esempio di forza apparente la forza centrifuga, percepita da un osservatore situato su di un sistema di riferimento in moto non rettilineo, che osservi un corpo solidale allo stesso.
È da notare infine come non abbia senso affermare che la descrizione della dinamica dei corpi in un sistema inerziale sia "più corretta" di quella effettuata in un sistema non inerziale: semplicemente, assumere la prospettiva del primo sistema è più funzionale ai fini di una rigorosa descrizione matematica dell'evento, perché permette di legare causalmente le forze con l'interazione con altri corpi, riguardo agli scambi di energia o di quantità di moto, ma ciò non toglie che in taluni casi sia invece più pratico considerare la prospettiva non inerziale.
</doc>

<doc id="2577462" url="https://it.wikipedia.org/wiki?curid=2577462" title="Fisica">
Fisica
La fisica () è la scienza della natura nel senso più ampio. 
Ha lo scopo di studiare i fenomeni naturali, ossia tutti gli eventi che possono essere descritti, ovvero quantificati o misurati, attraverso grandezze fisiche opportune, al fine di stabilire principi e leggi che regolano le interazioni tra le grandezze stesse e le loro variazioni, mediante astrazioni matematiche. Quest'obiettivo è raggiunto attraverso l'applicazione rigorosa del metodo scientifico, il cui scopo ultimo è fornire uno schema semplificato, o modello, del fenomeno descritto.
L'insieme di principi e leggi fisiche relative a una certa classe di fenomeni osservati definiscono una teoria fisica deduttiva, coerente e relativamente autoconsistente, costruita tipicamente a partire dall'induzione sperimentale.
La storia della fisica abbraccia certamente un lungo arco temporale, ma non vi è accordo sulla data di nascita della fisica. 
Tuttavia la fisica propriamente detta nasce con la Rivoluzione scientifica nel XVII secolo per opera di Niccolò Copernico, Keplero, Tycho Brahe, Galileo Galilei e il suo metodo scientifico, Leibniz e Newton che diedero contributi alla meccanica celeste e ai principi della meccanica classica fornendo anche gli strumenti matematici adattati allo scopo, come i fondamenti del calcolo infinitesimale.
La fisica rappresenta dunque la prima disciplina scientifica nella storia della scienza da cui nascerà nel XVIII secolo la chimica, nel XIX secolo la biologia e le scienze della Terra ecc... Nel XVIII e XIX secolo nascono e si sviluppano poi teorie come la termodinamica e l'elettromagnetismo. Sempre a livello storico si suole suddividere la fisica in fisica classica che comprende la meccanica classica, la termodinamica e l'elettromagnetismo fino alla fine del XIX secolo e la fisica moderna dall'inizio del XX secolo a partire dalla teoria della relatività, la meccanica quantistica e tutte le altre teorie fisiche della seconda metà del Novecento.
Conosciuta anche come la "regina delle scienze", originariamente branca della filosofia, la fisica è stata chiamata almeno fino al XVIII secolo "filosofia naturale". Solo in seguito alla codifica del metodo scientifico di Galileo Galilei, negli ultimi trecento anni si è talmente evoluta e sviluppata e ha conseguito risultati di tale importanza da conquistarsi piena autonomia e autorevolezza. Essa si è distinta dalla filosofia per ovvie ragioni di metodo di indagine.
L'indagine fisica viene condotta seguendo rigorosamente il metodo scientifico, anche noto come il "metodo sperimentale": all'osservazione del fenomeno segue la formulazione di ipotesi interpretativa, la cui validità viene messa alla prova tramite degli esperimenti. Le ipotesi consistono nella spiegazione del fenomeno attraverso l'assunzione di principi fondamentali, in modo analogo a quanto viene fatto in matematica con assiomi e postulati. L'osservazione produce come conseguenza diretta le leggi empiriche.
Se la sperimentazione conferma un'ipotesi, la relazione che la descrive viene detta "legge fisica". Il ciclo conoscitivo prosegue con il miglioramento della descrizione del fenomeno conosciuto attraverso nuove ipotesi e nuovi esperimenti.
Un insieme di leggi possono essere unificate in una "teoria" fondata su principi che permettano di spiegare il maggior numero possibile di fenomeni: questo processo permette anche di prevedere nuovi fenomeni che possono essere scoperti sperimentalmente.
Le leggi e le teorie fisiche, come tutte le leggi scientifiche, in quanto costruite a partire da processi conoscitivi di tipo induttivo-sperimentali, sono in linea di massima sempre provvisorie, nel senso che sono considerate "vere" finché non vengono in qualche modo "confutate", ossia finché non viene osservato il verificarsi di un fenomeno che esse non predicono o se le loro predizioni sui fenomeni si dimostrano errate. Infine ogni teoria può essere sostituita da una nuova teoria che permetta di predire i nuovi fenomeni osservati con un'accuratezza superiore ed eventualmente in un più ampio contesto di validità.
Cardine della fisica sono i concetti di grandezza fisica e misura: le grandezze fisiche sono ciò che è misurabile secondo criteri concordati (è stabilito per ciascuna grandezza un "metodo di misura" e un'unità di misura). Le misure sono il risultato degli esperimenti. Le leggi fisiche sono quindi generalmente espresse come relazioni matematiche fra grandezze, verificate attraverso misure. I fisici studiano quindi in generale il comportamento e le interazioni della materia attraverso lo spazio e il tempo.
Per queste sue caratteristiche, cioè il preciso rigore di studio dei fenomeni analizzati, è unanimemente considerata la scienza dura per eccellenza tra tutte le scienze sperimentali o scienze esatte grazie al suo approccio teso alla comprensione non solo "qualitativa", ma anche "quantitativa" con la stesura delle suddette leggi universali di natura matematica in grado di fornire una "previsione" sullo stato futuro di un fenomeno o di un sistema fisico.
Il "metodo scientifico" è la modalità con cui la scienza procede per raggiungere una conoscenza della realtà "oggettiva", "affidabile", "verificabile" e "condivisibile". Differisce dal metodo aristotelico, presente prima del 1600, per la presenza della sperimentazione. Esso consiste, da una parte, nella raccolta di evidenza empirica e misurabile attraverso l'osservazione e l'esperimento; dall'altra, nella formulazione di ipotesi e teorie da sottoporre nuovamente al vaglio dell'esperimento.
Esso fu applicato e codificato da Galileo Galilei nella prima metà del XVII secolo: precedentemente l'indagine della natura consisteva nell'adozione di teorie che spiegassero i fenomeni naturali senza una verifica sperimentale delle teorie stesse considerate vere in base al principio di autorità.
Il metodo sperimentale moderno richiede, invece, che le teorie fisiche debbano fondarsi sull'osservazione dei fenomeni naturali, debbano essere formulate come relazioni matematiche e debbano essere messe alla prova tramite esperimenti:
Il percorso seguito per arrivare alla stesura di una legge scientifica (e in particolare di una legge fisica) a partire dall'osservazione di un fenomeno si articola nei seguenti passi, ripetuti ciclicamente:
Dato che le condizioni in cui si svolge l'esperimento non sono mai ideali, al contrario di quanto supposto dalle ipotesi, è spesso necessario svolgere un elevato numero di misure e analizzare i risultati con metodi statistici.
Nel caso in cui l'ipotesi sia confermata la relazione che essa descrive diviene una legge fisica, ulteriormente sviluppabile attraverso:
Ogni osservazione di un fenomeno costituisce un caso a sé stante, una particolare istanza del fenomeno osservato. Ripetere le osservazioni vuol dire moltiplicare le istanze e raccogliere altri fatti, cioè altre "misure". Le diverse istanze saranno certamente diverse una dall'altra nei dettagli (ad esempio a causa di errori sperimentali), anche se nelle loro linee generali ci indicano che il fenomeno, a parità di condizioni, tende a ripetersi sempre allo stesso modo.
Per ottenere un risultato di carattere generale occorre sfrondare le varie istanze dalle loro particolarità e trattenere solo quello che è rilevante e comune ad ognuna di esse, fino a giungere al cosiddetto modello fisico.
Se l'ipotesi è smentita allora è rigettata ed è necessario formulare una nuova ipotesi e ripercorrere il percorso precedente.
Il ciclo conoscitivo proprio del metodo scientifico è di tipo induttivo: un procedimento che partendo da singoli casi particolari cerca di stabilire una legge universale.
Nella prima metà del XX secolo, il filosofo e logico inglese Bertrand Russell e il filosofo austriaco Karl Popper sollevarono obiezioni sul metodo dell'induzione. L'induzione non ha consistenza logica perché non si può formulare una legge universale sulla base di singoli casi; ad esempio, l'osservazione di uno o più cigni dal colore bianco non autorizza a dire che tutti i cigni sono bianchi; esistono cigni neri. Popper osservò che nella scienza non basta "osservare": bisogna saper anche che cosa osservare. L'osservazione non è mai neutra ma è sempre intrisa di quella teoria che, appunto, si vorrebbe mettere alla prova. Secondo Popper, la teoria precede sempre l'osservazione: anche in ogni approccio presunto "empirico", la mente umana tende inconsciamente a sovrapporre i propri schemi mentali, con le proprie categorizzazioni, alla realtà osservata.
Il metodo sperimentale non garantisce quindi che una legge fisica possa essere verificata in modo definitivo, ma si può limitare solamente a fornire la prova della falsità di un'ipotesi.
La misura è il processo che permette di conoscere una qualità di un determinato oggetto (ad esempio la lunghezza o la massa) dal punto di vista quantitativo, tramite un'unità di misura, cioè una grandezza standard che, presa "N" volte, associ un valore univoco alla qualità da misurare. La branca della fisica che si occupa della misurazione delle grandezze fisiche è chiamata metrologia. Il suo scopo è quello di definire alcune grandezze fisiche indipendenti, dette "fondamentali", dalle quali è possibile ricavare tutte le altre (che sono dette "derivate"), di definire i corretti metodi di misurazione e di costruire i campioni delle unità di misura adottate, in modo da avere un valore "standard" a cui fare riferimento in qualsiasi momento.
Il sistema di unità di misura universalmente accettato dai fisici è il Sistema Internazionale (SI): esso è basato su sette grandezze fondamentali, dalle quali derivano tutte le altre, ovvero:
Questo sistema di misurazione deriva direttamente dal sistema MKS, il quale ha come grandezze fondamentali solamente il metro, il secondo e il chilogrammo ed è stato sostituito con il sistema attuale poiché non sono considerati anche i fenomeni termodinamici, elettromagnetici e fotometrici
Altri sistemi usati in passato sono stati il sistema CGS, in cui le unità fondamentali sono il centimetro, il grammo e il secondo e il Sistema imperiale britannico (o anglosassone). Inoltre negli USA si utilizza attualmente il Sistema consuetudinario statunitense, derivato dal Sistema imperiale britannico.
In ogni procedimento di misura di una grandezza fisica, la misura è inevitabilmente accompagnata da un'incertezza o "errore" sul valore misurato. Una caratteristica fondamentale degli errori che influenzano le misure di grandezze fisiche è la sua "ineliminabilità", ossia una misura può essere ripetuta molte volte o eseguita con procedimenti o strumenti migliori, ma in ogni caso l'errore sarà sempre presente. L'incertezza fa parte della natura stessa dei procedimenti di misura. In un esperimento, infatti, non è mai possibile eliminare un gran numero di fenomeni fisici che possono causare dei disturbi alla misura, cambiando le condizioni nelle quali si svolge l'esperimento.
Una misura può quindi fornire solamente una "stima" del "valore vero" di una grandezza coinvolta in un fenomeno fisico.
Le incertezze che influenzano una misura sono solitamente suddivise a seconda delle loro caratteristiche in:
Nell'immagine a lato è rappresentato l'effetto delle incertezze su di una misura per analogia con il gioco delle freccette. Il valore vero della grandezza è il centro del bersaglio, ogni tiro (puntini blu) rappresenta una misura.
Quando si fa una misura, quindi, si deve procedere alla "stima dell'incertezza" ad essa associata, o, in altre parole, alla stima dell'errore sulla misura. Ogni misura deve essere quindi presentata accompagnata dalla propria incertezza segnalata dal segno di ± e dalla relativa unità di misura:
formula_1
In cui formula_2 è il simbolo relativo alla grandezza misurate, formula_3 è la "stima" del valore della misura, formula_4 è l'incertezza e formula_5 è l'unità di misura.
Quando una misura viene ripetuta molte volte è possibile valutare le incertezze casuali calcolando la deviazione standard delle misure (di solito indicata con la lettera greca, formula_4), la stima del valore vero si ottiene invece calcolando la media aritmetica dei valori delle misure. Se le misure sono ripetute poche volte si utilizza come incertezza la risoluzione dello strumento. L'incertezza deve fornire un intervallo di valori in cui, secondo la misura condotta dallo sperimentatore, cade il valore vero della misura secondo un certo livello di confidenza.
Ci si può servire dell'incertezza assoluta per quantificare la precisione della misura, il valore dell'incertezza con la relativa unità di misura è detto "incertezza assoluta", l<nowiki>'</nowiki>"incertezza relativa" si calcola come il rapporto fra l<nowiki>'</nowiki>incertezza assoluta e il valore vero della grandezza, in genere stimato dal valor medio delle misure effettuate. L'incertezza relativa è un numero adimensionale (ossia senza unità di misura). È possibile esprimere l'incertezza relativa anche con una percentuale.
Le incertezze si propagano quando i dati afflitti da incertezze vengono utilizzati per effettuare successivi calcoli (come ad esempio il calcolo dell'area di un tavolo a partire dalla lunghezza dei suoi lati), secondo delle precise regole dette della propagazione delle incertezze.
Infine, bisogna notare che in Fisica classica in linea di principio gli errori possono essere sempre ridotti fino alla sensibilità tipica dello strumento di misura, peraltro idealmente o in linea teorica sempre migliorabile, mentre in meccanica quantistica questo non è possibile a causa del principio di indeterminazione di Heisenberg.
Il tempo e lo spazio sono delle grandezze fondamentali della fisica, assieme a massa, temperatura, quantità di sostanza, intensità di corrente, e intensità luminosa: tutte le grandezze della fisica sono riconducibili a queste ultime.
L'unità di misura del tempo è il secondo, che è definito come la durata di 9 192 631 770 periodi della radiazione corrispondente alla transizione tra due livelli iperfini, da (F=4, MF=0) a (F=3, MF=0), dello stato fondamentale dell'atomo di cesio-133, mentre il metro è l'unità fondamentale dello spazio ed è definito come la distanza percorsa dalla luce nel vuoto in un intervallo di tempo pari a 1/299 792 458 di secondo.
Prima del Novecento i concetti di spazio e di tempo erano considerati assoluti e indipendenti: si pensava che lo scorrere del tempo e le estensioni spaziali dei corpi fossero indipendenti dallo stato di moto dell'osservatore che le misurava, ovvero dal sistema di riferimento scelto. Dopo l'avvento della teoria della relatività di Einstein i fisici dovettero cambiare opinione: le lunghezze e gli intervalli temporali misurati da due osservatori in moto relativo l'uno rispetto all'altro, possono risultare più o meno dilatati o contratti, mentre esiste un'entità, l'intervallo di Minkowski, che è invariante e se misurata da entrambi gli osservatori fornisce il medesimo risultato; quest'entità è costituita dalle 3 coordinate spaziali più una quarta, quella temporale, che rendono questo oggetto appartenente ad uno spazio a 4 dimensioni. Così facendo, lo spazio e il tempo non sono più due quantità fisse e indipendenti tra loro, ma sono correlate tra loro e formano un'unica e nuova base su cui operare, lo spazio-tempo.
Con la relatività generale, poi, lo spazio-tempo viene deformato dalla presenza di oggetti dotati di massa o energia (più in generale, di energia-impulso, vd. tensore energia impulso).
La massa è una grandezza fisica fondamentale. Essa ha come unità di misura nel Sistema internazionale il chilogrammo e viene definita nella meccanica newtoniana come la misura dell'inerzia offerta dai corpi al cambiamento del proprio stato di moto. Nella teoria della gravitazione universale di Newton svolge inoltre il ruolo di carica della forza gravitazionale. Questa doppia definizione della massa viene unita nella teoria della relatività di Einstein, tramite il principio di equivalenza, e inoltre essa viene legata all'energia di un corpo tramite la formula E = mc². La massa resta sempre costante a differenza del peso. Esempio: sulla luna la massa resta costante, mentre il peso diventa un sesto.
Nell'ambito della fisica, la forza viene definita come la rapidità di variazione della quantità di moto rispetto al tempo. Nel caso in cui la massa del corpo sia costante, la forza esercitata su un corpo è pari al prodotto della massa stessa per l'accelerazione del corpo.
In formule:
La forza esprime quantitativamente l'interazione di due corpi. L'interazione tra i corpi può avvenire attraverso una cosiddetta "area di contatto" (spesso assimilabile ad un punto) oppure può manifestarsi a distanza, attraverso quello che viene definito campo di forze.
Il concetto di campo di forze può essere chiarito se si pensa alla natura vettoriale della forza: la forza infatti viene descritta dal punto di vista matematico da un vettore, per cui un campo di forze è descritto in matematica come un campo vettoriale, cioè il campo di forze indica punto per punto la direzione, il verso e il modulo (o intensità) della forza che viene esplicata tra due corpi. Il campo di forze può essere visualizzato tramite le sue linee di campo o le linee di flusso.
Alcuni esempi di campi di forze sono: il campo magnetico, il campo elettrico e il campo gravitazionale.
Il modello fisico è una versione approssimata del sistema effettivamente osservato. Il suo impiego indiscriminato presenta dei rischi, ma ha il vantaggio di una maggiore generalità e quindi dell'applicabilità a tutti i sistemi simili al sistema in studio.
La costruzione del modello fisico è la fase meno formalizzata del processo conoscitivo, che porta alla formulazione di leggi quantitative e di teorie. Il modello fisico ha la funzione fondamentale di ridurre il sistema reale, e la sua evoluzione, ad un livello astratto ma traducibile in forma matematica, utilizzando definizioni delle grandezze in gioco e relazioni matematiche che li leghino. Tale traduzione può essere portata a termine anche attraverso l'uso del calcolatore, con programmi detti di simulazione, con i quali si studiano i fenomeni più disparati.
Il modello matematico, che ovviamente si colloca ad un livello di astrazione ancora superiore a quello del modello fisico, ovvero al massimo livello di astrazione nel processo conoscitivo, è costituito normalmente da equazioni differenziali che, quando non siano risolvibili in maniera esatta, devono essere semplificate opportunamente o risolte, più o meno approssimativamente, con metodi numerici (al calcolatore). Si ottengono in questo modo delle relazioni analitiche o grafiche fra le grandezze in gioco, che costituiscono la descrizione dell'osservazione iniziale.
Tali relazioni, oltre a descrivere l'osservazione, possono condurre a nuove previsioni. In ogni caso esse sono il prodotto di un processo che comprende diverse approssimazioni:
La soluzione del modello matematico va quindi interpretata tenendo conto delle varie approssimazioni che sono state introdotte nello studio del fenomeno reale, per vedere con quale approssimazione riesce a rendere conto dei risultati dell'osservazione iniziale e se le eventuali previsioni si verificano effettivamente e con quale precisione. Questo può venire confermato solo dall'esperienza, creando una sorta di schema in retroazione, che è il ciclo conoscitivo.
La fisica si compone di più branche che sono specializzate nello studio di diversi fenomeni oppure che sono caratterizzate dall'utilizzo estensivo delle stesse leggi di base. 
In base alla prima classificazione si possono distinguere quattro classi principali di fenomeni fisici: 
Ciascuna classe di fenomeni osservabili in natura è interpretabile in base a dei principi e delle leggi fisiche che insieme definiscono una teoria fisica deduttiva, coerente e relativamente autoconsistente. Benché ogni teoria fisica sia intrinsecamente falsificabile per la natura tipicamente induttiva del metodo di indagine scientifico, allo stato attuale esistono teorie fisiche più consolidate di altre seguendo il percorso storico di evoluzione della fisica stessa.
In base alla seconda classificazione si può invece distinguere tra fisica classica e fisica moderna, poiché quest'ultima fa uso continuamente delle teorie relativistiche, della meccanica quantistica e delle teorie di campo, che non sono invece parte delle teorie cosiddette classiche.
La fisica classica studia tutti i fenomeni che possono essere spiegati senza ricorrere alla relatività generale e alla meccanica quantistica. Le teorie principali che la compongono sono la meccanica classica (in cui si ricomprende l'acustica), la termodinamica, l'elettromagnetismo (in cui si ricomprende l'ottica) e la teoria newtoniana della gravità. Sostanzialmente tutte le teorie che sono state prodotte prima dell'inizio del XX secolo fanno parte della fisica classica. Le leggi della fisica classica, nonostante non siano in grado di spiegare alcuni fenomeni, come la precessione del perielio di Mercurio, o l'effetto fotoelettrico, sono in grado di spiegare gran parte dei fenomeni che si possono osservare sulla Terra. Le teorie, invece, falliscono quando è necessario spingersi oltre i limiti di validità delle stesse, ovvero nelle scale atomiche e subatomiche, o in quello dei corpi molto veloci, per cui è necessario fare ricorso alle leggi della fisica moderna.
La fisica classica utilizza un numero relativamente ridotto di leggi fondamentali che a loro volta si basano su una serie di principi assunti alla base della teoria. Fra questi quelli più importanti sono i concetti di spazio assoluto e tempo assoluto che sono poi alla base della relatività galileiana. Molto importanti sono anche i principi di conservazione.
Se la fisica classica aveva di per sé esaurito brillantemente quasi del tutto lo studio dei fenomeni fisici macroscopici, con il successivo passo, ovvero con la fisica moderna, lo studio fisico si incentra su tutti quei fenomeni che avvengono a scala atomica e subatomica o con velocità prossime a quelle della luce; le teorie principali che costituiscono questa nuova fisica sono la meccanica quantistica e la relatività generale. Più precisamente fanno parte di questa categoria tutte le teorie che sono state prodotte a partire dal XX secolo per cercare di spiegare alcuni fenomeni che le teorie classiche non riuscivano a dimostrare.
Queste nuove teorie rappresentarono una "spaccatura" netta nel disegno teorico tracciato dalla fisica classica precedente in quanto ne hanno completamente rivisto idee e concetti di fondo in cui l'uomo aveva sempre creduto fin dai tempi più antichi: 
Un'altra classificazione vuole la distinzione tra fisica sperimentale e fisica teorica in base alla suddivisione del processo di indagine scientifica rispettivamente nella fase dell'osservazione dei dati dell'esperimento e della loro successiva interpretazione ed elaborazione all'interno di teorie fisico-matematiche: stretto è dunque il loro legame di collaborazione. Entrambe queste distinzioni possono essere fatte all'interno sia della fisica classica che della fisica moderna.
La fisica matematica è quella disciplina scientifica che si occupa delle applicazioni della matematica ai problemi della fisica e dello sviluppo di metodi matematici adatti alla formulazione di teorie fisiche e alle relative applicazioni. È una branca della fisica tipicamente teorica.
In tempi recenti l'attività dei fisici-matematici si è concentrata principalmente sulle seguenti aree:
L'evoluzione della fisica in questo senso va verso la cosiddetta teoria del tutto ovvero una teoria omnicomprensiva che spieghi la totalità dei fenomeni fisici osservati in termini delle interazioni fondamentali a loro volta unificate.
La "fisica atomica" è invece la branca della fisica che studia l'atomo nella sua interezza ovvero comprendendo nucleo ed elettroni. Si tratta di un campo della fisica studiato all'inizio del XX secolo con la fornitura dei vari modelli atomici fino al modello attuale ritenuto più verosimile ovvero con nucleo interno ed elettroni esterni di tipo orbitale. Si tratta di un campo assestato già nella prima metà del XX secolo.
La più ampia branca della fisica della materia condensata (comunemente detta fisica della materia) è la fisica dello stato solido e riguarda lo studio delle proprietà dei solidi, sia elettroniche, che meccaniche, ottiche e magnetiche.
Il grosso della ricerca teorica e sperimentale della fisica dello stato solido è focalizzato sui cristalli, sia a causa della loro caratteristica struttura atomica periodica, che ne facilita la modellizzazione matematica, che per il loro ampio utilizzo tecnologico.
Con il termine "stato solido" in elettronica ci si riferisce in generale a tutti i dispositivi a semiconduttore. A differenza dei dispositivi elettromeccanici, quali ad esempio i relè, i dispositivi a stato solido non hanno parti meccaniche in movimento. Il termine è utilizzato anche per differenziare i dispositivi a semiconduttore dai primi dispositivi elettronici: le valvole e i diodi termoionici.
Il punto di partenza di gran parte della teoria nell'ambito della fisica dello stato solido è la formulazione di Schrödinger della meccanica quantistica non relativistica. La teoria si colloca generalmente all'interno dell'approssimazione di Born - Oppenheimer e dalla struttura periodica del reticolo cristallino si ricavano le condizioni periodiche di Born-von Karman e il Teorema di Bloch, che caratterizza la funzione d'onda nel cristallo. Le deviazioni dalla periodicità sono trattate ampiamente tramite approcci perturbativi o con altri metodi più innovativi, quali la rinormalizzazione degli stati elettronici.
Appartiene alla fisica dello stato solido anche la fisica delle basse temperature la quale studia gli stati della materia a temperature prossime allo zero assoluto e i fenomeni ad essi connessi (ad es. condensato di Bose-Einstein, superconduttività ecc..).
La "fisica nucleare" è la branca della fisica che studia il nucleo atomico nei suoi costituenti protoni e neutroni e le loro interazioni.
La fisica nucleare si distingue dalla fisica atomica che invece studia l'atomo, sistema composto dal nucleo atomico e dagli elettroni.
La fisica nucleare si distingue a sua volta dalla fisica delle particelle o fisica subnucleare che invece ha come oggetto lo studio delle particelle più piccole del nucleo atomico. La fisica delle particelle o subnucleare è stata per molto tempo considerata una branca della fisica nucleare. Il termine fisica subnucleare sta cadendo in disuso poiché si riferiva allo studio di particelle interne al nucleo, mentre oggi la maggior parte delle particelle note non sono costituenti nucleari.
L'energia nucleare è la più comune applicazione della fisica nucleare, ma il campo di ricerca è anche alla base di molte altre importanti applicazioni, come in medicina (medicina nucleare, risonanza magnetica nucleare), in scienza dei materiali (implantazioni ioniche) o archeologia (radiodatazione al carbonio).
La "fisica delle particelle" è la branca della fisica che studia i costituenti fondamentali e le interazioni fondamentali della materia; essa rappresenta la fisica dell<nowiki>'</nowiki>"infinitamente piccolo". Talvolta viene anche usata l'espressione fisica delle alte energie, quando si vuole far riferimento allo studio delle interazioni tra particelle elementari che si verificano ad altissima energia e che permettono di creare particelle non presenti in natura in condizioni ordinarie, come avviene con gli acceleratori di particelle.
In senso stretto, il termine "particella" non è del tutto corretto. Gli oggetti studiati dalla Fisica delle particelle, obbediscono ai principi della meccanica quantistica. Come tali, mostrano una dualità onda-corpuscolo, in base alla quale manifestano comportamenti da particella sotto determinate condizioni sperimentali e comportamenti da onda in altri. Teoricamente, non sono descritte né come onde né come particelle, ma come vettori di stato in un'astrazione chiamata spazio di Hilbert.
È una branca relativamente recente della fisica moderna che studia appunto il comportamento fisico di sistemi complessi come ad esempio il sistema economico (econofisica) o il sistema climatico assunti come sistemi dinamici non lineari.
Questa branca della fisica (la fisica cibernetica), nata nella seconda metà del XX secolo, si è sviluppata a tal punto che è ora ricompresa all'interno di varie discipline tecnico-applicative quali l'automatica, la meccatronica e l'informatica (intelligenza artificiale).
La "fisica medica" o "fisica sanitaria" è un'attività che riguarda, in generale, tutti i settori della fisica applicata alla medicina e alla radioprotezione. Più in particolare, le strutture di fisica sanitaria ospedaliere si occupano, in prevalenza, dell'impiego delle radiazioni ionizzanti e non ionizzanti (diagnostica per immagini, radioterapia, medicina nucleare, ...), ma anche di informatica, di modellistica, ecc.
L'astrofisica è una scienza che applica la teoria e i metodi delle altre branche della fisica per studiare gli oggetti di cui è composto l'universo, quali ad esempio le stelle, i pianeti, le galassie e i buchi neri.
L'astrofisica si differenzia dall'astronomia in quanto l'astronomia si pone come obiettivo la comprensione dei movimenti degli oggetti celesti, mentre l'astrofisica tenta di spiegare l'origine, l'evoluzione e il comportamento degli oggetti celesti stessi, rappresentando quindi la fisica dell<nowiki>'</nowiki>"infinitamente grande". Un'altra disciplina con cui l'astrofisica è intimamente correlata è la cosmologia, che ha come oggetto di studio l'origine dell'universo.
I telescopi spaziali (tra cui va ricordato il telescopio spaziale Hubble) sono strumenti indispensabili alle indagini dell'astrofisica: grazie ad essi gli astrofisici hanno trovato conferma di molte teorie sull'universo.
La "geofisica" (anche detta "fisica terrestre") è in generale l'applicazione di misure e metodi fisici allo studio delle proprietà e fenomeni fisici tipici del pianeta Terra.
La geofisica è una scienza di tipo preminentemente sperimentale, che condivide il campo di applicazione sia con la fisica che con la geologia e comprende al suo interno diverse branche, quali ad esempio:
La "geofisica applicata" studia la parte solida più superficiale della Terra e rivolge il suo campo di ricerche all'individuazione di strutture idonee per l'accumulo di idrocarburi, nonché alla risoluzione di problemi nel campo dell'ingegneria civile, ingegneria idraulica, ingegneria mineraria e per l'individuazione di fonti di energia geotermica.
Le prospezioni geofisiche (prospezioni sismiche, elettriche, elettromagnetiche, radiometriche, gravimetriche) rappresentano alcuni metodi fisici utilizzati nel campo dell'esplorazione geologica.
I principi fisici sono alla base di numerose discipline tecnico-scientifiche sia teoriche sia più vicine al campo applicativo (tecnica). Allo stesso tempo la fisica si avvale degli strumenti tecnici e matematici messi a disposizione da queste discipline per aiutarsi nel suo continuo processo di indagine scientifica dei fenomeni dell'universo.
Nel testo Il Saggiatore del 1623, Galileo Galilei afferma:
In generale, gli elementi che caratterizzano il modello matematico di un sistema fisico sono due: lo spazio degli stati e la dinamica. Il primo è un insieme che contiene tutti i possibili stati in cui il sistema si può trovare, dove per stato si intende una collezione di grandezze fisiche che, se conosciute in un certo istante, sono sufficienti per predire come evolverà il sistema, cioè quali stati saranno occupati negli istanti futuri; ad esempio, per un sistema meccanico di "n" particelle libere di muoversi nello spazio, uno stato è un insieme di 6"n" numeri reali, 3"n" per le posizioni (3 coordinate per ogni particella), e i restanti 3"n" per le velocità (3 componenti per ogni particella). Lo spazio degli stati può essere molto complicato, sia geometricamente (ad esempio nella meccanica dei sistemi vincolati e nella teoria della relatività generale, dove in genere è una varietà differenziale, i.e. uno spazio "curvo") che analiticamente (ad esempio in meccanica quantistica, dove è uno spazio di Hilbert proiettivizzato). La dinamica, invece, è la legge che, dato uno stato iniziale, descrive l'evoluzione del sistema. Solitamente, è data in forma differenziale, cioè collega lo stato in un certo istante a quello in un istante successivo "infinitamente vicino" nel tempo.
Le più grandi rivoluzioni della fisica moderna (la teoria della relatività generale, la meccanica quantistica e la teoria quantistica dei campi) si possono ricondurre all'inadeguatezza della fisica classica a descrivere i nuovi fenomeni sperimentali riscontrati verso la fine dell'Ottocento e l'inizio del Novecento (l'esperimento di Michelson-Morley e i vari esperimenti in cui si presentano fenomeni quantistici, tra cui, l'esperimento della doppia fenditura, il corpo nero, l'effetto fotoelettrico e l'effetto Compton).
Le maggiori aree della matematica che forniscono strumenti utili allo studio sia della forma dello spazio degli stati che della dinamica sono:
Gli strumenti della statistica sono utilizzati durante la fase di rilevamento dei dati a partire dal modello fisico e nella fase successiva di trattamento dei dati.
Particolarmente utile nella prima fase di rilevamento dei dati è la metodica del campionamento statistico (in inglese "sampling"), che consiste nel selezionare una particolare serie di dati all'interno dell'intervallo di condizioni studiate.
Una volta ottenuti i dati, viene effettuata la cosiddetta analisi di regressione, che permette di ottenere dall'insieme di dati più o meno sparsi (in quanto affetti da errori di varia natura) una relazione matematica precisa. Nel caso più semplice in cui la relazione matematica tra i dati venga rappresentata da una retta, si parla di regressione lineare.
Molti concetti statistici sono poi presi a prestito dalla fisica statistica laddove non è possibile avere informazioni deterministiche sui sistemi o fenomeni a molti gradi di libertà e variabili.
I computer vengono utilizzati in più fasi del processo conoscitivo: durante la fase di osservazione possono essere utilizzati ad esempio per effettuare un campionamento delle misurazioni, ovvero il valore della grandezza da misurare viene letto ad intervalli determinati, in modo da avere più misure in un ristretto lasso di tempo. Il calcolatore può svolgere anche la funzione di strumento registratore: i dati relativi all'osservazione vengono ad essere archiviati per lo svolgimento di operazioni successive di valutazione e/o confronto con altri dati. L'intero sistema per la misurazione, il trattamento e la registrazione dei dati, costituito dal calcolatore e da strumentazioni specifiche ad esso interfacciate, viene denominato sistema di acquisizione dati (o DAQ).
Gli strumenti informatici possono quindi fungere da "strumento" durante le diverse fasi dell'esperienza, ma possono anche andare oltre, costituendo un vero e proprio sistema virtuale, che sostituisce e "imita" il sistema fisico reale; si parla in questo caso di simulazione del processo in esame. Il sistema simulato presenta il vantaggio rispetto al sistema reale di avere un controllo su tutti gli elementi di disturbo che influenzano il fenomeno studiato; d'altra parte è necessaria una precedente conoscenza del modello matematico associato al modello fisico per la creazione del modello simulato. La simulazione quindi affianca "in primis" l'osservazione diretta durante il processo conoscitivo, con lo scopo di convalidare il modello matematico ipotizzato, e una volta che la corrispondenza tra modello fisico e modello simulato è stata accertata, è possibile utilizzare la simulazione per effettuare delle stime in condizioni contemplate dal modello matematico, ma che sono differenti da quelle in cui è avvenuta la precedente osservazione diretta.
La fisica è strettamente connessa alla chimica (la scienza delle molecole) con cui si è sviluppata di pari passo nel corso degli ultimi due secoli. La chimica prende molti concetti dalla fisica, soprattutto nei campi di termodinamica, elettromagnetismo, e meccanica quantistica. Tuttavia i fenomeni chimici sono talmente complessi e vari da costituire una branca del sapere distinta.
Nella chimica, come nella fisica, esiste il concetto di forza come "interazione tra i corpi". Nel caso della chimica "i corpi" hanno dimensioni dell'ordine dell'Ångström, e sono appunto le molecole, gli atomi, gli ioni, i complessi attivati, e altre particelle di dimensioni ad essi confrontabili. Le forze di interazione tra questi corpi sono i legami chimici (legami intramolecolari) e altre forze di interazione più blande (ad esempio le forze di Van der Waals, il legame a idrogeno e le forze di London).
È probabilmente la disciplina che più di ogni altra si avvale dei principi della fisica per sviluppare teorie proprie dedicate all'ideazione, progettazione, realizzazione e gestione di sistemi utili alle esigenze dell'uomo e della società: nel campo dell'ingegneria edile e dell'ingegneria civile strutture edili e opere civili (case, strade, ponti) sfruttano le conoscenze nel campo della statica e sulla resistenza meccanica dei materiali sottoposti a stress o sollecitazioni meccaniche e/o termiche; l'ingegneria meccanica e l'ingegneria motoristica sfruttano le conoscenze offerte dalla termodinamica per la progettazione e la realizzazione delle macchine termiche; l'ingegneria energetica sfrutta le conoscenze fisiche per la realizzazione di sistemi di produzione e distribuzione dell'energia (energia nucleare, energie rinnovabili, energia da combustibili fossili); l'ingegneria dell'informazione sfrutta i segnali e le onde elettromagnetiche emesse dalle sorgenti per il trasporto dell'informazione a distanza.
L'approccio metodologico utilizzato nel campo della fisica è applicato dall'inizio degli anni novanta anche a problematiche di tipo economico nell'ambito della disciplina denominata econofisica come tentativo di superamento dell'approccio classico economico di tipo semi-quantitativo.
Ad esempio vengono studiate le fluttuazioni dei mercati finanziari e i crash del mercato azionario a partire da modelli normalmente utilizzati per studiare fenomeni di tipo fisico quali: modelli di percolazione, modelli derivati dalla geometria frattale, modelli di arresto cardiaco, criticalità auto-organizzata e previsione dei terremoti, tipicamente modelli per sistemi complessi e caotici ovvero non-lineari.
Prima dell'avvento del metodo scientifico, l'interpretazione dei fenomeni naturali era riservata alla filosofia, per cui per lungo tempo la fisica fu denominata "filosofia naturale".
Tra i primi tentativi di descrivere la materia in ambito filosofico, si ricorda Talete.
Successivamente Democrito tentò di descrivere la materia attraverso i concetti di vuoto e atomo.
Ad oggi la fisica mantiene stretti rapporti con la filosofia attraverso branche come l'epistemologia e la filosofia della scienza.
Come in ogni altra disciplina scientifica i contributi scientifici alla nascita ed allo sviluppo di teorie fisiche avvengono attraverso pubblicazioni scientifiche su riviste scientifiche soggette ai ben noti e rigorosi processi di revisione paritaria.
</doc>

<doc id="221853" url="https://it.wikipedia.org/wiki?curid=221853" title="Elasticità (meccanica)">
Elasticità (meccanica)
In fisica, l"'elasticità" è la proprietà che permette ad un corpo di deformarsi sotto l'azione di una forza esterna e di riacquisire, se le deformazioni non risultano eccessive, la sua forma originale al venir meno della causa sollecitante. Se il corpo, cessata la sollecitazione, riassume esattamente la configurazione iniziale è detto perfettamente elastico. 
L'elasticità riguarda sia i corpi solidi che i fluidi. I primi possiedono sia elasticità di forma che di volume, reagiscono cioè elasticamente alle sollecitazioni che tendono a deformare il volume del corpo e a cambiare i suoi angoli; i fluidi invece presentano solo elasticità di volume, in quanto reagiscono elasticamente a una compressione o espansione ma non oppongono resistenza al cambiamento di forma, che dipende dal recipiente.
La sollecitazione massima che garantisce il comportamento elastico del materiale è detta limite di elasticità e, nel caso venga superata, si entra nella regione di comportamento plastico del pezzo, che consiste nel cedimento o nel flusso del materiale, a seconda che sia fragile o duttile rispettivamente. Il limite di elasticità trattandosi di una pressione è misurato in Pascal, ovvero una forza per unità di superficie: 
formula_1
Se il materiale è duttile, ovvero che permette plasticizzazioni, il limite elastico è la tensione di snervamento, mentre nel caso di materiali fragili, che sono privi del campo plastico, il limite elastico è la rottura del materiale.
Il modello matematico più semplice di rappresentazione del comportamento elastico è quello lineare della legge di Hooke (e della legge di Hooke generalizzata nel caso di stati tensionali pluriassiali), che nel caso di stato di sforzo monoassiale, tipico delle prove di trazione, è: formula_2 dove formula_3 è lo sforzo agente nel provino raffigurato in figura con formula_4 forza applicata alle sue estremità ed formula_5 superficie della sezione trasversale iniziale, formula_6 la deformazione del provino ovvero il suo allungamento relativo, con formula_7allungamento assoluto del provino, ossia la differenza fra la lunghezza finale formula_8 e quella iniziale formula_9 ed formula_10 il modulo di Young (o elastico), che è la costante di proporzionalità fra gli sforzi e le deformazioni in campo elastico.
Tale modello riveste un aspetto fondamentale sia in ambito teorico, per la possibilità di pervenire ad uno studio matematico completo dei problemi formulati, sia in ambito ingegneristico, per la ricaduta che esso ha nella modellazione e risoluzione di problemi di interesse tecnico e scientifico. Altri più complessi modelli matematici di elasticità nonlineare, importanti per la rappresentazione del comportamento delle gomme, fanno riferimento al modello di materiale iperelastico, mentre per mezzi porosi il modello si declina nella poroelasticità.
Lo studio dei corpi solidi elastici è oggetto della teoria dell'elasticità, una branca della meccanica dei solidi.
Il comportamento elastico dei diversi materiali ha origini microscopiche che si distinguono in base al particolare tipo di materiale. Possiamo parlare infatti di "elasticità entalpica" ed "elasticità entropica".
L'elasticità entalpica è caratteristica dei materiali cristallini, e deriva da un fenomeno che avviene a livello atomico. Le proprietà elastiche di questi materiali derivano dal tipo di interazione che si instaura fra i loro atomi costituenti, quando questi sono sottoposti ad un carico esterno. Se tali interazioni determinano un dislocamento degli atomi contenuto, questi, una volta rimosso il carico, riescono a rioccupare la loro posizione iniziale ed il materiale è detto elastico; se perdipiù il dislocamento è sufficientemente piccolo, è garantita la diretta proporzionalità fra deformazione e carico ed è valida pertanto la legge di Hooke. 
Il fitto reticolo cristallino di questi materiali permette solamente piccole deformazioni e spostamenti locali, da cui derivano l'alto limite di elasticità ed il grande modulo elastico. Questo comporta la necessità di esercitare elevate tensioni per ottenere deformazioni rilevanti. Nel caso in cui si rimanga al di sotto dello sforzo di snervamento del materiale, il rapporto tra sforzo e deformazione è pari alla costante modulo elastico o modulo di Young, che rappresenta la proporzionalità fra sforzo e deformazione nel campo lineare del materiale, descritta dalla legge di Hooke, e determina la pendenza del tratto rettilineo nel diagramma sforzo-tensione della prova monoassiale rappresentata in figura. 
L'elasticità dipende quindi dalla struttura microscopica del materiale e dalle forze di interazione che agiscono fra gli atomi che lo compongono. In particolare va considerata l'energia potenziale esistente tra ogni coppia di atomi, che può essere espressa in funzione della loro distanza. A una certa distanza d i due atomi sono in equilibrio, ossia la risultante delle forze di interazione tra i due è nulla. La variazione di tali forze (a causa della sollecitazione esterna) fa variare la distanza reciproca tra le particelle (producendo a livello macroscopico la deformazione del corpo: nel caso di trazione, per esempio, si ha uno "stiramento" dei legami). Per livelli relativamente bassi delle sollecitazioni, il lavoro meccanico necessario viene accumulato come energia elastica, all'interno del materiale, e viene restituito interamente al venir meno della causa sollecitante mentre le particelle ritornano alla loro posizione iniziale (il corpo riacquista la sua forma e dimensioni originarie). L'energia immagazzinata nel materiale è quantificabile dalla seguente relazione: formula_11 che graficamente è rappresentata dall'area sottesa alla curva tensione-deformazione rappresentata in figura, dove formula_12 è il lavoro svolto di deformazione, immagazzinato nel materiale come energia elastica, formula_13è l'andamento dello sforzo in funzione della deformazione formula_14, ed formula_15 è la deformazione finale che si raggiunge applicando il carico esterno.
Questo meccanismo è alla base del comportamento elastico macroscopico dei diversi materiali, ma al variare del tipo di materiale, e dunque della struttura microscopica, si delineano comportamenti elastici differenti.
L'elasticità entropica è caratteristica dei materiali polimerici costituiti a livello molecolare da catene; tale elasticità scaturisce da un movimento delle catene da uno stato ad elevata entropia (lo stato più probabile, in cui le catene sono aggrovigliate) a uno stato a bassa entropia (uno stato meno probabile, più ordinato, in cui le catene sono allineate), che avviene durante l'allungamento del materiale.
Materiali polimerici come la gomma, essendo costituiti a livello microscopico da molecole a catena, permettono grandi scorrimenti e deformazioni, e pertanto sono caratterizzati da bassi limiti di elasticità e piccolo modulo di elasticità. Ciò significa che a sforzi e tensioni relativamente bassi corrispondono già deformazioni apprezzabili macroscopicamente, così come punti di snervamento o rottura molto bassi. Questi materiali sono detti elastomeri, con un comportamento cosiddetto ad ""alta elasticità"" rispetto alla ""vera elasticità"" dei cristallini. Inoltre, a causa del precoce stiramento delle catene, causato da un ulteriore allungamento quando queste sono già state allineate, gli elastomeri hanno un comportamento elastico non lineare.
I materiali cellulari, come il legno, reagiscono in modo differente alla compressione e alla trazione. Grazie alla presenza di cavità nel materiale, la compressione mostra completa rigidità fino a quando le pareti di tali cavità non sono soggette a inflessione elastica, che permette di avere una notevole deformazione senza grande incremento di sforzo. Tali deformazioni inoltre, sono in gran parte recuperabili, ma una volta avvenute riportano il corpo a uno stato di rigidità, essendosi annullate le cavità. D'altra parte, queste non hanno la stessa influenza sulla trazione, che non permette la flessione elastica delle pareti nello stesso modo.
Per studiarne il comportamento se sottoposti a sforzo, i materiali possono essere modellati come privi di struttura interna e costituiti da un continuo solido. Rappresentando il corpo in un sistema di riferimento cartesiano, si può indicare la posizione di ogni suo punto tramite il vettore posizione: formula_16ed il loro spostamento con il vettore formula_17. Il vettore spostamento descrive come si deforma il corpo sotto carico, infatti: formula_18 è la distanza cartesiana fra due punti del corpo e formula_19è la stessa distanza dopo che il corpo si sia deformato ed è chiaramente funzione di formula_20. Si introduce la grandezza formula_21 detta deformazione, che al variare di formula_22 forma un tensore di rango 2, detto tensore delle deformazioni: formula_23dove i termini diagonali formula_24con formula_25 sono dette deformazioni normali e descrivono gli allungamenti o le contrazioni, le restanti formula_26con formula_27 sono detti scorrimenti e descrivono la variazione di forma, quindi degli angoli, rispetto al riferimento cartesiano.
Lo stato di sforzo è generalmente, e nella maggior parte dei casi, tridimensionale. Per studiarlo si sfrutta il Teorema di Cauchy ponendo una terna cartesiana sul punto formula_28 sotto studio e tagliando il corpo con un piano inclinato di normale formula_29 a distanza infinitesima formula_30 da formula_28, che individua insieme ai tre piani di riferimento un tetraedro, detto di Cauchy, rappresentato in figura. La faccia di normale formula_29ha superficie pari a formula_33, mentre le altre, di normale formula_34formula_35e formula_36, hanno superficie rispettivamente pari a formula_37, formula_38e formula_39dove formula_40, formula_41e formula_42sono i coseni direttori di formula_29. Il generico sforzo agente sul piano di normale formula_29 è formula_45, e sulle altre facce formula_46, formula_47 e formula_48, che per convenzione sono considerati positivi se entranti e quindi il meno sta ad indicare che sono uscenti dal volume infinitesimo. Per studiare il generico stato di sforzo formula_45di un punto appartenente al corpo basta imporre l'equilibrio statico nel tetraedro (I equazione cardinale della statica): 
formula_50 
che nel caso siano noti i tre vettori formula_46, formula_47 e formula_48si può determinare lo sforzo formula_45in qualsiasi punto del corpo. 
Si possono ora proiettare tutti e tre i vettori formula_46, formula_47 e formula_48nelle tre direzioni formula_58, formula_59e formula_60ed il vettore formula_45nella direzione normale e tangenziale del piano di normale formula_29, ottenendo: 
formula_63 
formula_64 
dove due delle tre componenti saranno tangenziali alla faccia di applicazione dello sforzo e la rimanente sarà normale alla faccia. Si compone infine il tensore degli sforzi, che descrive il generico stato di sforzo: 
formula_65
La densità di energia di deformazione è l'energia elastica immagazzinata dal materiale per unità di volume, e vale la relazione: formula_66ovvero l'incremento della densità di energia di deformazione formula_67è pari al lavoro svolto dagli sforzi formula_68per alterare le deformazioni formula_26. Si ricava allora che: formula_70.
La relazione formula_66 può essere espansa in serie con Taylor nell'intorno di formula_72nel caso di solido lineare e di stato iniziale scarico ed indeformato, ovvero con formula_73, ottenendo:
formula_74 alla quale se applichiamo formula_70 otteniamo la legge di Hooke generalizzata: formula_76 che nel caso di materiale isotropo diviene:
formula_77
dove formula_10 è il modulo elastico e formula_79 è il coefficiente di Poisson.
</doc>

<doc id="261778" url="https://it.wikipedia.org/wiki?curid=261778" title="Campo gravitazionale">
Campo gravitazionale
In fisica, il campo gravitazionale è il campo associato all'interazione gravitazionale.
In meccanica classica, il campo gravitazionale è trattato come un campo di forze conservativo. Secondo la relatività generale esso è espressione della curvatura dello spazio-tempo creata dalla presenza di massa o energia, quindi la forza di gravità sarebbe una forza apparente, ed è rappresentato matematicamente da un tensore metrico legato allo spazio-tempo curvo attraverso il tensore di Riemann. 
Il campo gravitazionale generato dalla Terra, ad esempio, in prossimità della superficie terrestre assume valori prossimi a 9,8 m·s e per convenzione si adotta tale valore di riferimento per l'accelerazione di gravità.
Il campo gravitazionale è un campo di forze conservativo. Il vettore del campo gravitazionale generato nel punto formula_1 nello spazio dalla presenza di una massa nel punto formula_2, origine del riferimento, è definito come:
dove formula_4 è la costante di gravitazione universale e formula_5 la massa. È quindi possibile esprimere la forza esercitata sul corpo di massa "m" come:
L'unità di misura del campo gravitazionale nel Sistema internazionale è:
dove formula_8 è il modulo di formula_9.
Il campo gravitazionale è descritto dal potenziale gravitazionale, definito come il valore dell'energia gravitazionale rilevato da una massa posta in un punto dello spazio per unità di massa. L'energia gravitazionale della massa è il livello di energia che la massa possiede a causa della sua posizione all'interno del campo gravitazionale; pertanto il potenziale gravitazionale della massa è il rapporto tra l'energia gravitazionale e il valore della massa stessa, cioè: 
Essendo il campo gravitazionale conservativo, è sempre possibile definire una funzione scalare "V" il cui gradiente, cambiato di segno, coincida con il campo:
Per ogni campo gravitazionale è possibile definire delle superfici ortogonali al campo in ogni punto dello spazio, dette superfici equipotenziali. Il significato fisico di queste superfici è chiaro se si considera il lavoro della forza di gravità lungo un cammino appartenente alla superficie: dato che lo spostamento è punto per punto ortogonale alla forza, il lavoro lungo questo cammino è nullo. Ciò vuol dire che masse uguali sulla stessa superficie equipotenziale hanno la stessa energia potenziale. Per esempio, nel caso di una sorgente sferica, le superfici equipotenziali sono sfere concentriche e le linee di flusso sono l'insieme delle semirette entranti nel centro delle sfere.
Indicato il campo gravitazionale come formula_12, a meno di fattori moltiplicativi e traslazionali, con formula_13 vettore posizione, si osserva che la sua divergenza in tre dimensioni è nulla. Infatti:
Il campo gravitazionale assume nell'ambito della teoria della relatività generale di Einstein una struttura molto più complessa. Esso rappresenta la differenza tra il tensore metrico dello spazio-tempo e il tensore metrico dello spazio-tempo piatto, o spazio-tempo di Minkowski. La deformazione dello spazio-tempo data dal campo gravitazionale viene talvolta rappresentata graficamente come la deformazione di un materasso, o di un telo elastico, ad opera di una palla pesante posta su di esso: qui lo spazio-tempo piatto è rappresentato dal telo perfettamente teso e, appunto, piatto.
Il tensore metrico dello spazio-tempo deformato dalla presenza di masse, oppure semplicemente energia, viene calcolato attraverso l'equazione di campo di Einstein:
dove formula_16 è il tensore metrico, formula_17 e formula_18 sono rispettivamente la curvatura scalare e il Tensore di Ricci, ottenuti come contrazione dal Tensore di Riemann, legato alle derivate del tensore metrico e formula_4 è la costante di gravitazione universale.
</doc>

<doc id="221400" url="https://it.wikipedia.org/wiki?curid=221400" title="Durezza">
Durezza
La durezza è un valore numerico che indica le caratteristiche di deformabilità plastica di un materiale. È definita come "la resistenza alla deformazione permanente".
Le prove di durezza determinano la resistenza offerta da un materiale a lasciarsi penetrare da un altro ("penetratore").
Esistono diverse scale per misurare la durezza dei materiali. Le più usate sono infatti:
Le prove di durezza si eseguono con macchine provviste di penetratori con forme diverse e con diverse metodologie.
Si basa, nel calcolo della durezza, sulla misura del diametro dell'impronta lasciata dal penetratore. Anche la Vickers si basa sullo stesso principio e la prova Vickers viene chiamata anche prova di microdurezza.
Rapida, economica, non distruttiva (oggetto riutilizzabile), possibilità di impiegare carichi particolarmente alti.
Se si moltiplica per 3,3 la durezza Brinell di un acciaio normalizzato si ottiene il suo carico di rottura.
Sono difficili i confronti tra diverse misure Brinell (a parità di carico applicato, il penetratore può affondare in misure diverse, cambiando anche l'inclinazione delle facce e la distribuzione degli sforzi).
Infatti è possibile confrontare prove eseguite con parametri diversi a patto che il rapporto formula_2 risulti verificato.
Si basa sulla misura dell'area dell'impronta lasciata dal penetratore. Viene chiamata prova di microdurezza, per via dei piccoli carichi applicati al penetratore.
Angolo di apertura: formula_6
L'inclinazione delle facce è costante; si usano anche carichi piccoli per fare misure di durezza ravvicinate, precisione della misurazione.
La scala è unica per tutti i materiali.
Costosa, notevole perdita di tempo nella lettura delle impronte che si può fare solo al microscopio.
Prova di durezza messa a punto da Steven Rockwell. Si basa sull'affondamento diretto dell'impronta e non sulla durezza misurata come pressione. Le scale di durezza ottenute sono convenzionali.
Velocità (usata in campo industriale).
È una misura di durezza solo convenzionale. L'unità di misura è rappresentata dai punti della scala utilizzata (HRC per la scala Rockwell, HV per la Vickers, HB per la Brinell, ecc.)
I valori di durezza ottenuti con le diverse scale non si possono confrontare perché utilizzano unità di misura diverse. Confronti empirici possono essere effettuati ma con molta attenzione (ad es. materiali simili, dimensione di impronte simili, ecc.) (vedi norma ISO 18265)
Misura la durezza in senso fisico, resistenza che il campione offre all'abrasione, si stabilisce la scala di durezza o misurando la larghezza del solco o il peso applicato. Per i metalli però non può fornire utili indicazioni.
Il principio su cui si basa lo Scleroscopio di Shore (durometro Shore) è il rimbalzo di una sfera d'acciaio che cade da una determinata altezza sul saggio. Il valore della durezza è definito dall'altezza del rimbalzo. 
La prova è impiegata per il collaudo di cilindri per laminatoi a freddo.
</doc>

<doc id="218922" url="https://it.wikipedia.org/wiki?curid=218922" title="Legge di Snell">
Legge di Snell
Nell'ottica geometrica la legge di Snell, nota anche come legge di Descartes o legge di Snell-Descartes (o legge di Cartesio o legge di Snell-Cartesio), descrive le modalità di rifrazione di un raggio luminoso nella transizione tra due mezzi con indice di rifrazione diverso, e deriva dall'equazione iconale.
Il nome della legge di Snell rispetta la legge dell'eponimia di Stigler. La legge è documentata per la prima volta in un manoscritto scritto intorno al 984 del matematico arabo Ibn Sahl, che la usò per ottenere i profili delle lenti asferiche (lenti che fuocheggiano la luce senza indurre aberrazioni geometriche). Fu poi scoperta di nuovo da Thomas Harriot nel 1602, che però non pubblicò il suo lavoro. Nel 1621, fu scoperta ancora una volta da Willebrord Snell, in una forma matematicamente equivalente, ma rimase inedita fino alla sua morte. René Descartes derivò indipendentemente la legge in termini di funzioni sinusoidali nel suo trattato "Discorso sul metodo" del 1637 e la usò per risolvere diversi problemi di ottica. In francese la legge di Snell è chiamata "di Descartes" o "di Snell-Descartes".
La luce si propaga nel vuoto alla velocità costante c.
La figura mostra due mezzi trasmissivi con indice di rifrazione "n" (a sinistra) e "n" (a destra) in contatto tra loro attraverso una superficie, che viene chiamata interfaccia (linea verticale in figura). Nel caso "n" > "n", la luce ha una velocità di fase più bassa nel secondo mezzo.
Il raggio luminoso PO proveniente dal mezzo di sinistra colpisce l'interfaccia nel punto O. A partire da tale punto O tracciamo una retta perpendicolare all'interfaccia stessa, che viene chiamata "normale" all'interfaccia (linea orizzontale in figura). L'angolo tra la normale e il raggio luminoso PO viene chiamato "angolo d'incidenza", θ.
Il raggio attraversa l'interfaccia e prosegue nel mezzo di destra, indicato come OQ. L'angolo che tale raggio (rifratto) forma con la normale si chiama "angolo di rifrazione", θ.
La legge di Snell fornisce la relazione tra gli angoli θ e θ:
formula_1
Si noti che nel caso θ = 0° (ovvero il raggio risulta perpendicolare all'interfaccia) la soluzione è θ = 0° per qualunque valore di "n" e "n". In altri termini, un raggio che entra in un mezzo in modo perpendicolare alla sua superficie non viene mai deviato.
Quanto detto sopra vale anche nel caso di un raggio luminoso che passa da un mezzo più denso a uno meno denso; la simmetria della legge di Snell mostra che gli stessi percorsi luminosi sono validi anche nella direzione opposta.
Una regola di carattere qualitativo per determinare la direzione della rifrazione è che il raggio luminoso è sempre più vicino alla normale dal lato del mezzo più denso.
La legge di Snell è valida in generale solo per mezzi isotropi, come il vetro. Nel caso di mezzi anisotropi (ad esempio alcuni cristalli) il fenomeno della birifrangenza può dividere in due il raggio rifratto. Si vengono allora ad avere due raggi, uno "ordinario" (raggio "o") che segue la legge di Snell, e uno "straordinario" (raggio "e") che può non essere complanare con quello incidente.
Si consideri l'equazione iconale nella forma:
formula_2
dove x è la ascissa curvilinea lungo il cammino ottico, n è l'indice di rifrazione e formula_3 il versore del raggio ottico ed effettuando il prodotto vettoriale per il versore formula_4 della posizione si ottiene:
formula_5
dato che la derivata è un operatore lineare, si può trasportare dentro nel modo più semplice il prodotto vettoriale:
formula_6
si arriva quindi alla legge di Snell:
formula_7
che si esprime nella forma più comune chiamando θ l'angolo fra la direzione della posizione e quella del raggio ottico:
formula_8
e considerando il fatto che la derivata parziale nulla equivale a un argomento costante rispetto alla variabile di derivazione:
formula_9
Il caso più semplice è quello in cui l'indice di rifrazione è uniforme lungo l'ascissa curvilinea:
formula_10
In questo caso si vede immediatamente che la traiettoria del raggio risulta rettilinea, con inclinazione uniforme:
formula_11
Nel caso invece l'indice di rifrazione sia lineare con l'ascissa curvilinea:
formula_12
il raggio risulta inclinato in ogni punto della sua traiettoria con legge:
formula_13
questa comporta uno smorzamento della deviazione lungo la traiettoria: col limite all'infinito si vede che l'inclinazione asintotica risulta nulla, qualunque sia l'inclinazione iniziale:
formula_14
quindi questo mezzo riesce ad allineare dei raggi ottici con qualsiasi direzione iniziale, anche se sempre in modo incompleto poiché fisicamente non può essere realizzato con un'estensione infinita, con efficienza proporzionale al cammino ottico e all'intensità dell'indice di rifrazione.
Nel passaggio da un mezzo più denso a uno meno denso (ovvero, "n" > "n") si può verificare facilmente che l'equazione formula_15 sia priva di soluzioni quando θ supera un valore che viene chiamato angolo critico:
Quando θ > θ non appare alcun raggio rifratto: la luce incidente subisce una riflessione interna totale ad opera dell'interfaccia. Si genera un'onda di superficie, o onda evanescente ("leaky wave"), che decade esponenzialmente all'interno del mezzo con indice di rifrazione "n".
Dal versore s del raggio luminoso incidente, e il versore p, normale all'interfaccia, è possibile ricavare i versori associati al raggio riflesso e rifratto:
La legge di Snell può essere legata al Principio di Fermat: 
Infatti si può verificare che il cammino seguito dalla luce è un punto stazionario per il cammino ottico, vale a dire che in corrispondenza di esso la derivata del cammino ottico si annulla. In alternativa, la relazione può essere ottenuta considerando l'interferenza di tutti i possibili percorsi che l'onda di luce può percorrere dalla sorgente all'osservatore - risulta che l'interferenza è distruttiva ovunque, eccetto che negli estremi di fase (dove è costruttiva) - che diventa il percorso effettivo.
In una classica analogia della brachistocrona proposta da Feynman una spiaggia è una regione a indice di rifrazione più basso del mare; il modo più rapido per un bagnino sulla spiaggia di raggiungere a velocità costante una persona che sta affogando è percorrere il cammino ottico ovvero seguire la legge di Snell.
</doc>

<doc id="213526" url="https://it.wikipedia.org/wiki?curid=213526" title="Diffrazione">
Diffrazione
In fisica la diffrazione è un fenomeno associato alla deviazione della traiettoria di propagazione delle onde (come anche la riflessione, la rifrazione, la diffusione o l'interferenza) quando queste incontrano un ostacolo sul loro cammino. È tipica di ogni genere di onda, come il suono, le onde sulla superficie dell'acqua o le onde elettromagnetiche come la luce o le onde radio; il fenomeno si verifica anche nelle particolari situazioni in cui la materia mostra proprietà ondulatorie, in accordo con il dualismo onda-particella.
Gli effetti di diffrazione sono rilevanti quando la lunghezza d'onda è comparabile con la dimensione dell'ostacolo: in particolare per la luce visibile (lunghezza d'onda attorno a 0,5 µm) si hanno fenomeni di diffrazione quando essa interagisce con oggetti di dimensione sub-millimetrica.
Qualunque deviazione di un raggio di luce non imputabile a riflessione o rifrazione è chiamato "diffrazione". Questa è la classica definizione riscontrata nel trattato classico di Ottica di Arnold Sommerfeld. È sorprendente notare che questa definizione ricalca quanto descritto per la prima volta dal Gesuita Francesco Maria Grimaldi (si veda l'originale definizione nella seconda figura che riproduce l'originale paragrafo nel trattato di F. M. Grimaldi), coniandone il termine che significa "frazionamento in più parti" nel 1665. Isaac Newton attribuì la causa del fenomeno a un "incurvamento" dei raggi luminosi (non osservando, come tutti gli Ottici Newtoniani, le frange all'interno dell'ombra di un capello). 
Il termine newtoniano che designa la diffrazione è "inflexion". Thomas Young studiò la diffrazione come sovrapposizione tra la luce direttamente trasmessa oltre una apertura in uno schermo (o un ostacolo) e un'onda avente origine dal bordo dell'apertura o dell'ostacolo. Lo stesso Augustin-Jean Fresnel adottò inizialmente il modello di Thomas Young, ma alcune esperienze atte ad evidenziare variazioni della figura di diffrazione dai parametri caratteristici del bordo (natura, geometria del bordo) e una inversione rispetto alla posizione prevista delle frange scure nella regione esterna all'ombra di un capello, lo indussero ad un abbandono della teoria dell'onda di bordo (stabilita da A. Fresnel in modo del tutto indipendente da Thomas Young), a favore della teoria basata sul principio di Huygens, riuscendo soprattutto a fornire una descrizione del fenomeno dal punto di vista matematico.
È da notare che la teoria dell'onda di bordo di Thomas Young ha precursori "Newtoniani" antecedenti a Thomas Young, la cui teoria è in alcuni punti non chiara e priva di supporto matematico. In genere la posizione di Thomas Young, cui si attribuisce il merito di avere per primo stabilito la natura "periodica" della luce, è in realtà incerta (il termine "lunghezza d'onda" non è "mai" usato) mentre è una costante delle sue ricerche l'analogia tra "suono" e "luce". Tuttavia, almeno all'epoca dei pionieri (T. Young e A. Fresnel) né la teoria dell'onda di bordo, né il principio di Huygens hanno un supporto teorico che giunge solo nel 1883 ad opera di G. Kirchhoff e, anche se inosservato, da G. A. Maggi nel 1886 per la teoria dell'onda di bordo.
Di fronte ad un fenomeno di diffrazione, nel caso ottico, si possono compiere alcune osservazioni preliminari.
Il caso generale del fenomeno è la "diffrazione di Fresnel" (o da campo vicino), dove la sorgente di luce e il piano di osservazione sono posti a distanza finita dalla fessura.
La "diffrazione di Fraunhofer" (o da campo lontano), invece, è un caso particolare della precedente, ma molto più semplice da analizzare: essa si ha infatti quando la sorgente e il piano sono posti a distanza infinita dal diaframma, così che i raggi incidenti possano essere considerati paralleli fra loro.
Un esempio di questo caso è quello di una sorgente di luce puntiforme (o rettilinea), come il tratto diritto del filamento di una lampadina o un fascio laser, vista da una distanza di un paio di metri attraverso due lamette distanti tra loro mezzo decimo di millimetro.
Le caratteristiche della diffrazione sono quindi che:
Fenomeni di diffrazione possono essere osservati quotidianamente, in particolare quelli che interessano la luce visibile: per esempio, le tracce incise sulla superficie di un CD o di un DVD agiscono come un reticolo di diffrazione, creando il familiare effetto arcobaleno; anche i piccoli ologrammi, ad esempio delle carte di credito, si basano sulla diffrazione. In natura, si possono osservare colori cangianti dovuti a diffrazioni interferenziali, come quelli delle piume del pavone, o della corazza di alcuni coleotteri, o delle ali di molte farfalle (figura a sinistra), che sono colorate grazie all'interferenza delle onde diffratte da parte di microscopiche scaglie disposte regolarmente.
La diffrazione atmosferica causata da microscopiche gocce d'acqua in sospensione è la responsabile degli anelli luminosi visibili attorno alle sorgenti di luce; la stessa ombra di un oggetto può mostrare deboli effetti di diffrazione sui bordi.
Una figura "policromatica" analoga alla farfalla nella foto si osserva tra le trame di un ombrello quando si guarda una luce lontana attraverso di esse. La diffrazione costituisce un limite nella risposta di qualunque strumento ottico e pertanto riguarda varie tecnologie: essa infatti pone un limite alla risoluzione di fotocamere, videocamere, telescopi e microscopi.
A causa della diffrazione le onde marine formano figure intricate quando incrociano un piccolo ostacolo, come un faro in mare, o attraversano una apertura stretta (figura a destra), come un canale o l'ingresso di un porto.
La diffrazione può venire "intuitivamente" "letta" come una richiesta di continuità da parte del fronte d'onda che subisce una discontinuità dal bordo (o dai bordi) di un ostacolo. La figura a fianco, che simula la diffrazione di un'onda piana attraverso la fenditura, ricorda quanto osservato in un'onda alla superficie dell'acqua quando passa attraverso una fenditura. Oltre la fenditura il fronte d'onda incidente è "tagliato" dai due bordi. La parte di fronte d'onda contigua a ciascun bordo piega attorno al bordo stesso fornendo così una perturbazione continua. 
Secondo la chiave di lettura della teoria dell'onda di bordo è come se l'ostacolo diventasse una sorgente (fittizia) di un'onda a simmetria cilindrica che si sovrappone tanto all'onda trasmessa secondo le leggi dell'ottica geometrica e, ovviamente, all'altra onda di bordo.
Secondo la chiave di lettura del principio di Huygens, il fronte d'onda incidente è l'inviluppo di onde elementari sferiche. Qui, le sorgenti ("fittizie") di tali onde sono nei punti della fenditura. L'inviluppo di tali onde sferiche in prossimità del bordo si propaga dando luogo a nuovi fronti d'onda successivi.
Nonostante la diversità nella descrizione del fenomeno, sia il modello dell'onda di bordo che il modello basato sul principio di Huygens sono pienamente equivalenti visto che la "matematizzazione" della teoria dell'onda di bordo discende dalla matematizzazione della teoria della propagazione secondo il principio di Huygens. Si vedano a questo proposito i riferimenti [6] e [7] della precedente sezione storica.
Per determinare gli effetti della diffrazione bisogna trovare innanzitutto la fase e l'intensità di ciascuna sorgente di Huygens in ogni punto dello spazio; ciò significa calcolare per ogni punto la sua distanza dal fronte d'onda: se la distanza di ciascun punto differisce a meno di un numero intero di lunghezze d'onda, tutte le sorgenti sono in fase e daranno luogo ad una "interferenza costruttiva"; se, al contrario, la distanza differisce di un numero intero più mezza lunghezza d'onda, l'interferenza sarà "distruttiva". In generale, è sufficiente determinare le posizioni di questi massimi e minimi per ottenere una completa descrizione del fenomeno.
La descrizione più semplice di diffrazione si ha nel caso di un problema in due dimensioni, come nel caso delle onde nell'acqua che si propagano solo sulla superficie del liquido; per quanto riguarda i raggi luminosi, si può trascurare una dimensione solo se la fenditura si estende in quella direzione per una distanza molto più grande della lunghezza d'onda della luce; nel caso di fenditure circolari, invece, si devono considerare tutte e tre le dimensioni.
Come esempio, si può ricavare un'equazione più precisa che leghi l'intensità delle bande di diffrazione all'angolo a cui si considerano, nel caso di una singola fenditura: partendo dalla rappresentazione matematica del principio di Huygens si considera un'onda monocromatica formula_1 sul piano complesso di lunghezza d'onda λ incidente su una fenditura di ampiezza "a"; se questa fenditura giace lungo il piano individuato dagli assi x′-y′ (con centro nell'origine), si può ipotizzare che la diffrazione generi un'onda complessa formula_2 che viaggia lungo una direzione radiale "r" rispetto alla fenditura e la cui equazione è:
Sia ora (x′,y′,0) un punto interno alla fenditura: se (x,0,z) sono le coordinate alle quali corrisponde l'intensità da misurare della figura di diffrazione, la fenditura si estenderà da formula_4 a formula_5 in un verso e da formula_6 a formula_7 nell'altro.
La distanza "r" dalla fenditura è:
Considerando il caso della diffrazione di Fraunhofer, risulterà che:
In altre parole, la distanza dello schermo è molto più grande dell'ampiezza della fenditura; con l'aiuto del teorema binomiale, questa distanza può essere ben approssimata come:
Sostituendo questo valore di "r" nella prima equazione si trova:
Per semplificare si possono raccogliere i termini costanti e chiamarli "C" ("C" può contenere numeri immaginari, anche se al termine ψ si potrà semplificare eliminando queste componenti). Ora, nella diffrazione di Fraunhofer formula_13 è molto piccolo, in modo da poter scrivere formula_14. Quindi, essendo formula_15, risulterà:
Si può notare con l'aiuto della formula di Eulero che formula_16 e formula_17:
formula_18
con la posizione: formula_19.
Infine, sostituendo in formula_20, l'intensità formula_21 delle onde diffratte a un dato angolo θ è data da:
Ripartendo dal principio di Huygens
si considerano ora "N" fenditure di uguale ampiezza ("a", formula_23, 0) distanti l'una dall'altra di una lunghezza "d" lungo l'asse x′. Come precedentemente trovato, la distanza "r" dalla prima fenditura sarà:
Per generalizzare questa situazione nel caso di "N" fenditure, si può innanzitutto osservare che mentre "z" e "y" restano costanti, x′ varia in questo modo:
Dunque si ha che:
e la somma di tutti gli "N" contributi all'onda è:
Di nuovo si può notare che formula_28 è trascurabile, 
in modo che formula_29; quindi risulta:
Ora si può usare la seguente identità
formula_30
per sostituire nell'equazione ed ottenere:
Di nuovo, sostituendo "k" e introducendo la variabile formula_31 al posto delle costanti non oscillanti, come nella diffrazione da una fenditura, si può semplificare il risultato; ricordandosi che:
si possono scartare gli esponenziali ed ottenere:
La diffrazione di un'onda piana incidente su un'apertura circolare dà come risultato il cosiddetto disco di Airy. La variazione dell'intensità dell'onda in funzione dell'angolo è data dall'espressione:
dove "a" è il raggio dell'apertura, "k" è pari a 2π/λ e J è una funzione di Bessel. Più piccola è l'apertura, più grande è la dispersione delle onde, a pari distanza.
Nel caso della diffrazione da un'apertura circolare, si rilevano una serie di anelli concentrici attorno al disco di Airy. L'analisi matematica di questo specifico caso è simile alla versione utilizzata per la diffrazione da una singola fenditura vista precedentemente.
Un'onda non deve necessariamente attraversare una fenditura per andare incontro a diffrazione: per esempio, anche un raggio di luce di ampiezza finita subisce un processo di diffrazione ed aumenta la propria ampiezza. Questo fenomeno limita l'ampiezza "d" dei dispositivi dove si raccoglie la luce, nel fuoco di una lente; ciò è conosciuto come "limite di diffrazione":
dove λ è la lunghezza d'onda della luce, "f" è la distanza focale della lente e "a" è il diametro del raggio di luce o (se il raggio di luce è più ampio della lente) il diametro della lente. L'ampiezza risultante contiene circa il 70% dell'energia della luce e corrisponde al raggio del primo minimo del disco di Airy, approssimato con il criterio di Rayleigh; il diametro del primo minimo, che contiene l'83.8% dell'energia della luce, è spesso utilizzato come "diametro di diffrazione".
Utilizzando il principio di Huygens, è possibile ricavare la superficie di diffrazione di un'onda che attraversa una fenditura di qualsiasi forma: se questa superficie viene osservata ad una certa distanza dall'apertura, risulterà essere la trasformata di Fourier in due dimensioni della funzione che rappresenta l'apertura.
La diffrazione da numerose fenditure descritta precedentemente è un fenomeno simile a ciò che si verifica quando un'onda viene diffusa da una struttura periodica, come il reticolo di atomi in un cristallo o le grate di un reticolo di diffrazione Ogni punto di diffusione, ad esempio ogni atomo del cristallo, agisce come una sorgente puntiforme di onde sferiche, le quali daranno luogo a fenomeni di interferenza costruttiva per formare un certo numero di onde diffratte. La direzione di queste onde è descritta dalla "Legge di Bragg":
dove λ è la lunghezza d'onda, "d" è la distanza tra ogni punto di diffusione, θ è l'angolo di diffrazione e "m" è un numero intero che indica l"'ordine" di ciascun onda diffratta.
La diffrazione di Bragg viene usata nella cristallografia a raggi X per ricavare la struttura di un qualsiasi cristallo analizzando gli angoli ai quali i raggi X vengono diffratti dal cristallo stesso: poiché l'angolo θ di diffrazione dipende dalla lunghezza d'onda λ, un reticolo di diffrazione causa una dispersione angolare di un raggio di luce.
L'esempio più semplice di diffrazione di Bragg è lo spettro di colori che si può vedere riflesso da un Compact disc: la breve distanza tra le tracce sulla superficie del disco costituisce un reticolo di diffrazione e ogni componente della luce bianca viene diffratta con differenti angoli, in accordo con la legge di Bragg.
La diffrazione di particelle materiali come gli elettroni è uno dei maggiori punti di forza della meccanica quantistica: osservare la diffrazione di un elettrone o di un neutrone consente di verificare l'esistenza della dualità onda-particella; questa diffrazione è anche un utile strumento scientifico: la lunghezza d'onda di queste particelle è sufficientemente piccola da essere usata nella scansione della struttura atomica dei cristalli.
La lunghezza d'onda associata ad una particella è la cosiddetta lunghezza d'onda di De Broglie:
dove "h" è la costante di Planck e "v" e "m" sono rispettivamente la velocità e la massa della particella; λ è caratteristica di qualsiasi oggetto materiale, anche se è rilevabile solo per entità con piccola massa, come gli atomi e altre particelle.
Recentemente, è stata osservata la diffrazione di particelle chiamate barioni e di un particolare tipo di fullereni chiamato "buckyball" ; il prossimo obiettivo della ricerca sarà quello di osservare la diffrazione dei virus, i quali, avendo molta più massa delle particelle elementari, hanno una lunghezza d'onda inferiore, cosicché devono attraversare molto lentamente una fenditura estremamente sottile affinché manifestino caratteri ondulatori.
Persino la Terra ha una sua lunghezza d'onda (in effetti, qualunque oggetto dotato di una quantità di moto la possiede): avendo una massa di circa 6×10 kg e una velocità orbitale media di circa 30000 ms, essa ha una lunghezza d'onda di De Broglie pari a 3.68×10 m.
La descrizione della diffrazione poggia, come detto in precedenza, sulla descrizione dell'interferenza tra onde generate dalla stessa sorgente che percorrono direzioni differenti, partendo dal medesimo punto; in questo modello, la differenza di fase tra le onde dipende solo dall'effettiva lunghezza del tragitto; può accadere però che due onde emesse in tempi diversi dalla sorgente arrivino sullo schermo in due punti diversi ma "allo stesso istante"; la fase iniziale con cui la sorgente genera le onde può anche cambiare nel tempo: onde emesse a intervalli di tempo sufficientemente lunghi non potranno quindi formare una stabile figura d'interferenza, dal momento che la loro differenza di fase non sarà più indipendente dal tempo.
La lunghezza correlata alla fase di un'onda elettromagnetica come la luce è detta lunghezza di coerenza: affinché si verifichi un'interferenza, la differenza dei tragitti di due onde deve essere inferiore alla lunghezza di coerenza.
Se le onde sono emesse da una sorgente estesa, ciò può produrre un'incoerenza lungo la direzione trasversale: osservando perpendicolarmente un raggio di luce, la lunghezza per la quale le fasi sono correlate è chiamata "lunghezza di coerenza trasversale"; nel caso della diffrazione dalla doppia fenditura, solo se questa lunghezza è minore della distanza tra le due aperture si osserverà il fenomeno della diffrazione.
Nel caso della diffrazione di particelle, la lunghezza di coerenza è legata all'estensione nello spazio della funzione d'onda che descrive tali particelle.
</doc>

<doc id="292637" url="https://it.wikipedia.org/wiki?curid=292637" title="Moto (fisica)">
Moto (fisica)
In fisica il moto è il cambiamento di posizione di un corpo in funzione del tempo, misurato da uno specifico osservatore in un determinato sistema di riferimento. Fino al XIX secolo, le leggi di Newton, incluse tra gli assiomi e i postulati del famoso "Philosophiae Naturalis Principia Mathematica",
erano alla base di quella parte della meccanica classica nota come cinematica. Lo studio del moto a partire dalle cause che lo generano ovvero le forze è noto invece come dinamica. 
Storicamente il problema del moto è stato dunque il primo problema affrontato dalla fisica, direttamente applicato al moto dei corpi celesti con la meccanica celeste nell'ambito della rivoluzione scientifica. I calcoli delle traiettorie e delle forze esercitate dai corpi in moto basati sulle leggi newtoniane e delle fisica classica, si dimostrarono efficaci fintanto che i fisici non si occuparono di fenomeni molto rapidi, come quelli della fisica atomica agli inizi del XX secolo. La forza del moto si chiama forza d'inerzia.
Nello studio del moto di un corpo particolarmente utile risulta il cosiddetto "Principio di indipendenza dei moti simultanei" il quale afferma che ""il moto di un corpo lungo una certa traiettoria nello spazio è la risultante ovvero la composizione di singoli moti ciascuno lungo le direzioni degli assi cartesiani nello spazio"". 
Esistono vari tipi di moto: rettilineo; curvilineo; circolare; parabola e ellissi: tutte queste sono dette traiettorie
Il principio è utile per determinare la traiettoria del corpo nello spazio componendo i singoli moti.
Tra i moti della "fisica classica", si ricordano:
Tra i moti della "fisica atomica e subatomica" e dell"'astrofisica", si ricordano:
</doc>

<doc id="11260" url="https://it.wikipedia.org/wiki?curid=11260" title="Legge di Hooke">
Legge di Hooke
In meccanica dei materiali, la legge di Hooke è la più semplice relazione costitutiva di comportamento dei materiali elastici. Essa è formulata dicendo che un corpo elastico subisce una deformazione direttamente proporzionale allo sforzo a esso applicato. La costante di proporzionalità che dipende dalla natura del materiale stesso.
I materiali per i quali la legge di Hooke è un'utile approssimazione del reale comportamento sono detti "materiali elastico-lineari". Definisce perciò un solido elastico allo stesso modo in cui la legge di Pascal definisce un fluido ideale.
Robert Hooke cominciò il suo studio sull'elasticità partendo dalla caratterizzazione del comportamento della "molla perfetta" o "ideale", cioè una molla priva di massa, di spessore trascurabile quando completamente compressa e in totale assenza di attrito e di altri fenomeni dissipativi; infatti, la molla ideale rappresenta il modello classico di elasticità lineare. La legge fu prima formulata nel 1675, nella forma dell'anagramma latino «"ceiiinosssttuv"», la cui soluzione fu pubblicata da Hooke nel 1678 come «"Ut tensio, sic vis"» («"come l'estensione, così la forza"»).
A partire dall'enunciato fornito originariamente da Hooke, l'equazione che esprime la forza elastica esercitata da una molla sollecitata longitudinalmente, in trazione o in compressione, lungo un asse formula_1 è:
quindi la forza formula_3 con cui la molla reagisce alla sollecitazione è direttamente proporzionale all'allungamento formula_4. La costante formula_5 rappresenta la "costante elastica longitudinale" della molla, espressa in formula_6.
In modo del tutto analogo, si ricava l'equazione che esprime il momento elastico, diretto lungo un asse formula_7 ortogonale al piano di torsione, esercitato da una molla torsionale sollecitata tangenzialmente:
quindi il momento meccanico formula_9 con cui la molla reagisce alla sollecitazione è direttamente proporzionale alla variazione dell'angolo formula_10. La costante formula_11 rappresenta la "costante elastica tangenziale" del corpo, espressa in formula_12.
Tuttavia, la formulazione odierna della legge di Hooke si serve di due grandezze vettoriali, la tensione formula_13 e la deformazione formula_14, legate tra loro da una relazione tensoriale.
Nel caso monodimensionale la relazione longitudinale diventa:
dove formula_16 è il "coefficiente di dilatazione lineare" e formula_17 è il modulo di elasticità longitudinale di Young, mentre la relazione inversa è:
dove l'inverso del modulo di Young è detto "modulo di cedevolezza longitudinale" formula_19.
Mentre il caso monodimensionale della relazione tangenziale diventa:
dove formula_21 è il c"oefficiente di scorrimento angolare" e formula_22 è il modulo di elasticità tangenziale.
Dalle relazioni precedenti si può dedurre che formula_23 e che formula_24, dove formula_25 è la sezione, formula_26 è la dimensione longitudinale e formula_27 è il braccio della forza che causa il momento.
Dato un sistema di riferimento cartesiano centrato un punto formula_28 appartenente ad un corpo deformabile, con formula_29 e detto formula_30, si ha che la cinematica del punto formula_31 è data dall'equazione:
mentre la trattazione statica di formula_31 la si ottiene attraverso il teorema di Cauchy-Poisson:
dove formula_35 è il vettore spostamento, formula_36 la giacitura e formula_37 e formula_38 sono, rispettivamente i tensori delle deformazioni e delle tensioni, che risultano entrambi simmetrici. Facendo uso della notazione di Voigt, a questi due tensori è possibile associare, rispettivamente, il vettore deformazione formula_14 e il vettore tensione formula_13.
In campo elastico, deformando un volume infinitesimo unitario formula_41, portandolo da uno stato formula_42 a uno stato formula_43, si applica un lavoro formula_44. Pertanto, il materiale rilascia tutta l'energia accumulata e ciò permette che si verifichi l'assenza di deformazioni residue.
Per i materiali iperelastici, l'energia di deformazione è definita come una funzione continua:
quindi essa rappresenta il "potenziale delle tensioni", mentre il "potenziale delle deformazioni" è rappresentato dall'energia complementare:
Essendo entrambe dei potenziali, entrambe le funzioni devono rispettare le condizioni di Schwarz.
A partire da queste considerazioni energetiche è possibile ricavare la legge di Hooke in termini tensoriali:
dove l'operatore lineare formula_48 è il "tensore di elasticità", la legge inversa, invece, è definita come:
dove l'operatore lineare formula_50 è il "tensore di cedevolezza". Pertanto si ha che:
Nonostante siano state ricavate per materiali iperelastici, queste leggi sono valide per tutti i tipi di materiali elastici.
Sia formula_48 che formula_50 sono tensori del quarto ordine, pertanto hanno 81 coefficienti scalari. In generale, entrambi i tensori hanno 36 coefficienti indipendenti, che si riducono a 21 nel caso di materiale iperelastico e a soli 2 nel caso il materiale sia anche omogeneo e isotropo. 
In tale ultimo caso il legame costitutivo è dato dalla relazione:
mentre, l'espressione inversa del legame costitutivo è la seguente:
dove formula_56 è la matrice identità e i due parametri scalari sono la costante di Lamé formula_57 e il modulo di elasticità tangenziale formula_22, che si legano al modulo di Young formula_17 e al modulo di Poisson formula_60 attraverso le relazioni:
La validità della legge di Hooke per una molla può essere verificata in laboratorio anche tramite semplici attrezzature. In genere, l'obiettivo dell'esperimento è la determinazione del valore della costante elastica longitudinale formula_5 di una molla. 
Per fare ciò occorre sottoporre la molla a carichi crescenti, misurando il relativo allungamento formula_4, pari alla differenza tra la lunghezza della molla sottoposta al carico, crescente, e la lunghezza della molla a riposo, ovvero non sottoposta ad alcun carico verticale, a meno del peso della molla stessa. Il rapporto tra la forza formula_3 applicata e l'allungamento formula_4 rappresenta esattamente il valore della costante elastica formula_5 di quella data molla. A questo punto occorre applicare forze verticali crescenti alla molla che, seguendo la legge di Hooke, produrrà allungamenti formula_4 direttamente proporzionali alle forze formula_3 applicate. I singoli valori di costante elastica formula_5 così determinati, se l'esperimento è svolto correttamente, risulteranno costanti, a meno di eventuali errori di misura da determinarsi con la teoria degli errori. 
Nel caso in cui formula_70 molle fossero poste in serie si può dimostrare e verificare sperimentalmente che il valore della costante elastica equivalente totale sarà pari a: 
</doc>

<doc id="39135" url="https://it.wikipedia.org/wiki?curid=39135" title="Spettro elettromagnetico">
Spettro elettromagnetico
Lo spettro elettromagnetico (abbreviato spettro EM) indica l'insieme di tutte le possibili frequenze delle radiazioni elettromagnetiche.
Pur essendo lo spettro continuo, è possibile una suddivisione puramente convenzionale ed indicativa in vari intervalli o "bande di frequenza", dettata a partire dallo spettro ottico. L'intero spettro è suddiviso nella parte di spettro visibile che dà vita alla luce e le parti di spettro non visibile a lunghezza d'onda maggiori e minori dello spettro visibile. Le onde di lunghezza nell'intervallo tra la luce visibile e le onde radio, a bassa intensità hanno poca energia e risultano scarsamente dannose, le radiazioni comprese tra l'ultravioletto e i raggi gamma invece hanno più energia, sono ionizzanti e quindi possono danneggiare gli esseri viventi.
Come l'orecchio ha dei limiti nella percezione del suono, l'occhio umano ha dei limiti nella visione della luce. 
In entrambi i casi, vi sono limiti superiori e inferiori.
Quantunque si distinguano varie zone nello spettro, non si può dire che esistano tra esse limiti netti.
La radiazione con una lunghezza d'onda inferiore a 400 nm è denominata luce ultravioletta. Questa zona scende fino a una lunghezza d'onda di circa 10 nm. Al di sotto di questa zona, si trova quella dei raggi X e si stende fino a una lunghezza d'onda di circa 0,006 nm. La parte inferiore dello spettro si compone di onde denominate raggi gamma. Questa zona si trova al di sotto della zona dei raggi X. Il campo di raggi gamma rappresenta il risultato della disintegrazione radioattiva.
Dalla parte dello spettro dove la luce ha lunghezza d'onda maggiore, cioè oltre il rosso, si trova la zona denominata infrarossa. Quest'ultima va da 0,7 µm a 0,4 mm. Quindi, viene la zona delle microonde, con lunghezze d'onda da 0,4 mm a 100 cm. Oltre a questa, vi sono tre campi di onde radio: onde corte da 1 m a 100 m; onde medie da 200 m a 600 m; onde lunghe superiori a 600 m. Le onde radio possono essere generate da scariche che producono onde elettromagnetiche.
È interessante rilevare che solo una parte assai limitata dello spettro contiene radiazioni visibili all'occhio.
L'occhio non può vedere la radiazione elettromagnetica oltre la zona violetta dello spettro e al di sotto della zona rossa. Lo spettro elettromagnetico si compone delle zone al di sopra e al di sotto di questi limiti, incluso il campo visibile. Anche se l'ultima lunghezza d'onda considerata nel campo visibile è di 0,4 µm, alcune persone possono vedere la radiazione con una lunghezza d'onda anche di solo 0,3 µm.
Per quanto le onde delle diverse zone abbiano tutte le stesse proprietà, si impiega il termine luce solo per la parte visibile dello spettro e le due zone circostanti. Le parti di luce visibile dello spettro sono emesse da corpi incandescenti, quali ad esempio il Sole o una lampadina. Vale la pena ricordare che, in base alla legge di Stefan-Boltzmann, ogni corpo a qualsiasi temperatura (purché superiore allo zero assoluto) emette radiazione elettromagnetica. Tuttavia i corpi che ci appaiono incandescenti sono appunto solo quelli che emettono un'apprezzabile quantità di radiazione nelle frequenze a cui il nostro occhio è sensibile.
Un utilizzo tipico applicativo dello spettro elettromagnetico è nelle telecomunicazioni per veicolare informazione attraverso segnali (portante modulata) sul canale di comunicazione tra mittente e destinatario, utilizzando la banda ottica e quella dell'infrarosso per le comunicazioni ottiche, quella a microonde e a radiofrequenza per le radiocomunicazioni (spettro radio).
Lo spettro infrarosso è coinvolto in tutti i processi di scambio di calore tra corpi per irraggiamento e quindi anche nei sistemi di riscaldamento, mentre un'altra applicazione è nei forni a microonde dove si utilizza appunto la banda delle microonde per la cottura dei cibi. Raggi X sono invece comunemente impiegati in diagnostica medica (radiografia).
</doc>

<doc id="21947" url="https://it.wikipedia.org/wiki?curid=21947" title="Interazione gravitazionale">
Interazione gravitazionale
L'interazione gravitazionale (o gravitazione o gravità nel linguaggio comune) è una delle quattro interazioni fondamentali note in fisica.
Nella fisica classica newtoniana la gravità è interpretata come una forza conservativa di attrazione a distanza agente fra corpi dotati di massa, secondo la legge di gravitazione universale; la sua manifestazione più evidente nell'esperienza quotidiana è la forza peso. 
Nella fisica moderna l'attuale teoria più completa, la relatività generale, interpreta l'interazione gravitazionale come una conseguenza della curvatura dello spaziotempo creata dalla presenza di corpi dotati di massa o energia (una piccola massa a grande velocità o una grande massa in quiete hanno lo stesso effetto di deformazione sulla curvatura dello spaziotempo circostante). Il campo gravitazionale che ne deriva è rappresentato matematicamente da un tensore metrico legato alla curvatura dello spaziotempo attraverso il tensore di Riemann. In tale contesto la forza peso diventa una forza apparente, conseguenza della geometria dello spaziotempo indotta dalla massa terrestre.
Le prime spiegazioni di una forza agente capace di aggregare i corpi vennero formulate, nella filosofia greca, all'interno di una visione animistica della natura, come nella dottrina di Empedocle, in cui domina l'alternanza di due princìpi, Amore e Odio, o in quella di Anassagora, dove prevale l'azione ordinatrice di una Mente suprema ("Nous"). 
Platone riteneva che la materia fosse pervasa da una "dynamis", cioè un'energia intrinseca, che spinge il simile ad attrarre il simile; concezione ripresa da Aristotele, per il quale tutto l'universo anela alla perfezione del primo motore immobile (Dio). Questo anelito si esprime nel movimento circolare di stelle, Sole, Luna e pianeti, giungendo tuttavia a corrompersi progressivamente fino a diventare rettilineo nella dimensione terrestre sublunare. Soltanto in quest'ambito, dunque, alcuni corpi, quelli che Platone e Aristotele chiamavano "gravi", risultano soggetti alla gravità: si trattava di composti dei quattro elementi fondamentali (fuoco, aria, acqua, terra), mentre l'etere fluttuava al di sopra di essi. Secondo la teoria aristotelica dei luoghi naturali, tutto ciò che è terra tende a ritornare lì dove risiede la terra, ovvero al centro dell'universo; al di sopra vi è la sfera dell'acqua che attrae tutto ciò che è liquido; analogamente si comportano i cerchi dell'aria e del fuoco. 
Come i suoi contemporanei, Aristotele interpretava la fisica dell'universo deducendola dalla fisiologia umana, sostenendo ad esempio che oggetti di peso diverso cadessero a velocità diverse, in analogia all'esperienza dell'uomo che tenti di contrastare il peso di un sasso, adottando così una prospettiva che, seppur contraddetta nel VI secolo d.C. da Giovanni Filopono, continuerà ad essere insegnata fino all'epoca di Galileo. Con lo stoicismo lo studio della gravità portò a scoprire una relazione tra il moto delle maree e i movimenti del Sole della Luna: l'universo è infatti concepito dagli stoici come un unico organismo vivente, animato dal "pneuma", forza vitale che tutto pervade, e che si esprime nella reciproca azione di un elemento attivo ("heghemonikòn") e di uno passivo ("hypàrchon") che ne subisce l'attrazione.
Anche per la dottrina neoplatonica, ripresa dalla teologia cristiana, il cosmo è animato dal "Logos" divino, dal quale le stelle e i pianeti risultano attratti: nel Medioevo il loro movimento viene spiegato in particolare con l'azione di intelligenze motrici, ordinate gerarchicamente in un coro di angeli. Si tratta di un universo retto da un principio armonico che si irradia in ogni sua parte, e strutturato perciò in maniera concentrica secondo l'insegnamento aristotelico. A fondamento di quest'ordine geometrico è posto Dio, il quale lo governa attraverso un atto d'amore: la gravità, dunque, come forza d'amore, così descritta ad esempio da Dante nell'ultimo verso della "Divina Commedia".
L'analogia neoplatonica tra Dio e il Sole condurrà tuttavia la filosofia rinascimentale a fare di quest'ultimo il centro di attrazione della Terra e dei pianeti. In Keplero, il primo a descrivere in maniera ellittica le loro orbite, permane la concezione animistica e astrologica dell'universo, basata sulla corrispondenza armonica tra i cieli e la terra; egli interpretava la forza immateriale della gravità come una sorta di emanazione magnetica.
A partire dal Seicento la visione animistica della gravità verrà progressivamente sostituita da una puramente meccanicista; Galileo Galilei ne fornì una descrizione limitata all'aspetto quantitativo, e riprendendo l'antica idea di Filopono teorizzò che, facendo cadere due corpi di masse differenti nello stesso momento, entrambi sarebbero arrivati al suolo in contemporanea. 
Cartesio negò che la gravità consistesse in una forza intrinseca, spiegandola sulla base di vortici di etere, e riconducendo ogni fenomeno fisico al principio di conservazione del moto, dato dalla massa per la velocità ("mv"). Leibniz obietterà a Cartesio che la quantità di moto non bastava a definire l'essenza di una forza, e ripristinò il concetto vitalistico di energia o "vis viva", espressa dal prodotto della massa per la velocità al quadrato ("e=mv"): era questa per lui ad essere conservata in natura.
Un concetto di forza affine a quello di Cartesio era stato peraltro espresso da Newton, che fece della massa, cioè della quantità di materia (data dal volume per la densità) il concetto fondamentale della meccanica gravitazionale: quanto più è grande la massa di un corpo, tanto più potente è la sua forza di gravità. Newton capì che la stessa forza che causa la caduta di una mela sulla Terra mantiene i pianeti in orbita attorno al Sole, e la Luna attorno alla Terra. Nel libro "Philosophiae Naturalis Principia Mathematica", del 1687, egli enunciò la legge di gravitazione universale, che dimostrò con il "metodo delle flussioni", un procedimento analogo alla derivazione. In seguito Huygens, nel suo "Horologium oscillatorium", chiarificò la natura delle forze centrifughe che impediscono ai pianeti di cadere sul sole pur essendone attratti.
Restava aperto tuttavia il problema di spiegare l'azione a distanza tra i corpi celesti, priva di contatto materiale, al quale verrà data una soluzione soltanto ai primi del Novecento da parte di Einstein, che sostituì l'etere con la tessitura dello spazio-tempo.
In meccanica classica l'interazione gravitazionale è generata da un campo vettoriale conservativo e descritta da una forza, detta forza peso, che agisce sugli oggetti dotati di massa.
La legge di gravitazione universale afferma che due punti materiali si attraggono con una forza di intensità direttamente proporzionale al prodotto delle masse dei singoli corpi e inversamente proporzionale al quadrato della loro distanza. Questa legge, espressa vettorialmente, diventa:
dove formula_2 è la forza con cui l'oggetto 1 è attratto dall'oggetto 2, "G" è la costante di gravitazione universale, che vale circa 6,67 × 10 Nm²/kg, "m" e "m" sono le masse dei due corpi, formula_3
è il vettore congiungente i due corpi (supposti puntiformi) e formula_4 è il suo modulo; nella seconda espressione della forza (che evidenzia il fatto che il modulo della forza è inversamente proporzionale al quadrato della distanza) formula_5 rappresenta il "versore (unitario)" che individua la retta congiungente i due punti materiali.
Definito il vettore accelerazione di gravità:
la legge di gravitazione universale può essere espressa come:
In prossimità della superficie terrestre il valore di formula_8 è convenzionalmente:
anche espressa in newton su chilogrammo.
Il campo gravitazionale è un campo di forze conservativo. Il campo generato nel punto formula_10 nello spazio dalla presenza di una massa nel punto formula_11 è definito come:
dove "G" è la costante di gravitazione universale e "M" la massa. È quindi possibile esprimere la forza esercitata sul corpo di massa "m" come:
L'unità di misura del campo gravitazionale nel Sistema internazionale è:
Il campo gravitazionale è descritto dal potenziale gravitazionale, definito come il valore dell'energia gravitazionale rilevato da una massa posta in un punto dello spazio per unità di massa. L'energia gravitazionale della massa è il livello di energia che la massa possiede a causa della sua posizione all'interno del campo gravitazionale; pertanto il potenziale gravitazionale della massa è il rapporto tra l'energia gravitazionale e il valore della massa stessa, cioè:
Essendo il campo gravitazionale conservativo, è sempre possibile definire una funzione scalare "V" il cui gradiente, cambiato di segno, coincida con il campo:
Nel precedente paragrafo si è detto che il valore medio dell'accelerazione di gravità nei pressi della superficie terrestre è stimato in 9,81 m/s². In realtà questo valore è diverso da quello reale perché non tiene conto di fattori, come la forza centrifuga causata dalla rotazione terrestre e la non perfetta sfericità della terra (la terra ha la forma di un geoide). Il valore convenzionalmente assunto è quindi "g" = 9,80665 m/s², deciso nella terza CGPM nel 1901 e corrisponde all'accelerazione subita da un corpo alla latitudine 45,5°.
Per molte applicazioni fisiche e ingegneristiche è quindi utile utilizzare una versione approssimata della forza di gravità, valida nei pressi della superficie terrestre:
dove formula_18 è un versore diretto lungo la "verticale". In sostanza la forza di gravità è approssimata con una forza di modulo costante, indipendente dalla quota del corpo, e come direzione il "basso", nel senso comune del termine. Naturalmente anche in questa approssimazione corpi con masse diverse hanno la stessa accelerazione di gravità.
L'energia potenziale gravitazionale "U" è data da:
dove "h" è la quota del corpo rispetto a un riferimento fisso.
In questo caso approssimato è molto semplice ricavare le leggi del moto, mediante integrazioni successive: per un corpo in caduta libera, chiamando "z" l'asse verticale (sempre diretto verso il basso) e proiettando il moto su di esso, valgono le seguenti leggi:
Inoltre, dalla conservazione dell'energia meccanica si ottiene un risultato notevole per corpi in caduta libera inizialmente fermi. Scriviamo l'energia meccanica del sistema a un tempo generico:
dove "v" è la velocità del corpo e "z" la sua quota. Supponiamo ora che all'istante iniziale formula_24 il corpo si trovi a una quota formula_25 e all'istante finale formula_26 abbia una velocità formula_27 e si trovi a quota formula_28; scriviamo quindi l'energia del sistema ai due istanti:
Dato che l'energia meccanica si conserva possiamo uguagliare le due ultime equazioni e ricavarci il modulo della velocità dopo una caduta di una quota "h":
Il problema generale della gravitazione, cioè la determinazione del campo gravitazionale creato da un insieme di masse, si può esprimere con il teorema di Gauss e il teorema della divergenza.
Essendo la forza di gravità conservativa, si può esprimere formula_8 come:
dove formula_34 è proporzionale all'energia potenziale gravitazionale come segue:
Dal teorema di Gauss:
Per il teorema della divergenza, il primo integrale, cioè il flusso della forza gravitazionale, è esprimibile come integrale di volume della sua divergenza:
Sostituendo a formula_8 la sua espressione come gradiente:
che, dovendo valere per ogni volume di integrazione, implica:
Quest'ultima è una equazione differenziale alle derivate parziali del secondo ordine, detta equazione di Poisson, da completare con le opportune condizioni al contorno.
La teoria di Newton della gravitazione ha permesso di descrivere con accuratezza la grande maggioranza dei fenomeni gravitazionali nel Sistema Solare.
Tuttavia, da un punto di vista sperimentale essa presenta alcuni punti deboli, successivamente affrontati a partire dalla teoria della relatività generale: 
Einstein sviluppò una nuova teoria della gravitazione, denominata relatività generale, pubblicata nel 1915.
Nella teoria di Einstein, la gravità non è una forza, come tutte le altre, ma è la proprietà della materia di deformare lo spazio-tempo. Propriamente, la gravità non è un'interazione a distanza fra due masse, ma è un "fenomeno mediato" da una deformazione dello spazio-tempo. La presenza di massa (più in generale, di energia e impulso) determina una curvatura della geometria (più esattamente, della struttura metrica) dello spazio-tempo: poiché i corpi che si muovono in "caduta libera" seguono nello spazio-tempo traiettorie geodetiche, e queste ultime non sono rettilinee se lo spazio-tempo è curvo, ecco che il moto degli altri corpi (indipendentemente dalla loro massa) subisce le accelerazioni che classicamente sono attribuite alla "forza di gravità".
I pianeti del Sistema Solare quindi hanno orbite ellittiche non per effetto di una forza di attrazione esercitata direttamente dal Sole, ma perché la massa del Sole incurva lo spazio-tempo. Il campo gravitazionale attorno a una stella è rappresentato dalla soluzione di Schwarzschild delle equazioni di Einstein, soluzione che si ottiene semplicemente assumendo le proprietà di simmetria sferica nello spazio tridimensionale di indipendenza dal tempo. Le equazioni del moto geodetico nella metrica di Schwarzschild permettono di calcolare l'orbita di un pianeta attorno a una stella: per quasi tutti i pianeti del Sistema Solare, la differenza fra queste orbite e i moti descritti dalle leggi di Keplero (soluzioni delle equazioni di Newton) non è osservabile in quanto è molto più piccola degli effetti perturbativi dovuti all'interazione dei pianeti fra loro. L'unica eccezione è rappresentata dal moto di Mercurio, in cui la precessione dell'asse dell'orbita che si osserva è molto maggiore di quanto previsto dalla gravità newtoniana (anche tenendo conto dell'influenza degli altri pianeti), ed è invece in perfetto accordo con la previsione delle equazioni relativistiche. L'osservazione della precessione del perielio di Mercurio è quindi una delle evidenze a favore della relatività generale rispetto alla teoria gravitazionale newtoniana.
Un'ulteriore evidenza osservativa, riscontrata per la prima volta nel corso dell'eclissi solare del 1919 (ma definitivamente confermata da osservazioni su scala extragalattica a partire dal 1980), consiste nell'effetto detto lente gravitazionale: l'immagine di un corpo celeste visto dalla Terra appare spostata rispetto alla posizione reale del corpo (talvolta l'immagine è anche sdoppiata) a causa della deflessione che la luce subisce quando rasenta una regione dello spazio con alta densità di massa. Questo conferma il fatto che la gravitazione deforma lo spazio-tempo, e che tale deformazione è avvertita anche da particelle prive di massa (i fotoni).
Un diverso approccio meccanicistico della gravità è dato dalla teoria del "Loop Quantum Gravity" e, nell'ambito della teoria delle stringhe, dall'esistenza dei gravitoni.
Il fisico matematico Erik Verlinde propone, rivedendo idee già in circolazione, che la gravità sia interpretabile come la manifestazione di una forza emergente in senso entropico: citando le sue parole la gravità altro non è che un ""effetto collaterale della propensione naturale verso il disordine"". Verlinde, con assoluta moderazione suggerisce che si tratta "di idee che dovrebbero servire da guida per ulteriori studi". Allo stato attuale degli studi, la teoria di Verlinde si delinea come l'ultima e la più motivata delle ipotesi speculative tra, e per, gli addetti ai lavori. Nel luglio 2010 la sua teoria è passata al grande pubblico, tramite la diffusione mediatica e attraverso internet, esautorando la teoria gravitazionale propagandata dal motto: "la gravità non esiste".
Nel 2009, Erik Verlinde formalizzò un modello concettuale che descrive la gravità come una forza entropica, che suggerisce che la gravità è una conseguenza del comportamento statistico dell'informazione associata alla posizione dei corpi materiali. Questo modello combina l'approccio termodinamico della gravità con il principio olografico, e implica che la gravità non sia una interazione fondamentale, ma un fenomeno che emerge dal comportamento statistico dei gradi di libertà microscopici codificati su uno schermo olografico.
La legge di gravità può essere derivata dalla meccanica statistica classica applicata al principio olografico, che afferma che la descrizione di un volume di spazio può essere rappresentato come formula_41 bit d'informazione binaria, codificata ai confini della regione, una superficie di area formula_42. 
L'informazione è distribuita casualmente su tale superficie e ciascun bit immagazzinato in una superficie elementare dell'area.
dove formula_44 è la lunghezza di Planck.
Il teorema statistico di equipartizione lega la temperatura formula_45 di un sistema (espressa in joule, basandosi sulla costante di Boltzmann) con la sua energia media:
Questa energia può essere identificata con la massa formula_47 per la relazione di equivalenza di massa ed energia:
La temperatura effettiva sperimentata da un rivelatore uniformemente accelerato in un campo di vuoto o stato di vuoto è data dall'effetto Unruh.
Questa temperatura è:
dove formula_50 è la costante di Planck ridotta,
e formula_51 è l'accelerazione locale, 
che è legata alla forza formula_52 dalla seconda legge di Newton del moto:
Assumendo ora che lo schermo olografico sia una sfera di raggio formula_4, la sua superficie è data da:
Da questi principi si deriva la legge di gravitazione universale di Newton:
L'iter è reversibile: leggendolo dal basso, dalla legge di gravitazione, risalendo per i principi della termodinamica si ricava l'equazione che descrive il principio olografico.
</doc>

<doc id="1770" url="https://it.wikipedia.org/wiki?curid=1770" title="Elettrone">
Elettrone
L'elettrone è una particella subatomica con carica elettrica negativa che si ritiene essere una particella elementare.
Insieme ai protoni e ai neutroni, è un componente dell'atomo e, sebbene contribuisca alla sua massa totale per meno dello 0,06%, ne caratterizza sensibilmente la natura e ne determina le proprietà chimiche: il legame chimico covalente si forma in seguito alla redistribuzione della densità elettronica tra due o più atomi..
La maggior parte degli elettroni presenti nell'universo è stata prodotta dal Big Bang, ma possono essere generati anche dal decadimento beta degli isotopi radioattivi e in collisioni ad alta energia, mentre possono essere annichilati dalla collisione con i positroni o assorbiti in un processo di nucleosintesi stellare.
Il moto dell'elettrone genera un campo magnetico, mentre la variazione della sua energia e della sua accelerazione causano l'emissione di fotoni; è inoltre responsabile della conduzione della corrente elettrica e del calore.
L'avvento dell'elettronica e il relativo sviluppo dell'informatica hanno reso l'elettrone protagonista dello sviluppo tecnologico del ventesimo secolo. Le sue proprietà vengono sfruttate in svariate applicazioni, come i tubi a raggi catodici, i microscopi elettronici, la radioterapia e il laser.
"Elettrone" deriva dalla parola greca "ήλεκτρον" (pronuncia "électron"), il cui significato è ambra. Tale nome è storicamente dovuto al fatto che l'ambra ebbe un ruolo fondamentale nella scoperta dei fenomeni elettrici: in particolare a partire dal VII secolo a.C. gli antichi Greci erano a conoscenza del fatto che strofinando un oggetto di ambra o ebanite con un panno di lana, l'oggetto in questione acquisiva la capacità di attirare a sé corpuscoli leggeri, quali ad esempio granelli di polvere. Queste evidenze sperimentali vennero riprese nel XVI secolo da William Gilbert, che individuò numerose sostanze, tra cui il diamante e lo zolfo, che presentavano lo stesso comportamento dell'ambra. Egli diede il nome di "forza elettrica" alla forza che attirava i corpuscoli, e chiamò "elettrizzati" i materiali che manifestavano tale proprietà.
Gli studi sull'elettricità e sul magnetismo furono continuati in epoca moderna fra gli altri da Benjamin Franklin e Michael Faraday, e in questo periodo nel contesto dell'atomismo fu avanzata l'idea che anche l'elettricità potesse essere costituita da piccoli corpuscoli indivisibili.
L'idea di una quantità fondamentale di carica elettrica fu introdotta dal filosofo Richard Laming nel 1838 per spiegare le proprietà chimiche dell'atomo..
Nel 1874 il fisico irlandese George Stoney introdusse il concetto di "unità di carica fondamentale". Nel 1891 ne stimò il valore e coniò il termine ""elettrone"" per riferirsi a tali "unità" (dalla combinazione del termine ""elettrico"" e del suffisso -one, che sarà utilizzato anche successivamente per designare altre particelle subatomiche, come il protone o il neutrone), scrivendo:
Le prime prove sperimentali dell'esistenza di questa particella si ebbero nel 1860, quando il fisico e chimico inglese Sir William Crookes effettuò esperimenti con il tubo di Geissler, inserendovi due lamine metalliche e collegandole a un generatore di corrente continua a elevato potenziale (circa ). Durante tale esperimento, Crookes si accorse che si generava una luce avente una colorazione differente a seconda del gas utilizzato. Tale emissione luminosa aveva origine dal catodo (polo negativo) e fluiva verso l'anodo (polo positivo).
In seguito all'esperienza di Crookes, anche il fisico tedesco Johann Wilhelm Hittorf nel 1869, mentre si stava dedicando ad uno studio sulla conduttività elettrica dei gas, evidenziò un bagliore emesso dal catodo e verificò che aumentava in intensità con il decrescere della pressione del gas. Nel 1876 il fisico tedesco Eugen Goldstein mostrò che i raggi di tale bagliore proiettano un'ombra e li chiamò "raggi catodici". Durante gli anni settanta del XIX secolo, Crookes sviluppò il primo tubo catodico con un vuoto spinto all'interno, dimostrando che i raggi luminescenti che appaiono all'interno del tubo trasportano energia e si muovono dal catodo all'anodo. Inoltre, applicando un campo magnetico, fu in grado di deflettere i raggi, dimostrando che il fascio si comporta come se fosse carico negativamente. Nel 1879, Crookes avanzò l'idea che queste proprietà potessero essere spiegate da quella che denominò "materia radiante""" e suggerì che si doveva trattare di un nuovo stato della materia, consistente di molecole cariche negativamente che sono espulse ad alta velocità dal catodo.
Il fisico inglese di origini tedesche Arthur Schuster proseguì gli esperimenti di Crookes posizionando delle piastre metalliche parallele ai raggi catodici e applicando un potenziale elettrico fra loro. Il campo deflesse i raggi verso la piastra carica positivamente, confermando che i raggi trasportano carica negativa. Misurando l'ammontare della deflessione per una data intensità di corrente elettrica, nel 1890 Schuster fu in grado di stimare il rapporto fra la massa e la carica dei componenti dei raggi catodici. Tuttavia, tale stima fu ritenuta poco attendibile dai suoi contemporanei poiché risultò migliaia di volte superiore alle attese.
Negli ultimi anni dell'Ottocento numerosi fisici sostennero la possibilità che l'elettricità fosse costituita da unità discrete, alle quali vennero conferiti vari nomi, ma delle quali non vi fu alcuna prova sperimentale convincente. Nel 1896, il fisico britannico J. J. Thomson, con i suoi colleghi John S. Townsend e H. A. Wilson, svolsero una serie di esperimenti che dimostrarono che i raggi catodici erano costituiti da singole particelle, piuttosto che onde, atomi o molecole come si riteneva in precedenza. Thomson stimò in maniera accurata la carica "e" la massa, trovando che le particelle dei raggi catodici, che lui chiamò "corpuscoli", avevano probabilmente una massa migliaia di volte inferiore a quella dell'idrogenione (H), lo ione più leggero che si conoscesse a quel tempo. Thomson mostrò come il rapporto carica/massa ("e"/"m"), uguale a e/g, fosse indipendente dal materiale del catodo. Inoltre mostrò come le particelle cariche negativamente prodotte dai materiali radioattivi, dai materiali riscaldati e dai raggi catodici fossero riconducibili tutte alla stessa entità. Il nome "elettrone" fu nuovamente proposto per identificare tali particelle dal fisico irlandese George F. Fitzgerald e da allora il nome venne universalmente accettato.
Mentre studiava i minerali naturalmente fluorescenti nel 1896, il fisico francese Henri Becquerel scoprì che essi emettono radiazione senza l'intervento di una sorgente di energia esterna. Tali materiali radioattivi divennero argomento di grande interesse da parte degli scienziati, fra cui anche il fisico neozelandese Ernest Rutherford, il quale scoprì che emettevano particelle, da lui chiamate particelle alfa e beta, sulla base della loro capacità di penetrare la materia. Nel 1900, Becquerel mostrò che i raggi beta emessi dal radio potevano essere deflessi da un campo elettrico e che il loro rapporto massa-carica era lo stesso dei raggi catodici. Tale evidenza sperimentale suggeriva che gli elettroni esistevano come componenti degli atomi.
La carica degli elettroni fu misurata con maggiore precisione dal fisici americani Robert Millikan e Harvey Fletcher nel loro esperimento della goccia d'olio del 1909, i cui risultati furono pubblicati nel 1911. In tale esperimento venne usato un campo elettrico per frenare la caduta, dovuta alla gravità, di una goccia d'olio elettricamente carica. Grazie a tale apparato strumentale, fu possibile misurare la carica elettrica prodotta da pochi ioni (tra 1 e 150) con un margine di errore inferiore allo 0,3%. Si ottenne un valore pari a e fu quindi possibile stimare che la massa dell'elettrone dovesse valere . Un simile esperimento era stato condotto in precedenza dal gruppo di Thomson, usando nubi di gocce di acqua cariche generate tramite l'elettrolisi, e nel 1911 da Abram Ioffe, che ottenne in maniera indipendente lo stesso risultato di Millikan usando microparticelle di metallo cariche, pubblicando i risultati nel 1913. Tuttavia, le gocce d'olio risultavano più stabili di quelle dell'acqua a causa della loro bassa velocità di evaporazione e quindi maggiormente adatte per svolgere esperimenti precisi per un lungo periodo di tempo.
Attorno all'inizio del ventesimo secolo, fu scoperto che sotto certe condizioni una particella carica che si muove ad elevata velocità causa una condensazione di vapore acqueo sovrassaturo lungo il suo cammino. Nel 1911, Charles Wilson sfruttò tale principio per sviluppare la prima camera a nebbia, uno strumento che permette di tracciare e fotografare il percorso seguito da particelle cariche, come gli elettroni veloci.
A partire dal 1914, gli esperimenti dei fisici Ernest Rutherford, Henry Moseley, James Franck e Gustav Hertz stabilirono definitivamente che l'atomo è formato da un nucleo massivo carico positivamente circondato da elettroni di massa minore. Nel 1913, il fisico danese Niels Bohr postulò che gli elettroni si trovano in stati di energia quantizzata, con l'energia determinata dal momento angolare delle orbite degli elettroni attorno al nucleo. La teoria avanzata da Bohr prevedeva inoltre che gli elettroni potessero muoversi tra questi stati (o orbite) in seguito all'assorbimento o all'emissione di un quanto di energia, un fotone di specifica frequenza. Tale teoria era in grado di spiegare la comparsa delle linee di emissione spettrale dell'idrogeno come conseguenza del suo contenuto energetico attraverso riscaldamento o facendolo attraversare da corrente elettrica. Nonostante ciò, il modello di Bohr non era in grado di predire l'intensità delle relative linee e di spiegare la struttura dello spettro di atomi più complessi.
La formazione di legami chimici tra atomi fu spiegata nel 1916 da Gilbert Newton Lewis, il quale asserì che il legame covalente sia generato dalla condivisione di una coppia di elettroni tra due atomi, mentre una descrizione completa sulla formazione di queste coppie e dei legami chimici venne fornita da Walter Heitler e Fritz London nel 1923 grazie alla meccanica quantistica. Nel 1919 il chimico statunitense Irving Langmuir rielaborò il modello statico dell'atomo di Lewis ipotizzando che tutti gli elettroni fossero distribuiti in una serie di gusci ("shell") sferici approssimativamente concentrici, tutti di uguale spessore"; tali gusci erano a loro volta suddivisi in celle, ognuna delle quali conteneva una coppia di elettroni. Tramite questo modello, Langmuir spiegò qualitativamente le proprietà chimiche di tutti gli elementi, le quali si ripetono secondo un ordine preciso stabilito dalla tavola periodica.
Nel 1924, il fisico austriaco Wolfgang Pauli osservò che la struttura a strati di un atomo poteva essere spiegata attraverso un insieme di quattro parametri che definivano univocamente lo stato quantico di un elettrone, e che un singolo stato non poteva essere occupato da più di un singolo elettrone (questa legge è nota come principio di esclusione di Pauli). Nonostante la sua intuizione, Pauli non riuscì a spiegare il significato fisico del quarto parametro, il quale poteva assumere solo due valori. La spiegazione teorica di tale parametro si deve invece ai fisici olandesi Samuel Goudsmit e George Uhlenbeck, i quali suggerirono che un elettrone, oltre al momento angolare associato alla sua orbita, può possedere un proprio momento angolare intrinseco. Fu così introdotto il concetto di spin e con questa scoperta era possibile spiegare anche la separazione delle linee spettrali osservata con uno spettrografo ad alta definizione.
Nel 1929, il fisico francese Louis de Broglie vinse il premio Nobel per la fisica per aver scoperto che anche gli elettroni, oltre alla luce, sono caratterizzati da una doppia natura, una corpuscolare e una ondulatoria. Questa nuova proprietà, presentata per la prima volta nella sua dissertazione del 1924 dal titolo "" (Ricerca sulla teoria dei quanti) è nota come dualismo onda-particella e comporta la possibilità di osservare fenomeni di interferenza fra elettroni sotto appropriate condizioni:
L'interferenza è una proprietà di tutte le onde: ad esempio nel caso della luce, se tra una sorgente luminosa e uno schermo illuminato da tale sorgente viene interposto un foglio con delle fessure parallele, la luce prodotta dalla sorgente attraversa tali fessure e si proietta sullo schermo producendo delle figure a bande in corrispondenza dello schermo. Nel 1927 furono osservati gli effetti dell'interferenza con un fascio di elettroni dal fisico inglese George Paget Thomson con una sottile pellicola metallica e dai fisici americani Clinton Davisson e Lester Germer, i quali studiarono il fenomeno di scattering degli elettroni incidenti su una lastra di nickel monocristallino. Niels Bohr nello stesso anno incluse l'ipotesi di de Broglie e queste evidenze sperimentali nel principio di complementarità, secondo il quale una descrizione completa dell'elettrone e della luce non può fare riferimento solo alla sua natura ondulatoria o solo alla sua natura particellare, ma deve necessariamente includerle entrambe.
Infatti la natura ondulatoria dell'elettrone si manifesta ad esempio nel fenomeno dell'interferenza, mentre la natura corpuscolare fa sì che un fascio di elettroni riesca a fare girare un piccolo mulinello posizionato lungo il suo tragitto.
Il successo della previsione di de Broglie portò alla pubblicazione dell'equazione di Schrödinger, formulata nel 1926 da Erwin Schrödinger, che descrive l'evoluzione temporale di uno stato quantico (e quindi della relativa funzione d'onda). Piuttosto che cercare una soluzione che determinasse la posizione di un elettrone nel tempo, questa equazione era usata per prevedere la probabilità di trovare un elettrone in un volume finito o infinitesimo dello spazio. Da questo approccio ebbe origine la branca della fisica denominata "meccanica quantistica", che garantì la possibilità di ricavare teoricamente i livelli energetici di un elettrone nell'atomo di idrogeno in buon accordo con i dati sperimentali. Una volta che vennero presi in considerazione lo spin e l'interazione fra più elettroni, la meccanica quantistica fu in grado di ricostruire l'andamento delle proprietà chimiche tipiche degli elementi nella tavola periodica.
Nel 1928, basandosi sul lavoro di Wolfgang Pauli, Paul Dirac formulò un modello dell'elettrone coerente con la teoria della relatività ristretta, applicando considerazioni relativistiche e di simmetria alla formulazione hamiltoniana della meccanica quantistica per un elettrone in un campo elettromagnetico; questa trattazione portò alla formulazione dell'equazione di Dirac. Per risolvere i problemi della sua equazione relativistica (in primo luogo l'esistenza di soluzioni a energia negativa), nel 1930 lo stesso Dirac sviluppò un modello del vuoto come un mare infinito di particelle con energia negativa, che fu poi chiamato mare di Dirac. Questo permise di prevedere l'esistenza del positrone, la corrispettiva antiparticella dell'elettrone, che fu scoperta sperimentalmente nel 1932 da Carl David Anderson. Anderson propose di chiamare gli elettroni "negatroni" e di usare il termine "elettroni" per indicare genericamente una delle varianti della particella sia a carica positiva che negativa. Questo uso del termine "negatroni" è occasionalmente utilizzato tuttora, anche nella sua forma abbreviata "negatone".
Gli elettroni nel mare di Dirac furono introdotti con lo scopo di impedire la perdita di energia senza limiti degli elettroni reali osservati. In questo contesto, i fotoni (cioè i quanti della radiazione elettromagnetica) possono essere assorbiti dagli elettroni del mare, permettendo a questi ultimi di uscire fuori da esso. Come risultato netto si generano degli elettroni a carica negativa e delle lacune di carica positiva nel mare. Una lacuna potrà essere rioccupata dall'elettrone che perde energia rilasciando in questo modo nuovamente un altro fotone.
Nel 1947 Willis Lamb, lavorando in collaborazione con lo studente Robert Retherford, trovò che certi stati quantistici dell'elettrone nell'atomo di idrogeno, che avrebbero dovuto avere la stessa energia, erano spostati uno rispetto all'altro e tale deviazione fu chiamata spostamento di Lamb. Circa nello stesso periodo, Polykarp Kusch, lavorando con Henry M. Foley, scoprì che il momento magnetico dell'elettrone è di poco più grande di quanto previsto dell'equazione di Dirac. Questa piccola differenza fu successivamente chiamata "momento magnetico di dipolo anomalo dell'elettrone". Per risolvere questo e altri problemi, una teoria avanzata chiamata elettrodinamica quantistica fu sviluppata da Sin-Itiro Tomonaga, Julian Schwinger e Richard P. Feynman alla fine degli anni quaranta.
Con lo sviluppo degli acceleratori di particelle nella prima metà del XX secolo, i fisici iniziarono ad approfondire le proprietà delle particelle subatomiche. Le proprietà di corpuscolo elementare puntiforme dell'elettrone hanno reso questa particella una sonda perfetta per esplorare la struttura dei nuclei atomici. Il primo tentativo riuscito di accelerare elettroni usando l'induzione elettromagnetica fu ad opera di Donald William Kerst nel 1942: il suo primo betatrone raggiunse energie di , mentre quelli successivi raggiunsero i . Nel 1947 fu scoperta la radiazione di sincrotrone con un sincrotrone di della General Electric; questa radiazione era causata dall'accelerazione degli elettroni che, in un campo magnetico, raggiungono velocità prossime a quelle della luce.
Il primo acceleratore di particelle ad alte energie è stato ADONE, con un fascio di particelle di energia pari a ; questa struttura, operativa a partire dal 1968, accelerava elettroni e positroni in direzioni opposte, raddoppiando in pratica l'energia prodottasi nelle loro collisioni se paragonata a quella ottenuta nelle collisioni degli elettroni con un bersaglio fisso. Il Large Electron-Positron Collider (LEP) al CERN, che operò dal 1989 al 2000, raggiunse energie di collisione pari a e fece importanti misure in merito al modello standard.
Il Large Hadron Collider (LHC), l'ultimo acceleratore del CERN, sostituisce gli elettroni con adroni, perché questi ultimi sono meno soggetti alla perdita di energia per radiazione di sincrotrone e quindi il rapporto fra energia acquisita dalla particella e l'energia spesa per ottenerla è maggiore.
L'elettrone possiede una massa a riposo di , pari a circa di quella del protone e una carica pari a (esatta). Si tratta della particella subatomica stabile più leggera che si conosca tra quelle dotate di carica elettrica. La carica elettrica è la carica elementare cambiata di segno e lo spin ha valore semi intero, per cui l'elettrone è un fermione. Appartiene alla prima generazione dei leptoni ed è soggetto all'interazione gravitazionale, a quella debole e a quella elettromagnetica. La sua antiparticella è il positrone, che si differenzia solo per la carica elettrica di segno opposto.
Nel modello standard della fisica delle particelle gli elettroni appartengono al gruppo delle particelle subatomiche chiamate leptoni, che si ritiene siano particelle elementari, ed hanno massa minore rispetto a ogni altra particella carica conosciuta. L'elettrone appartiene alla prima generazione di particelle fondamentali, mentre la seconda e la terza generazione contengono altri leptoni carichi, il muone e il tauone, che possiedono identica carica e spin, ma massa a riposo maggiore. L'elettrone e tutti i leptoni differiscono dagli altri componenti fondamentali della materia (che sono i quark, costituenti i protoni e i neutroni) per il fatto che non risentono della forza di interazione nucleare forte.
La massa a riposo di un elettrone è di approssimativamente o che, in base al principio di equivalenza massa ed energia, corrisponde a un'energia a riposo di , con un rapporto rispetto alla massa del protone di circa 1 a 1836. Misure astronomiche hanno mostrato che il rapporto fra le masse del protone e dell'elettrone è rimasto costante per almeno metà dell'età dell'universo, come è previsto nel modello standard.
L'elettrone ha una carica elettrica di , che viene chiamata "carica elementare" ed è usata come unità standard per la carica delle particelle subatomiche. Entro i limiti dell'errore sperimentale, il valore della carica dell'elettrone è uguale a quella del protone, ma con il segno opposto. Il valore della carica elementare è indicato con il simbolo "e", mentre l'elettrone viene comunemente indicato con il simbolo "e", dove il segno meno indica il fatto che tale particella presenta carica negativa; analogamente, per il positrone, che ha la stessa massa dell'elettrone e carica di segno opposto, è utilizzato come simbolo "e".
L'elettrone non ha sotto strutture conosciute e viene descritto come un punto materiale, dal momento che esperimenti effettuati con la trappola di Penning hanno mostrato che il limite superiore per il raggio della particella è di 10 metri. Esiste inoltre una costante fisica, il raggio classico dell'elettrone, a cui corrisponde un valore di ; questa costante deriva tuttavia da un calcolo che trascura gli effetti quantistici presenti.
Si ritiene che l'elettrone sia stabile poiché, dal momento che la particella possiede carica unitaria, il suo decadimento violerebbe la legge di conservazione della carica elettrica. Il limite inferiore sperimentale per la vita media dell'elettrone è di anni, con un intervallo di confidenza al 90%.
In meccanica quantistica l'elettrone può essere trattato sia come onda che come particella, in accordo con il dualismo onda-particella. Nel formalismo delle funzioni d'onda l'elettrone è descritto matematicamente da una funzione di variabile complessa, la funzione d'onda appunto. Il quadrato del valore assoluto della funzione d'onda rappresenta una densità di probabilità, cioè la probabilità che l'elettrone sia osservato nell'intorno di una determinata posizione.
Da tale distribuzione si può calcolare l'incertezza della posizione dell'elettrone. Un calcolo analogo si può fare sulla quantità di moto dell'elettrone. Le incertezze sulla posizione e la quantità di moto sono legate dal principio di indeterminazione di Heisenberg.
Gli elettroni sono particelle identiche, ovvero non possono essere distinte l'una dall'altra per le loro proprietà fisiche intrinseche: è possibile cambiare la posizione di una coppia di elettroni interagenti senza che si verifichi un cambiamento osservabile nello stato del sistema. La funzione d'onda dei fermioni, di cui gli elettroni fanno parte, è antisimmetrica: il segno della funzione d'onda cambia quando la posizione dei due elettroni viene scambiata, ma il valore assoluto non varia con il cambio di segno e il valore della probabilità resta immutato. Questo differenzia i fermioni dai bosoni, che hanno una funzione d'onda simmetrica.
Il momento angolare intrinseco è caratterizzato dal numero quantico di spin, pari a 1/2 in unità di ħ, e l'autovalore dell'operatore di spin è √3⁄2 ħ. Il risultato di una misura della proiezione dello spin su ognuno degli assi di riferimento può inoltre valere soltanto ±ħ⁄2.
Oltre allo spin, l'elettrone ha un momento magnetico intrinseco, allineato al suo spin, che ha un valore approssimativamente simile al magnetone di Bohr, che è una costante fisica che vale . La proiezione del vettore di spin lungo la direzione della quantità di moto definisce la proprietà delle particelle elementari conosciuta come elicità.
L'evoluzione temporale della funzione d'onda di una particella è descritta dall'equazione di Schrödinger, che nel caso di un sistema di elettroni interagenti mostra una probabilità nulla che una coppia di elettroni occupi lo stesso stato quantico: questo fatto è responsabile del principio di esclusione di Pauli, il quale afferma che due elettroni del sistema non possono avere i medesimi numeri quantici. Tale principio è alla base di molte proprietà dei sistemi con molti elettroni, in particolare genera la loro configurazione all'interno degli orbitali atomici.
Quando un elettrone si muove con velocità prossima a quella della luce è necessario ricorrere alla teoria della relatività speciale per descriverne il moto. Secondo tale teoria, la massa relativistica dell'elettrone aumenta dal punto di vista di un osservatore esterno, e di conseguenza è necessaria una forza sempre più intensa per mantenere costante l'accelerazione. In questo modo un elettrone non può mai raggiungere la velocità della luce nel vuoto "c", essendo richiesta un'energia infinita. Tuttavia, se un elettrone che si muove a una velocità prossima a quella della luce entra in un mezzo dielettrico, per esempio l'acqua, in cui la velocità della luce è significativamente minore di quella dell'elettrone, l'interazione con esso può generare un fronte d'onda di luce causato dall'effetto Čerenkov. Tale effetto è simile al boom sonico, che accade quando un oggetto supera la velocità del suono.
L'effetto della relatività speciale è descritto da una quantità nota come fattore di Lorentz, definita da:
dove formula_2 è la velocità della particella e l'energia cinetica formula_3 associata a un elettrone che si muove con velocità formula_2 è:
dove "m" è la massa a riposo dell'elettrone. Per esempio, l'acceleratore lineare di Stanford (SLAC) può accelerare un elettrone a circa 51 GeV. Questo fornisce un valore per formula_6 vicino a 100 000, dal momento che la massa a riposo dell'elettrone è circa 0,51 MeV/c. La quantità di moto relativistica è 100 000 volte la quantità di moto dell'elettrone prevista dalla meccanica classica alla stessa velocità.
Dal momento che l'elettrone ha anche un comportamento ondulatorio, a una data velocità esso ha una caratteristica lunghezza d'onda di de Broglie. Questa è data da "λ" = "h"/"p" dove "h" è la costante di Planck e "p" è la quantità di moto. Per un elettrone con energia di 51 GeV, come quelle raggiunte dall'acceleratore SLAC, la lunghezza d'onda è di circa , piccola a sufficienza per esplorare la scala infinitesima del nucleo atomico e dei protoni.
La teoria dei campi quantistica interpreta i fenomeni di interazione fra gli elettroni e la radiazione elettromagnetica in termini di scambi di particelle generate nel vuoto dalle fluttuazioni quantistiche. Ad esempio, secondo l'elettrodinamica quantistica, gli elettroni e il campo elettromagnetico interagiscono fra loro puntualmente tramite lo scambio di fotoni e particelle virtuali aventi vita breve e non direttamente osservabili. Le fluttuazioni quantistiche creano continuamente nel vuoto coppie di particelle virtuali, fra le quali vi sono l'elettrone e il positrone, che si annichilano in breve tempo senza poter essere misurate effettivamente. In base al principio di indeterminazione di Heisenberg, la variazione dell'energia necessaria a produrre la coppia di particelle e la loro vita media non si possono conoscere contemporaneamente, tuttavia se la vita media è estremamente breve l'incertezza riguardo all'energia è molto ampia, e il processo e la fluttuazione possono avvenire senza violare la conservazione dell'energia.
La presenza delle particelle virtuali, sebbene non direttamente osservabile, è responsabile tuttavia della differenza delle caratteristiche dell'elettrone al variare della scala di energie dei processi in cui è coinvolto. Le correzioni virtuali sono all'origine di correzioni divergenti di tipo logaritmico della massa dell'elettrone rispetto al valore nominale classico. La rimozione di queste divergenze, alla base della teoria della rinormalizzazione, comporta una ridefinizione del concetto di costante fisica, che viene ad assumere nel contesto quantistico un valore differente in base alla scala di osservazione. Per esempio la carica elettrica dell'elettrone non è costante ed aumenta lentamente all'aumentare dell'energia dei processi in cui è coinvolto.
Questo importante risultato delle teorie di campo quantistiche può essere interpretato come l'effetto di schermo prodotto dalle particelle virtuali. La presenza di un elettrone isolato permette attraverso il campo elettromagnetico di creare una coppia positrone-elettrone dal vuoto; il positrone virtuale appena creato, di carica positiva, sarà attratto dall'elettrone isolato, mentre l'elettrone virtuale ne sarà respinto. Questo fenomeno produce uno schermo positivo attorno all'elettrone isolato, la cui carica a grande distanza sarà quindi considerevolmente ridotta rispetto a quella a corta distanza. Una particella carica ad alta energia sarà in grado di penetrare lo schermo e per questo motivo entra in interazione con una carica elettrica efficace più alta. In base a processi analoghi, anche la massa dell'elettrone tende a crescere quando le scale di energie crescono. Questo tipo di comportamento delle costanti fisiche è caratteristico di tutte le teorie che presentano un polo di Landau, come l'elettrodinamica quantistica.
La mutua interazione fra fotoni e elettroni spiega anche la piccola deviazione dal momento magnetico intrinseco dell'elettrone dal magnetone di Bohr. I fotoni virtuali, responsabili del campo elettrico, possono permettere infatti all'elettrone di avere un moto agitato nell'intorno della sua traiettoria classica, che genera l'effetto globale di un moto circolare con una precessione. Questo moto produce sia lo spin che il momento magnetico dell'elettrone. Negli atomi, poi, la creazione di fotoni virtuali spiega lo spostamento di Lamb osservato nelle linee spettrali e il fenomeno del decadimento spontaneo di elettrone da uno stato eccitato a uno di energia inferiore. Questo tipo di polarizzazione è stata confermata sperimentalmente nel 1997 usando l'acceleratore giapponese TRISTAN.
L'elettrone è responsabile delle proprietà chimiche fondamentali degli atomi e delle molecole. L'interazione elettromagnetica fra gli elettroni è infatti all'origine dei legami fra gli atomi e della struttura macroscopica della materia, oggetto di studio della chimica e della fisica dello stato solido.
Gli elettroni sono i costituenti fondamentali degli atomi, assieme a protoni e neutroni. Essi sono confinati nella regione in prossimità del nucleo atomico e nel caso di un atomo neutro isolato sono in numero pari al numero atomico, cioè al numero di protoni contenuti nel nucleo. Se il numero di elettroni è differente dal numero atomico, l'atomo è detto ione e possiede una carica elettrica netta.
Secondo la meccanica classica, un elettrone in moto circolare uniforme attorno al nucleo, essendo accelerato, emetterebbe radiazione elettromagnetica per effetto Larmor, perdendo progressivamente energia e impattando infine sul nucleo. Il collasso degli atomi è smentito dall'osservazione sperimentale della stabilità della materia: per questo motivo il modello atomico di Bohr è stato introdotto nel 1913 per fornire una descrizione semiclassica nella quale un elettrone può muoversi soltanto su alcune determinate orbite non-radiative caratterizzate da precisi valori dell'energia e del momento angolare. Nello sviluppo successivo della meccanica quantistica, per rappresentare lo stato degli elettroni nell'atomo, la traiettoria classica è stata sostituita dalla funzione d'onda nota con il nome di orbitale atomico.
Ad ogni orbitale è associato uno degli stati energetici degli elettroni che interagiscono con il potenziale elettrico generato dal nucleo. Il valore della funzione d'onda associata a tali stati è fornito dalla soluzione dell'equazione d'onda di Schrödinger, che può essere risolta per l'atomo di idrogeno notando la simmetria radiale del potenziale elettrico indotto dal nucleo. Le soluzioni dell'equazione d'onda sono enumerate da numeri quantici che assumono un insieme discreto di valori, che rappresentano il valore di aspettazione dell'energia e del momento angolare, in particolare:
Gli atomi con più elettroni richiedono una descrizione degli stati più complessa di quella dell'atomo di idrogeno, in quanto è necessaria l'introduzione di approssimazioni a causa dell'impossibilità di risolvere esattamente l'equazione di Schrödinger per via analitica. Le approssimazioni più utilizzate sono il metodo di Hartree-Fock, che sfrutta la possibilità di scrivere la funzione d'onda degli elettroni come un determinante di Slater, l'accoppiamento di Russell-Saunders e l'accoppiamento jj, che invece riescono ad approssimare l'effetto dovuto all'interazione spin-orbita nel caso di nuclei rispettivamente leggeri e pesanti.
Per il principio di esclusione di Pauli, due o più elettroni non possono trovarsi nel medesimo stato, cioè non possono essere descritti dai medesimi numeri quantici. Questo fatto determina la distribuzione degli elettroni negli orbitali. Gli orbitali sono occupati dagli elettroni in modo crescente rispetto all'energia. Lo stato di momento angolare è definito dal numero quantico azimutale "l"; dove il quadrato del valore assoluto del momento angolare è formula_7. Il numero quantico magnetico può assumere valori interi compresi tra -"l" e +"l": il numero di tali valori è il numero delle coppie di elettroni, con valore di spin opposto, che possiedono il medesimo numero quantico azimutale. Ad ogni livello energetico corrisponde un numero crescente di possibili valori del numero quantico azimutale, a ogni valore del numero quantico azimutale corrispondono "2l + 1" valori di "m", e a ogni valore di "m" corrispondono i due valori possibili di spin.
All'interno della nuvola elettronica è possibile che un elettrone effettui una transizione da un orbitale a un altro principalmente attraverso l'emissione o l'assorbimento di fotoni (cioè di quanti di energia), ma anche in seguito alla collisione con altre particelle o tramite l'effetto Auger. Quando un elettrone acquista un'energia pari alla differenza di energia con uno stato non occupato all'interno degli orbitali, esso effettua una transizione in tale stato. Una delle applicazioni più importanti di tale fenomeno è l'effetto fotoelettrico, in cui l'energia fornita da un fotone è tale da separare l'elettrone dall'atomo. Inoltre, dal momento che l'elettrone è carico, il suo moto attorno al nucleo, che in una descrizione semiclassica è circolare uniforme, produce un momento di dipolo magnetico proporzionale al momento angolare orbitale. Il momento magnetico totale di un atomo è equivalente alla somma vettoriale dei momenti di dipolo magnetici e di spin di tutti i suoi elettroni e dei costituenti del nucleo. Il momento magnetico dei costituenti del nucleo è tuttavia trascurabile rispetto a quello degli elettroni. L'interazione tra il momento di dipolo magnetico e il momento di spin è descritto dall'interazione spin-orbita, mentre l'interazione con un campo magnetico esterno è descritta dai limiti di Paschen-Back e Zeeman, a seconda che l'interazione spin-orbita sia rispettivamente trascurabile o meno rispetto al campo applicato.
Nelle molecole gli atomi sono uniti dal legame chimico covalente, in cui uno o più elettroni sono condivisi fra due o più atomi. In una molecola gli elettroni si muovono sotto l'influenza attrattiva dei nuclei e il loro stato è descritto da orbitali molecolari, più grandi e complessi di quelli di un atomo isolato, che in prima approssimazione si possono ottenere attraverso la sommatoria di più orbitali degli atomi considerati singolarmente. Differenti orbitali molecolari hanno differenti distribuzioni spaziali di densità di probabilità: nel caso di una molecola costituita da due atomi, per esempio, gli elettroni che ne formano l'eventuale legame si troveranno con maggiore probabilità in una ristretta regione posta fra i due nuclei.
Un composto ionico può essere definito come un composto chimico formato da ioni, aventi ciascuno una carica elettrica positiva o negativa, ma l'insieme di tali ioni ha carica elettrica complessiva neutra. Alla base dei composti ionici vi è il legame ionico, di natura elettrostatica, che si forma quando le caratteristiche chimico-fisiche dei due atomi sono nettamente differenti e vi è una notevole differenza di elettronegatività. Per convenzione si suole riconoscere un legame ionico tra due atomi quando la differenza di elettronegatività Δχ è maggiore di 1,9. Al diminuire di tale differenza cresce il carattere covalente del legame.
L'elettrone genera un campo elettrico che esercita una forza attrattiva su particelle con una carica positiva (come il protone) e una forza repulsiva su particelle con carica negativa. L'intensità di tale forza è determinata dalla legge di Coulomb. Un elettrone in movimento genera un campo magnetico: tale proprietà prende il nome di "induzione elettromagnetica" ed è responsabile ad esempio della generazione del campo magnetico che permette il funzionamento del motore elettrico. Tramite la legge di Ampère tale movimento rispetto all'osservatore può essere messo in relazione al campo magnetico generato. In generale, i campi elettrici e magnetici prodotti da cariche o correnti elettriche sono calcolati risolvendo le equazioni di Maxwell. Il campo elettromagnetico di una particella carica in movimento è espresso tramite il potenziale di Liénard-Wiechert, anche quando la velocità della particella è prossima a quella della luce.
Quando un elettrone è in moto in corrispondenza di un campo magnetico è soggetto alla forza di Lorentz, la quale esercita una variazione della componente della velocità dell'elettrone perpendicolare al piano definito dal campo magnetico e dalla velocità iniziale dell'elettrone e la forza centripeta che viene generata costringe l'elettrone a seguire una traiettoria elicoidale. L'accelerazione che deriva da questo moto curvilineo, nel caso di velocità relativistiche, causa una radiazione di energia da parte dell'elettrone sotto forma di radiazione di sincrotrone. L'emissione di energia causa a sua volta un rinculo dell'elettrone, conosciuto come forza di Abraham-Lorentz-Dirac, che rallenta il moto dell'elettrone; questa forza è generata da un effetto di retroazione del campo dell'elettrone su sé stesso.
In elettrodinamica quantistica, l'interazione elettromagnetica tra le particelle è trasmessa dai fotoni: un elettrone isolato nello spazio vuoto che non subisce un'accelerazione non è in grado di emettere o di assorbire un fotone reale, poiché così facendo violerebbe le leggi di conservazione dell'energia e della quantità di moto. Invece i fotoni virtuali possono trasferire la quantità di moto tra due particelle cariche ed è questo scambio di fotoni virtuali che genera, per esempio, la forza di Coulomb. L'emissione di energia può avvenire quando un elettrone viene deviato da una particella carica, come per esempio un protone; l'accelerazione dell'elettrone porta all'emissione della radiazione di "bremsstrahlung", detta anche radiazione di frenamento.
Una collisione anelastica tra un fotone e un elettrone libero produce l'effetto Compton: questo urto è associato a un trasferimento dell'energia e della quantità di moto tra le particelle, che porta alla variazione della lunghezza d'onda del fotone incidente. Il valore massimo di questa variazione della lunghezza d'onda è "h"/"m"c ed è noto come lunghezza d'onda Compton e per l'elettrone vale . Se la lunghezza d'onda della luce incidente è sufficientemente lunga (come ad esempio quella della luce visibile, che ha una lunghezza d'onda che va da a ), la variazione della lunghezza d'onda dovuta all'effetto Compton diventa trascurabile e l'interazione tra radiazione e particelle può essere descritta tramite lo scattering Thomson.
La forza dell'interazione elettromagnetica tra due particelle cariche è data dalla costante di struttura fine α che è una quantità adimensionale formata dal rapporto di due contributi energetici: l'energia elettrostatica di attrazione o repulsione data dalla separazione di una lunghezza d'onda Compton e dall'energia a riposo della carica. Il suo valore è , che è possibile approssimare con la frazione 1/137.
Quando elettroni e positroni collidono si annichilano l'un l'altro, originando due o più fotoni dei raggi gamma. Se invece la quantità di moto dell'elettrone e del positrone è trascurabile si può formare il positronio prima che il processo di annichilamento porti alla formazione di due o tre fotoni dei raggi gamma con un'energia totale di . D'altra parte i fotoni molto energetici possono trasformarsi in un elettrone e in un positrone tramite un processo chiamato produzione di coppia, ma questo avviene solo in presenza di una particella carica nelle vicinanze, come un nucleo atomico.
Nella teoria dell'interazione elettrodebole la componente sinistrorsa della funzione d'onda dell'elettrone forma un doppietto di isospin debole con il neutrino elettronico, cioè a causa dell'interazione elettrodebole il neutrino si comporta come un elettrone. Ciascuna componente di questo doppietto può subire l'interazione della corrente debole carica tramite l'emissione o l'assorbimento di un bosone W e può essere trasformata nell'altra componente. La carica è conservata durante questo processo poiché anche il bosone W porta una carica che annulla ogni variazione netta durante la reazione. Le interazioni della corrente debole carica sono responsabili del decadimento beta negli atomi radioattivi. Sia l'elettrone che il neutrino possono subire l'interazione della corrente debole neutra tramite uno scambio di bosoni Z e questo è responsabile dello scattering elastico tra elettrone e neutrino.
Se un corpo ha un numero di elettroni maggiore o minore rispetto a quelli necessari per bilanciare la carica positiva dei nuclei, esso presenterà una carica elettrica netta: nel caso di un eccesso di elettroni, il corpo è carico negativamente, mentre nel caso di un difetto di elettroni, il corpo è carico positivamente; se invece il numero di elettroni e il numero di protoni sono uguali, le loro cariche si annullano a vicenda e il corpo è dunque elettricamente neutro. Un corpo macroscopico può sviluppare una carica elettrica ad esempio attraverso lo sfregamento, per via dell'effetto triboelettrico.
Gli elettroni indipendenti che si muovono nel vuoto sono detti "elettroni liberi" e anche gli elettroni nei metalli hanno un comportamento simile a quelli liberi. Il flusso di carica elettrica dovuto al moto degli elettroni liberi o in un materiale è detto corrente elettrica.
I materiali sono classificati in base alla resistenza che oppongono al passaggio di corrente: si dividono in conduttori, semiconduttori e isolanti (o dielettrici).
In generale, ad una data temperatura, ciascun materiale ha una conducibilità elettrica che determina il valore della corrente quando è applicato un potenziale elettrico. Esempi di buoni conduttori, cioè materiali capaci di far scorrere facilmente al proprio interno elettricità, sono i metalli come il rame e l'oro, mentre vetro e plastica sono cattivi conduttori.
I metalli sono spesso anche buoni conduttori di calore. Nonostante questo, al contrario della conducibilità elettrica, la conducibilità termica è quasi indipendente dalla temperatura; ciò è espresso matematicamente dalla legge di Wiedemann-Franz, la quale afferma che il rapporto fra la conduttività termica e la conduttività elettrica è proporzionale alla temperatura.
Le proprietà di conduzione di un solido cristallino sono determinate dagli stati quantistici degli elettroni, la cosiddetta struttura elettronica a bande. Nel caso di solidi amorfi, cioè senza struttura cristallina, la descrizione è più complessa.
Nei solidi cristallini gli atomi sono disposti regolarmente in un reticolo. La simmetria di tale distribuzione spaziale permette di semplificare il calcolo degli stati energetici degli elettroni nel cristallo e ricavare la struttura a bande. Con questa descrizione è possibile approssimare il comportamento degli elettroni nei solidi con quello di elettroni liberi, ma con una diversa massa, detta massa efficace. Un elettrone all'interno di un reticolo cristallino è descritto da una funzione d'onda detta funzione di Bloch, alla quale è associato un vettore detto "quasi-impulso" o "impulso cristallino", che è l'analogo della quantità di moto per gli elettroni liberi. L'analogia con gli elettroni liberi è particolarmente adeguata per alcuni valori di impulso cristallino, per i quali si ha una relazione di dispersione quadratica, come nel caso libero.
Nei solidi gli elettroni sono trattati come quasiparticelle poiché, a causa dell'interazione reciproca e con gli atomi del reticolo, assumono delle proprietà diverse da quelle degli elettroni liberi. Inoltre, nei solidi si introduce una quasiparticella, detta lacuna, che descrive la "mancanza" di un elettrone. Tale particella ha una sua massa efficace ed ha carica positiva, uguale in valore assoluto a quella dell'elettrone.
Nei materiali isolanti gli elettroni rimangono confinati in prossimità dei loro rispettivi nuclei. Al contrario, i metalli hanno una struttura elettronica a bande, alcune delle quali sono parzialmente riempite dagli elettroni. La presenza di queste bande permette agli elettroni nei metalli di muoversi come elettroni liberi o delocalizzati; essi non sono associati a uno specifico atomo e quindi, quando è applicato un campo elettrico, si muovono liberamente come un gas, chiamato gas di Fermi.
Un'altra categoria di materiali è quella dei semiconduttori, in cui la conducibilità può variare di molto fra i valori estremi di conduzione e isolante.
A causa delle collisioni fra elettroni e atomi la velocità di deriva degli elettroni in un conduttore è dell'ordine di pochi millimetri per secondo. Ciò nonostante, la velocità di propagazione di un segnale elettrico, cioè la velocità con la quale si propaga la variazione di corrente in un conduttore, è tipicamente di circa il 75% della velocità della luce. Questo accade perché i segnali elettrici si propagano come onde, con una velocità dipendente dalla costante dielettrica del materiale.
Il disordine termico nel reticolo cristallino del metallo causa un aumento della resistività del materiale, producendo quindi la dipendenza dalla temperatura per la corrente elettrica.
Quando alcuni materiali sono raffreddati al di sotto di una certa temperatura critica, avviene una transizione di fase a causa della quale essi perdono la resistività alla corrente elettrica, in un processo noto come superconduttività. Nella teoria BCS, gli elettroni sono legati in coppie che entrano in uno stato quantistico noto come condensato di Bose-Einstein. Tali coppie, dette coppie di Cooper, si accoppiano nel loro moto per mezzo delle vibrazioni di reticolo chiamate fononi, evitando le collisioni con gli atomi che normalmente causano la resistività elettrica (le coppie di Cooper hanno un raggio di circa , quindi si possono scavalcare a vicenda). La teoria BCS non descrive tutti i materiali superconduttori, e non esiste ancora un modello teorico in grado di spiegare completamente la superconduttività ad alta temperatura.
Gli elettroni all'interno dei solidi conduttivi, che sono a loro volta trattati come quasi-particelle, quando sono strettamente confinati intorno a temperature vicine alle zero assoluto si comportano globalmente come due nuove differenti quasi-particelle: gli spinoni e gli oloni. Il primo trasporta spin e momento magnetico, mentre il secondo la carica elettrica.
Gli elettroni possono, secondo la teoria di Eugene Paul Wigner, formare essi stessi una struttura cristallina, disponendosi nei punti di un reticolo. Tale stato della materia è detto cristallo di Wigner.
Per spiegare gli istanti iniziali dell'evoluzione dell'universo è stata sviluppata la teoria del Big Bang, che è la più accettata dalla comunità scientifica. Nel primo millisecondo dell'esistenza dell'universo noto, la temperatura era di circa un miliardo di kelvin e i fotoni avevano un'energia media nell'ordine del milione di elettronvolt; questi fotoni erano sufficientemente energetici da poter reagire l'un l'altro per formare coppie di elettroni e positroni:
dove formula_6 è il fotone, formula_10 è il positrone e formula_11 è l'elettrone. Contemporaneamente le coppie elettrone-positrone si annichilivano e producevano fotoni energetici. I due processi erano in equilibrio durante la prima fase di evoluzione dell'universo, ma dopo 15 secondi la temperatura dell'universo calò sotto la soglia di formazione delle coppie di elettroni-positroni. La maggior parte degli elettroni e positroni rimasti si annichilirono e produssero raggi gamma che in breve tempo irradiarono l'universo.
Per ragioni non ancora ben comprese, durante il processo di leptogenesi vi era un numero maggiore di elettroni rispetto a quello dei positroni, perciò circa un elettrone ogni miliardo sopravvisse durante il processo di annichilazione. Questo eccesso era analogo a quello dei protoni sugli antiprotoni, in una condizione nota come asimmetria barionica, perciò la carica netta presente nell'universo risultava nulla. I protoni e i neutroni superstiti iniziarono a interagire in un processo noto come nucleosintesi primordiale, durato fino a circa 5 minuti dopo l'istante iniziale, in cui si assistette alla formazione dei nuclei degli isotopi di idrogeno, elio e in minima parte litio. I neutroni rimasti subirono il decadimento beta, con una vita media di circa quindici minuti, con la formazione di un protone, un elettrone e un antineutrino:
dove formula_13 è il neutrone, formula_14 è il protone e formula_15 è l'antineutrino elettronico. Per i successivi - anni gli elettroni liberi erano troppo energetici per legarsi ai nuclei atomici; passato questo periodo, seguì un processo di ricombinazione, in cui gli elettroni si legarono ai nuclei atomici per formare atomi elettricamente neutri e a causa di ciò l'universo divenne trasparente alla radiazione elettromagnetica.
Circa un milione di anni dopo il Big Bang, si iniziò a formare la prima generazione di stelle; all'interno di queste stelle, la nucleosintesi portò alla produzione di positroni derivanti dalla fusione di nuclei atomici e queste particelle di antimateria si annichilirono immediatamente con gli elettroni formando raggi gamma. Ciò portò a una continua riduzione nel numero di elettroni e a un corrispettivo aumento di neutroni; nonostante questo il processo di evoluzione stellare portò alla sintesi di isotopi radioattivi i quali potevano decadere con un decadimento di tipo beta, emettendo in questo modo un elettrone e un antineutrino dal nucleo.
Alla fine della sua vita, una stella di massa superiore di 20 volte la massa solare può subire un collasso gravitazionale e formare un buco nero; in base alle leggi della fisica classica, questo oggetto stellare massivo esercita un'attrazione gravitazione così grande da impedire a qualsiasi cosa, anche alla radiazione elettromagnetica, di potergli sfuggire una volta che è stato superato il raggio di Schwarzschild. Si pensa tuttavia che gli effetti quantistici possano permettere l'emissione di una radiazione di Hawking a tale distanza, infatti si ritiene che sull'orizzonte degli eventi di questi oggetti vengano prodotte coppie virtuali di elettroni e positroni e quando esse vengono formate in prossimità dell'orizzonte degli eventi, la distribuzione spaziale casuale di queste particelle può permettere a una particella della coppia di apparire all'esterno dell'orizzonte grazie all'effetto tunnel. Il potenziale gravitazionale del buco nero può fornire l'energia sufficiente per trasformare la particella virtuale in una particella reale, facendo in modo da diffonderla nello spazio, mentre all'altra particella della coppia è stata fornita energia negativa e ciò comporta una perdita netta di energia del buco nero. La velocità della radiazione di Hawking cresce con il diminuire della massa e questo comporta l'evaporazione del buco nero che alla fine esplode.
Un altro modo di formazione degli elettroni è dato dall'interazione dei raggi cosmici con gli strati alti dell'atmosfera: i raggi cosmici sono particelle che viaggiano nello spazio con energie anche dell'ordine dei e, quando esse collidono con le particelle presenti nell'alta atmosfera terrestre, vi è la produzione di una cascata di particelle, tra le quali pioni e muoni, con questi ultimi che sono i responsabili di più della metà della radiazione cosmica osservata a Terra. Il decadimento del pione porta alla formazione dei muoni tramite il seguente processo:
mentre a suo volta il muone può decadere formando elettroni:
Le prime osservazioni degli elettroni come particella, hanno sfruttato fenomeni elettrostatici o la produzione di raggi catodici. Oggi si eseguono esperimenti in laboratorio in cui vengono osservati elettroni sia per lo studio delle proprietà di queste particelle, sia per studiare le proprietà di corpi macroscopici.
In condizioni di laboratorio, l'interazione di elettroni individuali possono essere osservate con l'uso di rilevatori di particelle, che permettono misure precise di specifiche proprietà come energia, spin e carica elettrica. Lo sviluppo della trappola ionica quadrupolare ha permesso di contenere particelle in piccole regioni dello spazio per lunghi periodi. Questo ha permesso la misura precisa delle proprietà particellari. Per esempio in una misurazione si è riusciti a contenere un singolo elettrone per un periodo di dieci mesi. Il momento magnetico di un elettrone fu misurato con una precisione di 11 cifre significative, che, nel 1980, è la misura migliore di una costante fisica.
La prima immagine video della distribuzione di energia di un elettrone è stata catturata da un team dell'università di Lund in Svezia, nel febbraio 2008. Gli scienziati hanno usato flash estremamente piccoli di luce, che hanno permesso di osservare il moto di un elettrone per la prima volta.
Tramite la misura dell'energia irradiata da elettroni, gran parte delle misure spettroscopiche sono collegati allo studio degli elettroni liberi o legati, misurando l'energia dei fotoni emessi.
Per esempio, nell'ambiente ad alta energia come la corona di una stella, gli elettroni liberi formano un plasma che emette energia per gli effetti di Bremsstrahlung. Il gas elettronico può formare delle oscillazioni di plasma, ovvero oscillazioni regolari della densità degli elettroni, e queste possono produrre emissioni di energia che possono essere rilevate usando i radiotelescopi.
Nel caso di atomi e molecole, un elettrone confinato a muoversi attorno a un nucleo può transire fra i diversi livelli energetici di questo consentiti, assorbendo o emettendo fotoni di frequenza caratteristica. Per esempio, quando un atomo è irraggiato da una sorgente con uno spettro continuo, appariranno delle distinte linee spettrali per la radiazione trasmessa. Ciascun elemento o molecola esibisce un insieme caratteristico proprio di serie di linee spettrali, che lo distinguono dagli altri atomi, come per esempio il noto caso delle serie dello spettro dell'atomo di idrogeno. Lo studio dell'intensità e la larghezza di queste linee permette di indagare le proprietà fisico-chimiche delle sostanza in analisi.
La distribuzione di elettroni nei materiali solidi può essere visualizzata dallo spettroscopio ARPES ("Angle resolved photoemission spectroscopy", ovvero spettroscopia fotoelettrica angolarmente risolta). Questa tecnica si basa sull'effetto fotoelettrico per misurare il reticolo reciproco, una rappresentazione matematica della struttura periodica di un cristallo. ARPES può essere usato per determinare la direzione, la velocità e la diffusione di elettroni nel materiale.
I fasci di elettroni sono usati nella saldatura di materiali, permettendo di raggiungere densità di energia superiori ai nello stretto diametro focale di e spesso non richiedono un materiale di riempimento. Questa tecnica di saldatura deve essere eseguita nel vuoto, in modo tale che gli elettroni non interagiscano con l'aria prima di raggiungere il bersaglio e può essere usata per unire materiali conduttori che altrimenti sarebbero difficili da saldare.
La litografia a fasci di elettroni (EBL) è un metodo per stampare i semiconduttori a risoluzioni più basse del micron. Questa tecnica è limitata dagli alti costi, basse performance, dalla necessità di operare con fascio nel vuoto e dalla tendenza degli elettroni a essere diffusi nei solidi. L'ultimo problema limita la risoluzione a circa . Per questa ragione, l'EBL è principalmente usata per la produzione di un piccolo numero di circuiti integrati specializzati.
La lavorazione con fasci di elettroni è usata per irradiare i materiali in modo da cambiare le loro proprietà fisiche o per la sterilizzazione medica e la produzione di cibo. Nella radioterapia, i fasci di elettroni generati da acceleratori lineari sono usati per il trattamento di tumori superficiali: dato che un fascio di elettroni può penetrare solamente uno spessore limitato prima di essere assorbito, tipicamente intorno a per elettroni di energia nel range 5–, la radioterapia è utile per il trattamento di lesioni della cute come il carcinoma basocellulare. Un fascio di elettroni può essere usato per integrare il trattamento di aree che sono state irraggiate da raggi X.
Gli acceleratori di particelle usano campi elettrici per far raggiungere agli elettroni e alle loro antiparticelle alte energie. Nel momento in cui queste particelle passano in una regione in cui c'è campo magnetico, questi emettono radiazione di sincrotrone. L'intensità di questa radiazione dipende dallo spin e questo può permettere la polarizzazione dei fasci di elettroni in un processo noto come effetto Sokolov-Ternov. La polarizzazione di fasci di elettroni può essere molto utile per numerosi esperimenti. La radiazione di sincrotrone può anche essere usata per raffreddare il fascio di elettroni, in modo da ridurre la quantità di moto persa dalle particelle. Una volta che le particelle sono state accelerate sino alla energia richiesta, i fasci separati di elettroni e positroni sono portati alla collisione e la risultante emissione di radiazione è osservata dai rivelatori di particelle ed è studiata dalla fisica particellare.
Gli elettroni possono essere utilizzati anche per ottenere immagini microscopiche grazie ai microscopi elettronici, che indirizzano un fascio focalizzato direttamente sul campione. A causa dell'interazione del fascio con il materiale, alcuni elettroni cambiano le loro proprietà, come una variazione della direzione, della fase relativa e dell'energia. Registrando questi cambiamenti del fascio elettronico, si possono produrre immagini a risoluzione atomica del materiale. Questa elevata risoluzione, maggiore dei microscopi ottici (che è di circa in luce blu), è possibile poiché i microscopi elettronici sono limitati dalla lunghezza d'onda di De Broglie degli elettroni (a titolo d'esempio, un elettrone ha una lunghezza d'onda di quando questo viene accelerato da un potenziale di ). Il microscopio elettronico a trasmissione corretto in aberrazione è in grado di avere una risoluzione inferiore a , che è sufficiente per risolvere i singoli atomi. Queste caratteristiche tecniche rendono il microscopio elettronico uno strumento di laboratorio utile per le immagini ad alta risoluzione; a fronte di questi vantaggi, i microscopi elettronici sono strumenti molto costosi da mantenere.
Vi sono due tipi di microscopi elettronici: a trasmissione e a scansione. Il primo funziona in maniera analoga a una lavagna luminosa, ovvero il fascio di elettroni passa attraverso una parte del campione e viene successivamente proiettato tramite lenti su diapositive o su un CCD. Nel secondo invece l'immagine è prodotta con un fascio elettronico molto fine che scansione riga per riga una piccola regione del campione; l'ingrandimento varia da 100× a o più per entrambi i microscopi. Un altro tipo di microscopio elettronico è quello a effetto tunnel sfrutta l'effetto tunnel quantistico degli elettroni che fluiscono da una punta conduttrice appuntita al materiale di interesse e può riprodurre immagini a risoluzione atomica delle superfici.
Altre tecniche permettono di studiare la struttura cristallina dei solidi; una tecnica che sfrutta questo principio è il Low Energy Electron Diffraction (LEED) che permette di visualizzare su uno schermo fluorescente la figura di diffrazione di un cristallo utilizzando un fascio collimato di elettroni avente un'energia tra i 20 e i . Un altro metodo che sfrutta la diffrazione è il Reflection high-energy electron diffraction (RHEED) che sfrutta la riflessione di un fascio di elettroni incidente a piccoli angoli in modo da caratterizzare la superficie del materiale di studio; l'energia tipica del fascio è tra 8 e , mentre l'angolo di incidenza varia tra 1° e 4°.
Nel laser a elettroni liberi, un fascio di elettroni a energia relativistica passa attraverso una coppia di ondulatori che contengono una serie di dipoli magnetici, i cui campi sono orientati in direzioni alternate; l'elettrone emette radiazione di sincrotrone che, a turno, interagisce coerentemente con lo stesso elettrone e ciò porta a un grosso aumento del campo di radiazione alla frequenza di risonanza. Il laser può emettere una radiazione elettromagnetica coerente ad alta radianza con un ampio intervallo di frequenze, che va dalle microonde ai raggi X morbidi. Questo strumento potrà essere utilizzato per l'industria, per le comunicazioni e per varie applicazioni mediche, come la chirurgia dei tessuti molli.
Gli elettroni sono fondamentali per il funzionamento dei tubi catodici, che sono largamente usati nei dispositivi come computer e televisori. In un tubo fotomoltiplicatore ogni fotone che colpisce il fotocatodo dà inizio a una cascata di elettroni che produce un impulso di corrente rivelabile. I tubi a vuoto sfruttano il flusso di elettroni per manipolare i segnali elettrici e svolgono un ruolo importante nello sviluppo nell'elettronica; nonostante ciò essi sono stati in gran parte soppiantati dai dispositivi a semiconduttori come i transistor.
</doc>

<doc id="334" url="https://it.wikipedia.org/wiki?curid=334" title="Accelerazione">
Accelerazione
In fisica, in primo luogo in cinematica, l'accelerazione è una grandezza vettoriale che rappresenta la variazione della velocità nell'unità di tempo. In termini differenziali, è pari alla derivata rispetto al tempo del vettore velocità.
Nel SI l'unità di misura del modulo dell'accelerazione è il m/s², ovvero metro al secondo quadrato.
Quando non specificato, per "accelerazione" si intende l'"accelerazione traslazionale", sottintendendo che lo spostamento a cui si fa riferimento è una traslazione nello spazio. Il termine, "accelerazione", infatti, può essere utilizzato con un significato più generale per indicare la variazione di una velocità in funzione del tempo. Ad esempio, nella descrizione del moto rotatorio, per definire l"'accelerazione di rotazione" si usano l'accelerazione angolare e l'accelerazione areolare.
Le derivate temporali della velocità di ordine superiore al primo vengono studiate nel moto vario.
L'accelerazione di un punto materiale è la variazione della sua velocità rispetto al tempo. Il modo più immediato per quantificare tale variazione consiste nel definire l"'accelerazione media" formula_1 come il rapporto tra la variazione di velocità formula_2 al tempo finale formula_3 e iniziale formula_4 posseduta dall'oggetto, e l'intervallo finito di tempo formula_5 di durata del moto:
Un modo preciso per caratterizzare l'accelerazione si ottiene considerando la velocità in ogni istante di tempo, ovvero esprimendo la velocità in funzione del tempo e, ove la funzione è continua, calcolandone la derivata. Si definisce in questo modo l"'accelerazione istantanea":
Si tratta del limite per l'intervallo di tempo tendente a zero del rapporto incrementale che definisce l'accelerazione media:
L'accelerazione media coincide con l'accelerazione istantanea quando quest'ultima è costante nel tempo (formula_9), e si parla in tal caso di "moto uniformemente accelerato".
Nel moto del punto materiale su di una curva, il vettore accelerazione in un punto è orientato verso la concavità della traiettoria in quel punto. Può succedere che durante il moto il vettore velocità cambi soltanto in direzione e verso, restando costante in modulo, come ad esempio nel caso di moto circolare uniforme. La componente del vettore accelerazione nella direzione del moto è in questo caso nulla, e il vettore è quindi radiale (perpendicolare alla traiettoria). Data una traiettoria curvilinea arbitraria e continua, per individuare la direzione ed il verso dell'accelerazione di un oggetto che la percorre si utilizza il metodo del cerchio osculatore.
In un contesto più formale, sia formula_10 la lunghezza di un arco della curva percorsa dall'oggetto in moto. Se formula_11 è lo spostamento dell'oggetto nel tempo formula_12, la norma della velocità istantanea nel punto formula_13 è la derivata dello spostamento rispetto al tempo:
con il vettore velocità che è quindi scritto come:
dove formula_16 è il vettore unitario tangente alla curva. Il modulo dell'accelerazione istantanea è allora:
ed il vettore accelerazione è dato da:
dove formula_19 è la curvatura e si sono evidenziate la componente in direzione del moto e la componente in direzione perpendicolare, con formula_20 vettore unitario normale alla curva. In generale è possibile introdurre una terna di versori ortonormali, detta triedro di Frenet, costituita ortogonalizzando i vettori velocità, accelerazione ed un terzo vettore, generato dal prodotto vettoriale dei primi due. I versori così generati prendono il nome di "versore tangente", "normale" e "binormale". L'accelerazione giace sempre, per costruzione, nel piano individuato dal versore tangente e da quello normale. La geometria differenziale sfrutta il triedro di Frenet per permettere di calcolare in ogni punto la curvatura e la torsione della traiettoria.
In uno spazio a tre dimensioni si può scrivere l'accelerazione come:
dove formula_22, formula_23 e formula_24 sono i versori del sistema di riferimento cartesiano utilizzato. Poiché, nella sua definizione generale, l'accelerazione è il vettore che quantifica la variazione di direzione e modulo della velocità, data una traiettoria qualsiasi, è sempre possibile scomporre l'accelerazione del corpo in una componente ad essa tangente, detta "accelerazione tangenziale", e in una componente perpendicolare, detta "accelerazione normale":
L'accelerazione tangenziale descrive il cambiamento in norma della velocità, mentre quella normale è associata alla variazione della direzione della velocità.
Sapendo che la velocità lineare formula_26, che è sempre tangente alla traiettoria, è legata alla velocità angolare formula_27 dalla relazione:
dove formula_29 denota il prodotto vettoriale, formula_27 la velocità angolare e formula_31 il raggio di curvatura della traiettoria nel punto considerato. Pertanto formula_26 è ortogonale al piano formato da formula_27 e da formula_31, e viceversa, il vettore formula_27 è ortogonale al piano formato da formula_26 e da formula_31, cioè dal piano sul quale avviene il moto.
Data una traiettoria formula_38 giacente in un piano, e tracciato per un punto formula_39 in moto il cerchio osculatore, ovvero la circonferenza tangente in ogni istante alla traiettoria in "formula_39", la quale approssima al meglio la traiettoria in quel punto, si trova che:
dove formula_42 è l'accelerazione angolare. Considerando la derivata del vettore velocità formula_43, si ha:
Eguagliando quanto ottenuto dalle equazioni precedenti e identificando i termini si ha che le componenti sono:
In due dimensioni il versore normale è univocamente determinato, mentre in tre dimensioni bisogna specificarlo; infatti, esso risulta parallelo al raggio del cerchio osculatore.
Da quanto mostrato segue inoltre che se la componente normale dell'accelerazione è nulla, allora il moto si svolge su una retta; infatti, la direzione del vettore velocità è costante, e dato che la velocità è sempre tangente alla traiettoria, quest'ultima è rettilinea. Nel caso in cui l'accelerazione tangenziale sia costante si ha un moto rettilineo uniformemente accelerato. Se, invece, anche la componente tangenziale dell'accelerazione sia nulla, il vettore velocità è allora costante e si ha un moto rettilineo uniforme.
Viceversa, se a essere costante è la componente normale la traiettoria risulterà circolare. In questo caso, essa prenderà il nome di "accelerazione centripeta" perché punta istante per istante verso il centro della circonferenza. Se l'accelerazione angolare, quindi anche l'accelerazione tangenziale, è costante, si ha un moto circolare uniformemente accelerato. Invece, nel caso di moto circolare uniforme l'accelerazione angolare è nulla, per cui l'accelerazione si riduce alla sola componente centripeta, pertanto la velocità angolare sarà costante nel tempo. 
Un osservatore solidale a un sistema di riferimento non inerziale sperimenterà delle accelerazioni apparenti. Per il teorema delle accelerazioni di Coriolis, le accelerazioni apparenti dall'osservatore sono due: la prima detta "accelerazione centrifuga", avente modulo e direzione identici all'accelerazione centripeta, ma con verso opposto, e la seconda che prende il nome di "accelerazione complementare", o "accelerazione di Coriolis", il cui valore è: 
L'accelerazione media si rappresenta con il grafico velocità-tempo, dal quale si comprende come l'accelerazione media sia uguale alla pendenza della retta che congiunge i punti iniziale e finale del grafico velocità-tempo in cui andiamo a calcolare la media.
L'accelerazione istantanea è la tangente alla curva velocità-tempo nel punto fissato, così come è il significato geometrico della derivata prima. Essa è quindi uguale alla pendenza della retta tangente alla curva nel punto in cui viene calcolata.
Attraverso lo studio della curva nel grafico velocità-tempo si possono ricavare ulteriori importanti informazioni: dall'angolo che la tangente forma con l'asse del tempo si evince che l'accelerazione è negativa se la tangente forma un angolo superiore ai 90 gradi con l'asse delle ascisse, è positiva se rimane sotto i 90 gradi mentre è nulla se la tangente è parallela all'asse. Inoltre, si noti come a valori positivi della curva accelerazione-tempo corrispondano valori crescenti della curva velocità-tempo. Poiché l'accelerazione è la derivata seconda della posizione, si può anche ricavare l'andamento della relazione accelerazione-tempo anche studiando la concavità del grafico.
Se gli formula_47 punti materiali di un sistema sono in movimento, solitamente, la posizione del centro di massa varia. Pertanto, nell'ipotesi in cui la massa totale formula_48 sia costante, l'accelerazione del centro di massa sarà:
dove formula_50 la quantità di moto totale del sistema e formula_51 è la sommatoria delle forze esterne.
</doc>

<doc id="4441" url="https://it.wikipedia.org/wiki?curid=4441" title="Velocità">
Velocità
In fisica, in primo luogo in cinematica, la velocità (dal latino "vēlōcitās", a sua volta derivato da "vēlōx", cioè "veloce") è una grandezza vettoriale definita come la variazione della posizione di un corpo in funzione del tempo, ossia, in termini matematici, come la derivata del vettore posizione rispetto al tempo.
Nel Sistema Internazionale la velocità si misura in m·s (metri al secondo).
Quando non specificato, per "velocità" si intende la "velocità traslazionale", sottintendendo che lo spostamento a cui si fa riferimento è una traslazione nello spazio. Il termine, "velocità", infatti, può essere utilizzato con un significato più generale per indicare la variazione di una coordinata spaziale in funzione del tempo. Ad esempio, nella descrizione del moto rotatorio, per definire la "velocità di rotazione" si usano la velocità angolare e la velocità areolare.
Talvolta si usa il termine "rapidità" per indicare il modulo della velocità. Ciò viene fatto in analogia con la lingua inglese, nella quale si indica con "speed" la rapidità e con "velocity" la velocità in senso vettoriale.
La variazione della velocità, sia in aumento che in diminuzione, è l'accelerazione, anche se nel linguaggio comune a volte si parla di "decelerazione" quando la velocità diminuisce.
La velocità è un vettore che indica la rapidità del moto, la direzione e il verso di un punto materiale in movimento. Essa si riduce a una grandezza scalare soltanto in casi particolari, ad esempio, nel moto rettilineo uniforme, in cui il vettore ha una sola componente diversa da zero.
Si definisce "velocità media" formula_1 il rapporto tra lo spostamento, inteso come la variazione dello posizione, formula_2 e l'intervallo di tempo formula_3 impiegato a percorrerlo:
dove formula_5 e formula_6 sono i vettori posizione agli istanti iniziale formula_7 e finale formula_8. La velocità media può essere vista come il coefficiente angolare della retta in un grafico spazio-tempo. In particolare si parla di velocità positiva , se l'angolo che la retta forma con l'asse delle ascisse è acuto e di velocità negativa , se l'angolo che la retta forma con l'asse delle ascisse è ottuso. 
Si definisce "velocità istantanea" formula_9 il limite della velocità media per intervalli di tempo molto brevi, ovvero la derivata della posizione rispetto al tempo:. In parole povere la velocità istantanea è il valore limite della velocità media nell'intorno di un determinato istante quando la variazione di tempo formula_10 considerata tende al valore 0.
Si noti che la velocità media è proprio la media della velocità istantanea in un tempo finito formula_12:
avendo usato il teorema fondamentale del calcolo integrale.
In un contesto più formale, sia formula_14 la lunghezza di un arco della curva percorsa dall'oggetto in moto, ovvero lo spostamento dell'oggetto al tempo formula_15. La norma della velocità istantanea nel punto formula_16 è la derivata dello spostamento rispetto al tempo:
ed il vettore velocità ha la direzione del moto:
con formula_19 il vettore unitario tangente alla curva.
Utilizzando uno spazio bidimensionale, la velocità media e quella istantanea si possono scomporre nel seguente modo:
dove formula_21 e formula_22 sono due versori in direzione degli assi formula_23 e formula_24. Il modulo del vettore velocità è a sua volta scomponibile nei suoi componenti:
dove formula_26 è il momento statico e formula_27 la quantità di moto totale del sistema.
In caso di caduta di un oggetto immerso in un campo gravitazionale, la velocità finale dell'oggetto può essere determinata utilizzando la conservazione dell'energia, ottenendo così una semplice espressione:
dove formula_29 è la differenza di quota tra il punto di caduta e quello in cui l'oggetto si ferma.
In quest'ultimo caso si parla di velocità di impatto.
Per velocità terminale di caduta, o "velocità limite", si intende la velocità massima che raggiunge un corpo in caduta. Cadendo attraverso un fluido infatti il corpo incontra una crescente resistenza all'aumentare della velocità e quando l'attrito eguaglia la forza di attrazione gravitazionale la velocità si stabilizza.
La velocità della luce, o di qualsiasi altra onda elettromagnetica, è identica nel vuoto per tutti i sistemi di riferimento. Questa invarianza, implicita nelle simmetrie delle equazioni di Maxwell per la propagazione delle onde elettromagnetiche e verificata sperimentalmente alla fine del 1800 con l'esperimento di Michelson-Morley, ha portato alla necessità di modificare le equazioni del moto e della dinamica. Una delle conseguenze della teoria della relatività ristretta di Albert Einstein è che la velocità massima raggiungibile al limite da un qualunque oggetto fisico è quella della luce nel vuoto.
</doc>

<doc id="14789" url="https://it.wikipedia.org/wiki?curid=14789" title="Campo elettrico">
Campo elettrico
In fisica, il campo elettrico è un campo di forze generato nello spazio dalla presenza di una o più cariche elettriche o di un campo magnetico variabile nel tempo. Insieme al campo magnetico esso costituisce il campo elettromagnetico, responsabile dell'interazione elettromagnetica.
Introdotto da Michael Faraday, il campo elettrico si propaga alla velocità della luce ed esercita una forza su ogni oggetto elettricamente carico. Nel sistema internazionale di unità di misura si misura in newton su coulomb (N/C), o in volt su metro (V/m). Se è generato dalla sola distribuzione stazionaria di carica spaziale, il campo elettrico è detto elettrostatico ed è conservativo.
Sperimentalmente si verifica l'attrazione o la repulsione tra corpi dotati di carica elettrica, corrispondente a due stati di elettrizzazione della materia. La carica si definisce positiva quando vi è una carenza di elettroni nell'oggetto, negativa in presenza di un eccesso. Corpi elettrizzati entrambi positivamente o entrambi negativamente si respingono, mentre corpi elettrizzati in modo opposto si attraggono.
Per misurare l'elettrizzazione di un corpo si usa uno strumento chiamato elettroscopio a foglie, costituito da un'ampolla di vetro nella quale è inserita un'asta metallica la quale, all'interno dell'ampolla, ha due linguette metalliche molto sottili, dette "foglie", mentre all'esterno essa può essere messa a contatto con un corpo carico. Mettendo a contatto con l'asta un corpo carico, le linguette si allontanano l'una dall'altra in proporzione all'elettrizzazione del corpo che è stato messo a contatto.
A partire da tali evidenze sperimentali, nella seconda metà del diciottesimo secolo Charles Augustin de Coulomb formulò la legge di Coulomb, che quantifica la forza elettrica attrattiva o repulsiva che due corpi puntiformi carichi elettricamente si scambiano a distanza. A partire da tale legge si può affermare che un corpo carico elettricamente produce nello spazio circostante un campo tale per cui, se si introduce una carica elettrica, questa risente dell'effetto di una forza, detta forza di Coulomb, direttamente proporzionale al prodotto delle due cariche e inversamente proporzionale al quadrato della loro distanza.
Nel vuoto, il campo elettrico formula_1 in un punto dello spazio è definito come la forza per unità di carica elettrica positiva alla quale è soggetta una carica puntiforme formula_2, detta carica "di prova", se posta nel punto:
Il vettore campo elettrico formula_1 in un punto è quindi definito come il rapporto tra la forza elettrica agente sulla carica di prova ed il valore della carica stessa, purché la carica di prova sia sufficientemente piccola da provocare una perturbazione trascurabile sull'eventuale distribuzione di carica che genera il campo. Il campo è dunque indipendente dal valore della carica di prova usata, essendone indipendente il rapporto tra la forza e la carica stessa, e questo mostra che il campo elettrico è una proprietà caratteristica dello spazio. Dalla definizione si ricava che l'unità di misura del campo elettrico è formula_5, che equivale a formula_6.
Dalla legge di Coulomb segue che una carica formula_7 posta in formula_8 genera un campo elettrico che in un punto qualsiasi formula_9 è definito dalla seguente espressione:
dove formula_11 è la costante dielettrica del vuoto.
Per un numero "n" di cariche puntiformi formula_12 distribuite nello spazio il campo elettrostatico nella posizione formula_9 è dato da:
In generale, per una distribuzione continua di carica si ha:
dove formula_16 rappresenta la densità di carica nello spazio:
e formula_18 rappresenta la regione di spazio occupata dalla distribuzione di carica. Il campo elettrico si può esprimere come gradiente di un potenziale scalare, il potenziale elettrico:
Essendo il potenziale elettrico un campo scalare, il campo elettrico è conservativo.
Il campo elettrico è un campo vettoriale rappresentato attraverso linee di campo: una carica puntiforme positiva produce le linee di campo radiali uscenti da essa, ed è definita sorgente delle linee di forza, mentre per una carica puntiforme negativa le linee di campo sono radiali ed entranti verso la carica, che è così definita pozzo di linee di forza. Le linee di livello a potenziale elettrico costante sono dette superfici equipotenziali, e sono perpendicolari alle linee di flusso del campo elettrico.
Il fatto che una superficie chiusa che racchiuda la sorgente del campo sia attraversata da tutte le linee di forza generate dalla sorgente, si formalizza attraverso il teorema del flusso, anche detto teorema di Gauss, che definisce una proprietà matematica generale per il campo vettoriale elettrico. Nel vuoto il teorema afferma che il flusso del campo elettrico attraverso una superficie chiusa contenente una distribuzione di carica caratterizzata dalla densità di carica volumetrica formula_20 è pari alla carica totale contenuta nel volume racchiuso dalla superficie diviso per la costante dielettrica del vuoto:
Applicando il teorema della divergenza alla prima relazione ed uguagliando gli integrandi si ottiene:
Tale equazione è la prima delle equazioni di Maxwell, e costituisce la forma locale del teorema di Gauss per il campo elettrico.
Il campo elettrostatico viene generato da una distribuzione di carica indipendente dal tempo. Condizione necessaria e sufficiente perché un campo vettoriale sia conservativo in un insieme semplicemente connesso, ad esempio, un insieme stellato o convesso, è che la circuitazione del campo, cioè l'integrale del campo lungo una linea chiusa, sia nulla:
Questo avviene solamente in condizioni stazionarie.
In maniera equivalente, il campo elettrostatico è conservativo dal momento che esiste una funzione scalare, il potenziale elettrico, tale che l'integrale per andare da un punto A ad un punto B non dipenda dal cammino percorso ma solo dal valore della funzione agli estremi:
Dal teorema della divergenza e dal teorema del flusso si ricava la prima equazione di Maxwell nel vuoto:
Per la conservatività del campo elettrostatico è possibile enunciare la terza equazione di Maxwell nel vuoto nelle forme:
Combinando la prima con la seconda si ottiene l'equazione di Poisson:
dove con formula_28 si indica l'operatore differenziale laplaciano. La soluzione dell'equazione di Poisson è unica se sono date le condizioni al contorno. In particolare, un potenziale che soddisfi l'equazione di Poisson e che sia nullo a distanza infinita dalle sorgenti del campo coincide necessariamente con il potenziale elettrico, dato dall'espressione:
In assenza di cariche sorgenti del campo l'equazione diventa omogenea, e prende il nome di equazione di Laplace:
dalla quale risulta che in assenza di cariche il potenziale è una funzione armonica.
Risolvere l'equazione di Poisson in regioni di spazio limitate significa risolvere il problema generale dell'elettrostatica per opportune condizioni al contorno, come l'assenza o la presenza di conduttori e cariche elettriche localizzate. In particolare se ne distinguono tre tipi:
In questo caso non sono presenti cariche localizzate, ed il campo elettrostatico è generato da un sistema di conduttori di geometria nota e potenziale noto. In questo caso vale l'equazione di Laplace, dove le condizioni al contorno sono che il potenziale sia nullo all'infinito e valga formula_31 sulla superficie dei conduttori. Una volta ricavati i potenziali per ogni punto nello spazio risolvendo l'equazione di Laplace, si ricava il campo elettrostatico, ed è possibile determinare la densità di carica superficiali formula_32 sui conduttori mediante il teorema di Coulomb. Infine, si può trovare la carica netta totale su tutti i conduttori e i coefficienti di capacità su questi tramite il seguente sistema, che consente di ricavare i coefficienti.
In questo caso il campo elettrostatico è dato da un sistema di conduttori di geometria nota di cui sono note le cariche su ognuno. Si danno quindi dei potenziali arbitrari sui conduttori formula_34 e si risolve il "problema di Dirichlet" come sopra. Dal momento che le cariche sono note ed i coefficienti di capacità sono indipendenti dalle cariche e dai potenziali, essendo dipendenti solo dal loro rapporto, dal sistema del caso precedente si ricavano i reali potenziali formula_35.
Un esempio può essere quello di avere una distribuzione di carica formula_20 nota nello spazio ed un sistema di conduttori di cui si conoscono solo le cariche su ognuno. Il problema è quello di risolvere l'equazione di Poisson, e dal momento che non si conoscono i potenziali il problema diventa un sistema di equazioni del tipo:
dove i numeri formula_38 sono i coefficienti della matrice di potenziale. Per calcolare i potenziali si utilizza poi il metodo dei potenziali di prova.
L'elettrostatica e la magnetostatica rappresentano due casi particolari di una teoria più generale, l'elettrodinamica, dal momento che trattano i casi in cui i campi elettrico e magnetico non variano nel tempo. In condizioni stazionarie il campo elettrico ed il campo magnetico possono essere infatti trattati indipendentemente l'uno dall'altro, tuttavia in condizioni non stazionarie i due campi appaiono come le manifestazioni di una stessa entità fisica: il campo elettromagnetico.
Le stesse cariche che sono sorgente del campo elettrico, infatti, quando sono in moto generano un campo magnetico. Questo fatto è descritto dalle due leggi fisiche che correlano i fenomeni elettrici con quelli magnetici: la legge di Ampere-Maxwell e la sua simmetrica legge di Faraday, descritte nel seguito.
La legge di Faraday afferma che la forza elettromotrice indotta in un circuito chiuso da un campo magnetico è pari all'opposto della variazione del flusso magnetico del campo attraverso l'area abbracciata dal circuito nell'unità di tempo:
dove formula_40 è il flusso del campo magnetico formula_41. Dalla definizione di forza elettromotrice la precedente relazione può essere scritta come:
applicando il teorema del rotore al primo membro:
si giunge a:
Uguagliando gli integrandi segue la forma locale della legge di Faraday, che rappresenta la terza equazione di Maxwell:
Ovvero il campo elettrico può essere generato da un campo magnetico variabile nel tempo. Una conseguenza fondamentale della legge di Faraday è che il campo elettrico in condizioni non stazionarie non è più conservativo, dal momento che la sua circuitazione non è più nulla. Inoltre, avendo definito:
dove formula_47 è il potenziale vettore magnetico, dalla legge di Faraday segue che:
Dal momento che il rotore è definito a meno di un gradiente, si ha:
Il campo elettrico è così scritto in funzione dei potenziali associati al campo elettromagnetico.
L'estensione della legge di Ampère al caso non stazionario mostra come un campo elettrico variabile nel tempo sia sorgente di un campo magnetico. Ponendo di essere nel vuoto, la forma locale della legge di Ampère costituisce la quarta equazione di Maxwell nel caso stazionario:
Tale relazione vale solamente nel caso stazionario poiché implica che la divergenza della densità di corrente sia nulla, contraddicendo in questo modo l'equazione di continuità per la corrente elettrica:
Per estendere la legge di Ampère al caso non stazionario è necessario inserire la prima legge di Maxwell nell'equazione di continuità:
Il termine
è detto corrente di spostamento, e deve essere aggiunto alla densità di corrente nel caso non stazionario.
Inserendo la densità di corrente generalizzata così ottenuta nella legge di Ampère:
si ottiene la quarta equazione di Maxwell nel vuoto. Tale espressione mostra come la variazione temporale di un campo elettrico sia sorgente di un campo magnetico.
La presenza di materiale dielettrico nello spazio ove esista un campo elettrico formula_1 modifica il campo stesso. Questo è dovuto al fatto che gli atomi e le molecole che compongono il materiale si comportano come dipoli microscopici e si polarizzano in seguito all'applicazione di un campo elettrico esterno. L'effetto della polarizzazione elettrica può essere descritto riconducendo la polarizzazione dei dipoli microscopici ad una grandezza vettoriale macroscopica, che descriva il comportamento globale del materiale soggetto alla presenza di un campo elettrico esterno. Il vettore "intensità di polarizzazione", anche detto vettore di "polarizzazione elettrica" e indicato con formula_56, è il dipolo elettrico per unità di volume posseduto dal materiale.
La polarizzazione del dielettrico crea entro il materiale una certa quantità di carica elettrica indotta, detta carica di polarizzazione formula_57. Introducendo tale distribuzione di carica nella prima delle equazioni di Maxwell, che esprime la forma locale del teorema del flusso per il campo elettrico, si ha:
dove formula_59 è la densità di cariche libere e nel secondo passaggio si è utilizzata la relazione tra la densità volumica di carica di polarizzazione ed il vettore di polarizzazione. Si ha quindi:
L'argomento dell'operatore differenziale è il vettore induzione elettrica, definito come:
E la prima equazione di Maxwell assume la forma:
La maggior parte dei materiali isolanti può essere trattata come un dielettrico lineare omogeneo ed isotropo, questo significa che tra il dipolo indotto nel materiale ed il campo elettrico esterno sussista una relazione lineare. Si tratta di un'approssimazione di largo utilizzo, ed in tal caso i campi formula_63 e formula_64 sono equivalenti a meno di un fattore di scala:
e di conseguenza:
La grandezza formula_67 è la costante dielettrica relativa, e dipende dalle caratteristiche microscopiche del materiale. Se il materiale non è omogeneo, lineare ed isotropo, allora formula_68 dipende da fattori come la posizione all'interno del mezzo, la temperatura o la frequenza del campo applicato.
Nel dominio delle frequenze, per un mezzo lineare e indipendente dal tempo sussiste la relazione:
dove formula_70 è la frequenza del campo.
Inserendo il vettore di induzione elettrica nelle equazioni di Maxwell nei materiali, considerando il caso in cui il dielettrico sia perfetto e isotropo e ponendo che anche per il campo magnetico nei materiali sussista una relazione di linearità, si ha:
dove formula_72 è il campo magnetico nei materiali, e costituisce l'analogo del vettore induzione elettrica per la polarizzazione magnetica.
Considerando dielettrici perfetti ed isotropi, è possibile definire le condizioni di raccordo del campo elettrostatico quando attraversa due dielettrici di costante dielettrica relativa formula_73 e formula_74. Sulla superficie di separazione si consideri una superficie cilindrica di basi formula_75 e altezza formula_76 infinitesima, di ordine di grandezza superiore alla base. Applicando il flusso di Gauss uscente dalle basi si evince che il flusso infinitesimo è nullo poiché non vi sono cariche libere localizzate al suo interno:
dove formula_78 sono le componenti normali del campo di spostamento elettrico. In termini di campo elettrico si ha quindi:
Per la componente tangenziale del campo elettrico vale il teorema di Coulomb, ovvero la direzione del campo elettrico è normale alla superficie del conduttore, e pertanto la componente tangenziale si conserva:
In termini di campo di spostamento elettrico:
Attraversando la superficie di separazione tra due dielettrici perfetti ed isotropi, quindi, la componente normale del campo elettrico subisce una discontinuità mentre quella tangenziale non si modifica, viceversa per il campo di spostamento elettrico. Unendo le due relazioni si ottiene la legge di rifrazione delle linee di forza del campo elettrico:
e dunque:
dove
è l'angolo di rifrazione.
Il campo elettromagnetico è dato dalla combinazione del campo elettrico formula_1 e del campo magnetico formula_41, solitamente descritti con vettori in uno spazio a tre dimensioni. Il campo elettromagnetico interagisce nello spazio con cariche elettriche e può manifestarsi anche in assenza di esse, trattandosi di un'entità fisica che può essere definita indipendentemente dalle sorgenti che l'hanno generata. In assenza di sorgenti il campo elettromagnetico è detto onda elettromagnetica, essendo un fenomeno ondulatorio che non richiede di alcun supporto materiale per diffondersi nello spazio e che nel vuoto viaggia alla velocità della luce. Secondo il modello standard, il quanto della radiazione elettromagnetica è il fotone, mediatore dell'interazione elettromagnetica.
La variazione temporale di uno dei due campi determina il manifestarsi dell'altro: campo elettrico e campo magnetico sono caratterizzati da una stretta connessione, stabilita dalle quattro equazioni di Maxwell. Le equazioni di Maxwell, insieme alla forza di Lorentz, definiscono formalmente il campo elettromagnetico e ne caratterizzano l'interazione con oggetti carichi. Le prime due equazioni di Maxwell sono omogenee e valgono sia nel vuoto che nei mezzi materiali, e rappresentano in forma differenziale la Legge di Faraday e la legge di Gauss per il campo magnetico. Le altre due equazioni descrivono il modo con cui il materiale in cui avviene la propagazione interagisce, polarizzandosi, con il campo elettrico e magnetico, che nella materia sono denotati con formula_87 e formula_88. Esse mostrano in forma locale la Legge di Gauss elettrica e la Legge di Ampère-Maxwell.
La forza di Lorentz è la forza formula_89 che il campo elettromagnetico genera su una carica formula_2 puntiforme:
dove formula_92 è la velocità della carica.
Le equazioni di Maxwell sono formulate anche in elettrodinamica quantistica, dove il campo elettromagnetico viene quantizzato. Nell'ambito della meccanica relativistica, i campi sono descritti dalla teoria dell'elettrodinamica classica in forma covariante, cioè invariante sotto trasformazione di Lorentz. Nell'ambito della teoria della Relatività il campo elettromagnetico è rappresentato dal tensore elettromagnetico, un tensore a due indici di cui i vettori campo elettrico e magnetico sono particolari componenti.
</doc>

<doc id="15637" url="https://it.wikipedia.org/wiki?curid=15637" title="Energia potenziale elettrica">
Energia potenziale elettrica
L<nowiki>'</nowiki>energia potenziale elettrica, anche detta energia potenziale elettrostatica, in fisica ed in elettrotecnica, è l'energia potenziale del campo elettrostatico. Si tratta dell'energia posseduta da una distribuzione di carica elettrica, ed è legata alla forza esercitata dal campo generato dalla distribuzione stessa. Insieme all'energia magnetica, l'energia potenziale elettrica costituisce l'energia del campo elettromagnetico.
L'energia potenziale elettrostatica può essere definita come il lavoro svolto per creare una distribuzione di carica partendo da una configurazione iniziale in cui ogni componente della distribuzione non interagisce con gli altri. Ad esempio, per un sistema discreto di cariche essa coincide con il lavoro svolto per portare le singole cariche da una posizione in cui esse hanno potenziale elettrico nullo alla loro disposizione finale. L'energia potenziale elettrostatica può anche essere definita a partire dal campo elettrostatico generato dalla distribuzione stessa, ed in tale caso la sua espressione è indipendente dalla sorgente del campo.
Si tratta di una quantità che può essere sia negativa che positiva, a seconda che il lavoro svolto per portarle nella configurazione assunta sia positivo o negativo. Due cariche interagenti dello stesso segno hanno energia positiva, poiché il lavoro svolto per avvicinarle deve vincere la loro repulsione, mentre per lo stesso motivo due cariche di segno opposto hanno energia negativa.
L'energia potenziale elettrica formula_1 posseduta da una carica elettrica puntiforme formula_2 nella posizione formula_3 in presenza di un campo elettrico formula_4 è l'opposto del lavoro formula_5 compiuto dalla forza elettrostatica formula_6 per portare formula_2 da una posizione di riferimento formula_8, in cui la carica ha un'energia nota, alla posizione formula_3.
L'energia elettrostatica è definita come il lavoro necessario per portare un sistema di cariche elettriche, o più in generale una distribuzione di carica, in una data configurazione spaziale.
Si consideri dunque un sistema di cariche puntiformi. Per disporre nello spazio la prima carica elettrica formula_10 non si compie lavoro, e quindi formula_11. Per portare la seconda carica, tenendo conto della prima, il lavoro è:
dove formula_13 è la distanza tra le posizioni formula_14 e formula_15 di formula_10 e formula_17. Per la terza si ha, analogamente:
Considerando un sistema di cariche puntiformi si ha in definitiva:
con formula_20. In una forma più simmetrica:
dove il termine formula_22 è introdotto in quanto in tale sommatoria il lavoro per formula_23, che è lo stesso per formula_24, è contato due volte. Separando le due sommatorie si riconosce il potenziale elettrico:
e l'energia potenziale elettrostatica è data da:
L'estensione al caso continuo mostra che, data una distribuzione continua di cariche descritta da una densità di carica formula_27 contenuta nel volume formula_28, l'energia elettrostatica associata alla distribuzione è data dall'integrale:
dove formula_30 è il potenziale elettrico nel punto formula_31.
L'energia di sistemi elettricamente interagenti, così come le altre proprietà meccaniche, può essere descritta in modo analogo in termini del campo elettrico. Tale approccio, equivalente al precedente, permette di descrivere l'energia del sistema attraverso il campo che esso genera, indipendentemente dalle sue sorgenti.
Considerando un volume formula_28, l'energia del campo elettrostatico contenuta in tale regione è:
dove:
è la densità di energia elettrica nel vuoto.
Nel caso ci si trovi in presenza di un dielettrico, tramite gli stessi passaggi si ottiene:
dove formula_36 è il vettore di spostamento elettrico, e:
è la densità di energia elettrica nella materia.
Nel caso di distribuzioni continue di carica si ha:
con formula_27 densità di carica e formula_40 volume infinitesimo. Sfruttando la prima equazione di Maxwell formula_41 si ha:
applicando al contrario l'identità vettoriale formula_43 si ottiene:
Dalla definizione di potenziale tale espressione è pari a:
ed applicando il teorema della divergenza:
A questo punto, si può estendere il dominio di integrazione su tutta la regione dello spazio nel quale il campo elettrico sia apprezzabilmente diverso da zero, e quindi trascurare il primo dei due integrali. Dal punto di vista fisico, l'integrale di flusso che si è trascurato rappresenta il termine energetico aggiuntivo che si deve considerare nel caso la superficie di integrazione non sia sufficientemente estesa da contenere tutto lo spazio in cui il campo non è nullo.
L'utilizzo dell'energia elettrica è diffusissimo nella società moderna e attuale attraverso l'allaccio alla rete elettrica oppure tramite batterie o accumulatori: basta pensare all'uso nell'illuminazione di edifici (pubblici e privati) e strade, nell'alimentazione elettrica degli elettrodomestici e dei computer nonché nei processi produttivi-industriali ovvero nelle macchine elettriche quali i motori elettrici. 
La sua scoperta ha rappresentato dunque una vera e propria rivoluzione tecnologica, economica e sociale innescando una forte e irreversibile dipendenza/pervasività grazie ai suoi vantaggi rispetto all'energia meccanica prodotta dai motori endotermici. Tra questi si ricorda il fatto di poter essere trasportata a distanza, il basso rumore di esercizio delle apparecchiature elettriche, l'assenza di fumi di scarico nei luoghi di utilizzazione e il minor ingombro di una macchina elettrica. 
Tra gli svantaggi si annovera invece proprio il fatto di non essere una fonte primaria e quindi la necessità di una infrastruttura di conversione che inevitabilmente introduce una perdita di efficienza nel processo di conversione a monte e nel trasporto lungo le linee elettriche.
L'energia elettrica, se si eccettua l'elettricità atmosferica dei fulmini e il potenziale debolmente negativo della Terra, non è una fonte di energia primaria sulla Terra per cui deve essere prodotta per trasformazione a partire da una fonte di energia primaria, risultando così una fonte di energia secondaria. Il processo di trasformazione, a rendimento sempre inferiore al 100%, avviene all'interno di centrali elettriche. In queste, escludendo il fotovoltaico, qualunque altra sia la fonte da cui si intende generare energia, tre sono le macchine indispensabili allo scopo che si vuole ottenere:
Altro elemento del quale non si poteva fare a meno per produrre energia elettrica è l'acqua, in forma liquida (come nelle centrali idroelettriche) o di vapore (nelle centrali termoelettriche, geotermoelettriche, a fissione nucleare ed a solare termodinamico), ma sempre ad alta pressione, allo scopo di far girare le turbine ad un numero di giri tale da produrre in maniera il più possibile costante la "corrente alternata" per mezzo dell'alternatore.
L'utilizzo di acqua che, in quasi tutti i casi, deve essere riscaldata fino a divenire vapore presenta due ordini di problemi:
Una volta raggiunta la produzione di energia elettrica, il trasporto su vasta scala e la distribuzione dell'energia elettrica prodotta dalle centrali fino agli utenti finali avviene attraverso la rete di trasmissione e la rete di distribuzione.
</doc>

<doc id="38911" url="https://it.wikipedia.org/wiki?curid=38911" title="Onda">
Onda
In fisica con il termine onda si indica una perturbazione che nasce da una sorgente e si propaga nel tempo e nello spazio, trasportando energia o quantità di moto senza comportare un associato spostamento della materia.
Le onde possono propagarsi sia attraverso la materia, sia nel vuoto. Ad esempio la radiazione elettromagnetica e la radiazione gravitazionale possono esistere e propagarsi anche in assenza di materia, mentre altri fenomeni ondulatori esistono unicamente in un mezzo fisico, che deformandosi produce le forze elastiche di ritorno in grado di permettere all'onda di propagarsi.
Dal punto di vista matematico un'onda è una soluzione dell'equazione delle onde (o di altre equazioni più complicate), la cui espressione varia a seconda del tipo di perturbazione.
Non è semplice dare una definizione autonoma e precisa del termine "onda", sebbene questo termine sia comunemente molto usato in contesti molto differenti fra loro. La definizione delle caratteristiche necessarie e sufficienti che identificano il fenomeno ondulatorio è flessibile. Intuitivamente il concetto di onda è qualificato come il trasporto di una perturbazione nello spazio senza comportare un trasporto netto della materia del mezzo, qualora presente, che occupa lo spazio stesso. I fisici Albert Einstein e Leopold Infeld hanno cercato di rispondere alla domanda "Cos'è un'onda?" unendo questo fatto all'esperienza comune:
Una vibrazione può essere definita come il moto avanti e indietro intorno a un punto definito "x", tuttavia una vibrazione non è necessariamente un'onda. Infatti in un'onda sulla superficie dell'acqua, oppure lungo una stringa, l'energia vibrazionale si muove dalla sorgente sotto forma di perturbazione senza un moto collettivo delle particelle dell'acqua o della corda in cui si propaga. Questa rappresentazione diventa però problematica quando si ha a che fare con le onde stazionarie (per esempio le onde sulle corde di una chitarra), dove l'energia in tutte le direzioni è identica e non viene trasportata lungo lo spazio, perciò talvolta nella definizione di onda si cita solamente la propagazione di un disturbo senza richiedere il trasporto di energia o quantità di moto. Per le onde elettromagnetiche (ad esempio la luce) bisogna considerare ulteriormente che il concetto di mezzo non può essere applicato, in quanto queste si propagano anche nello spazio vuoto.
Per queste ragioni la teoria delle onde rappresenta una particolare branca della fisica teorica riguardante lo studio delle onde indipendentemente dalla loro origine fisica. Questa peculiarità deriva dal fatto che la teoria matematica delle onde si può applicare per descrivere fenomeni ondulatori in contesti anche molto differenti. Per esempio l'acustica si distingue dall'ottica per il fatto che la prima si occupa del trasporto vibrazionale di energia meccanica, mentre la seconda di perturbazioni del campo elettrico e magnetico. Concetti come massa, inerzia, quantità di moto, elasticità diventano quindi cruciali per descrivere i processi ondulatori acustici, al contrario dell'ottica. La struttura particolare del mezzo introduce inoltre alcuni fattori di cui bisogna tenere conto, come ad esempio i fenomeni vorticosi per l'aria e l'acqua o la complessa struttura cristallina nel caso di alcuni solidi.
Altre proprietà tuttavia possono essere usate per descrivere indifferentemente tutti i tipi di onde. Per esempio, basandosi sull'origine meccanica delle onde acustiche, ci può essere un movimento nello spazio e nel tempo di una perturbazione se e solo se il mezzo non è né infinitamente flessibile né infinitamente rigido. Se tutte le parti che compongono il mezzo si dispongono in modo rigido l'una rispetto all'altra, non sarà possibile alcun movimento infinitesimo e quindi non ci sarà alcuna onda (ad esempio l'idealizzazione del corpo rigido). Al contrario, se tutte le parti sono indipendenti l'una dall'altra senza alcun tipo di interazione reciproca, non vi sarà alcuna onda in quanto non ci sarà trasmissione di energia fra le varie parti componenti del corpo. Nonostante queste considerazioni non si possano applicare alle onde che non si propagano in alcun senso, si possono comunque trovare caratteristiche comuni a tutte le onde: ad esempio, in un'onda la fase è differente per punti adiacenti nello spazio, perché la vibrazione raggiunge questi punti in tempi differenti.
Similmente, alcuni fenomeni che si sono scoperti in determinati contesti, sono poi stati generalizzati ad altri fenomeni ondulatori. L'interferenza è stata studiata da Young nel caso particolare delle onde luminose, tuttavia è stata recentemente analizzata in alcuni problemi riguardanti le proprietà atomiche quantistiche dell'elettrone.
Un'onda può essere caratterizzata da una singola oscillazione oppure da un "treno" o successione di onde aventi caratteristiche simili, come ad esempio la periodicità intrinseca. In generale le onde sono caratterizzate da una "cresta" (punto alto), da un "ventre" (punto più basso) e da fronti d'onda di propagazione nel caso di treni di onde e sono in prima istanza classificabili come longitudinali o trasversali. Nelle onde trasversali la vibrazione è perpendicolare alla direzione di propagazione (ad esempio le onde su una corda, le parti infinitesime si muovono in alto e in basso in verticale, mentre l'onda si propaga orizzontalmente).
Le onde longitudinali sono invece caratterizzate da una vibrazione concorde con la direzione di propagazione dell'onda (ad esempio le onde sonore, le particelle dell'aria si muovono infinitesimamente nella stessa direzione di propagazione del suono). Esistono onde che sono sia longitudinali che trasversali e sono dette "onde miste" (ad esempio le onde sulla superficie del mare). Parametri di riferimento di un'onda sono l'ampiezza, la lunghezza d'onda, il periodo, la frequenza, la fase, la velocità di propagazione, l'energia e la potenza ad essa associata. Per quanto riguarda la velocità di un'onda sono definibili la velocità di fase e la velocità di gruppo.
Il mezzo in cui le onde viaggiano può essere classificato a seconda delle seguenti proprietà:
Durante la propagazione nel mezzo l'onda è soggetta ad attenuazione da parte del mezzo fino all'esaurimento dell'energia trasportata.
Tutte le onde hanno un comportamento comune in situazioni standard e possono subire i seguenti effetti o fenomeni:
Un'onda è polarizzata se può oscillare solo in una direzione. La polarizzazione di un'onda trasversale descrive la direzione di oscillazione, nel piano perpendicolare alla direzione di moto. Onde longitudinali come quelle sonore non hanno polarizzazione, in quanto per queste onde la direzione di oscillazione è lungo la direzione di moto. Un'onda può essere polarizzata con un filtro polarizzatore.
A seconda delle caratteristiche, le onde si possono classificare in molti modi.
Riguardo al tipo di mezzo:
Riguardo alle dimensioni del mezzo in cui si propagano:
Riguardo alla direzione del moto di oscillazione rispetto a quella di propagazione:
Riguardo alla propagazione:
A seconda del mezzo in cui si propagano e della caratteristica fisica che usiamo per rappresentarle:
Alcune onde caratteristiche sono:
I fenomeni ondulatori possono essere matematicamente descritti dall'equazione delle onde, almeno in prima approssimazione. Questa semplice equazione fornisce utili strumenti di analisi di tutte le onde e spesso, come nel caso di una stringa vibrante, le sue soluzioni rappresentano una prima approssimazione valida per piccole perturbazioni.
L'equazione delle onde per una funzione scalare formula_1 è un'equazione differenziale alle derivate parziali iperbolica della forma:
In una dimensione questa equazione si riduce a:
la cui soluzione generale si ottiene definendo le variabili:
e riscrivendo l'equazione:
la cui soluzione è dunque:
ovvero:
Tale soluzione di basa sul principio di Duhamel.
Una funzione formula_8 rappresenta quindi un'onda dotata di ampiezza costante che si propaga lungo l'asse formula_9 di un sistema di riferimento cartesiano se la dipendenza dallo spazio formula_9 e dal tempo formula_11 è data dalla sola combinazione formula_12:
dove formula_14 è una costante positiva. A seconda che l'argomento sia formula_15 o formula_16, l'onda si dice rispettivamente "regressiva" o "progressiva".
Un'onda progressiva di velocità formula_14 dipende dall'argomento formula_12 e trasla lungo spazio e nel tempo a velocità costante, senza cambiare la sua forma. Se si considera infatti la stessa perturbazione al tempo formula_19, si ha, per la definizione di onda:
Dato che l'onda è funzione solo di formula_21, allora la traslazione formula_22 può essere vista come una semplice traslazione spaziale di formula_23:
e quindi l'onda ad un istante successivo formula_25 non è altro che la stessa onda dell'istante formula_11, con la medesima forma, ma solo traslata di formula_27.
Si definisce fronte d'onda il luogo dei punti nello spazio in cui formula_21 assume il medesimo valore ad un determinato istante temporale.
Se formula_29 è periodica nel suo argomento allora descrive un'onda periodica. La periodicità dell'onda è identificata dal periodo formula_30, che rappresenta il tempo necessario affinché un ciclo completo di oscillazione venga completato. La frequenza formula_31 dell'onda è inoltre il numero di periodi per unità di tempo; se l'unità di tempo è il secondo la frequenza si misura in hertz.
Periodo e frequenza sono legate dalla relazione:
Ad un periodo temporale corrisponde un periodo spaziale detto lunghezza d'onda formula_33, e vale la relazione:
Nel caso di un'onda periodica, la rappresentazione in serie di Fourier permette di descrivere l'onda come somma di termini sinusoidali del tipo:
Equivalentemente, usando la formula di Eulero, questi termini possono essere rappresentati come la parte reale di una funzione immaginaria:
In queste formule formula_37 è il vettore d'onda, che identifica la direzione di propagazione dell'onda al posto della velocità di propagazione. Il suo modulo è chiamato "pulsazione spaziale", ed è legato alla lunghezza d'onda dalla relazione:
Lo scalare formula_39 è l'ampiezza dell'onda, e rappresenta il massimo valore della grandezza rappresentativa dell'onda in un periodo. Il termine formula_40 rappresenta la fase iniziale dell'onda.
Un'onda può essere descritta per mezzo della sua frequenza angolare, che è correlata alla frequenza formula_31 secondo la relazione:
In certi casi le onde presentano caratteristiche, come la dispersione (la velocità di propagazione dipende dalla frequenza) o la non linearità (il comportamento dell'onda dipende dalla sua ampiezza) che non possono essere descritte dalle soluzioni dell'equazione delle onde. Per questo motivo tali onde devono essere descritte da equazioni più complicate, come l'equazione di Sine-Gordon (che nel caso classico può descrivere la propagazione di un'onda di torsione in una stringa elastica, a cui è agganciato un sistema di pendoli che oscillano nel piano trasverso alla stringa), l'equazione di Schrödinger non lineare, l'equazione di Korteweg-de Vries o quella di Boussinesq. Tali equazioni permettono di descrivere fenomeni non previsti dall'equazione delle onde di D'Alembert, come i solitoni, le onde cnoidali o la turbolenza d'onda. Fenomeni di questo tipo si osservano in un gran numero di branche della fisica, come la fluidodinamica, la fisica dei plasmi, l'ottica non lineare, i condensati di Bose-Einstein o la relatività generale, anche se nei casi più semplici ci si può spesso ricondurre all'approssimazione lineare fornita dall'equazione delle onde di D'Alembert. 
I fenomeni ondulatori rappresentano una classe di fenomeni naturali estremamente importante in fisica; alcuni esempi di onde sono: le onde elastiche, le onde di pressione (onde acustiche e onde d'urto), le onde marine, le onde elettromagnetiche (la luce), le onde gravitazionali, le onde sismiche. In prima approssimazione, secondo il modello concettuale della fisica classica, si può affermare che in natura, al di là delle nozioni di spazio, tempo, energia e carica elettrica, tutto ciò che non è materia (cioè dotato di massa) è un'onda, cioè "energia in propagazione". La differenza sostanziale tra onda e corpuscolo materiale è che mentre il corpuscolo in un certo istante temporale è sempre "localizzato" in un preciso volume dello spazio, l'onda appare invece più "delocalizzata" nello spazio.
È solo con la fisica moderna che si raggiunge un punto di contatto nella realtà fisica tra le due diversissime classi di fenomeni, ondulatori e corpuscolari: agli inizi del XX secolo la meccanica quantistica, attraverso il principio di complementarità, sancisce infatti il cosiddetto dualismo onda-particella nei fenomeni fisici che avvengono a scala atomica e subatomica, secondo il quale le stesse particelle microscopiche dotate di massa propria, oltre alle proprietà classiche quali energia meccanica e quantità di moto, assumono proprietà ondulatorie nell'interpretazione di determinati contesti e fenomeni.
Un caso particolare di onda, descrivibile matematicamente a partire dall'equazione delle onde imponendo opportune condizioni al contorno, è l'onda stazionaria cioè un'onda che rimane in una posizione spaziale costante fissa nel tempo senza propagarsi oscillando tra punti fissi detti nodi. Questo fenomeno può accadere per esempio quando il mezzo si muove in direzione opposta all'onda oppure come risultato di una interferenza fra due onde, di eguale ampiezza e frequenza, che viaggiano in opposte direzioni.
In un'onda stazionaria vi sono alcuni punti, detti nodi, che restano fissi e non oscillano. Questo fatto determina a stretto rigore per questo tipo di perturbazione delle caratteristiche intrinsecamente differenti da una "onda" nel senso stretto del termine. In quanto tale, un'onda stazionaria può permettere per esempio di immagazzinare energia in una regione spaziale ma non rappresenta quindi alcun trasporto energetico netto fra differenti punti dello spazio.
La sovrapposizione di due onde che si muovono in direzione opposte con uguale ampiezza e frequenza, ma fase opposta è un fenomeno tipico indotto dalla riflessione di una singola onda contro un ostacolo fisso, esattamente quanto accade per esempio in una onda elettromagnetica che incide contro una lastra di materiale conduttore. Questo meccanismo è usato per generare onde stazionarie ed è alla base del funzionamento di alcuni strumenti musicali a corda, a fiato
e delle cavità risonanti.
Le onde che possono svilupparsi lungo una stringa sono di tipo trasversale e soddisfano l'equazione delle onde di d'Alembert solamente se l'ampiezza della perturbazione che genera il fenomeno ondulatorio è piccola. In questo limite si ricava che la velocità di propagazione è pari a:
dove formula_30 è la tensione a cui è sottoposta la stringa mentre formula_45 è la sua densità lineare o massa lineica, cioè la massa per unità di lunghezza. Un'onda su una stringa può essere riflessa in seguito all'urto contro un estremo fisso oppure essere parzialmente trasmessa e parzialmente riflessa in seguito all'incontro di una giunzione fra due stringhe di differente densità lineare formula_45. Questo tipo di onde, insieme al fenomeno delle onde stazionarie, sono alla base del funzionamento di molti strumenti a corda.
Un'onda elettromagnetica è un fenomeno ondulatorio dato dalla propagazione in fase del campo elettrico e del campo magnetico, oscillanti in piani tra loro ortogonali e ortogonali alla direzione di propagazione. Tale fenomeno è descritto matematicamente come soluzione dell'equazione delle onde, a sua volta ottenuta a partire dalle equazioni di Maxwell secondo la teoria dell'elettrodinamica classica. Questo tipo di radiazione viaggia nella direzione sempre perpendicolare alle direzioni di oscillazione dei campi, ed è quindi un'onda trasversale. Nel diciannovesimo secolo James Clerk Maxwell ha scoperto infatti che i campi elettrici e magnetici soddisfano l'equazione delle onde, con una velocità di propagazione vuoto pari alla velocità della luce, come determinato sperimentalmente da Heinrich Hertz. Le onde elettromagnetiche, come ad esempio la luce visibile, hanno caratteristiche di propagazione nei mezzi o in presenza di ostacoli dipendenti dalla frequenza (e quindi dalla lunghezza d'onda), alcuni materiali sono trasparenti al passaggio della radiazione elettromagnetica sulla lunghezza d'onda del visibile (come alcuni tipi di vetro), mentre le onde radio sono difficilmente ostacolate nella propagazione da oggetti di piccola dimensione, come anche piccoli edifici, infine la radiazione elettromagnetica a lunghezza d'onda inferiore a quella degli ultravioletti può essere dannosa per la salute dell'uomo. Un caso particolare di onda elettromagnetica è l'onda monocromatica.
Le onde gravitazionali sono distorsioni della curvatura dello spazio tempo che viaggiano come un'onda, propagandosi da una sorgente, come ad esempio un corpo massivo. La loro esistenza è stata prevista da Albert Einstein nel 1916 in base alla relatività generale e dovrebbero teoricamente trasportare energia sotto forma di radiazione gravitazionale. Le principali possibili sorgenti dovrebbero essere i sistemi binari composti da pulsar o buchi neri. L'11 febbraio 2016 il team del rivelatore Advanced LIGO ha annunciato di aver rilevato il 14 settembre 2015 onde gravitazionali causate dalla collisione di due buchi neri.
Le onde gravitazionali sono in generale non lineari e interagiscono con la materia e l'energia. Tuttavia questo fenomeno può essere lineare quando le onde sono molto lontane dalle sorgenti e per piccole perturbazioni dello spazio tempo.
Le onde hanno applicazioni diffusissime nella vita comune e in molti campi di studio tecnico-scientifico: dallo studio delle proprietà delle onde elettromagnetiche emesse da un corpo è possibile risalire alle caratteristiche chimico-fisiche del corpo (spettroscopia); le stesse onde elettromagnetiche (ad es. le onde portanti modulate oppure onda quadra) sono anche il mezzo utilizzato per veicolare informazione all'interno dei sistemi di telecomunicazioni attraverso segnali (es. onda radio nelle radiocomunicazioni); inoltre molte tecniche di diagnostica medica, prospezione geofisica, telerilevamento, applicazioni radar ecc., utilizzano particolari onde elettromagnetiche o acustiche.
</doc>

<doc id="18888" url="https://it.wikipedia.org/wiki?curid=18888" title="Effetto fotoelettrico">
Effetto fotoelettrico
Nella fisica dello stato solido l'effetto fotoelettrico è il fenomeno fisico di interazione radiazione-materia caratterizzato dall'emissione di elettroni da una superficie, solitamente metallica, quando questa viene colpita da una radiazione elettromagnetica, ossia da fotoni aventi una certa lunghezza d'onda.
La scoperta dell'effetto fotoelettrico va fatta risalire alla seconda metà del XIX secolo e ai tentativi di spiegare la conduzione nei liquidi e nei gas.
Nel 1887 Hertz, riprendendo e sviluppando gli studi di Schuster sulla scarica dei conduttori elettrizzati stimolata da una scintilla elettrica nelle vicinanze, si accorse che tale fenomeno è più intenso se gli elettrodi vengono illuminati con luce ultravioletta.
Nello stesso anno Wiedemann e Ebert stabilirono che la sede dell'azione di scarica è l'elettrodo negativo e Hallwachs trovò che la dispersione delle cariche elettriche negative è accelerata se i conduttori vengono illuminati con luce ultravioletta.
Nei primi mesi del 1888 il fisico italiano Augusto Righi, nel tentativo di capire i fenomeni osservati, scoprì un fatto nuovo: una lastra metallica conduttrice investita da una radiazione UV si carica positivamente. Righi introdusse, per primo, il termine "fotoelettrico" per descrivere il fenomeno.
Hallwachs, che aveva sospettato ma non accertato il fenomeno qualche mese prima di Righi, dopo qualche mese dimostrava, indipendentemente dall'italiano, che non si trattava di trasporto, ma di vera e propria produzione di elettricità.
Sulla priorità della scoperta tra i due scienziati si accese una disputa, riportata sulle pagine del Nuovo Cimento. La comunità scientifica tagliò corto e risolse la controversia chiamando il fenomeno "effetto Hertz-Hallwachs".
Fu poi Einstein nel 1905 a darne l'interpretazione corretta, intuendo che l'estrazione degli elettroni dal metallo si spiegava molto più coerentemente ipotizzando che la radiazione elettromagnetica fosse costituita da pacchetti di energia o quanti, poi denominati fotoni.
L'ipotesi quantistica di Einstein non fu accettata per diversi anni da una parte importante della comunità scientifica, tra cui Hendrik Lorentz, Max Planck e Robert Millikan (vincitori del Premio Nobel per la fisica, rispettivamente, nel 1902, 1918 e 1923), secondo i quali la reale esistenza dei fotoni era un'ipotesi inaccettabile, considerato che nei fenomeni di interferenza le radiazioni elettromagnetiche si comportano come onde. L'iniziale scetticismo di questi grandi scienziati dell'epoca non deve sorprendere dato che perfino Max Planck, che per primo ipotizzò l'esistenza dei quanti (anche se con riferimento agli atomi, che emettono e assorbono "pacchetti di energia"), ritenne, per diversi anni, che i quanti fossero un semplice artificio matematico e non un reale fenomeno fisico. Ma successivamente lo stesso Robert Millikan dimostrò sperimentalmente l'ipotesi di Einstein sull'energia del fotone, e quindi dell'elettrone emesso, che dipende soltanto dalla frequenza della radiazione, e nel 1916 effettuò uno studio sugli elettroni emessi dal sodio che contraddiceva la classica teoria ondulatoria di Maxwell.
L'aspetto corpuscolare della luce fu confermato definitivamente dagli studi sperimentali di Arthur Holly Compton. Infatti il fisico statunitense nel 1921 osservò che, negli urti con gli elettroni, i fotoni si comportano come particelle materiali aventi energia e quantità di moto che si conservano; nel 1923 pubblicò i risultati dei suoi esperimenti (effetto Compton) che confermavano in modo indiscutibile l'ipotesi di Einstein: la radiazione elettromagnetica è costituita da quanti (fotoni) che interagendo con gli elettroni si comportano come singole particelle. Per la scoperta dell'effetto omonimo Arthur Compton ricevette il premio Nobel nel 1927.
Per i suoi studi sull'effetto fotoelettrico e la conseguente scoperta dei quanti di luce Einstein ricevette il Premio Nobel per la fisica nel 1921.
L'effetto fotoelettrico fu rivelato da Hertz nel 1887 nell'esperimento ideato per generare e rivelare le onde elettromagnetiche; in quell'esperimento, Hertz usò uno spinterometro in un circuito accordato per generare onde e un altro circuito simile per rivelarle. Nel 1900 Lenard studiò tale effetto, trovando che la luce incidente su una superficie metallica provoca l'emissione di elettroni, la cui energia non dipende dall'intensità della luce, ma dal suo colore, cioè dalla frequenza.
Quando la luce colpisce una superficie metallica pulita (il catodo "C") vengono emessi elettroni. Se alcuni di questi colpiscono l'anodo "A", si misura una corrente nel circuito esterno. Il numero di elettroni emessi che raggiungono l'anodo può essere aumentato o diminuito rendendo l'anodo positivo o negativo rispetto al catodo.
Detta "V" la differenza di potenziale tra "A" e "C", si può vedere che solo da un certo potenziale in poi (detto "potenziale d'arresto") la corrente inizia a circolare, aumentando fino a raggiungere un valore massimo, che rimane costante. Questo massimo valore è, come scoprì Lenard, direttamente proporzionale all'intensità della luce incidente. Il potenziale d'arresto è legato all'energia cinetica massima degli elettroni emessi dalla relazione
dove "m" è la massa dell'elettrone, "v" la sua velocità, "e" la sua carica.
Ora, la relazione che lega le due grandezze è proprio quella indicata perché se "V" è negativo, gli elettroni vengono respinti dall'anodo, tranne se l'energia cinetica consente loro, comunque, di arrivare su quest'ultimo. D'altra parte si notò che il potenziale d'arresto non dipendeva dall'intensità della luce incidente, sorprendendo lo sperimentatore, che si aspettava il contrario. Infatti, classicamente, il campo elettrico portato dalla radiazione avrebbe dovuto mettere in vibrazione gli elettroni dello strato superficiale fino a strapparli al metallo. Usciti, la loro energia cinetica sarebbe dovuta essere proporzionale all'intensità della luce incidente e non alla sua frequenza, come invece sembrava risultare sperimentalmente.
Come comprese Einstein, riprendendo la teoria di Planck, l'effetto fotoelettrico evidenzia la natura quantistica della luce. Nella radiazione elettromagnetica, l'energia non è distribuita in modo uniforme sull'intero fronte dell'onda ma è concentrata in singoli quanti (pacchetti discreti) di energia, i fotoni. Un solo fotone per volta, e non l'intera onda nel suo complesso, interagisce singolarmente con un elettrone, al quale cede la sua energia. Affinché ciò si verifichi è necessario che il singolo fotone abbia un'energia sufficiente a rompere il legame elettrico che tiene legato l'elettrone all'atomo. Questa "soglia minima" di energia del fotone si determina in base alla relazione di Planck
dove h è la costante di Planck, f e λ sono rispettivamente la frequenza e la lunghezza d'onda del fotone, c è la velocità della luce (ricordando la relazione formula_3).
In altri termini, l'elettrone può uscire dal metallo solo se l'energia del fotone è almeno uguale al “lavoro di estrazione” (formula_4). Esiste, pertanto, una “soglia minima” di estrazione per ogni metallo, che fa riferimento o alla lunghezza d'onda o alla frequenza del fotone incidente e, quindi, alla sua energia “hf”, la quale coincide con il “lavoro di estrazione” (Wₑ).
Il valore di soglia varia in base al tipo di materiale considerato (in genere metalli) e dipende, pertanto, dalle sue caratteristiche atomiche; anche il grado di purezza del metallo influisce sul valore di soglia (per tale motivo i testi o i siti specializzati riportano spesso valori di soglia differenti per lo stesso metallo).
Nella tabella che segue sono riportati i valori di soglia di alcuni metalli. Il dato iniziale noto è quello del lavoro di estrazione in eV (col. 2), che equivale al valore di soglia del fotone (in eV) incidente sul metallo considerato; i valori di soglia riportati nelle colonne 3, 4 e 5 sono stati ricavati dalle rispettive formule.
VALORI DI SOGLIA PER L'EMISSIONE DI ELETTRONI DA UN METALLO
Si precisa che:
Spesso il parametro di soglia iniziale noto è:
Con l'aumentare dell'energia dei fotoni incidenti (ossia quando aumenta “ f ” oppure quando diminuisce “ λ ”) aumenta anche l'energia cinetica degli elettroni estratti.
Va in proposito sottolineato che aumentando l'intensità della radiazione elettromagnetica (ossia il numero di fotoni al secondo, di pari energia, che colpiscono l'unità di superficie) aumenta il numero degli elettroni estratti ma non la loro energia cinetica, la quale dipende esclusivamente dall'energia dei fotoni incidenti. Questa è una conseguenza della teoria quantistica di Einstein, in base alla quale ogni fotone incidente interagisce soltanto con un singolo elettrone. Infatti secondo la teoria ondulatoria classica di Maxwell l'estrazione di elettroni dal metallo dipende dall'intensità dell'irradiamento per unità di superficie (che deve raggiungere un valore sufficiente) e prescinde, quindi, dalla frequenza della radiazione incidente (ipotesi, questa, smentita dalle evidenze sperimentali).
L'effetto fotoelettrico, oggetto di studi da parte di molti fisici, è stato fondamentale per comprendere la natura quantistica della luce.
Un caso particolare di effetto fotoelettrico è l'effetto fotovoltaico.
Einstein, nel lavoro del 1905 che gli fruttò il Premio Nobel per la fisica nel 1921, fornisce una spiegazione dei fatti sperimentali partendo dal principio che la radiazione incidente possiede energia quantizzata. Infatti i fotoni che arrivano sul metallo cedono energia agli elettroni dello strato superficiale del solido; gli elettroni acquisiscono così l'energia necessaria per rompere il legame: in questo senso l'ipotesi più semplice è che il fotone ceda all'elettrone tutta l'energia in suo possesso. A questo punto l'elettrone spenderà parte dell'energia per rompere il legame e parte incrementerà la sua energia cinetica che gli permetterà di arrivare in superficie e abbandonare il solido: da qui si può capire che saranno gli elettroni eccitati più vicini alla superficie ad avere la massima velocità normale alla stessa. Per questi, posto "P" il lavoro (che varia da sostanza a sostanza) utile all'elettrone per uscire, si avrà che l'energia cinetica è pari a:
A questo punto detta formula_6 la carica dell'elettrone e "Π" il potenziale positivo del corpo e tale da impedire perdita di elettricità allo stesso (il potenziale di arresto), si può scrivere:
oppure, con i simboli consueti
che diventa
dove "E" è la carica di un grammo-equivalente di uno ione monovalente e P il potenziale di questa quantità.
Ponendo, poi, "E" = 9,6 · 10, "Π" · 10 rappresenterà il potenziale in volt del corpo in caso di irradiazione nel vuoto.
Ora, ponendo "P"' = 0, "ν" = 1,03·10 (limite dello spettro solare dalla parte ultravioletta), "β" = 4,866·10, si ottiene "Π"·10 = 4,3V: il risultato trovato è così in accordo, per quanto riguarda gli ordini di grandezza, con quanto trovato da Lenard.
Si può concludere che:
I risultati matematici cambiano se si rifiuta l'ipotesi di partenza (energia trasmessa totalmente)
che diventa
per la fotoluminescenza, che è il processo inverso.
Se poi la formula è corretta, "Π"("ν") riportata sugli assi cartesiani risulterà una retta con pendenza indipendente dalla sostanza. Nel 1916 Millikan eseguì la verifica sperimentale di tale fatto, misurando il potenziale d'arresto e trovando che questo è una retta di "ν" con pendenza "h"/"e", come previsto.
Le normali cellule fotelettriche dei cancelli automatici funzionano basandosi sull'effetto fotoelettrico: una sorgente elettromagnetica di una cellula sorgente irradia elettromagneticamente a distanza una cellula ricevente che funge da ricevitore trasformandosi in interruttore per il sistema elettromeccanico.
</doc>

<doc id="8916" url="https://it.wikipedia.org/wiki?curid=8916" title="Pianeta">
Pianeta
Un pianeta è un corpo celeste che orbita attorno a una stella e che, a differenza di questa, non produce energia tramite fusione nucleare, la cui massa è sufficiente a conferirgli una forma sferoidale, laddove la propria dominanza gravitazionale gli permette di mantenere libera la sua fascia orbitale da altri corpi di dimensioni comparabili o superiori.
Questa definizione è entrata ufficialmente nella nomenclatura astronomica il 24 agosto 2006, con la sua promulgazione ufficiale da parte dell'Unione Astronomica Internazionale. In precedenza non esisteva una definizione precisa, ma un'atavica indicazione derivante dall'antica astronomia greca, per cui si considerava pianeta qualunque corpo celeste dotato di massa significativa che si muovesse su orbite fisse.
Nell'antichità, come rivela l'etimologia del termine "pianeta" (in greco antico "plànētes astéres", stelle vagabonde), venivano considerati tali tutti gli astri che si spostavano nel cielo notturno rispetto allo sfondo delle stelle fisse, ovvero la Luna, il Sole, Mercurio, Venere, Marte, Giove e Saturno, escluse le comete, che venivano considerate fenomeni atmosferici.
Nel XVI secolo, con l'affermarsi del sistema eliocentrico, divenne chiaro che Luna e Sole non condividevano in realtà la natura fisica e le caratteristiche orbitali proprie degli altri pianeti e che anche la Terra doveva essere inclusa nel novero dei pianeti.
Nel 1781 venne scoperto Urano, il primo pianeta che non era noto agli astronomi greci. Nei successivi 150 anni sarebbero stati individuati, in successione, altri due pianeti, Nettuno e Plutone; quest'ultimo è stato annoverato tra i pianeti dalla scoperta nel 1930 fino al 2006, anno in cui venne promulgata la nuova definizione di pianeta.
Inoltre a partire dal 1801 vennero progressivamente scoperti oltre centomila corpi di dimensioni subplanetarie, orbitanti attorno al Sole principalmente nella regione di spazio compresa fra l'orbita marziana e quella gioviana, la cosiddetta fascia principale. Sebbene in un primo tempo questi corpi fossero designati come pianeti, in virtù del loro numero sempre crescente vennero presto definiti come una classe di oggetti a sé: gli asteroidi. Fra di essi, solo poche decine sono caratterizzati da una forma approssimativamente sferica.
Lo schema dei nove pianeti classici rimase inalterato fino agli anni novanta del XX secolo; tuttavia alla fine del 2002 le moderne tecniche osservative avevano già permesso l'individuazione di oltre cento corpi di questo tipo, fra pianeti extrasolari e planetoidi ghiacciati orbitanti nelle regioni periferiche del sistema solare esterno. In particolare nel caso di questi ultimi la scoperta di corpi dalle dimensioni confrontabili o addirittura maggiori di quelle di Plutone, il più piccolo dei nove pianeti, riaccese un forte dibattito sulla necessità di promulgare una definizione precisa di "pianeta". Il problema nasceva dal fatto che la classificazione dei corpi celesti derivava in parte dall'astronomia dell'antica Grecia, che si limitava a chiarire che un pianeta era un qualsiasi corpo celeste che si muovesse lungo orbite fisse (o "schemi"). Questa descrizione era stata limata col tempo fino a quella corrente, che tuttavia peccava in vaghezza e in genericità.
Nel 2005 l'Unione Astronomica Internazionale (UAI) istituì il "Comitato per la definizione di pianeta" (PDC), composto da sette esperti riconosciuti a livello mondiale, cui assegnò il compito di fornire una definizione precisa del termine. Nel corso della ventiseiesima Assemblea generale dell'UAI, avvenuta dal 14 al 25 agosto 2006, la risoluzione proposta dal comitato fu discussa e modificata e il 24 agosto 2006 venne ufficializzata. Precedentemente considerato un pianeta, da questa data Plutone fu ridefinito, assieme ad altri corpi di recente scoperta, come pianeta nano.
I nomi dei pianeti nella cultura occidentale sono derivati dalle consuetudini dei Romani, che in ultima analisi derivano da quelle dei Greci e dei Babilonesi. Nell'antica Grecia, il Sole e la Luna erano chiamati "Elio" e "Selēnē"; il pianeta più lontano era chiamato "Phàinōn", il "più luminoso"; il penultimo pianeta era "Phaéthon", il "brillante"; il pianeta rosso era indicato come "Pyróeis", l'"ardente"; il più luminoso era conosciuto come "" "Phōsphóros", il "portatore di luce", mentre il fugace pianeta più interno era chiamato "Stílbōn", "lo splendido". Inoltre i Greci associarono ogni pianeta a una divinità del loro pantheon, gli Olimpi: Elio e Selene erano i nomi sia dei pianeti, sia degli dèi; "Phainon" era sacro a Crono, il Titano che generò gli Olimpi; "Phaethon" era sacro a Zeus, figlio di Crono; "Pyroeis" ad Ares, figlio di Zeus e dio della guerra; "Phosphoros" era retto da Afrodite, la dea dell'amore; mentre Hermes, messaggero degli dei e dio dell'apprendimento e dell'ingegno, dominava "Stilbon".
L'abitudine greca di dare i nomi dei propri dei ai pianeti derivò quasi certamente da quella dei Babilonesi, che indicavano Phosphoros con il nome della propria dea dell'amore, Ishtar; Pyroeis era identificato dal dio della guerra, Nergal; Stilbon dal dio della saggezza, Nabu, e Phaethon dal capo degli dei, Marduk. Le concordanze tra i due sistemi di nomenclatura sono troppe, perché essi possano essere stati sviluppati in modo indipendente. La corrispondenza tra le divinità non era perfetta. Per esempio, Nergal fu identificato con Ares; tuttavia Nergal era per i Babilonesi, oltre che il dio della guerra, anche la divinità delle pestilenze e dell'oltretomba.
Oggi i nomi utilizzati per designare i pianeti nella maggior parte delle culture occidentali derivano da quelli delle divinità olimpiche, spesso in una versione mutuata dalla mitologia romana. Infatti l'influenza dell'Impero romano prima e della Chiesa cattolica poi ha portato all'adozione dei nomi in latino. Inoltre il "pantheon" romano, in conseguenza della comune origine indoeuropea, aveva numerose similitudini con quello greco, sebbene mancasse di una ricca tradizione narrativa. Durante l'ultimo periodo della Repubblica romana, gli scrittori romani attinsero ai miti greci e li estesero alle proprie divinità, al punto che i due pantheon divennero quasi indistinguibili. In seguito, quando i Romani studiarono i testi di astronomia dei Greci, assegnarono ai pianeti i nomi delle proprie divinità: Mercurio (per Hermes), Venere (per Afrodite), Marte (per Ares), Giove (per Zeus) e Saturno (per Crono). Quando nei secoli XVIII e XIX furono scoperti nuovi pianeti, la comunità internazionale scelse di proseguire nella tradizione e furono nominati Urano e Nettuno.
Secondo una credenza originatasi in Mesopotamia, sviluppatasi nell'Egitto ellenistico e in seguito diffusasi anche tra i Romani, le sette divinità da cui i pianeti erano nominati si prendevano cura degli affari della Terra con turni orari, stabiliti in base alla distanza dal nostro pianeta nell'ordine seguente: Saturno, Giove, Marte, il Sole, Venere, Mercurio e la Luna. Il giorno era intitolato al dio che ne reggeva la prima ora, così al giorno dedicato a Saturno, che reggeva la prima ora del primo giorno e della settimana, seguiva quello dedicato al Sole, che reggeva la venticinquesima ora della settimana e la prima del secondo giorno, a cui seguivano i giorni dedicati alla Luna, a Marte, Mercurio, Giove e Venere. Quest'ordine è stato quindi ripreso dall'ordine dei giorni della settimana nel calendario romano che sostituì il ciclo nundinale e che ancora oggi è preservato in numerose lingue e culture. Nella maggior parte delle lingue romanze, i nomi dei primi cinque giorni della settimana sono traduzione diretta delle originarie espressioni latine: ad esempio da "lunae dies" derivano lunedì, in italiano; "lundi" in francese, "lunes" in spagnolo. Differentemente è accaduto per il sabato e la domenica, i cui nomi hanno subito l'influsso della tradizione della Chiesa. Invece nelle lingue germaniche è stato preservato il significato originario dei nomi di questi due giorni. A titolo di esempio, le parole inglesi "Sunday" e "Saturday" tradotte letteralmente significano: "giorno del Sole" e "giorno di Saturno"; analogamente è accaduto per il lunedì. Invece i nomi dei restanti giorni della settimana sono stati riassegnati a dèi considerati simili o equivalenti alle corrispondenti divinità romane.
Poiché la Terra fu classificata tra i pianeti solo nel XVII secolo, a essa non è generalmente associato il nome di una divinità. Nelle lingue romanze il suo nome deriva dalla parola latina "terra"; mentre nelle lingue germaniche dalla parola "*erþā", da cui derivano le forme "Earth" in inglese, "Erda" e, la più recente, "Erde" in tedesco, "Aarde" in olandese e "Jorden" (forma determinata di "jord") nelle lingue scandinave; tutte col significato di "suolo". In greco si è preservato il nome originario: "Ghê" (Gea o Gaia).
Le culture non europee adottano altri sistemi di nomenclatura planetaria. In India essa è basata sul Navagraha, che include i sette pianeti tradizionali (Sūrya per il Sole, Chandra per la Luna e Budha, Shukra, Mangala, Bṛhaspati e Shani per i pianeti Mercurio, Venere, Marte, Giove e Saturno) e i nodi ascendente e discendente dell'orbita della Luna come Rahu e Ketu. La Cina e i Paesi dell'Estremo Oriente influenzati dalla sua cultura (come Giappone, Corea e Vietnam) usano una nomenclatura basata sul Wu Xing (la teoria dei cinque elementi): Mercurio è identificato con l'acqua, Venere con il metallo, Marte con il fuoco, Giove con il legno e Saturno con la terra.
Tutti i pianeti, a eccezione dei pianeti interstellari, orbitano attorno a stelle o comunque oggetti sub-stellari. L'orbita percorsa da un pianeta attorno alla propria stella è descritta dalle leggi di Keplero: «i pianeti orbitano su orbite ellittiche, di cui la stella occupa uno dei fuochi.» Nel sistema solare tutti i pianeti orbitano intorno al Sole nella stessa direzione di rotazione del Sole, quindi in senso anti-orario, se visto dal polo nord della nostra stella. Tuttavia si è visto che almeno un pianeta extrasolare, WASP-17b, si muove in direzione opposta a quella in cui ruota la stella.
Il periodo che un pianeta impiega per compiere una rivoluzione completa intorno alla stella è conosciuto come periodo siderale o anno. La massima distanza tra il pianeta e il centro dell'orbita è detta semiasse maggiore. L'anno di un pianeta dipende dal valore del semiasse maggiore dell'orbita che esso percorre: più è grande, maggiore è la distanza che deve percorrere il pianeta lungo la propria orbita e con minor velocità, perché meno attratto dalla gravità della stella. La distanza tra il pianeta e la stella varia nel corso del periodo siderale. Il punto in cui il pianeta è più vicino alla stella viene chiamato periastro (perielio nel sistema solare), mentre il punto più lontano è chiamato afastro o apoastro (afelio nel sistema solare). Al periastro la velocità del pianeta è massima, convertendo l'energia gravitazionale in energia cinetica; all'apoastro la velocità assume il suo valore minimo.
L'orbita di ogni pianeta è descritta attraverso sei parametri orbitali: il semiasse maggiore; l'eccentricità, l'inclinazione orbitale, l'ascensione retta del nodo ascendente, l'argomento del perielio o pericentro e l'anomalia vera. L'eccentricità descrive la forma dell'orbita: le orbite caratterizzate da una piccola eccentricità sono più circolari, mentre quelle con eccentricità maggiori ellissi più schiacciate. I pianeti del sistema solare percorrono orbite con basse eccentricità e pertanto quasi circolari. Invece le comete e gli oggetti della fascia di Kuiper, così come alcuni pianeti extrasolari, hanno orbite molto eccentriche e quindi particolarmente allungate.
L'inclinazione e l'ascensione retta del nodo ascendente sono due parametri angolari che individuano la disposizione del piano orbitale nello spazio. L'inclinazione è misurata rispetto al piano dell'orbita della Terra (piano dell'eclittica) per i pianeti del sistema solare, mentre per i pianeti extrasolari si usa il piano di vista dell'osservatore da terra. Gli otto pianeti del sistema solare giacciono molto vicini al piano dell'eclittica; le comete e gli oggetti della fascia di Kuiper invece possono discostarsene molto.
I punti in cui il pianeta attraversa il piano dell'eclittica sono detti nodi, ascendente o discendente in base alla direzione del moto. L'ascensione retta del nodo ascendente è misurata rispetto a una direzione di riferimento, individuata nel sistema solare dal punto d'Ariete. "L<nowiki>'</nowiki>argomento del pericentro" specifica l'orientazione dell'orbita all'interno del piano orbitale, mentre "l<nowiki>'</nowiki>anomalia vera" la posizione dell'oggetto sull'orbita in funzione del tempo. A questi parametri possono essere affiancati o sostituiti degli altri che sono una loro rielaborazione, come il tempo di passaggio al perielio, equivalente nella meccanica kepleriana all'indicazione dell'argomento del pericentro, o il periodo orbitale, equivalente all'asse maggiore per la terza legge di Keplero.
Diversi pianeti e pianeti nani del sistema solare, come Nettuno e Plutone e alcuni pianeti extrasolari, hanno periodi orbitali che sono in risonanza l'un con l'altro o con corpi più piccoli. Questo fenomeno è comune anche nei sistemi dei satelliti.
I pianeti ruotano attorno ad assi invisibili che passano per il loro centro. Il periodo di rotazione di un pianeta è conosciuto come il suo giorno. La maggior parte dei pianeti del sistema solare ruota nello stesso verso in cui orbitano attorno al Sole, ovvero in verso antiorario se guardati dal polo nord celeste; le uniche eccezioni sono Venere e Urano che ruotano in verso orario. A causa dell'estrema inclinazione dell'asse di Urano esistono due convenzioni che si differenziano nel polo che scelgono come nord e, di conseguenza, nell'indicare come oraria o antioraria la rotazione attorno a questo polo; la rotazione di Urano è retrograda rispetto alla sua orbita, indipendentemente dalla convenzione adottata. Grande è la variabilità della durata del giorno tra i pianeti, con Venere che completa una rotazione in 243 giorni terrestri e i giganti gassosi che la completano in poche ore. Non sono noti i periodi di rotazione dei pianeti extrasolari finora scoperti. Tuttavia, per quanto riguarda i pianeti gioviani caldi, la loro prossimità alle stelle attorno a cui orbitano suggerisce che siano in rotazione sincrona, ovvero che il loro periodo di rotazione sia uguale al periodo di rivoluzione; di conseguenza essi mostrano sempre la stessa faccia alla stella intorno a cui orbitano e mentre su un emisfero è perpetuamente giorno, sull'altro è perpetuamente notte.
L'asse intorno a cui ruota il pianeta può essere, e in genere è, inclinato rispetto al piano orbitale. Ciò determina che nel corso dell'anno il quantitativo di luce che ogni emisfero riceve dalla stella vari: quando l'emisfero settentrionale è diretto verso essa e riceve maggiore illuminazione, quello meridionale si trova nella condizione opposta, e viceversa. È l'inclinazione dell'asse di rotazione a comportare l'esistenza delle stagioni e i cambiamenti climatici annuali a esse associate.
I momenti in cui la stella illumina la superficie massima o minima di un emisfero sono detti solstizi. Ve ne sono due nel corso dell'orbita e a essi corrisponde la durata massima (solstizio d'estate) e minima (solstizio d'inverno) del giorno. I punti dell'orbita in cui il piano equatoriale e il piano orbitale del pianeta vengono a giacere sullo stesso piano sono detti equinozi. Agli equinozi la durata del giorno eguaglia la durata della notte e la superficie illuminata si divide equamente tra i due emisferi geografici.
Tra i pianeti del sistema solare la Terra, Marte, Saturno e Nettuno possiedono valori dell'inclinazione dell'asse di rotazione prossimi ai 25°. Mercurio, Venere e Giove ruotano attorno ad assi inclinati di pochi gradi rispetto ai rispettivi piani orbitali e le variazioni stagionali sono minime. Urano possiede l'inclinazione assiale maggiore, pari a circa 98° e ruota praticamente su un fianco. I suoi emisferi in prossimità dei solstizi sono quasi perennemente illuminati o perennemente in ombra. La durata delle stagioni è determinata dalla dimensione dell'orbita: su Venere durano circa 55-58 giorni, sulla Terra 90-93 giorni, su Marte sei mesi, su Nettuno quarant'anni.
Le inclinazioni assiali dei pianeti extrasolari non sono state determinate con certezza. Gli studiosi ritengono che la maggior parte dei pianeti gioviani caldi possegga inclinazioni assiali nulle o quasi, in conseguenza della prossimità alla loro stella.
La caratteristica dinamica che definisce un pianeta è la dominanza orbitale. Un pianeta è gravitazionalmente dominante, o "avrà ripulito le proprie vicinanze orbitali" (riportando le parole utilizzate nella definizione di pianeta approvata dall'Unione Astronomica Internazionale) se nella propria zona orbitale non orbiteranno altri corpi di dimensioni comparabili a quelle del pianeta che non siano o suoi satelliti o comunque a esso gravitazionalmente legati. Questa caratteristica è la discriminante tra pianeti e pianeti nani. Sebbene questo criterio a oggi sia applicato soltanto al sistema solare, sono stati scoperti diversi sistemi planetari extrasolari in formazione in cui si osserva in atto il processo che condurrà alla formazione di pianeti gravitazionalmente dominanti.
La principale caratteristica fisica che consente di identificare un pianeta è la sua massa. Un pianeta deve possedere una massa sufficientemente elevata affinché la sua gravità domini sulle forze elettromagnetiche, presentandosi in uno stato di equilibrio idrostatico; più semplicemente, ciò significa che tutti i pianeti possiedono una forma sferica o sferoidale. Infatti un corpo celeste può assumere una forma irregolare se possiede una massa inferiore a un valore limite, che è funzione della propria composizione chimica; superato questo valore si innesca un processo di collasso gravitazionale che lo conduce, con tempi più o meno lunghi, ad assumere una forma sferica.
La massa è anche il principale attributo che consente di distinguere un pianeta da una nana bruna. Il limite superiore per la massa di un corpo planetario equivale a circa 13 volte la massa di Giove, valore oltre il quale nel nucleo del corpo celeste si raggiungono le condizioni adatte per la fusione del deuterio, il che rende l'oggetto una nana bruna. A parte il Sole, nel sistema solare non esiste alcun altro oggetto con una massa superiore a questo valore; tuttavia sono stati scoperti numerosi oggetti extra-solari con masse che si avvicinano a questo valore limite e che possono essere definiti pertanto pianeti. L<nowiki>'</nowiki>"Extrasolar Planets Encyclopedia" (Enciclopedia dei pianeti extrasolari) ne riporta una lista, che comprende HD 38529 c, AB Pictoris b, HD 162020 b, e HD 13189 b.
Il più piccolo pianeta conosciuto, escludendo pianeti nani e satelliti, è PSR B1257+12A, uno dei primi pianeti extrasolari scoperti, individuato nel 1992 in orbita intorno a una pulsar; la sua massa è circa la metà di quella del pianeta Mercurio.
 
Ogni pianeta ha iniziato la sua esistenza in uno stato fluido; nelle fasi iniziali della sua formazione i materiali più densi e più pesanti sono affondati verso il centro del corpo, lasciando i materiali più leggeri in prossimità della superficie. Ogni pianeta ha quindi un interno differenziato, costituito da un nucleo denso circondato da un mantello, che può presentarsi allo stato fluido.
I pianeti terrestri sono sigillati all'interno di una crosta dura, mentre nei giganti gassosi il mantello si dissolve semplicemente negli strati nuvolosi superiori.
I pianeti terrestri posseggono nuclei di elementi ferromagnetici, quali ferro e nichel, e mantelli di silicati. Si ritiene che Giove e Saturno posseggano nuclei composti da rocce e metalli, circondati da idrogeno metallico. Urano e Nettuno, più piccoli, posseggono nuclei rocciosi, circondati da mantelli composti da ghiacci d'acqua, ammoniaca, metano e di altre sostanze volatili. I moti dei fluidi in prossimità dei nuclei planetari determina l'esistenza di un campo magnetico.
Tutti i pianeti del sistema solare hanno un'atmosfera, dal momento che la gravità associata alle loro grandi masse è abbastanza forte da intrappolare le particelle gassose. I giganti gassosi sono sufficientemente massicci da trattenere grandi quantitativi di gas leggeri come idrogeno ed elio, mentre i pianeti più piccoli li perdono nello spazio. L'atmosfera terrestre è diversa rispetto a quelle degli altri pianeti. Infatti i processi vitali che hanno luogo sul pianeta ne hanno alterato la composizione, arricchendola di ossigeno molecolare (O). Mercurio è l'unico pianeta del sistema solare che possiede un'atmosfera estremamente tenue, che è stata soffiata via per la maggior parte, sebbene non totalmente, dal vento solare.
Le atmosfere planetarie ricevono energia in vario grado dal Sole e dagli strati planetari più interni; ciò determina il verificarsi di fenomeni meteorologici quali cicloni sulla Terra, tempeste di sabbia che interessano l'intero Marte, tempeste cicloniche e anticicloniche, come ad esempio la celebre Grande Macchia Rossa su Giove, e forti venti sui giganti gassosi. Anche sui pianeti extrasolari sono state identificate tracce di attività meteorologica: su HD 189733 b è stata individuata una tempesta simile alla Grande Macchia Rossa, ma due volte più ampia.
Si è visto che alcuni pianeti gioviani caldi perdono la loro atmosfera nello spazio a causa delle radiazioni e del vento stellare in modo molto simile a quanto accade alle code delle comete: è quanto accade ad esempio per HD 209458 b. È stato ipotizzato che su questi pianeti si verifichi una grande escursione termica diurna e che possono pertanto svilupparsi venti supersonici tra l'emisfero illuminato e quello in ombra, con velocità che nel caso di HD 209458 b sono comprese tra 5000 e 10 000 km/h. Osservazioni eseguite su HD 189733 b sembrano indicare che l'emisfero buio e l'emisfero illuminato abbiano temperature molto simili, a indicazione del fatto che l'atmosfera del pianeta ridistribuisce globalmente e con elevata efficienza l'energia ricevuta dalla stella.
Una caratteristica importante dei pianeti è l'esistenza di un momento magnetico intrinseco, che indica che il pianeta è ancora geologicamente attivo o, in altre parole, che al suo interno esistono ancora moti convettivi di materiali elettricamente conduttivi che generano il campo. La presenza di un campo magnetico planetario modifica significativamente l'interazione tra il pianeta e il vento stellare; infatti attorno al pianeta si crea una "cavità" (una zona dello spazio in cui il vento solare non riesce a entrare) chiamata magnetosfera, che può raggiungere dimensioni molto più grandi rispetto al pianeta stesso. Al contrario, pianeti che non posseggono un campo magnetico intrinseco sono circondati da piccole magnetosfere indotte dall'interazione della ionosfera con il vento solare, che non sono in grado di proteggere efficacemente il pianeta.
Degli otto pianeti del sistema solare, solo Venere e Marte mancano di un campo magnetico intrinseco, mentre ne possiede uno la più grande luna di Giove, Ganimede. Il campo magnetico intrinseco di Ganimede è diverse volte più forte di quello di Mercurio, il più debole tra quelli posseduti dai pianeti e appena sufficiente a deflettere il vento solare. Il campo magnetico planetario più forte all'interno del sistema solare è quello di Giove. Le intensità dei campi magnetici degli altri giganti gassosi sono pressappoco simili a quella del campo terrestre, sebbene i loro momenti magnetici siano significativamente più grandi. I campi magnetici di Urano e Nettuno sono fortemente inclinati rispetto ai rispettivi assi di rotazione e scostati rispetto al centro del pianeta.
Nel 2004 un gruppo di astronomi delle Hawaii ha osservato un pianeta extrasolare creare una macchia sulla superficie della stella attorno a cui era in orbita, HD 179949. I ricercatori hanno ipotizzato che la magnetosfera del pianeta stesse interagendo con la magnetosfera stellare, trasferendo energia alla fotosfera stellare e incrementando localmente la già alta temperatura di 14 000 K di ulteriori 750 K.
Tutti i pianeti, a esclusione di Mercurio e Venere, hanno satelliti naturali, chiamati comunemente "lune". La Terra ne ha una, Marte due, mentre i giganti gassosi ne hanno un elevato numero, organizzate in sistemi complessi simili a sistemi planetari. Alcune lune dei giganti gassosi hanno caratteristiche simile a quelle dei pianeti terrestri e dei pianeti nani e alcune di esse sono state studiate come possibili dimore di forme di vita (specialmente Europa, uno dei satelliti di Giove).
Attorno ai quattro giganti gassosi orbitano degli anelli planetari di dimensione e complessità variabili. Gli anelli sono composti principalmente da polveri ghiacciate o silicati e possono ospitare minuscoli satelliti pastore la cui gravità ne delinea la forma e ne conserva la struttura. Sebbene l'origine degli anelli planetari non sia nota con certezza, si crede che derivino da un satellite naturale che ha sofferto un grosso impatto oppure siano il risultato piuttosto recente della disgregazione di un satellite naturale, distrutto dalla gravità del pianeta dopo aver oltrepassato il limite di Roche.
Nessuna caratteristica secondaria è stata osservata attorno agli esopianeti fino scoperti, anche se si ipotizza che alcuni di questi, in particolare i giganti più massicci, potrebbero ospitare uno stuolo di esosatelliti simili a quelli che orbitano attorno a Giove. Tuttavia si crede che la sub-nana bruna Cha 110913-773444, classificata come un pianeta interstellare, sia circondata da un disco da cui in futuro potrebbero avere origine dei piccoli pianeti o satelliti.
Il modello maggiormente accettato dalla comunità scientifica per spiegare la formazione dei sistemi planetari è il modello della nebulosa solare, formulato originariamente, come arguibile dal nome, per spiegare la formazione del sistema solare.
In accordo con il modello standard della formazione stellare, la nascita di una stella avviene attraverso il collasso di una nube molecolare, il cui prodotto è la protostella. Non appena la stella nascente conclude la fase protostellare e fa ingresso nella pre-sequenza principale (fase di T Tauri), il disco che ne ha mediato l'accrescimento diviene protoplanetario; la sua temperatura diminuisce, permettendo la formazione di piccoli grani di polvere costituiti da roccia (in prevalenza silicati) e ghiacci di varia natura, che a loro volta possono fondersi tra loro per dar luogo a blocchi di diversi chilometri detti planetesimi. Se la massa residua del disco è sufficientemente grande, in un lasso di tempo astronomicamente breve (100 000–300 000 anni) i planetesimi possono fondersi tra loro per dar luogo a embrioni planetari, detti protopianeti, i quali, in un arco temporale compreso tra 100 milioni e un miliardo di anni, vanno incontro a una fase di violente collisioni e fusioni con altri corpi simili; il risultato sarà la formazione, alla fine del processo, di alcuni pianeti terrestri.
La formazione dei giganti gassosi è invece un processo più complicato, che avverrebbe al di là della cosiddetta "frost line" (chiamata in letteratura anche "limite della neve"). I protopianeti ghiacciati posti oltre questo limite possiedono una massa superiore e sono in maggior numero rispetto ai protopianeti esclusivamente rocciosi. Non è completamente chiaro cosa succeda in seguito alla formazione dei protopianeti ghiacciati; sembra tuttavia che alcuni di questi, in forza delle collisioni, crescano fino a raggiungere una massa superiore alle 10 masse terrestri – M – (secondo recenti simulazioni si stima 14-18), necessaria per poter innescare un fenomeno di accrescimento, simile a quello cui è andata incontro la stella ma su scala ridotta, a partire dall'idrogeno e dall'elio che sono stati spinti nelle regioni esterne del disco dalla pressione di radiazione e dal vento della stella neonata. L'accumulo di gas da parte del nucleo protopianetario è un processo inizialmente lento, che prosegue per alcuni milioni di anni fino al raggiungimento di circa 30 M, dopo di che subisce un'imponente accelerazione che lo porta in breve tempo (poche migliaia di anni) ad accumulare il 90% di quella che sarà la sua massa definitiva: si stima che pianeti come Giove e Saturno abbiano accumulato la gran parte della loro massa in appena 10 000 anni. L'accrescimento si conclude all'esaurimento dei gas disponibili; successivamente il pianeta subisce, a causa della perdita di momento angolare dovuta all'attrito con i residui del disco, un decadimento dell'orbita che risulta in un processo di migrazione planetaria, più o meno accentuato a seconda dell'entità dell'attrito; questo spiega come mai in alcuni sistemi extrasolari siano stati individuati dei giganti gassosi a brevissima distanza dalla stella madre, i cosiddetti pianeti gioviani caldi ("Hot Jupiters"). Si ritiene che i giganti ghiacciati, come Urano e Nettuno, costituiscano dei "nuclei falliti", formatisi quando oramai gran parte dei gas erano stati esauriti.
I protopianeti che non sono stati inglobati dai pianeti son potuti diventare loro satelliti, in seguito a un processo di cattura gravitazionale, o hanno mantenuto un'orbita eliosincrona raggruppati in fasce con altri oggetti simili, diventando pianeti nani o altri corpi minori.
Gli impatti con i planetesimi, così come il decadimento radioattivo dei loro costituenti, hanno riscaldato i pianeti in formazione, causandone una parziale fusione. Ciò ha permesso che il loro interno si sia differenziato conducendo alla formazione di un nucleo più denso, di un mantello e di una crosta (si veda anche il paragrafo Differenziazione interna). Nel processo, i pianeti terrestri, più piccoli, hanno perduto la maggior parte della loro atmosfera; i gas perduti sono stati in parte reintegrati da quelli eruttati dal mantello e dagli impatti di corpi cometari. I pianeti più piccoli in seguito hanno continuato a perdere la propria atmosfera attraverso vari meccanismi di fuga.
È importante notare che esistono dei sistemi planetari estremamente diversi dal sistema solare: sono stati scoperti, ad esempio, sistemi planetari intorno a pulsar; in merito a questi ultimi non vi sono ancora teorie certe sulla loro formazione, ma si pensa che possano originarsi a partire da un disco circumstellare costituitosi dai materiali espulsi dalla stella morente durante l'esplosione in supernova.
Si è scoperto inoltre che la metallicità, ovvero l'abbondanza di elementi più pesanti dell'elio, è un parametro importante nel determinare se una stella possegga o meno pianeti: si ritiene che sia meno probabile che una stella povera di metalli, appartenente alla popolazione stellare II, possa essere circondata da un sistema planetario articolato, mentre le probabilità aumentano per le stelle ricche di metalli, appartenenti alla popolazione stellare I.
Ogni pianeta, pur nella propria unicità, condivide con gli altri delle caratteristiche comuni; alcune di queste, come la presenza di anelli o satelliti naturali, sono state osservate solo nel sistema solare; altre invece, quali l'atmosfera, sono comuni anche ai pianeti extrasolari.
Gli otto pianeti che, in base alla definizione ufficiale del 24 agosto 2006, compongono il sistema solare, in ordine di distanza crescente dal Sole, sono:
Dal 1930 al 2006 era considerato pianeta anche Plutone (), che possiede cinque satelliti naturali: Caronte, Notte, Idra, Cerbero; il quinto satellite, Stige, è stato scoperto dal telescopio spaziale Hubble l'11 luglio 2012.
Nel 2006 Plutone è stato riclassificato come pianeta nano.
Tutti i pianeti del sistema solare (eccetto la Terra) possiedono nomi derivati dalla mitologia romana; al contrario, i nomi dei principali satelliti naturali sono derivati da quelli di divinità o personaggi della mitologia greca (a eccezione di quelli di Urano, che portano nomi di personaggi delle opere di Shakespeare e Pope).
Gli asteroidi, al contrario, possono essere battezzati, a discrezione del loro scopritore e con l'approvazione dell'UAI, con un nome qualunque.
Non sono ancora chiare le convenzioni di nomenclatura che verranno adottate per la categoria dei pianeti nani.
I pianeti del sistema solare, secondo la loro composizione, possono essere divisi in pianeti terrestri e pianeti gioviani.
I pianeti di tipo terrestre si trovano nel sistema solare interno e sono costituiti principalmente da roccia (da cui il nome alternativo di "pianeti rocciosi"). Il termine deriva direttamente dal nome del nostro pianeta, per indicare quei pianeti simili alla Terra. Essi sono caratterizzati da una temperatura superficiale relativamente alta, dovuta alla vicinanza del Sole, assenza o basso numero di satelliti naturali, con un'atmosfera molto sottile se confrontata a quella dei giganti gassosi. Raggiungono dimensioni relativamente piccole (meno di 15 000 chilometri di diametro).
Nel sistema solare essi sono quattro:
I pianeti di tipo gioviano sono composti principalmente da gas, donde il nome di "giganti gassosi". Prototipo di tali pianeti è Giove. Essi sono caratterizzati da un elevato valore della massa, che consente loro di trattenere un'estesa atmosfera ricca di idrogeno ed elio, e da dimensioni notevoli. Sono accompagnati da un elevato numero di satelliti naturali e da elaborati sistemi di anelli.
Nel sistema solare sono presenti quattro giganti gassosi:
I pianeti nani sono oggetti celesti orbitanti attorno a una stella e caratterizzati da una massa sufficiente a conferire loro una forma sferoidale (avendo raggiunto la condizione di equilibrio idrostatico), ma che non sono stati in grado di "ripulire" la propria fascia orbitale da altri oggetti di dimensioni confrontabili; da ciò deriva il fatto che i pianeti nani si trovano all'interno di cinture asteroidali. Nonostante il nome, un pianeta nano non è necessariamente più piccolo di un pianeta. Si osservi inoltre che la classe dei pianeti è distinta da quella dei pianeti nani, e non comprende quest'ultima. Inoltre, i pianeti nani posti oltre l'orbita di Nettuno sono detti plutoidi.
L'UAI riconosce cinque pianeti nani:
Il termine "pianetino" e la locuzione "pianeta minore" sono solitamente utilizzate per designare gli asteroidi. Ciò deriva dal fatto che i primi quattro asteroidi scoperti (Cerere – oggi classificato come pianeta nano, Pallade, Giunone e Vesta), furono in effetti considerati dei pianeti veri e propri per circa quarant'anni. Il primo a suggerire di distinguerli dai pianeti fu William Herschel, che propose il termine "asteroide", ovvero "di aspetto stellare", riferendosi al fatto che sono oggetti troppo piccoli perché possa essere risolto il loro disco e, di conseguenza, osservati con un telescopio appaiono come le stelle.
La maggior parte degli astronomi, comunque, preferì continuare a utilizzare il termine pianeta almeno fino alla seconda metà dell'Ottocento, quando il numero degli asteroidi conosciuti superò le cento unità. Allora, diversi osservatori in Europa e negli Stati Uniti iniziarono a riferirsi loro collettivamente come a "pianeti minori", espressione ancora in uso.
La prima scoperta confermata di un pianeta extrasolare è avvenuta il 6 ottobre 1995, quando Michel Mayor e Didier Queloz dell'Università di Ginevra hanno annunciato l'individuazione di un pianeta attorno a 51 Pegasi, nella costellazione di Pegaso. La maggior parte degli oltre 600 pianeti extrasolari scoperti fino a ottobre 2011 hanno masse pari o superiori a quella di Giove. Il motivo di questa apparente difformità nella distribuzione di masse osservata nel sistema solare è dato da un classico effetto di selezione, in virtù del quale i nostri strumenti sono capaci di vedere solo pianeti molto grandi e prossimi alla rispettiva stella madre, perché i loro effetti gravitazionali sono maggiori e più agevoli da individuare.
Tra le eccezioni più rilevanti ci sono tre pianeti orbitanti la pulsar PSR B1257+12, il resto di un'esplosione di supernova. Sono stati individuati, inoltre, circa una dozzina di esopianeti con masse comprese tra le 10 e le 20 masse terrestri (confrontabili dunque con la massa di Nettuno, pari a 17 masse terrestri), come quelli che orbitano intorno alle stelle , 55 Cancri e GJ 436, a cui a volte ci si riferisce chiamandoli appunto "pianeti nettuniani".
Al maggio del 2011 il numero dei pianeti rocciosi individuati supera il centinaio. Essi appartengono, per lo più, alla categoria delle "Super Terre", caratterizzate da una massa superiore a quella della Terra, ma inferiore a quella di Urano e Nettuno. Gliese 876 d, con una massa pari a circa 6 masse terrestri, è stato il primo a essere scoperto, nel 2005. OGLE-2005-BLG-390Lb e MOA-2007-BLG-192Lb, mondi glaciali, sono stati scoperti attraverso l'effetto delle microlenti gravitazionali, COROT-Exo-7b, un pianeta con un diametro stimato in circa 1,7 volte quello della Terra (la cui scoperta fu annunciata con grande enfasi nel 2009), ma che orbita attorno alla sua stella alla distanza di 0,02 UA e ciò determina che sulla sua superficie si raggiungano temperature di 1 500 °C e due pianeti in orbita attorno a una vicina nana rossa, Gliese 581.
Di particolare interesse è il sistema planetario in orbita attorno alla nana rossa Gliese 581, composto da sei pianeti, due dei quali non confermati. Gliese 581 d ha una massa pari a circa 7,7 volte quella della Terra, mentre Gliese 581 c è cinque volte la Terra e al momento della sua scoperta si pensò che fosse il primo pianeta terrestre extrasolare scoperto in prossimità della zona abitabile di una stella. Tuttavia, studi più approfonditi hanno rivelato che il pianeta è leggermente troppo vicino alla sua stella per essere abitabile, mentre Gliese 581 d, sebbene sia molto più freddo della Terra, potrebbe esserlo, se la sua atmosfera contenesse una quantità sufficiente di gas serra. Gliese 581 g, se confermato, sarebbe il primo pianeta scoperto nella zona abitabile della propria stella.
Il 2 febbraio 2011 la NASA ha diffuso una lista di 1235 probabili pianeti extrasolari individuati attraverso il telescopio spaziale Kepler. Essa comprende 68 possibili pianeti di dimensioni simili alla Terra (R < 1,25 R) e altre 288 possibili super Terre (1,25 R < R < 2 R). Inoltre, 54 probabili pianeti sono stati individuati nella zona abitabile del loro sistema, sei dei quali hanno dimensioni inferiori al doppio di quelle terrestri.
È probabile che alcuni pianeti fin qui scoperti non siano molto simili ai giganti gassosi del Sistema solare, perché ricevono un quantitativo di radiazione stellare molto superiore rispetto a essi, dal momento che presentano orbite circolari ed estremamente vicine alle proprie stelle. Corpi di questo tipo sono noti con l'appellativo di pianeti gioviani caldi ("Hot Jupiters"). Potrebbero esistere, inoltre, dei pianeti gioviani caldi (indicati come pianeta ctonii) che orbitano tanto vicini alla propria stella da aver perduto la propria atmosfera, soffiata via dalla radiazione stellare. Sebbene siano stati individuati dei processi di dissoluzione dell'atmosfera su numerosi pianeti gioviani caldi, al 2009 non è stato individuato alcun pianeta che possa essere qualificato come ctonio.
L'individuazione di un numero maggiore di pianeti extrasolari e una loro migliore conoscenza richiede la costruzione di una nuova generazione di strumenti. Il programma COROT, del CNES, in collaborazione con l'Agenzia Spaziale Europea, e Kepler della NASA sono le principali missioni spaziali attualmente operative. È prevista per la primavera del 2011 l'entrata in funzione del telescopio "Automated Planet Finder", che farà parte dell'Osservatorio Lick. Le principali agenzie spaziali hanno allo studio diversi progetti che prevedono la creazione di una rete di telescopi spaziali per l'individuazione di pianeti delle dimensioni della Terra, anche se il loro finanziamento rimane ancora incerto.
La probabilità dell'occorrenza dei pianeti terrestri è una delle variabili dell'equazione di Drake, che cerca di stimare il numero di civiltà extraterrestri evolute presenti nella nostra Galassia.
Un pianeta interstellare è un corpo celeste avente una massa equivalente a quella di un pianeta ("planemo"), ma non legato gravitazionalmente a nessuna stella: questi corpi celesti si muovono dunque nello spazio interstellare come oggetti indipendenti da qualsiasi sistema planetario, il che giustifica l'appellativo di "pianeta orfano" attribuito a volte, in maniera alternativa, a questo tipo di oggetti.
Sebbene siano state annunciate diverse scoperte di questi oggetti, nessuna di esse è stata finora confermata. La comunità scientifica, inoltre, dibatte sull'opportunità di considerarli o meno pianeti; alcuni astronomi hanno suggerito infatti di chiamarli sub-nane brune. La differenza principale tra i due oggetti starebbe nel processo che ha condotto alla loro formazione: una sub-nana bruna si forma dalla contrazione di una nube di gas e polveri, in maniera simile a quanto avviene per una stella o una nana bruna; un pianeta, invece, dall'accrescimento di gas e polveri intorno a un embrione planetario orbitante all'interno di un disco circumstellare, con un processo analogo a quello descritto precedentemente (si veda a tal proposito il paragrafo Formazione dei pianeti e dei sistemi planetari). Successivamente, il pianeta verrebbe espulso nello spazio interstellare in seguito a instabilità dinamiche proprie dei sistemi planetari neoformati, come è stato suggerito da diverse simulazioni computerizzate.
L'Unione Astronomica Internazionale non è entrata nel merito della diatriba, salvo indicare, in una dichiarazione del 2003, che "gli oggetti vaganti in giovani ammassi stellari con valori della massa inferiori al valore della massa limite per la fusione termonucleare del deuterio non sono "pianeti", ma sono "sub-nane brune" (o qualunque altro nome sarà ritenuto appropriato)." Va notato come la definizione data si riferisca espressamente a oggetti vaganti in giovani ammassi stellari.
Un "pianeta ipotetico" è un pianeta o corpo planetario la cui esistenza è ritenuta possibile ma non è stata confermata da dati empirici.
Diversi corpi planetari rientrano in questo novero; non di meno, vi sono state nel passato o vi sono tutt'oggi credenze occasionali pseudoscientifiche, teorie complottiste o gruppi religiosi volti ad accettare tali ipotesi come scientifiche e fondate. Si distinguono dai pianeti immaginari della fantascienza per il fatto che questi gruppi credono nella loro reale esistenza. Esempi di questi pianeti ipotetici sono Antiterra, Lilith, Kolob e il Pianeta X.
In altri casi, l'esistenza di pianeti ipotetici è stata postulata come possibile spiegazione di fenomeni astronomici osservati nel sistema solare, al momento della loro scoperta. Successivamente, il miglioramento delle conoscenze astronomiche ha condotto alla smentita della loro esistenza.
Infine, lo studio dei meccanismi di formazione dei sistemi planetari e l'osservazione dei pianeti extrasolari finora scoperti ha portato a ipotizzare l'esistenza di nuove classi di pianeti quali: i pianeti oceano, la cui superficie sarebbe ricoperta da un oceano profondo centinaia di chilometri; pianeti di carbonio, che potrebbero essersi formati a partire da dischi protoplanetari ricchi dell'elemento e poveri di ossigeno; pianeti ctoni, l'ultimo stadio di un pianeta gioviano caldo tanto prossimo alla propria stella da essere privato della caratteristica atmosfera.
Per pianeti immaginari si intendono tutti i luoghi genericamente abitabili di carattere astronomico, completamente inventati o ridescritti immaginariamente a partire da quelli realmente esistenti che si trovano in opere letterarie, cinematografiche e d'animazione. Non costituiscono quindi un pianeta ipotetico, perché i lettori non credono nella loro reale esistenza.
L'esplorazione di altri pianeti è un tema costante della fantascienza, specie in relazione al contatto con forme di vita aliene. Durante le prime fasi dello sviluppo della fantascienza, Marte rappresentò il pianeta più frequentemente utilizzato e romanzato del nostro sistema solare; le sue condizioni in superficie erano ritenute le più favorevoli alla vita.
Gli scrittori nelle loro opere hanno creato migliaia di pianeti immaginari. Molti di questi sono quasi indistinguibili dalla Terra. In questi mondi, le differenze rispetto alla Terra sono prevalentemente di tipo sociale; altri tipici esempi sono i pianeti prigione, le culture primitive, gli estremismi politici e religiosi, e così via.
Pianeti più insoliti e descrizioni più accurate dal punto di vista fisico si possono trovare soprattutto nelle opere di fantascienza hard o classica; tipici esempi sono quelli che presentano su gran parte della loro superficie un unico ambiente climatico, ad esempio i pianeti desertici, i mondi acquatici, artici o interamente ricoperti da foreste.
Alcuni scrittori, scienziati e artisti hanno poi speculato riguardo a mondi artificiali o pianeti-equivalenti.
Alcune delle più celebri serie televisive fantascientifiche, come "Star Trek" e "Stargate SG-1", sono basate sulla scoperta e sull'esplorazione di nuovi pianeti e di civiltà aliene.
</doc>

<doc id="8925" url="https://it.wikipedia.org/wiki?curid=8925" title="Massa (fisica)">
Massa (fisica)
La massa (dal greco: , "máza", torta d'orzo, grumo di pasta) è una grandezza fisica propria dei corpi materiali che ne determina il comportamento dinamico quando sono soggetti all'influenza di forze esterne.
Nel corso della storia della fisica, in particolare della fisica classica, la massa è stata considerata una proprietà intrinseca della materia, rappresentabile con un valore scalare e che si conserva nel tempo e nello spazio, rimanendo costante in ogni sistema isolato. Inoltre, il termine massa è stato utilizzato per indicare due grandezze potenzialmente distinte: l'interazione della materia con il campo gravitazionale e la relazione che lega la forza applicata a un corpo con l'accelerazione su di esso indotta. Tuttavia, è stata verificata l'equivalenza delle due masse in numerosi esperimenti (messi in atto già da Galileo Galilei per primo).
Nel quadro più ampio della relatività ristretta, specialmente in una prospettiva storica, la massa relativistica non è più una proprietà intrinseca della materia, ma dipende anche dallo stato della materia stessa e dal sistema di riferimento in cui viene osservata. Il concetto di massa relativistica non è centrale alla teoria, al punto che alcuni autori la ritengono un concetto fuorviante. Nella relatività ristretta un corpo ha una massa relativistica direttamente proporzionale alla sua energia, tramite la famosa formula "E" = "mc"². È possibile invece definire un invariante relativistico, detto massa a riposo o massa invariante, al quale la massa relativistica si riconduce nel caso in cui la particella sia ferma. La massa a riposo è definita in termini dell'energia e dell'impulso della particella ed è la stessa in ogni sistema di riferimento, risultando una grandezza fisica molto più utile della massa relativistica, al posto della quale può essere usata l'energia della particella.
A differenza di spazio e tempo, per cui si possono dare definizioni operative in termini di fenomeni naturali, per definire il concetto di massa occorre fare esplicito riferimento alla teoria fisica che ne descrive significato e proprietà. I concetti intuitivi pre-fisici di "quantità di materia" (da non confondere con "quantità di sostanza", misurata in moli) sono troppo vaghi per una definizione operativa, e fanno riferimento a proprietà comuni, l'inerzia e il peso, che vengono considerati ben distinti dalla prima teoria che introduce la massa in termini quantitativi, la dinamica newtoniana.
Il concetto di massa diventa più complesso al livello della fisica subatomica dove la presenza di particelle elementari con massa (elettroni, quark, ...) e prive di massa (fotoni, gluoni) non ha ancora una spiegazione in termini fondamentali. In altre parole, non è chiaro il perché alcune particelle siano dotate di massa e altre no. Le principali teorie che cercano di dare una interpretazione alla massa sono: il meccanismo di Higgs, la teoria delle stringhe e la gravità quantistica a loop; di queste, a partire dal 4 luglio 2012 grazie all'acceleratore di particelle LHC, soltanto la Teoria di Higgs ha avuto i primi riscontri sperimentali.
Nell'attuale Sistema internazionale di unità di misura (SI) la massa è stata scelta come grandezza fisica fondamentale, cioè non esprimibile in termini di altre grandezze. La sua unità di misura è il chilogrammo, indicato col simbolo kg.
Nel sistema CGS l'unità di massa è il grammo. Nel Regno Unito e negli Stati Uniti viene comunemente usata la libbra (circa 454 g) e la "stone" (letteralmente "pietra", 14 libbre). Altre unità di misura vengono comunemente utilizzate in specifici campi della fisica.
In fisica atomica e fisica della materia vengono comunemente utilizzate le unità di misura di Hartree, basate sulla massa dell'elettrone o l'unità di massa atomica, equivalente grossomodo alla massa di un protone. In chimica si usa frequentemente la mole che, pur non essendo una unità di massa, vi è legata da un semplice fattore di proporzionalità.
In fisica nucleare e sub-nucleare è comune l'utilizzo dell'unità di massa atomica. Tuttavia, soprattutto nel campo delle alte energie, si usa esprimere la massa (a riposo o invariante) tramite la sua energia equivalente E = mc². L'energia viene a sua volta espressa in eV. Per esempio un elettrone ha una massa di circa
L'elettrone ha quindi una massa a riposo equivalente a 0,511 MeV. Negli esperimenti di fisica sub-nucleare l'energia cinetica delle particelle studiate è spesso dello stesso ordine di grandezza, il che rende questa scelta di unità di misura particolarmente conveniente.
Le unità di misura della massa, in particolare il chilogrammo e la libbra, vengono talvolta usate anche per misurare una forza. Quest'uso pur essendo tecnicamente scorretto è molto diffuso nell'uso comune e giustificato dal fatto che l'accelerazione di gravità sulla terra ("g") è grossomodo costante. Una forza può quindi essere espressa come massa equivalente tramite la costante di proporzionalità "g". In altre parole, affermare che una forza ha l'intensità di un chilogrammo è equivalente ad affermare che un corpo del peso di un chilogrammo, al livello del mare, sarebbe soggetto a una forza gravitazionale di entità equivalente. Quest'uso non è tuttavia conforme al Sistema Internazionale. Massa e forza sono due grandezze concettualmente distinte, con unità di misura SI diverse, rispettivamente il chilogrammo per la massa e il newton per la forza; ed è bene sottolineare che il peso di un oggetto è una forza, non una proprietà fisica intrinseca dell'oggetto (quale invece è la massa).
In meccanica classica il termine massa si può riferire a tre diverse grandezze fisiche scalari, distinte tra loro:
La massa inerziale e quella gravitazionale sono state sperimentalmente provate come equivalenti, anche se concettualmente sono distinte. I primi esperimenti mirati a stabilire questa equivalenza sono stati quelli di Galileo Galilei.
La massa inerziale "m" di un corpo viene definita nei Principia come quantità di materia legandola al principio di proporzionalità come costante di proporzionalità tra la forza applicata formula_2 e l'accelerazione subita formula_3:
La massa inerziale si può in effetti ottenere "operativamente" misurando l'accelerazione del corpo sottoposto a una forza nota, essendo l'indice della tendenza di un corpo ad accelerare quando è sottoposto a una forza, cioè dell'inerzia del corpo.
Il problema di utilizzare questa proprietà come definizione è che necessita del concetto pregresso di forza; per evitare il circolo vizioso generato da Newton che non specificava lo strumento per misurarla; spesso la forza viene allora definita legandola all'allungamento di una molla che segua la legge di Hooke, definizione chiaramente insoddisfacente in quanto particolare e non generale.
Inoltre questa definizione ha dato origine a diverse problematiche, legate in particolare al sistema di riferimento nel quale si effettua la misura: il concetto di inerzia, come quello di forza, fu infatti storicamente criticato da molti pensatori, tra i quali Berkeley, Ernst Mach, Percy Williams Bridgman e Max Jammer.
Il capitolo più importante per la storia del concetto venne dal tentativo di Mach di eliminare gli elementi metafisici che persistevano nell'edificio della Meccanica classica, riformulando la massa in una definizione chiara divenuta classica anche in quanto da questa prese poi le mosse la Relatività generale, anche se non risolutiva tant'è che Einstein stesso disperò nell'includere il principio di Mach all'interno della Teoria.
Questa si basa invece sul principio di azione-reazione, lasciando che il principio di proporzionalità definisca successivamente la forza. Consideriamo un sistema isolato formato da due corpi (puntiformi) interagenti tra loro. Qualunque sia la forza che agisce fra i due corpi, si osserva sperimentalmente che le accelerazioni subite dai due corpi sono sempre proporzionali e in rapporto costante fra loro:
Ciò che è particolarmente rilevante è che il rapporto formula_6 fra le due accelerazioni istantanee non solo è costante nel tempo, ma non dipende dallo stato iniziale del sistema: è quindi associato a una proprietà fisica intrinseca dei due corpi in esame. Cambiando uno dei due corpi, varia anche la costante di proporzionalità. Supponiamo quindi di utilizzare tre corpi, ed effettuare separatamente tre esperimenti con le tre possibili coppie (assumiamo sempre l'assenza di forze esterne). In questo modo potremo misurare le costanti formula_7 Si noti che per definizione
Confrontando i valori delle costanti osservate, si troverà invariabilmente che questi soddisfano la relazione formula_9 Quindi il prodotto formula_10 non dipende dalla natura del corpo 1, poiché uguale all'inverso di formula_11, vale a dire formula_12, che ne risulta indipendente per via della indipendenza di formula_11. Da questo si ricava che ogni coefficiente formula_14 deve poter essere espresso come prodotto di due costanti, ognuna dipendente solo da uno dei due corpi. Chiamiamo formula_15; ma deve valere identicamente
quindi
in ogni istante, per qualunque coppia di corpi. La quantità "m" che risulta così definita (a meno di un fattore costante, che corrisponde alla scelta dell'unità di misura) è chiamata massa inerziale del corpo: è quindi possibile misurare la massa di un corpo misurando le accelerazioni dovute alle interazioni tra questo e un altro corpo di massa nota, senza bisogno di conoscere quali siano le forze agenti fra i due punti (purché il sistema formato dai due corpi si possa considerare isolato, ossia non soggetto a forze esterne). Il legame tra le masse è dato da:
Consideriamo un corpo, per esempio una palla da tennis. Notiamo che se la palla è lasciata libera in aria, essa è attratta verso il basso da una forza, in prima approssimazione costante, chiamata forza peso. Tramite una bilancia a piatti si può notare che corpi diversi, in generale, sono attratti diversamente dalla forza peso, cioè "pesano" diversamente. La bilancia a piatti si può usare per dare una definizione operativa della massa gravitazionale: si assegna massa unitaria a un oggetto campione e gli altri oggetti hanno una massa pari al numero di campioni necessari a bilanciare i piatti.
La massa gravitazionale passiva è una grandezza fisica proporzionale all'interazione di ciascun corpo con il campo gravitazionale. All'interno dello stesso campo gravitazionale, un corpo con massa gravitazionale piccola sperimenta una forza minore di quella di un corpo con massa gravitazionale grande: la massa gravitazionale è proporzionale al peso, ma mentre quest'ultimo varia a seconda del campo gravitazionale, la massa resta costante. Per definizione, possiamo esprimere la forza peso P come il prodotto della massa gravitazionale "m" per un vettore g, chiamato "accelerazione di gravità", dipendente dal luogo nel quale si effettua la misurazione e le cui unità di misura dipendono da quella della massa gravitazionale. La direzione del vettore g è chiamata "verticale".
Come detto precedentemente, la massa gravitazionale attiva di un corpo è proporzionale all'intensità del campo gravitazionale da esso generata. Maggiore è la massa gravitazionale attiva di un corpo, più intenso è il campo gravitazionale da esso generato, e quindi la forza esercitata dal campo su un altro corpo; per fare un esempio, il campo gravitazionale generato dalla Luna è minore (a parità di distanza dal centro dei due corpi celesti) di quello generato dalla Terra perché la sua massa è minore. Misure di masse gravitazionali attive si possono eseguire, per esempio, con bilance di torsione come quella usata da Henry Cavendish nella determinazione della costante di gravitazione universale.
L'equivalenza tra la massa gravitazionale attiva e quella passiva è una diretta conseguenza del terzo principio della dinamica di Newton: chiamiamo "F" il modulo della forza che il corpo 1 esercita sul corpo 2, "F" il modulo della forza che il corpo 2 esercita sul corpo 1 e "m", "m", "m" e "m" le masse gravitazionali, attive e passive, dei due corpi. Abbiamo:
da cui:
cioè
Data l'arbitrarietà dei corpi, le leggi della meccanica classica stabiliscono la sostanziale equivalenza tra le masse gravitazionali attive e passive; molte verifiche sperimentali si sono aggiunte nel tempo, come per esempio quella di D. F. Bartlett e D. Van Buren del 1986 compiuta sfruttando la diversa composizione della crosta e del mantello lunari, con una precisione sull'uguaglianza del rapporto "massa gravitazionale attiva"/"massa gravitazionale passiva" pari a 4×10.
Da qui in poi le masse gravitazionali attiva e passiva saranno identificate dall'unico termine "massa gravitazionale".
La massa gravitazionale è a tutti gli effetti la carica del campo gravitazionale, esattamente nello stesso senso in cui la carica elettrica è la carica del campo elettrico: essa contemporaneamente "genera" e "subisce gli effetti" del campo gravitazionale. Notiamo che eventuali oggetti con massa gravitazionale nulla ("es. fotoni") non subirebbero gli effetti del campo: in realtà un risultato della relatività generale è che qualunque corpo segue una traiettoria dovuta al campo gravitazionale. Per ulteriori informazioni, vedi il paragrafo riguardante la massa nella relatività generale.
Gli esperimenti hanno dimostrato che la massa inerziale e quella gravitazionale sono sempre proporzionali con la stessa costante di proporzionalità, entro la precisione delle misure effettuate sinora. I primi esperimenti furono condotti da Galileo; si dice comunemente che Galileo ottenne i suoi risultati lasciando cadere oggetti dalla torre di Pisa, ma ciò è probabilmente apocrifo: più verosimilmente studiò il moto di biglie tramite l'uso di piani inclinati. La biografia scritta da Vincenzo Viviani asserisce che Galileo abbia lasciato cadere sfere dello stesso volume ma di materiale diverso, cioè di diversa massa, dalla torre di Pisa, ma fu probabilmente un esperimento mentale che non fu mai eseguito realmente; Galileo usò invece piani inclinati per rallentare la caduta dei corpi.
Supponiamo di avere un oggetto di massa inerziale e gravitazionale rispettivamente "m" ed "m". Se la forza peso è la sola forza agente sugli oggetti la seconda legge di Newton ci fornisce:
da cui:
Un esperimento di verifica dell'equivalenza tra le due definizioni di massa, una volta fissato il luogo (altrimenti potrebbe variare g) potrebbe consistere, per esempio, nel misurare a per diversi corpi cercando eventuali variazioni; in parole povere, verificare se due corpi qualsiasi, cadendo, accelerano nello stesso modo ("universalità della caduta libera", oppure "UFF" dall'inglese "universality of free fall"). Come detto sopra, sperimentalmente non si riscontrano violazioni dell'equivalenza, quindi scegliendo la stessa unità di misura per le due masse il rapporto vale esattamente 1: per ogni corpo non solo massa gravitazionale e massa inerziale hanno le stesse unità di misura, ma sono anche espresse dallo stesso numero. Di conseguenza g è un'accelerazione, e viene chiamata infatti "accelerazione di gravità".
Le verifiche sperimentali dell'equivalenza tra massa inerziale e gravitazionale e dell'UFF sono state effettuate mediante l'uso di piani inclinati (Galileo), pendoli (Newton), fino ad arrivare alle bilance di torsione (Loránd Eötvös).
Attualmente la precisione raggiunta dagli esperimenti è nell'ordine di una parte su 10, precisione ottenuta dalla misurazione della distanza lunare tramite laser. Sono previsti, o comunque in pianificazione, i lanci di diversi satelliti artificiali come STEP ("Satellite Test of the Equivalence Principle"), MICROSCOPE ("Micro-Satellite à traînée Compensée pour l'Observation du Principe d'Equivalence") e Galileo Galilei, che dovrebbero testare l'equivalenza a meno di una parte su 10.
Un pendolo è formato da un lungo filo leggero (di massa trascurabile), vincolato al soffitto, alla cui estremità inferiore sia agganciato un corpo, per esempio una sfera metallica. Una misura del periodo del pendolo fornisce una misura del rapporto tra la massa gravitazionale e la massa inerziale del corpo: ripetendo la misura con corpi di vari materiali, densità e dimensioni è possibile verificare se questo rapporto rimanga costante o no. La misura è tanto più accurata quanto è piccolo l'angolo di oscillazione massimo "θ".
L'equazione del moto del pendolo è data da:
Se "θ" è sufficientemente piccolo abbiamo:
dove ω è la pulsazione del pendolo. Il periodo d'oscillazione è dato da:
da cui:
Sperimentalmente, si osserva che "T" è costante per ogni massa usata, perciò per ogni corpo il rapporto "m" / "m" deve essere costante.
Un esperimento decisamente più accurato fu compiuto da Loránd Eötvös a partire dal 1895
sfruttando la bilancia di torsione la cui invenzione è accreditata a Charles-Augustin de Coulomb nel 1777 (sebbene anche John Michell in maniera del tutto indipendente ne costruì una in periodo antecedente al 1783) e che fu successivamente perfezionata da Henry Cavendish. Una bilancia di torsione è formata da un braccio con due masse uguali alle estremità, vincolato al soffitto tramite un filo di un materiale opportuno (es. quarzo). Applicando una forza alle masse si applica un momento torcente al manubrio: grazie al fatto che la forza peso agente sulle masse ha anche una componente dovuta alla forza centrifuga causata dalla rotazione della terra sul suo asse, è possibile correlare massa inerziale e gravitazionale, che risultano sperimentalmente essere di diretta proporzionalità.
Sia il manubrio inizialmente diretto verso la direzione est-ovest. Definiamo un sistema di riferimento con l'asse x da sud a nord, l'asse y da ovest a est e l'asse z dal basso verso l'alto; α è la latitudine alla quale si svolge l'esperimento. Proiettando le forze gravitazionale e centrifuga sull'asse z abbiamo all'equilibrio:
che si può anche scrivere come:
Se il rapporto tra le masse gravitazionali e le masse inerziali fosse diverso, ciò implicherebbe la diversità delle masse inerziali dei due corpi: ma ciò causerebbe una rotazione sul piano "xy", dovuta alla componente orizzontale della forza centrifuga. I momenti delle forze, proiettati sull'asse orizzontale danno:
Se questa relazione non fosse verificata si avrebbe un momento torcente agente sulla bilancia e di conseguenza una rotazione dell'apparato sperimentale; invertendo le masse si otterrebbe ovviamente una rotazione nel senso opposto. Eötvös non notò nessuna torsione del filo entro gli errori sperimentali, e quindi stabilì l'equivalenza delle masse gravitazionali e inerziali a meno di un fattore nell'ordine di 10 (una parte su un miliardo)
Nella meccanica classica vige la fondamentale legge della conservazione della massa, in varie formulazioni. In generale, dato un volume di controllo "V", fissato, la variazione della massa contenuta in esso è pari al flusso uscente della massa attraverso la frontiera formula_31, cioè attraverso la superficie chiusa che delimita il volume "V", cambiato di segno: in parole povere, la variazione di massa di un sistema è uguale alla massa entrante meno la massa uscente; ciò implica, per esempio, che la massa non può venire né creata né distrutta, ma solo spostata da un luogo a un altro. In chimica, Antoine Lavoisier stabilì nel XVIII secolo che in una reazione chimica la massa dei reagenti è uguale alla massa dei prodotti.
Il principio di conservazione della massa vale con ottima approssimazione nell'esperienza quotidiana, ma cessa di valere nelle reazioni nucleari e, in generale, nei fenomeni che coinvolgono energie relativistiche: in questo caso esso viene incorporato nel principio di conservazione dell'energia (vedi oltre).
Oggetti carichi possiedono una inerzia maggiore rispetto agli stessi corpi scarichi. Ciò si spiega con una interazione delle cariche elettriche in moto con il campo da esse stesse generato, detta "reazione di campo"; l'effetto è interpretabile come un aumento della massa inerziale del corpo ed è ricavabile dalle equazioni di Maxwell.
L'interazione delle cariche elettriche con il campo dipende dalla geometria del sistema: l'inerzia di un corpo carico assume un carattere tensoriale, in contraddizione con la meccanica classica, e bisogna perciò distinguere tra una componente parallela al moto e due componenti trasversali. Si dimostra che si può dividere la massa inerziale di un corpo carico in due componenti, la massa elettromagnetica e la massa "non-elettromagnetica". Mentre la massa elettromagnetica dipende dalla geometria del sistema, la massa non-elettromagnetica ha le stesse caratteristiche "standard" di invarianza della massa inerziale, e a essa si riconduce la massa inerziale se il corpo è scarico.
Il concetto di massa elettromagnetica esiste anche nella teoria della relatività ristretta e nella teoria quantistica dei campi. La massa elettromagnetica ebbe una grande importanza nella storia della fisica a cavallo tra i secoli XIX e XX a causa del tentativo, portato avanti principalmente da Max Abraham e Wilhelm Wien, inizialmente supportato dai lavori sperimentali di Walter Kaufmann, di ricavare la massa inerziale unicamente dall'inerzia elettromagnetica; questa interpretazione dell'inerzia fu però in seguito abbandonata con l'accettazione della teoria della relatività; esperimenti più precisi, eseguiti per la prima volta da A.H. Bucherer nel 1908, mostrarono che le relazioni corrette per la massa longitudinale e la massa trasversa non erano quelle fornite da Abraham, ma quelle di Hendrik Antoon Lorentz ("vedi il paragrafo successivo").
Nella relatività ristretta, il termine "massa" (o massa propria, o massa a riposo) si riferisce solitamente alla massa inerziale di un corpo così come viene misurata nel sistema di riferimento nel quale è in quiete. Anche in questo caso la massa è una proprietà intrinseca di un corpo e l'unità di misura è la stessa, il kilogrammo. Si può ancora determinare la massa di un oggetto come rapporto tra forza e accelerazione, a patto che si faccia in modo che la velocità del corpo sia molto più piccola di quella della luce. Infatti, ad alte velocità, il rapporto tra la forza impressa F e l'accelerazione a del corpo dipende in maniera sostanziale dalla sua velocità nel sistema di riferimento scelto, o meglio dal fattore di Lorentz relativo alla velocità alla quale si trova il corpo: in particolare se facciamo tendere la velocità all'infinito, il rapporto diverge. Il legame tra forza F e accelerazione A per un corpo con massa a riposo non nulla formula_32, con velocità "v" lungo l'asse "x" in un sistema di riferimento inerziale ("del laboratorio"), si ricava esprimendo le componenti spaziali della quadriaccelerazione e della quadriforza nel sistema di riferimento del laboratorio:
Sostituendo formula_35, con semplici passaggi si ottengono le seguenti relazioni, dovute a Lorentz:
Se la velocità del corpo è molto minore della velocità della luce "c", i fattori di Lorentz "γ" tendono a 1, perciò la massa a riposo del corpo è proprio equivalente alla massa inerziale.
Storicamente, nell'ambito della relatività ristretta si hanno altre definizioni di massa oltre a quella di "massa a riposo". Definendo massa il rapporto tra quantità di moto e la velocità otteniamo quella che viene indicata con massa relativistica formula_37. Se invece cerchiamo di identificare la massa come rapporto tra forza e accelerazione dobbiamo distinguere tra massa longitudinale formula_38 e massa trasversa formula_39, introdotte dal fisico tedesco Max Abraham; notiamo che questa distinzione tra le componenti della massa è analoga al caso della massa elettromagnetica. Sia la massa relativistica che le masse longitudinale/trasversa non sono considerate buone definizioni di massa in quanto dipendono dal sistema di riferimento nel quale la massa è misurata, e sono oggi in disuso. Utilizzando questi concetti, il sistema di equazioni precedente diventa:
L'energia "E" è definita in relatività ristretta come il prodotto tra la velocità della luce "c" e la componente temporale "P" del quadrimpulso (o quadrivettore quantità di moto). In formule:
dove γ è il fattore di Lorentz relativo alla velocità del corpo. Se misuriamo l'energia di un corpo fermo, chiamata "energia a riposo" "E", otteniamo:
Questa equazione stabilisce una corrispondenza tra massa a riposo di un corpo ed energia: in altri termini, ogni corpo con massa a riposo diversa da zero possiede una energia a risposo "E" dovuta unicamente al fatto di avere massa.
Questa equazione permette inoltre di incorporare il principio di conservazione della massa nel principio di conservazione dell'energia: per esempio l'energia del Sole è dovuta a reazioni termonucleari nelle quali la massa a riposo degli atomi che intervengono nella reazione è maggiore della massa dei prodotti, ma si conserva l'energia totale in quanto il "difetto di massa" viene convertito in energia (cinetica) e liberato successivamente dai prodotti sotto forma di fotoni e neutrini oppure negli urti con altri atomi.
L'equazione implica di fatto che la massa inerziale totale di un sistema isolato, in generale, non si conserva.
La conservazione della massa in meccanica classica può essere interpretata come parte della conservazione dell'energia quando non si verificano reazioni nucleari o subnucleari, che implicano variazioni significative della somma delle masse a riposo del sistema; al contrario, data la piccolezza del difetto di massa nei legami chimici, la massa è praticamente conservata nelle reazioni chimiche.
Nella meccanica relativistica abbiamo una relazione notevole che lega massa a riposo di un corpo, la sua energia e la sua quantità di moto. Dalla definizione di energia abbiamo:
dove γ è il fattore di Lorentz. Le componenti spaziali "P" del quadrimpulso sono invece:
D'altra parte il vettore è uno scalare "m" per una quadrivelocità: la norma quadra di un tale quadrivettore vale sempre -m²c², perciò, chiamando "p" la norma euclidea del vettore tridimensionale quantità di moto (cioè l'intensità dell'usuale quantità di moto moltiplicata per il fattore γ):
Sostituendo nell'ultima equazione quelle precedenti otteniamo l'equazione cercata:
Da questa equazione si nota come anche particelle con massa nulla possano avere energia/quantità di moto diverse da zero. Nella meccanica classica invece una forza piccola a piacere produrrebbe un'accelerazione infinita su una ipotetica particella di massa nulla ma la sua energia cinetica e quantità di moto resterebbero pari a zero. Invece all'interno della relatività ristretta quando "m = 0", la relazione si semplifica in:
Per esempio, per un fotone si ha formula_48, dove ν è la frequenza del fotone: la quantità di moto del fotone è quindi pari a:
La meccanica classica si limita a prendere atto della proporzionalità tra massa inerziale e massa gravitazionale come fenomeno empirico ma tenendo queste due grandezze ben distinte e separate. Solo con la teoria della relatività generale si ha una unificazione dei due concetti, risultato che, secondo Albert Einstein, dà ""alla teoria generale della relatività una tale superiorità rispetto alla meccanica classica che tutte le difficoltà che si incontrano nel suo sviluppo vanno considerate ben poca cosa"".
Uno dei principi sui quali si basa la relatività generale è il principio di equivalenza. Nella sua versione "forte", esso afferma che in un campo gravitazionale è sempre possibile scegliere un sistema di riferimento che sia localmente inerziale, cioè che in un intorno sufficientemente piccolo del punto le leggi del moto assumono la stessa forma che avrebbero in assenza di gravità. È facile verificare che questo principio implica il principio di equivalenza debole, che sancisce proprio l'equivalenza tra massa inerziale e gravitazionale: infatti supponiamo di avere due corpi sottoposti unicamente alla forza di gravità (supponiamo che siano abbastanza vicini da poter trascurare eventuali variazioni del campo gravitazionale).
Se la massa inerziale e quella gravitazionale dei due corpi fossero diverse, esse subirebbero accelerazioni diverse, ma allora sarebbe impossibile trovare un sistema di riferimento nel quale viaggino entrambe di moto rettilineo uniforme, cioè in condizione di assenza di forze.
Un celebre esperimento mentale che si basa sull'equivalenza tra la massa inerziale e quella gravitazionale è quello dell'ascensore di Einstein. In una delle versioni di questo esperimento, una persona si trova all'interno di una cabina chiusa, senza la possibilità di osservare l'esterno; lasciando cadere una palla, osserva che cade con una accelerazione "g" = 9,81 m/s². Schematizzando, ciò può essere dovuto a due motivi:
Einstein diede molta importanza al fatto che l'osservatore non possa decidere, dal suo punto di vista, quale delle due situazioni si verifichi realmente: ciò determina una sostanziale equivalenza tra i sistemi di riferimento accelerati e quelli sottoposti alla forza di gravità. Questo esperimento mentale è una delle linee-guida che hanno portato Albert Einstein alla formulazione della teoria della relatività generale, tramite una rivisitazione del principio d'inerzia: infatti i corpi liberi non percorrono sempre delle rette, ma delle geodetiche nello spazio-tempo, curvato dalla presenza di masse. Si noti che in uno spazio-tempo piatto, cioè nel quale vige la metrica di Minkowski, in assenza di forze gravitazionali, le geodetiche sono proprio rette e ci si riconduce quindi al principio d'inerzia newtoniano.
Sul finire degli anni trenta si è capito che l'unione della meccanica quantistica con la relatività ristretta doveva portare allo sviluppo di teorie fisiche delle interazioni elementari in termini di campi quantizzati. In questa rappresentazione le particelle elementari sono descritte come eccitazioni quantizzate dello stato di vuoto, che può contenere un numero intero di particelle e/o antiparticelle di ogni tipo, create e distrutte nelle interazioni fra i campi. Il formalismo necessario a questo salto concettuale è contenuto nella procedura della seconda quantizzazione.
In prima quantizzazione, l'evoluzione dei campi relativistici è governata da varie equazioni, analoghe dell'equazione di Schrödinger, la cui forma dipende dai gradi di libertà e dal tipo di particelle che sono descritte. Ad esempio un campo scalare soddisfa l'equazione di Klein-Gordon:
e descrive i bosoni di spin nullo; l'equazione di Dirac:
descrive invece i fermioni di spin 1/2. Le soluzioni di queste equazioni soddisfano esattamente la relazione di dispersione fra energia e momento richiesta dalla relatività ristretta:
Nonostante questo, la probabilità per una particella di spin nullo di propagarsi al di fuori del cono luce è non nulla, sebbene esponenzialmente decrescente. Per risolvere questa e altre inconsistenze si rese necessario lo sviluppo della teoria di campo quantistica.
Nell'ambito delle teorie di campo, e quindi della seconda quantizzazione, la situazione è più complicata a causa del fatto che le particelle fisiche sono descritte in termini di campi e interagiscono tra di loro attraverso lo scambio di particelle virtuali. Per esempio, nell'elettrodinamica quantistica, un elettrone ha una probabilità non nulla di emettere e riassorbire un fotone, oppure un fotone può creare una coppia elettrone-positrone che a loro volta, annichilendosi, formano un fotone identico all'originale. Questi processi sono inosservabili direttamente, ma producono effetti sulla misura delle "costanti" delle teorie fisiche che dipendono dalla scala di energie a cui queste stesse costanti vengono misurate. Ad esempio, in una teoria asintoticamente libera, come la cromodinamica quantistica per le interazioni nucleari forti, la massa dei quark tende a decrescere logaritmicamente con l'aumentare dell'energia. Questa dipendenza dalla scala delle masse e delle costanti di accoppiamento è il principale risultato ottenuto dalla teoria della rinormalizzazione.
La predizione teorica del bosone di Higgs nasce dal fatto che alcune particelle mediatrici di forza sono massive e per descriverle consistentemente con le procedure della rinormalizzazione, la relativa teoria deve essere invariante rispetto alle simmetrie interne di gauge. È facile mostrare che le lagrangiane contenenti termini espliciti di massa (come quelli con la "m" nelle equazioni del moto del paragrafo precedente) rompono la simmetria di gauge. Per ovviare a questo problema si introduce un campo, detto campo di Higgs, accoppiato agli altri campi (fermioni e campi di gauge) in modo da fornire, sotto determinate ipotesi, un termine di massa che mantenga la simmetria del sistema sotto trasformazioni interne. Il meccanismo di Higgs è il metodo più semplice di dare massa alle particelle in modo completamente covariante, e il bosone di Higgs è stato a lungo considerato il "tassello mancante" del modello standard. Una particella consistente con il bosone di Higgs è stata infine scoperta nel 2012 dagli esperimenti ATLAS e CMS presso l'acceleratore LHC presso il CERN. A rigor di termini, il meccanismo di Higgs è l'accoppiamento necessario a dare massa ai bosoni vettori W e Z, mentre la massa dei leptoni (elettroni, muoni, tauoni) e dei quark, ovverosia dei fermioni, è regolata dalla interazione di Yukawa; si noti che gli accoppiamenti del bosone di Higgs con i fermioni non sono calcolabili da principi primi, ma sono anch'essi numeri introdotti "ad hoc" nelle equazioni.
Articoli:
</doc>

<doc id="8997" url="https://it.wikipedia.org/wiki?curid=8997" title="Lavoro (fisica)">
Lavoro (fisica)
In fisica, il lavoro è l'energia scambiata tra due sistemi quando avviene uno spostamento attraverso l'azione di una forza, o una risultante di forze, che ha una componente non nulla nella direzione dello spostamento. Pertanto, ha le dimensioni di una forza applicata lungo una determinata distanza.
Dunque, il lavoro complessivo esercitato su un corpo è pari alla variazione della sua energia cinetica. In presenza, invece, di un campo di forza conservativo, cioè in assenza di effetti dissipativi, il lavoro svolto è pari alla variazione di energia potenziale tra gli estremi del percorso. Il lavoro compiuto da una forza è nullo se lo spostamento è nullo o se questa non ha componenti lungo la direzione dello spostamento.
Il termine "lavoro" è stato introdotto nel 1826 dal matematico francese Gaspard Gustave de Coriolis. Il simbolo più utilizzato per indicare il lavoro è la lettera formula_1, dall'inglese "work", anche se, spesso, nella letteratura italofona lo si indica con la lettera formula_2.
Nel Sistema Internazionale l'unità di misura del lavoro è il joule.
Il lavoro lineare elementare di un campo vettoriale, come una forza formula_3, associato allo spostamento elementare formula_4 è definito come:
che in termini di coordinate cartesiane, si può esprimere come:
Di conseguenza, il lavoro totale lungo una curva formula_7 è definito come l'integrale di linea di seconda specie dell'1-forma differenziale formula_8:
ovvero l'integrale di linea del campo vettoriale formula_3 lungo la curva formula_7.
Nel caso di un corpo che ruota, il lavoro elementare può essere espresso in funzione del momento meccanico, calcolato attorno a un polo formula_12, con formula_13 come braccio:
Pertanto, il lavoro totale del momento corrispondente ai due spostamenti angolari formula_15 e formula_16 è formalmente uguale all'integrale:
Dalla definizione di integrale curvilineo, si hanno le seguenti conseguenze immediate:
Per la proprietà di linearità dell'operatore integrale si ha che:
Per il teorema dell'energia cinetica, il lavoro compiuto dalla risultante delle forze agenti su un corpo è uguale alla variazione della sua energia cinetica: formula_27.
In generale, a causa della generalità del campo formula_28, che varia da punto a punto, il lavoro dipende dalla traiettoria per andare da formula_29 a formula_30. Vi sono però casi di notevole rilevanza fisica nei quali è possibile limitarsi a forze per le quali il lavoro non dipende dalla traiettoria seguita ma solo dalle posizioni iniziale e finale della traiettoria.
Nel caso di un campo di forza conservativo il lavoro è la variazione di energia potenziale tra gli estremi del percorso. In questo caso il lavoro non dipende dal particolare cammino seguito, ma solo dalla posizione iniziale formula_31 e dalla posizione finale formula_32.
A partire dal lavoro è possibile definire la conservatività di un campo di forze: il campo è conservativo "se e solo se" il lavoro della forza lungo una traiettoria chiusa qualsiasi è zero. Infatti:
Nel caso in cui la forza sia costante e il percorso rettilineo, il lavoro diventa pari al prodotto scalare del vettore forza per il vettore spostamento formula_45:
dove formula_47 l'angolo tra la direzione della forza e la direzione dello spostamento.
Il lavoro può essere sia positivo sia negativo, il segno dipende dall'angolo "formula_47" compreso tra il vettore forza formula_49 ed il vettore spostamento formula_45.
Il lavoro svolto dalla forza è positivo seformula_51ovvero se formula_52. Un lavoro positivo è causato da una forza detta "motrice", uno negativo (formula_53), invece, da una forza "resistente".
Il termine utilizzato in fisica differisce dalla definizione usuale di lavoro, che è decisamente legata all'esperienza quotidiana e si può ricondurre, ad esempio, alla fatica muscolare. Infatti si compie un lavoro se si ha uno spostamento: se per esempio si spinge contro un muro naturalmente esso rimarrà fermo e non si avrà lavoro.
Quando la forza ha la stessa direzione dello spostamento, il prodotto scalare equivale al prodotto aritmetico dei moduli dei due vettori:
Anche nel caso di forza parallela ma opposta allo spostamento, l'espressione del lavoro si riduce al prodotto aritmetico dei moduli, ma con segno opposto:
Quando forza e spostamento sono perpendicolari, il lavoro è nullo:
Per i campi conservativi è possibile definire una funzione scalare, detta energia potenziale, la cui variazione tra i punti formula_31 e formula_32 rappresenta il lavoro compiuto dalle forze per andare da formula_29 a formula_30, per quanto detto prima lungo un qualunque percorso.
si indica formula_62 perché, per convenzione, si considera solitamente la variazione di qualcosa dal punto finale a quello iniziale, cioè formula_63.
Il concetto continua a valere se formula_64 non dipende dalla "posizione" ma da uno "stato", ovvero da una posizione nello spazio delle fasi del sistema: ovviamente sostituendo formula_65 con l'equivalente nel caso in questione. Un esempio è il diagramma pressione/volume usato per le macchine termiche.
Considerando campi conservativi, dal teorema dell'energia cinetica (formula_66), si ha che la variazione di energia potenziale è contraria alla variazione di energia cinetica:
e quindi la somma dell'energia cinetica e dell'energia potenziale, detta energia meccanica, è costante in virtù del teorema dell'energia meccanica.
ovvero
L'esempio classico di campi non conservativi si ha considerando le forze d'attrito: l'attrito si oppone sempre al moto, quindi lungo qualsiasi traiettoria avremo l'integrale di una funzione costantemente negativa. E il risultato sarà un lavoro costantemente negativo anche lungo traiettorie chiuse.
Avremmo un lavoro pari a zero, e quindi un campo conservativo solo se l'attrito fosse zero lungo tutto il percorso, solo se, cioè, non avessimo attriti.
Scomponendo, nel teorema dell'energia cinetica, il lavoro in due addendi: formula_70 quello derivante da forze conservative, pari alla variazione di energia potenziale, e quello derivante da forze non conservative abbiamo:
e quindi:
cioè la variazione dell'energia meccanica, cioè la somma di energia cinetica e potenziale, è uguale al lavoro compiuto dalla forze non conservative.
Nel Sistema Internazionale l'unità di misura per il lavoro è il joule che corrisponde allo spostamento di un metro di una forza di un newton:
Tra le altre unità di misura del lavoro ricordiamo:
In termodinamica, il "lavoro" viene scomposto per comodità in due contributi: un contributo relativo alla variazione di volume, detto lavoro di volume, e un contributo indipendente dalla variazione di volume, che prende il nome di lavoro isocoro.
In termodinamica un gas esercita una pressione formula_78 interna sulle pareti del recipiente in cui è contenuto. Se una di queste pareti di superficie formula_79 è mobile e si sposta di una quantità infinitesima formula_80 sotto l'azione di questa pressione, allora il lavoro infinitesimo compiuto dal gas è dato da:
dove formula_82 è la variazione del volume corrispondente. Questo è vero se la trasformazione è reversibile, infatti solo se il sistema è in equilibrio termodinamico è possibile conoscere il valore della pressione formula_78 interna al contenitore. La notazione formula_84 è usata per indicare che il lavoro in fisica non è una funzione di stato, ed invece dipende dalla particolare trasformazione eseguita sul sistema: in termini matematici si dice che il lavoro non è, in generale, esprimibile come un differenziale esatto.
Infine, se il sistema termodinamico subisce una trasformazione generica, quindi per lo più irreversibile, allora possiamo ancora quantificare il lavoro fatto dal gas o dal sistema così:
lavoro fatto contro la pressione esterna formula_86.
Sotto il termine di lavoro isocoro si annoverano tutti i tipi di lavoro che non si riflettono in una variazione di volume, ad esempio: il lavoro elettrico, il lavoro di un campo magnetico, oppure il lavoro svolto da una girante.
In un circuito elettrico il lavoro infinitesimo compiuto dalla batteria che genera la differenza di potenziale formula_87 per far circolare una corrente elettrica formula_88 per un tempo infinitesimo formula_89 è data da formula_90, il segno di tale lavoro sarà positivo o negativo a seconda che rispettivamente la pila eroghi o assorba corrente.
Il valore del lavoro elettrico scambiato tra il tempo formula_91 e il tempo formula_92 si può ottenere integrando l'equazione precedente, dalla quale si ottiene:
nel caso in cui la differenza di potenziale "formula_87" rimanga costante durante l'intervallo di tempo considerato, si può scrivere:
essendo:
La forza che agisce su una particella carica in movimento, immersa in un campo magnetico, è detta forza di Lorentz, che in assenza di campi elettrici esterni è data da:
dove formula_100 è la carica della particella, formula_103 è la sua velocità, formula_104 è il vettore di campo magnetico, legati dal prodotto vettoriale.
Se il moto della carica avviene in senso parallelo alle linee di forza del campo magnetico, significa che la forza è ortogonale allo spostamento, dunque il lavoro è nullo.
</doc>

<doc id="2090986" url="https://it.wikipedia.org/wiki?curid=2090986" title="Energia meccanica">
Energia meccanica
L'energia meccanica è la somma di energia cinetica ed energia potenziale attinenti allo stesso sistema, da distinguere dall'energia totale del sistema in cui rientra anche l'energia interna.
Quando due sistemi si scambiano tra loro energia meccanica, tale energia in transito è definita lavoro. Dunque l'energia meccanica può essere posseduta da un sistema e scambiata con altri sistemi, mentre il lavoro corrisponde solamente alla parte di energia meccanica che è scambiata.
Per un sistema scleronomo e in presenza di sole forze conservative si dimostra che l'energia meccanica costituisce un integrale di moto, cioè si conserva, e coincide con l'hamiltoniana meccanica. La dimostrazione più semplice discende direttamente dal teorema dell'energia cinetica: se il lavoro compiuto dalle forze è pari alla variazione di energia cinetica del sistema:
Se le forze sono conservative è possibile esprimere il lavoro come variazione di energia potenziale:
si ottiene che la variazione di energia cinetica più la variazione di energia potenziale è identicamente nulla, cioè:
avendo battezzato la quantità "T+U" energia meccanica totale del sistema.
Un corpo in un campo gravitazionale (conservativo) è dotato di una certa energia potenziale dipendente unicamente dall'altezza rispetto ad un punto di riferimento. Se lo lasciamo libero, in assenza di forze dissipative come l'attrito con l'aria, l'energia potenziale iniziale, a mano a mano che cade, si trasforma in energia cinetica (cresce la velocità) mentre la somma delle due energie rimane la stessa.
Chiamando formula_5 e formula_6 rispettivamente la quota rispetto ad un riferimento fisso e la velocità di un corpo all'istante "t", e formula_7 e formula_8 le stesse quantità all'istante iniziale "t"=0, abbiamo:
cioè
che possiamo scrivere come
Il primo membro della precedente esprime l'energia meccanica totale "T + U" del sistema al tempo "t", che è costante ed uguale all'energia meccanica formula_12 del sistema all'istante "t"=0. Quindi:
Alla fine della caduta, quando il corpo urta il pavimento ed è di nuovo fermo, l'energia cinetica è nuovamente nulla, e poiché anche l'energia potenziale è diminuita, concludiamo che in questo evento l'energia meccanica si sia dissipata (in seguito ad un urto anelastico). In realtà l'energia meccanica "scomparsa" risulta essersi convertita in energia termica e, eventualmente, onde sonore: misurando la temperatura dell'oggetto possiamo infatti riscontrarne un lieve aumento, oltre a notare, nel caso in cui sia presente, una perturbazione dell'eventuale mezzo in cui avviene l'urto. Questo è un fatto generale: le leggi di conservazione della fisica implicano la conservazione dell'energia nei sistemi isolati.
Il "pendolo di Maxwell" fornisce un ottimo esempio del principio di conservazione dell'energia meccanica. Il sistema è costituito da un volano. Due fili sono avvolti nello stesso verso attorno all'asse del volano, mentre le estremità opposte sono collegate ad un sostegno orizzontale. Il volano è "caricato" avvolgendo i fili attorno all'asse, in modo tale che il volano si trovi ad una certa altezza rispetto al piano di riferimento. Se lasciato andare, il volano inizia a scendere ed acquista velocità. Arrivato al punto più basso consentito dallo srotolamento dei fili, il pendolo si riavvolge nel verso opposto e risale. In condizioni ideali, esso tornerebbe alla stessa quota di partenza; tuttavia, per la presenza di attriti con i fili e con il mezzo (l'aria), il moto risulta essere invece smorzato, e dopo un certo numero di oscillazioni il pendolo si ferma nel punto più basso consentito dai fili.
Per determinare il periodo di tale pendolo, ovvero il tempo impiegato dal volano per scendere e risalire, si utilizza il principio di conservazione dell'energia:
ovvero le variazioni di energia cinetica, sia di traslazione che rotazionale, compensano le variazioni di energia potenziale. Avendo preso come asse di riferimento l'asse h diretto verso l'alto e come piano di riferimento quel piano orizzontale sul quale giace il punto più basso raggiunto dal volano, alla massima altezza "h" l'energia è tutta potenziale, mentre nel punto più basso ("h" = 0) l'energia è tutta cinetica. Se h e v sono le generiche altezza e velocità all'istante "t", possiamo esplicitare la conservazione dell'energia:
Se esprimiamo "I", il momento di inerzia del volano, come "kmr", con "k" coefficiente adimensionale, la precedente può essere scritta nel seguente modo (ricordando che "v" = ω "r"):
Deriviamo ambo i membri rispetto al tempo (ricordando di includere tutte le dipendenze temporali e di applicare correttamente la regola di derivazione delle funzioni composte):
La legge oraria di un corpo uniformemente accelerato è data da:
Imponendo che Δh sia pari alla massima estensione del volano (cioè "h"("t") = 0), si ricava il tempo "t" nel quale il volano raggiunge il fondo (il periodo "T" è esattamente il doppio):
Quindi, in definitiva:
Consideriamo un corpo di massa "m" con velocità iniziale formula_21 che urti elasticamente un altro corpo, inizialmente fermo, di massa "M".
Dato che l'urto è elastico, l'energia meccanica dell'intero sistema deve conservarsi. Dato che in un urto agiscono forze impulsive è possibile trascurare le altre forze in gioco (es. gravitazionale), quindi l'energia del sistema è data dalla somma delle energie cinetiche dei corpi. Inoltre, dato che in un urto, per definizione, si considera il sistema come isolato, si conserva la quantità di moto. Chiamando "v" la velocità finale del bersaglio, otteniamo il sistema:
Si ricava facilmente, ricavando "v" dalla seconda equazione e sostituendo nella prima:
dove μ è un coefficiente adimensionale che indica il rapporto tra la velocità finale e quella iniziale. Si ricava immediatamente l'energia cinetica finale del proiettile
ovvero
l'energia cinetica del corpo, dopo l'urto, è uguale a quella iniziale per un coefficiente positivo μ detto "di restituzione".
Non sempre le forze che agiscono su un sistema sono conservative, e non sempre l'energia meccanica, dunque, si conserva. Siano allora F e F rispettivamente la somma di tutte le forze conservative e non conservative. Il lavoro da esse compiuto è allora:
Per il teorema dell'energia cinetica, il lavoro corrisponde alla variazione totale di energia cinetica del sistema:
mentre, essendo F forze conservative, è possibile ad esse associare una funzione potenziale "U" tale che il lavoro di tali forze possa essere espresso come:
In questo modo, sostituendo nell'espressione del lavoro, si ha:
Ora a primo membro si riconosce la variazione di energia meccanica del sistema, prova che le variazioni di energia meccanica di un sistema sono dovute esclusivamente al lavoro compiuto dalle forze non conservative sul sistema.
Un esempio di forza non conservativa, preso dall'esperienza di tutti i giorni, è la forza d'attrito. Sebbene in natura non esistano forze non conservative (a livello microscopico), la forza d'attrito è considerata non conservativa, in primo luogo perché essa, in generale, non è costante, perlomeno in direzione e verso; in secondo luogo perché gli effetti che essa produce (generalmente surriscaldamento delle parti a contatto) non sono conteggiati nel computo dell'energia meccanica. Analogamente, non sono conteggiati i contributi del campo elettromagnetico che produce un lavoro non conservativo e dipendente dallo spostamento.
</doc>

<doc id="6247197" url="https://it.wikipedia.org/wiki?curid=6247197" title="Sistema di riferimento">
Sistema di riferimento
In fisica e geodesia un sistema di riferimento è un sistema rispetto al quale viene osservato e misurato un certo fenomeno fisico o un oggetto fisico oppure vengono compiute determinate misurazioni. La nozione nasce nell'ambito della meccanica classica, in cinematica e dinamica, con la descrizione del moto dei corpi e con la constatazione che il moto è sempre relativo ad un sistema rispetto al quale lo osserviamo.
In fisica la primaria distinzione tra sistemi di riferimento è quella tra sistemi di riferimento inerziali e sistemi di riferimento non inerziali. Dalla teoria della relatività speciale discende il principio secondo il quale le leggi fisiche sono invarianti in tutti i sistemi di riferimento inerziali, cosa che in fisica classica falliva applicando le trasformazioni di Galileo alle equazioni di Maxwell. 
A partire dalla definizione di un sistema di riferimento nell'osservazione di un certo fenomeno è possibile definire o costruire un sistema di coordinate per la misura oggettiva dei parametri fisici o grandezze fisiche di riferimento come ad esempio spazio, tempo, posizione, velocità, accelerazione ecc...
</doc>

<doc id="71428" url="https://it.wikipedia.org/wiki?curid=71428" title="Moto parabolico">
Moto parabolico
Il moto parabolico è un tipo di moto bidimensionale esprimibile attraverso la combinazione di due moti rettilinei simultanei ed indipendenti:
Il moto parabolico può essere descritto mediante le relazioni della cinematica che legano i vettori posizione, velocità, ed accelerazione.
La più significativa realizzazione di tale moto è fornita dal "moto del proiettile" in cui si utilizzano le seguenti semplificazioni (approssimazioni della fisica e della geometria del problema):
Si supponga che un corpo sia lanciato all'istante "t=0" nell'origine "O" di un sistema di coordinate cartesiano "Oxy", e che la velocità iniziale abbia modulo "v" e formi un angolo θ con l'asse "x" orizzontale.
Dalle leggi del moto uniformemente accelerato si ha:
Ipotizzando che il corpo si trovi in prossimità della terra, è possibile considerare la funzione formula_2 come costante, con valore pari a formula_3 diretta lungo la perpendicolare al terreno (asse y), per cui si ha:
Come si può notare dalla formula, la velocità giace sempre nel piano formato dai vettori costanti formula_5 e formula_6, ovvero quello su cui si svolge il moto.
Il vettore velocità può essere scomposto lungo le due componenti "x" e "y":
Dalla relazione precedente, si ricava:
Proiettando le velocità sugli assi si ottengono le componenti:
costante nel tempo, e
da cui, integrando, si ricavano le leggi orarie dei moti lungo gli assi "x" e "y":
La traiettoria viene ricavata eliminando la variabile temporale, ossia, esprimendo il rapporto:
e esplicitando il parametro formula_13 dalla legge oraria formula_14:
In tal modo si arriva all'equazione cartesiana:
da cui, moltiplicando per x ambo i membri, si ottiene
che rappresenta una parabola con concavità rivolta verso il basso, il cui grafico è rappresentato in figura. Inoltre se la posizione del lancio del corpo non si trova nell'origine, quindi ad esempio nel punto formula_18 si può approssimare la curva con una traslazione degli assi paralleli agli assi cartesiani con origine in formula_19 (l'approssimazione è dovuta al fatto che stiamo considerando il corpo in prossimità della terra, ergo g è costante)
La gittata è la distanza percorsa in orizzontale dal corpo prima che tocchi terra.
Se consideriamo la traiettoria espressa in un piano cartesiano Oxy, per calcolare la gittata possiamo utilizzare la funzione y(x) vista sopra. Ci interessa sapere a che coordinata x si ha la coordinata y pari a zero, cioè:
Si tratta di una parabola, ci aspettiamo quindi due soluzioni. Se il corpo parte da terra una delle due soluzioni sarà proprio la posizione di partenza e può essere scartata. Se il corpo non parte da terra, una delle due soluzioni si troverà "dietro" la posizione di partenza e non ha significato fisico. Svolgiamo l'equazione di secondo grado per ottenere la gittata x
Studiamo ora il caso in cui l'altezza di partenza non è zero. Non ci servirà altro che riutilizzare la funzione y(x) aggiungendo la costante formula_22. Svolgiamo quest'altra equazione di secondo grado per ottenere:
formula_23
Sono necessarie varie semplificazioni e trasformazioni, ma in questa forma è facile notare come questo risultato vale sia per un corpo lanciato da terra che per un corpo lanciato da un'altezza data. 
A questo punto è possibile ricavare l'angolo di massima gittata. Fissati formula_24 per un punto lanciato da terra, ci si chiede per quale angolo la gittata è massima. formula_25 ha massimo relativo per l'argomento del seno uguale a formula_26 quindi per formula_27°
Siccome il moto parabolico è simmetrico rispetto all'asse passante per il vertice e parallelo all'asse y ("proprietà della parabola"), l'ascissa del punto di atterraggio è due volte l'ascissa del vertice della parabola, ovvero il doppio dell'ascissa del punto di massima altezza. Tale ascissa è dunque:
Sostituendo nell'equazione della parabola esplicitata precedentemente si ha che:
Gli stessi risultati si ottengono considerando il fatto che il punto di altezza massima è un punto di massimo della curva della traiettoria e quindi il punto di massimo della parabola. Trovarlo quindi consiste nel porre la derivata prima dell'equazione della traiettoria uguale a zero e ricavare dall'equazione ottenuta l'ascissa del punto cercato formula_30(Che sarebbe la gittata) sostituendo nell'equazione della traiettoria si ottiene anche l'ordinata formula_31.
Il tempo di volo è il tempo fra l'istante di lancio e quello di arrivo del corpo, che coincide con il tempo necessario a percorrere il tratto OG con la velocità v:
Un tipico esempio di moto parabolico è quello del proiettile, di cui si occupa la balistica. Un proiettile in volo è sottoposto alla forza di gravità della Terra. Nell'ipotesi di attrito dell'aria trascurabile, il secondo principio della dinamica porta ad un'accelerazione che può essere scomposta nel seguente modo:
Se il proiettile viene sparato con velocità iniziale v secondo un angolo θ, si ottengono le seguenti componenti di velocità:
Le componenti della posizione del proiettile sono quindi:
Il moto lungo l'asse x è quindi uniforme, e quello lungo l'asse y accelerato. Se la velocità iniziale fosse stata pari a zero, il moto sarebbe stato di caduta libera.
</doc>

<doc id="65999" url="https://it.wikipedia.org/wiki?curid=65999" title="Energia potenziale">
Energia potenziale
In fisica, l<nowiki>'</nowiki>energia potenziale di un oggetto è l'energia che esso possiede a causa della sua posizione o del suo orientamento rispetto ad un campo di forze. Nel caso si tratti di un sistema, l'energia potenziale può dipendere dalla disposizione degli elementi che lo compongono. Si può vedere l'energia potenziale anche come la capacità di un oggetto (o sistema) di trasformare la propria energia in un'altra forma di energia, come ad esempio l'energia cinetica.
Il termine "energia potenziale" fu coniato da Rankine nel 1853.
Nel sistema internazionale è misurata in joule ("J").
Si tratta di una funzione scalare delle coordinate dell'oggetto nel sistema di riferimento utilizzato. Dato un campo vettoriale conservativo, l'energia potenziale è la sua capacità di compiere lavoro: il lavoro relativo a una forza che agisce su un oggetto è l'integrale di linea di seconda specie della forza valutato sul cammino compiuto dall'oggetto, e se essa è conservativa il valore di questo integrale non dipende dal tipo di cammino seguito. Quando si ha a che fare con forze conservative si può definire un potenziale scalare definito in tutto lo spazio, usualmente il potenziale è definito come l'energia potenziale fratto la variabile che è responsabile della forza. In particolare, dal punto di vista matematico tale potenziale esiste solo se la forza è conservativa, e del resto si assume che per tutte le forze conservative si può sempre definire fisicamente un'energia potenziale.
L'energia potenziale può essere definita anche per il campo magnetico, che non è conservativo, nelle regioni in cui vi è assenza di correnti elettriche. In tal caso, infatti, il rotore del campo è nullo. L'energia potenziale magnetica di un magnete in un campo magnetico è definita come il lavoro della forza magnetica (il momento meccanico) nel ri-allineare il momento di dipolo magnetico.
Se in una regione di spazio sono presenti una qualche forza e un oggetto che è sensibile alla presenza della forza, l'energia potenziale (associata alla forza) posseduta dall'oggetto è definita come la differenza tra l'energia che esso possiede a causa della forza in una data posizione nello spazio e l'energia posseduta in una posizione scelta come riferimento. Spesso nella posizione scelta come riferimento l'energia potenziale è nulla.
L'energia potenziale è definibile come il lavoro necessario a portare a distanza infinita due molecole, ed è pari a zero quando la distanza tra le molecole è infinita.
Data una forza formula_1, il lavoro formula_2 lungo una curva formula_3 è dato in generale dalla relazione:
che in forma locale si scrive:
Nel caso il campo di forze sia conservativo, il lavoro non dipende dal tipo di percorso compiuto, ma soltanto dall'entità della forza agli estremi del cammino (gli estremi di integrazione): il differenziale formula_6 è allora un differenziale esatto, e il campo conservativo corrisponde (per definizione) al gradiente di un campo scalare, chiamato potenziale. In questo caso, se l'oggetto si sposta da un punto formula_7 a un punto formula_8 la forza esercitata dal campo compie un lavoro pari all'opposto formula_9 della differenza formula_10 tra l'energia potenziale posseduta dall'oggetto nelle due posizioni iniziale e finale:
Il motivo del segno meno, per cui il lavoro è pari all'opposto dell'energia, è il fatto che in questo modo a un lavoro positivo corrisponde una riduzione del potenziale. Poiché è possibile fissare arbitrariamente il livello zero dell'energia potenziale, essa viene definita a meno di una costante additiva. Nel caso più semplice, in cui il moto si svolge in una sola direzione, l'energia potenziale di una forza conservativa è pari a una qualche primitiva della forza, cambiata di segno:
dove formula_13 è la costante additiva. Fissando formula_14 si determina qual è la primitiva, e pertanto si rende necessario imporre delle condizioni al contorno: per le forze nulle all'infinito si utilizza la condizione al contorno di Dirichlet formula_15, detta "condizione di località".
Nel caso tridimensionale, se il dominio è un insieme stellato il lemma di Poincaré fornisce una condizione sufficiente e necessaria affinché nel punto formula_16 la forza sia l'opposto formula_17 del gradiente formula_18 di un potenziale scalare formula_19 (ovvero sia conservativa):
Inoltre, l'integrale si può separare:
dove il punto formula_22 è scelto arbitrariamente e i vettori formula_23, formula_24 e formula_25 sono i versori canonici di formula_26.
formula_32 dove formula_33 è la costante di gravitazione universale e formula_34 la massa del corpo maggiore. In quest'ultima il livello di zero di formula_19 è posto a distanza infinita dal corpo celeste, e di conseguenza i valori di formula_19 sono sempre negativi.
Una forza di posizione agente su un punto materiale qualsiasi dello spazio tridimensionale in un sistema di riferimento viene definita in particolare come:
dove formula_46, formula_49, formula_50 sono le coordinate cartesiane di un generico punto nel riferimento, e agisce su un punto materiale di massa formula_27.
Si nota subito che questa forza è di tipo "non locale", in quanto non è nulla all'infinito:
Il calcolo del lavoro della forza lungo la curva formula_53 parametrizzata da:
avviene tramite un integrale curvilineo, oppure controllando che possa esistere una funzione energia potenziale associata alla forza formula_55. La forza è definita su tutto formula_56. Per il lemma di Poincaré se il campo è irrotazionale esiste una funzione energia potenziale associata. 
Il rotore di formula_55 è:
Il campo è quindi conservativo: ciò significa che il lavoro compiuto dalla forza non dipende dalla traiettoria del corpo. La funzione energia potenziale si calcola nel seguente modo:
Imponendo la condizione di località:
Risulta quindi che il campo di energia potenziale è di tipo "non locale" (come anche la forza che la origina).
Chiamando:
il lavoro compiuto dalla forza lungo la traiettoria formula_53 è funzione dei soli estremi del percorso e pari a:
Come si vede l'imposizione della condizione di località non ha alcun influsso sul lavoro (né l'avrebbe sulla forza). Inoltre, se la forza formula_55 è l'unica forza presente, si conserva l'energia meccanica del sistema formula_66, anche se risulta infinita:
e quindi la conservatività della quantità meccanica:
non dipende dalla condizione di località.
</doc>

<doc id="69468" url="https://it.wikipedia.org/wiki?curid=69468" title="Spostamento (cinematica)">
Spostamento (cinematica)
In cinematica si definisce spostamento il cambiamento di posizione di un punto nello spazio.
Date due posizioni formula_1 e formula_2 dello stesso punto, lo spostamento è dato da:
in pratica è il vettore differenza dei due vettori posizione formula_2 e formula_5 in quanto:
Utilizzando i versori, il vettore spostamento si può ricavare componendo il vettore. Ad esempio in due dimensioni si avrà:
Ovvero abbiamo che la differenza tra due coordinate lungo un asse di riferimento (ad esempio "x") denota un segmento formula_9 che ha un orientamento determinato dal segno della differenza, ovvero un segmento orientato, o anche è un vettore formula_10.
Lo possiamo vedere anche dal fatto che è un prodotto tra uno scalare formula_11 per un vettore formula_12 di lunghezza formula_13, che per il prodotto di uno scalare per un vettore restituisce un vettore lungo formula_14 di lunghezza formula_11 e lungo il verso formula_12.
La composizione dei vettori formula_10 e formula_18
mi dà il vettore spostamento formula_19, come la composizione delle posizioni formula_20, formula_21.
Analogamente in uno spazio a tre dimensioni:
Lo spostamento risulta così indipendente dalla traiettoria effettivamente percorsa per muoversi fra i due punti.
Lo spostamento non è mai tangente alla traiettoria, tranne che nel semplice caso di traiettoria rettilinea.
Sappiamo che la velocità media formula_24 è il rapporto tra lo spostamento, formula_25,
e l'intervallo di tempo impiegato per compierlo formula_26
</doc>

<doc id="69546" url="https://it.wikipedia.org/wiki?curid=69546" title="Posizione">
Posizione
In fisica, la posizione è l'insieme delle quantità misurabili, dette "coordinate", le quali devono essere di numero pari o superiore ai suoi gradi di libertà, che definisce dove si trovi nello spazio un punto rispetto ad un sistema di riferimento.
L'evidenza sperimentale ha finora dimostrato che per determinare la posizione di un punto materiale nello spazio, quindi un oggetto incapace di ruotare su sé stesso, sono sufficienti tre coordinate, pertanto si ritiene che lo spazio che ci circonda sia tridimensionale, almeno fino alla scala delle dimensioni sondabili sperimentalmente. La ricerca di extradimensioni a scale di dimensioni molto piccole è oggetto di studio della fisica delle particelle.
Per determinare la posizione di un punto vincolato su una curva o su una superficie sono sufficienti rispettivamente una o due coordinate.
In meccanica classica, la posizione di un punto nello spazio è determinata dal vettore posizione definito all'interno di un sistema di riferimento, ad esempio cartesiano.
Il vettore posizione è definito da:
Scegliendo come sistema di riferimento il sistema cartesiano il vettore posizione è un vettore applicato nell'origine degli assi.
Si tratta di un segmento orientato OP che spesso si trova rappresentato anche mediante coordinate del punto P da cui si ottiene la rappresentazione dello stesso per componenti, per esempio nella forma
dove formula_8 e formula_9 rappresentano i versori degli assi cartesiani.
Esistono altre forme di rappresentazione del vettore posizione, ad esempio in coordinate cilindriche e sferiche.
Il vettore posizione è legato alla definizione di spostamento, il quale è in corrispondenza biunivoca con la legge oraria dato che se si rappresenta il vettore in funzione del tempo si ha che
formula_10
In relatività ristretta e in relatività generale le tre coordinate che determinano la posizione di un punto nello spazio insieme all'istante preso in considerazione costituiscono le quattro coordinate che determinano un evento nello spaziotempo. Il concetto di vettore posizione in relatività ristretta può essere generalizzato in quadrivettore con l'aggiunta della quarta coordinata temporale. In relatività generale, tuttavia, l'interpretazione della posizione di un evento nello spaziotempo quadridimensionale richiede l'uso di concetti della geometria differenziale che danno allo spaziotempo proprietà diverse rispetto allo spazio euclideo ipotizzato per la meccanica classica.
</doc>

<doc id="73449" url="https://it.wikipedia.org/wiki?curid=73449" title="Sistema di riferimento inerziale">
Sistema di riferimento inerziale
In fisica un sistema di riferimento inerziale è un sistema di riferimento in cui è valido il primo principio della dinamica. Con un'accettabile approssimazione è considerato inerziale il sistema solidale con il Sole e le stelle (il cosiddetto sistema delle stelle fisse), ed ogni altro sistema che si muova di moto rettilineo uniforme rispetto ad esso (e che quindi né acceleri né ruoti): in questo modo si viene a definire una classe di equivalenza per questi sistemi.
Un sistema di riferimento inerziale è un sistema di riferimento caratterizzato dalla seguente condizione: se un punto materiale è libero, cioè non sottoposto a forze oppure sottoposto ad una risultante nulla di forze, allora persevererà il suo stato di quiete o di moto rettilineo uniforme finché esso non viene perturbato. In altre parole un osservatore S o sistema di riferimento si dice inerziale se, di un punto materiale isolato (P, m), misura accelerazione nulla, qualunque sia l'istante t in cui si effettua tale misura e qualunque sia lo stato cinematico (P, Ṗ) del punto nel medesimo istante t.
Si può verificare che gli osservatori, che di un punto materiale isolato misurano accelerazione nulla, sono tutti e solo quelli che si muovono di moto traslatorio rettilineo uniforme rispetto all'osservatore S sopra citato.
Un sistema di riferimento inerziale nella dinamica newtoniana è un sistema di riferimento in cui sia valida la prima legge della dinamica. Questo assunto è basato sull'osservazione e sulla geometria euclidea.
L'espressione (Forza = massa × accelerazione) formula_1 ha un importante valore: non menziona la velocità e quindi questa non fa parte del calcolo; in pratica la variazione di velocità dovuta a una forza è indipendente dalla velocità. Questa proprietà esprime una proprietà fondamentale del moto: le velocità sono indistinguibili. Pertanto un osservatore può scegliere un qualsiasi sistema di riferimento inerziale come sistema di coordinate per i calcoli, e può applicare le leggi del moto. I sistemi di riferimento inerziali sono gli unici sistemi di riferimento in cui sono valide le stesse leggi del moto.
Nel modello concettuale della dinamica newtoniana, è sufficiente definire il moto inerziale come moto a velocità costante lungo una linea retta.
La Terra non è un vero e proprio sistema di questo tipo, a causa dei suoi movimenti di rivoluzione e di rotazione. In particolare, il moto di rotazione sottopone gli oggetti sulla sua superficie lontani dai poli a una piccola forza centrifuga. Tuttavia questa accelerazione è irrilevante in certi casi, per cui la Terra è un sistema di riferimento che approssima un sistema di riferimento inerziale.
Il moto di rotazione sottopone inoltre i corpi lontani dall'equatore alla forza di Coriolis, che devia verso destra il moto di tutti i corpi dell'emisfero nord e verso sinistra quelli dell'emisfero sud, come dimostrato dal famoso pendolo di Foucault.
Gli osservatori posti in un sistema inerziale si dicono osservatori inerziali, ed assumono una particolare importanza nell'ambito della meccanica. Due osservatori in moto relativo uniforme osservano che una medesima massa, sottoposta alla medesima forza, varia nel medesimo modo il suo stato di moto. Nel linguaggio corrente si può dire che presenta la medesima inerzia nei confronti di un tentativo di variarne lo stato di moto. Le leggi della meccanica classica sono invarianti solo per osservatori inerziali. Un osservatore dentro una capsula in caduta libera verso la Terra non può verificare la legge di caduta dei gravi, che pure descrive la sua stessa caduta.
In formule possiamo dire che:
La terza forza si annulla e i due osservatori misurano la medesima forza agente su formula_4
Se questo accade i due osservatori sono detti "inerziali".
La Relatività Speciale non solo ha identificato nelle trasformazioni di Lorentz lo strumento appropriato di trasformazione tra sistemi di riferimento inerziali, ma ha anche introdotto il concetto che le leggi che valgono per tutti i sistemi di riferimento inerziali includono anche le leggi dell'elettrodinamica, non solo quelle della meccanica.
In dinamica relativistica si riconosce che l'assunzione che lo spazio sia euclideo non è giustificata in generale. È comunque possibile creare una dinamica coerente ed accurata sulla base del concetto di "sistema di riferimento inerziale", verificando se ci sia un'accelerazione causata da una forza applicata. Una scatola contenente un peso, collegato a tutte le pareti del contenitore con molle che lo tengono sospeso, agisce come un accelerometro. Quando l'accelerometro viene messo in movimento da una forza, il peso in esso "oscilla" in verso opposto alla forza, a causa della propria inerzia. Se non ci sono manifestazioni di inerzia, questo è un sistema di riferimento inerziale.
In relatività generale esso è così definito, perché l'osservazione che anche gli accelerometri più accurati danno misura "zero" quando sono in caduta libera in un campo gravitazionale è presa come indicazione che "fondamentalmente" non è presente accelerazione nello spazio che circonda un oggetto in caduta libera.
In relatività generale, sia la gravità che l'inerzia sono definite come interazioni di materia con la geometria dello spazio-tempo. Detto molto sinteticamente: la geometria dello spazio-tempo indica alla materia come muoversi, e la materia deforma la geometria dello spazio-tempo.
Quando la materia si sta muovendo attraverso dello spazio-tempo non deformato (in altre parole: spazio-tempo rettilineo), essa segue traiettorie analoghe alle linee rette euclidee. Queste linee sono rettilinee universalmente, nel senso che esse sono diritte anche se osservate a distanza: esse sono universalmente linee rette. Oggetti che si muovono in uno spazio-tempo rettilineo possono muoversi solo in linea retta e a velocità costante: esso è l'unico movimento libero che questa forma di spazio-tempo permette. Quando la velocità cambia in seguito all'esercizio di una forza, allora si manifesta l'inerzia.
Quando la materia si muove liberamente attraverso uno spazio-tempo deformato descrive delle geodetiche. Queste sono percorsi in cui l'inerzia non si manifesta. Quando una forza devia un oggetto dalla propria geodetica si manifesta l'inerzia. In uno spazio-tempo deformato, un sistema di riferimento inerziale è definibile solo localmente, a causa della deformazione dello spazio-tempo causata dalla materia. La materia è riunita nelle stelle, nei pianeti ecc., e nelle vicinanze di questi gravi si osserva una deformazione sferica della geometria dello spazio-tempo. Parte di questa è un'alterazione del ritmo del tempo. Un oggetto in caduta libera verso il centro di gravità di un grave si muove lungo la propria geodetica. Il sistema di riferimento locale, ancorato all'oggetto, è un sistema di riferimento inerziale locale.
Secondo la relatività generale, esiste un solo campo, che può essere chiamato gravito-inerziale.
L'interazione gravitazionale è mediata dalla deformazione di qualcosa che è comunque presente: la geometria dello spazio-tempo.
La proprietà dell'inerzia esiste a causa dell'universale presenza del campo gravito-inerziale; è l'interazione con la geometria dello spazio-tempo a dire a un pianeta rotante quanto si deve rigonfiare all'equatore.
Un accelerometro interagisce con la geometria locale dello spazio-tempo per misurare se sta accelerando rispetto ad essa. È importante notare che un accelerometro non misura la sua "velocità" rispetto a qualche sistema assoluto.
La velocità è fondamentalmente relativa.
Un giroscopio in rotazione interagisce con la geometria locale dello spazio-tempo per misurare se sta ruotando rispetto ad essa. Siccome è molto raro che la geometria locale dello spazio-tempo ruoti significativamente rispetto all'universo, un giroscopio in rotazione mostra effettivamente quale sistema di riferimento non sta ruotando rispetto all'universo.
Il concetto di sistema di riferimento inerziale, come riconosciuto nella relatività generale, viene presentato solitamente discutendo solamente i fenomeni cinematici: accelerometri e giroscopi.
Ma non si tratta solo di dinamica: nei sistemi di riferimento inerziale, come riconosciuti nella relatività generale, valgono tutte le leggi della fisica.
</doc>

<doc id="958210" url="https://it.wikipedia.org/wiki?curid=958210" title="Campo (fisica)">
Campo (fisica)
In fisica, un campo è una grandezza esprimibile come funzione della posizione nello spazio e del tempo, o nel caso relativistico nello spaziotempo.
Un campo può essere scalare, spinoriale, vettoriale o tensoriale, a seconda che la grandezza rappresentata sia rispettivamente uno scalare, uno spinore di Dirac, un vettore o un tensore. Per esempio, il campo gravitazionale può essere modellato come campo vettoriale dove un vettore indica l'accelerazione esercitata su una massa per ogni punto. Questo intuitivamente, anche se il campo gravitazionale indica la forza che agisce su una massa unitaria e non un'accelerazione (un libro posto su un tavolo non subisce nessun accelerazione ma un campo di forza). Altri esempi possono essere il campo di temperatura o quello della pressione atmosferica, che sono spesso illustrati tramite le isoterme e le isobare collegando i punti che hanno rispettivamente la stessa temperatura o pressione.
Sotto questo punto di vista un campo può essere più semplicemente definito come l'insieme dei valori che una data grandezza fisica, scalare o vettoriale, assume nello spazio. Il teorema di Helmholtz è fondamentale per la comprensione dei campi in quanto fornisce una classe di parametri che li determinano univocamente.
Nel caso di un campo di forze, come il campo gravitazionale e il campo elettrico, il concetto di campo è strettamente correlato con quello di interazione a distanza.
La teoria dei campi descrive la dinamica di un campo, cioè la sua variazione nel tempo. Di solito questa viene descritta da una lagrangiana o una hamiltoniana di campo, trattate come un sistema con infiniti gradi di libertà. La teoria risultante può essere classica o quantistica. In fisica moderna i campi più studiati sono quelli relativi alle forze fondamentali.
La teoria della relatività generale afferma l'impossibilità di fenomeni simultanei (sempre distanziati a meno di un infinitesimo di spazio-tempo) e sostituisce con il concetto di campo le forze simultanee agenti a distanza utilizzate nella fisica newtoniana.
Fenomeni simultanei anche a grandi distanze sono invece possibili per l'entanglement quantistico, che negli esperimenti è misurato e riproducibile solo per singole particelle e che, in accordo con il teorema di non-comunicazione, non è sfruttabile a livello macroscopico per la trasmissione di dati.
Ci sono numerosi esempi di campi classici. La dinamica di questi campi è di solito specificata dalla densità di Lagrangiana in termini di componenti del campo; la dinamica può essere ottenuta usando il principio d'azione.
Michael Faraday per primo capì l'importanza del campo come oggetto fisico, durante la sua ricerca sul magnetismo. Egli capì che il campo elettrico e magnetico non erano solo campi di forza che influenzavano il moto delle particelle, ma avevano un'interpretazione fisica reale, perché essi possono trasportare energia.
Queste idee portarono alla creazione, da parte di James Clerk Maxwell, della prima teoria unificata dei campi con l'introduzione delle equazioni per il campo elettromagnetico. La versione moderna di queste equazioni sono chiamate equazioni di Maxwell. Alla fine del diciannovesimo secolo il campo elettromagnetico fu capito come una collezione di due campi vettoriali nello spazio. Oggi, questo è raggruppato in un singolo campo tensoriale del secondo ordine nello spaziotempo.
La teoria della gravità di Einstein, chiamata relatività generale, è un altro esempio di una teoria di campo. Qui il campo principale è un tensore metrico, un campo tensoriale del secondo ordine nello spaziotempo.
Si pensa attualmente che la meccanica quantistica sia alla base di tutti i fenomeni fisici; cosicché anche la teoria classica dei campi dovrebbe essere riformulata in modo da tenerne conto.
Ciò è stato fatto con la cosiddetta seconda quantizzazione che rende la funzione d'onda della meccanica quantistica (scalare, in quella sede) un operatore.
Tale meccanismo è stato applicato dapprima, con successo, al campo elettromagnetico; la corrispondente teoria di campo quantizzata è nota come elettrodinamica quantistica o QED.
Successivamente si sono ottenute le teorie di campo quantizzato per due delle altre forze fondamentali: la forza forte descritta dalla cromodinamica quantistica (QCD) e quella debole descritta dalla teoria elettrodebole (che mostra come in realtà la forza elettromagnetica e quella debole abbiano un'origine comune).
Queste tre teorie possono esse derivate come casi particolari del cosiddetto modello standard della fisica delle particelle.
A tutt'oggi non è stata invece trovata una soddisfacente teoria di campo quantizzato per il campo gravitazionale.
Le teorie di campo classico rimangono comunque utili per lo studio di fenomeni in cui le proprietà quantistiche della materia non siano rilevanti; ad esempio lo studio dell'elasticità dei materiali o la fluidodinamica.
I campi classici, come quello elettromagnetico, sono usualmente funzioni derivabili a tutti gli ordini, e in ogni caso almeno due volte. Al contrario le funzioni generalizzate non sono continue.
Il metodo dei campi casuali continui deve essere usato per uno studio dei campi classici a temperature finite, poiché un campo classico variabile con la temperatura è ovunque non differenziabile.
I campi casuali sono insiemi di variabili casuali munite di indice; un campo casuale continuo è un campo casuale i cui indici sono funzioni continue. In particolare, conviene talvolta considerare i campi casuali che hanno come insieme degli indici uno spazio di Schwartz di funzioni, nel qual caso il campo casuale continuo è una distribuzione temperata.
In modo (molto) rude possiamo pensare ai campi casuali continui come a funzioni ordinarie che valgano formula_1 quasi ovunque, e tali che quando si effettui una media pesata di tutti questi infiniti in una regione finita, si ottenga un risultato finito, e che può essere ben definito. Possiamo definire un campo casuale continuo in modo più preciso come una mappa lineare dallo spazio delle funzioni in quello dei numeri reali.
Un modo conveniente per classificare i campi (classici e quantistici) sono le simmetrie che possiedono. Le simmetrie sono di due tipi:
I campi sono spesso classificati per il loro comportamento rispetto a trasformazioni dello spaziotempo:
Nella relatività vale una classificazione simile, ad eccezione del fatto che i campi scalari, vettoriali e tensoriali sono definiti rispetto alla simmetria di Poincaré dello spaziotempo.
I campi possono avere simmetrie interne oltre a quelle spaziotemporali.
</doc>

<doc id="896150" url="https://it.wikipedia.org/wiki?curid=896150" title="Urto">
Urto
L'urto è il termine fisico con il quale si identifica la collisione di due corpi che si scontrano.
Un'interpretazione più corretta viene fornita dalla meccanica del continuo: i corpi sono dotati di "elasticità" e l'intervallo di tempo durante il quale tali oggetti sono a contatto si compone di un periodo di compressione, nel quale si compie una deformazione spesso impercettibile, e di un periodo di ritorno elastico durante il quale la forma torna allo stato iniziale.
Viene inizialmente presa in considerazione la classe degli urti "normali" a due corpi, cioè quelli in cui la direzione del moto avviene lungo la normale comune per il punto di contatto sia prima che dopo l'urto in quanto moto unidimensionale, e successivamente si può estendere lo studio agli urti "obliqui" a n>2 corpi in d>1 dimensioni.
Per un urto "normale" la velocità relativa dei corpi dopo l'urto è proporzionale a quella precedente l'urto attraverso un coefficiente di ritorno legato alle elasticità dei due corpi:
Se formula_3 l'urto è detto "totalmente anelastico";
se formula_4 l'urto è detto "elastico";
L'applicazione del principio di conservazione:
dove:
al caso di un urto tra due corpi a coefficiente di ritorno qualsiasi si ha:
dove:
La quantità di moto totale dopo l'urto è data dalla quantità di moto iniziale più l'impulso totale. Perciò le forze sono uguali e contrarie per i due corpi, e la loro somma vettoriale è nulla.
Dalle due equazioni precedenti si ricava che:
Che applicata al caso di urto totalmente anelastico si traduce in:
e quindi, :formula_11
in particolare se :formula_12, allora: formula_13
Applicata invece al caso di urto elastico si traduce in:
se poi :formula_16, allora: formula_17
Si capisce perciò meglio come ε sia un fattore legato all'elasticità dell'urto.
Dalle equazioni delle velocità:
dove:
Si può verificare facilmente infatti che :formula_19 il principio di conservazione della quantità di moto che abbiamo imposto precedentemente.
Se si considera un sistema di bersaglio 2 molto più grande del proiettile 1,
ma allora:
dove "formula_22" è l'accelerazione relativa: a parità di questa, la forza impulsiva è perciò massima e doppia per un urto elastico rispetto ad un urto perfettamente anelastico.
Affinché l'energia cinetica totale dei corpi rimanga invariata (e quindi le velocità dei due corpi dopo l'urto abbiano o direzione o verso o intensità diverse tra loro), si deve avere un urto elastico, e viceversa, come dimostra questa catena di doppie implicazioni:
Se l'energia cinetica dei corpi è stata parzialmente dissipata nell'urto, allora si parla genericamente di urto anelastico. In quest'ultimo caso, si può dimostrare analogamente che l'energia cinetica dissipata è la massima possibile (dovendo rispettare la conservazione della quantità di moto totale) nel caso di urto totalmente anelastico, poiché i due corpi procedono alla stessa velocità dopo l'urto.
Secondo il primo principio della termodinamica, la parte di energia cinetica dissipata viene convertita in energia interna dei corpi coinvolti nell'urto, cioè in generale in parte in calore e in parte in lavoro termodinamico dei corpi stessi.
</doc>
