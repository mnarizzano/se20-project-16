<document>
<doc id="109852" url="https://it.wikipedia.org/wiki?curid=109852">
<title>Triangolo rettangolo</title>
<text>
Il triangolo rettangolo è un triangolo in cui l'angolo formato da due lati, detti cateti, è retto, ovvero di 90° (o radianti). Il lato opposto all'angolo retto si chiama ipotenusa. L'ipotenusa è per il teorema di Pitagora uguale alla radice quadrata della somma dei quadrati dei cateti. 
Il triangolo rettangolo rappresenta un caso particolare di triangolo generico, per cui molte relazioni fondamentali si semplificano. Il caso più particolare è quello del triangolo rettangolo isoscele, caso per il quale 
Aggiungendo a un triangolo rettangolo il triangolo ottenuto con la sua riflessione rispetto all'ipotenusa si ottiene un aquilone. Aggiungendogli il triangolo ottenuto sottoponendolo alla rotazione di π intorno al punto medio dell'ipotenusa si ottiene il rettangolo per il quale l'ipotenusa è diagonale principale.
Dal triangolo rettangolo isoscele con entrambe le costruzioni si ottiene il quadrato di lato formula_2.
Ogni similitudine trasforma un triangolo rettangolo in un triangolo rettangolo. Le classi di similitudine dei triangoli rettangoli si possono quindi rappresentare fedelmente con i triangoli rettangoli aventi l'ipotenusa "c" di lunghezza 1 e il vertice opposto appartenente a una delle semicirconferenze aventi come diametro l'ipotenusa. La collezione delle classi di similitudine si può parametrizzare con il rapporto "a"/"b" delle lunghezze dei cateti ovvero con uno dei due angoli non retti, ad esempio con l'angolo formula_15 relativo al vertice "A". Dalla trigonometria segue che:
È chiamato anche Triangolo 90-45 per le ampiezze degli angoli che lo formano, invero è composto da un angolo retto e due angoli da 45°. Per costruzione, il triangolo rettangolo isoscele è la meta di un quadrato ed ha come ipotenusa la diagonale del quadrato e come cateti i suoi lati. Viene frequentemente rappresentato come un triangolo isoscele che ha come base l'ipotenusa. Ingloba le proprietà dei triangoli rettangoli e dei triangoli isosceli infatti, rispettivamente:
Per ciò, la caratteristica principale di questo triangolo è che l'altezza è congruente alla semi ipotenusa, cioè formula_19. Inoltre ha i due cateti uguali e misurano formula_20.
L'ortocentro di un triangolo rettangolo coincide con il vertice dell'angolo retto.
Il circocentro è il punto medio dell'ipotenusa.
Per individuare il baricentro può essere comodo riferire il triangolo ad una coppia di assi cartesiani ortogonali con l'origine nel vertice "C" relativo all'angolo retto, l'asse delle "x" contenente il lato "a" dalla parte delle ascisse positive e l'asse delle "y" contenente il lato "b". Scrivendo in tale riferimento le equazioni di due rette comprendenti due delle mediane e mettendole a sistema per trovarne l'intersezione si calcola che le coordinate del baricentro sono "a"/3 e "b"/3.
</text>
</doc>
<doc id="109857" url="https://it.wikipedia.org/wiki?curid=109857">
<title>Triangolo equilatero</title>
<text>
Nella geometria euclidea, un triangolo equilatero è un triangolo avente i suoi tre lati congruenti tra loro. Si dimostra che i suoi angoli sono tutti congruenti e pari a 60° = formula_1rad. Poiché è sia equilatero sia equiangolo è il poligono regolare con tre lati.
I triangoli equilateri sono particolari triangoli isosceli. Tutti i triangoli equilateri sono simili tra di loro: per caratterizzare metricamente un triangolo equilatero, ovvero per caratterizzare la classe dei triangoli equilateri nel piano ottenibili gli uni dagli altri mediante traslazioni e rotazioni, serve e basta un parametro estensivo; tipicamente si usa la lunghezza dei suoi lati. 
Nei triangoli equilateri, le bisettrici, le mediane, le altezze e gli assi si sovrappongono cosicché lo stesso punto rappresenta l'ortocentro, il baricentro, l'incentro e il circocentro. 
Il gruppo delle simmetrie del "triangolo equilatero" è costituito dall'identità, dalle rotazioni intorno al suo centro di 120° e di 240° e dalle riflessioni rispetto alle bisettrici degli angoli. Tale gruppo è isomorfo al gruppo simmetrico di 3 oggetti S.
Come mostra Euclide in Elementi I, 1 (è la prima proposizione di tutta l'opera), il triangolo equilatero dato il lato AB si può costruire con riga e compasso in questo modo:
La dimostrazione è semplice: essendo, per definizione, tutti i punti della circonferenza equidistanti dal centro, il segmento AB è congruente ad AC, e AB è congruente a BC. Ma allora per la proprietà transitiva della congruenza, AB = AC = BC e il triangolo è equilatero.
Indicando con formula_2 il lato del triangolo, con formula_3 il perimetro, con formula_4 l'area, con formula_5 la base e con formula_6 l'altezza si ha:
Il centro geometrico del triangolo è il centro delle circonferenze inscritta e circoscritta al triangolo equilatero
Il raggio della circonferenza circoscritta è formula_15
da cui formula_16
Il raggio della circonferenza inscritta è formula_17
da cui formula_18
L'area, noto R, è formula_19
</text>
</doc>
<doc id="102044" url="https://it.wikipedia.org/wiki?curid=102044">
<title>Prisma</title>
<text>
Il prisma in geometria solida è un poliedro le cui basi sono due poligoni congruenti di n lati posti su piani paralleli e connessi da un ciclo di parallelogrammi (le "facce laterali").
Se il poligono che forma le basi è un particolare poligono, ad esempio un triangolo, quadrato, pentagono, etc. si parla rispettivamente di "prisma triangolare", "prisma quadrato", '"pentagonale", etc. In generale, si parla di "prisma n-gonale".
Se le facce laterali sono tutte dei rettangoli il poliedro è un "prisma retto": in questo caso infatti le facce laterali formano degli angoli retti con entrambe le basi. In caso contrario si parla di "prisma obliquo".
Un prisma che ha tutte le facce a forma di parallelogramma è un parallelepipedo. Si tratta, quindi, di un prisma le cui basi sono parallelogrammi.
Un "prisma regolare" è un prisma retto la cui base è un poligono regolare.
Il poliedro duale di un prisma è una bipiramide.
Il volume di un prisma è dato dal prodotto dell'area di una delle sue basi per la distanza tra i piani (paralleli) ai quali appartengono. Se il prisma è retto, questa distanza è pari alla lunghezza di uno spigolo verticale (altrimenti no).
Un prisma regolare con formula_1 lati ha formula_2 simmetrie. Per formula_3 il prisma regolare è in realtà un cubo e le simmetrie sono di più (48), perché è possibile scambiare una faccia laterale con una base.
Più precisamente, il gruppo di simmetria di un prisma regolare con formula_4 lati è il prodotto diretto formula_5 del gruppo diedrale di ordine formula_6 con il gruppo ciclico di ordine 2. Il gruppo diedrale rappresenta infatti tutte le simmetrie che preservano ciascuna base, ed è quindi isomorfo al gruppo formula_7 di simmetrie di un formula_8-gono regolare, mentre il secondo fattore rappresenta l'isometria che scambia le due basi.
</text>
</doc>
<doc id="129655" url="https://it.wikipedia.org/wiki?curid=129655">
<title>Esagono</title>
<text>
Un esagono è un poligono con sei lati e sei vertici, il suo simbolo di Schläfli è {6}. In esso si possono tracciare nove diagonali.
La parola esagono è composta da ἕξ che significa sei e da γωνία che significa angolo a ricordare il fatto che tale poligono contiene sei angoli interni.
Un esagono regolare è un esagono con i sei lati di uguale lunghezza e con i sei angoli congruenti (uguale ampiezza).
Gli angoli interni di un esagono regolare misurano tutti 120°.
Il perimetro formula_1 di un esagono regolare di lato formula_2 è dato da
formula_3
L'apotema formula_4 dell'esagono regolare, dato dal raggio della circonferenza inscritta, è pari a:
Il diametro della circonferenza circoscritta è pari a formula_6 mentre il diametro della circonferenza inscritta è formula_7.
L'area dell'esagono regolare di lato formula_2, apotema formula_4 e perimetro formula_1 è data da:
L'area dell'esagono regolare, essendo formula_12, con raggio della circonferenza circoscritta formula_13 è data da:
Come succede per i quadrati e i triangoli equilateri, gli esagoni regolari si possono unire per ricoprire porzioni di piano senza lasciare spazi vuoti (tre esagoni intorno a ogni vertice): per questo motivo sono molto utili per costruire tassellazioni. Non a caso, le cellette del favo di un alveare sono esagonali, stante l'uso efficiente di spazio e di materiali da costruzione che tale forma consente. Il diagramma di Voronoi di una rete di triangoli equilateri è equivalente alla tassellazione di un favo con esagoni regolari.
Non esistono solidi platonici che hanno come facce esagoni regolari: I solidi archimedei con alcune facce esagonali sono il tetraedro troncato, l'ottaedro troncato, l'icosaedro troncato (che incontriamo nel classico pallone da calcio e nella molecola di fullerene), il cubottaedro troncato e l'icosidodecaedro troncato.
In francese, il termine "L'Hexagone" (L'Esagono) è spesso utilizzato come soprannome della Francia, a causa della forma esagonale del profilo dei suoi confini.
Un esagono regolare è costruibile con riga e compasso. L'immagine seguente è un'animazione che mostra passo-passo il metodo suggerito da Euclide nei suoi "Elementi" (libro IV, proposizione 15).
</text>
</doc>
<doc id="147738" url="https://it.wikipedia.org/wiki?curid=147738">
<title>Corda (geometria)</title>
<text>
In geometria, si dice corda un segmento che unisce due punti distinti di una curva. La retta su cui giace la corda si chiama secante.
Nel caso particolare delle corde di una circonferenza valgono le seguenti proprietà:
</text>
</doc>
<doc id="126887" url="https://it.wikipedia.org/wiki?curid=126887">
<title>Disuguaglianza triangolare</title>
<text>
In matematica, la disuguaglianza triangolare afferma che, in un triangolo, la somma delle lunghezze di due lati è maggiore della lunghezza del terzo. Una sua conseguenza, la disuguaglianza triangolare inversa, afferma invece che la differenza tra le lunghezze dei due lati è minore della lunghezza del rimanente.
Nel contesto della geometria euclidea, la disuguaglianza triangolare è un teorema, conseguenza del teorema del coseno, e, nel caso di triangoli rettangoli, conseguenza del teorema di Pitagora. Essa può essere usata per dimostrare che il percorso più breve tra due punti è il segmento rettilineo che li congiunge.
Nell'ambito degli spazi normati e degli spazi metrici, la disuguaglianza triangolare è una proprietà che ogni norma o distanza deve possedere per essere considerata tale.
Euclide dimostrò la disuguaglianza triangolare usando la costruzione in figura. Iniziando con un triangolo formula_1, si costruisce un triangolo isoscele prendendo il lato formula_2 e un segmento formula_3 della stessa lunghezza lungo il lato formula_4. Poiché l'angolo formula_5 è maggiore dell'angolo formula_6, per i corrispondenti lati opposti vale la stessa disuguaglianza: quindi formula_7. Ma poiché formula_8, si ha che formula_9, ovvero la disuguaglianza cercata. Questa dimostrazione compare negli Elementi di Euclide, libro 1, proposizione 20. Nel 1752, la proposizione euclidea è oggetto di una dissertazione di Tommaso Maria Gabrini, che ne conferma la tesi.
Nel caso di triangolo rettangoli, la disuguaglianza afferma che la somma dei due cateti è maggiore dell'ipotenusa, mentre la differenza è minore di essa.
La disuguaglianza triangolare può essere estesa, tramite induzione matematica, ad un poligono con un numero qualsiasi di lati. In questo caso, essa afferma che la lunghezza di un lato è minore della somma di tutti i rimanenti.
La disuguaglianza triangolare può essere usata per provare che la distanza più breve tra due punti è realizzata dal segmento rettilineo che li congiunge.
Nella sua forma per poligoni generali, essa già prova che ogni percorso lungo una linea spezzata è più lungo di quello lungo il segmento rettilineo che congiunge i due punti. Poiché la lunghezza di una curva qualsiasi è definita come l'estremo superiore della lunghezza delle spezzate che approssimano la curva, si ha che essa è più lunga proprio di queste spezzate, e quindi anche del segmento rettilineo tra i due punti.
Nell'ambito degli spazi metrici, la disuguaglianza triangolare è una proprietà che deve soddisfare una distanza per essere tale. Essa afferma che, in uno spazio metrico formula_10, comunque si scelgano tre punti formula_11, formula_12 e formula_13, vale che:
La disuguaglianza triangolare è responsabile di molte proprietà interessanti delle metriche, tra cui quelle riguardanti la convergenza: è grazie ad essa che si può dimostrare che ogni successione convergente in uno spazio metrico è una successione di Cauchy.
Nell'ambito degli spazi normati, ogni norma deve soddisfare la disuguaglianza triangolare per essere tale. Quindi, considerato uno spazio vettoriale normato formula_15, comunque si scelgano due vettori formula_11 e formula_12 deve valere che
ovvero la norma della somma di due vettori è minore o uguale della somma delle loro norme.
Grazie a tale proprietà, ponendo per ogni formula_11 e formula_12
la funzione formula_22 è una metrica, detta metrica indotta dalla norma. Vale infatti la disuguaglianza triangolare:
Il valore assoluto è una norma per i numeri reali, e quindi soddisfa la disuguaglianza triangolare. Infatti, poiché valgono le seguenti relazioni per ogni formula_11 e formula_12:
sommando membro a membro si ottiene
da cui la disuguaglianza triangolare (applicando una delle proprietà del valore assoluto)
Più precisamente,
Se su uno spazio è definito un prodotto scalare formula_34, è possibile definire la norma indotta da esso:
Come conseguenza della disuguaglianza di Cauchy-Schwarz, essa soddisfa la disuguaglianza triangolare:
da cui, estraendo la radice:
La disuguaglianza triangolare inversa è una immediata conseguenza della disuguaglianza triangolare, che dà un limite dal basso invece che dall'alto. Nell'ambito della geometria euclidea essa afferma che ogni lato è maggiore della differenza degli altri due.
Nel caso di spazi normati, essa afferma che:
Nel caso di spazi metrici, invece:
Questa proprietà implica che sia la funzione norma formula_39 che la funzione distanza da un punto formula_40 sono funzioni di Lipschitz con costante di Lipschitz uguale a 1.
</text>
</doc>
<doc id="1616204" url="https://it.wikipedia.org/wiki?curid=1616204">
<title>Endecagono</title>
<text>
Un endecagono è un poligono con 11 lati e 11 angoli. Gli angoli interni di un endecagono regolare misurano circa 147,27°. L'area di un endecagono regolare con lato lungo "a" è data da
Un endecagono regolare "non" può essere costruito in modo esatto con l'uso di soli riga e compasso. Qui sotto ne è mostrata una costruzione che fornisce un'ottima approssimazione (circa un centesimo di grado sull'angolo al centro):
Se si vuole trovare il perimetro di un endecagono regolare si moltiplica un suo lato per 11
P= a • 11
</text>
</doc>
<doc id="1656374" url="https://it.wikipedia.org/wiki?curid=1656374">
<title>Seno (matematica)</title>
<text>
In matematica, in particolare in trigonometria, dato un triangolo rettangolo il seno di uno dei due angoli interni adiacenti all'ipotenusa è definito come il rapporto tra le lunghezze del cateto opposto all'angolo e dell'ipotenusa. 
Più in generale il seno di un angolo formula_1, espresso in gradi o radianti, è una quantità che dipende solo da formula_1, costruita usando la circonferenza unitaria.
Definendo come formula_3 il seno nell'angolo formula_4 si ottiene la funzione seno, una funzione trigonometrica di fondamentale importanza nell'analisi matematica. In ambito italiano questa funzione viene spesso indicata con formula_5.
Nel triangolo rosso in figura, il seno di formula_4 è dato da
Più in generale si definisce il seno di un angolo formula_4 a partire dalla circonferenza goniometrica, ovvero dalla circonferenza di raggio unitario nel piano cartesiano. Presa la semiretta uscente dall'origine che forma un angolo formula_4 con l'asse delle ascisse come in figura, il seno dell'angolo è quindi definito come il valore della coordinata formula_10 del punto di intersezione tra la semiretta e la circonferenza (in figura, è la lunghezza del segmento formula_11). 
Il dominio della funzione seno è l'insieme dei numeri reali, mentre il codominio è l'intervallo reale formula_12, ossia applicando tale funzione a qualunque numero reale si ottiene sempre un numero reale compreso tra formula_13 e formula_14, estremi inclusi.
La tabella seguente elenca i principali valori notevoli assunti dalla funzione seno:
D'altra parte il teorema di Pitagora applicato al triangolo formula_31 fornisce la relazione
e quindi 
Vale anche la relazione:
Come per il coseno, la cosecante di un angolo è formula_35 diviso il seno dell'angolo.
La derivata della funzione seno è la funzione coseno: 
La derivata seconda è invece
La funzione seno è una funzione analitica, la cui espansione in serie di Taylor è
In analisi matematica questa uguaglianza è spesso usata per definire il seno. La stessa serie definisce il seno come funzione olomorfa su tutto il piano complesso.
Seguono alcune equazioni fondamentali riguardanti la funzione seno:
con l'aggiunta della condizione che:
Esiste anche un'identità trigonometrica che mette in relazione la funzione seno alla funzione tangente:
Questa identità si rivela di fondamentale importanza nella risoluzione di equazioni goniometriche in cui l'incognita figuri come argomento sia di un seno sia di un coseno o di funzioni derivate da queste. Esiste infatti un'analoga identità per quanto riguarda il coseno e l'uso congiunto di queste due identità permette la risoluzione dell'equazione nell'incognita formula_44.
Il reciproco del seno, definito dove il seno è diverso da zero, è la cosecante:
La funzione seno ristretta all'intervallo formula_46 come dominio e con codominio formula_47 è biettiva, e quindi ha un’inversa, chiamata arcoseno e indicata con formula_48 o con formula_49 che riprende la notazione della funzione inversa. Per definizione si ha quindi: 
Dalla formula di Eulero si deduce che la funzione seno è in relazione con la funzione esponenziale e con la funzione seno iperbolico. Infatti per ogni numero reale formula_4 si ha 
Alcune formule particolari riguardanti la funzione seno coinvolgono l'operazione di prodotto. 
Per esempio risulta per ogni intero formula_53
In analisi complessa, applicando teorema di fattorizzazione di Weierstrass alla funzione seno, la si può esprimere come prodotto infinito, mediante la seguente formula che vale per ogni numero complesso formula_55
La funzione seno è anche in relazione con alcune funzioni speciali, come si evince, per esempio, dalla formula di riflessione della funzione Gamma 
e dall'equazione funzionale soddisfatta dalla funzione zeta di Riemann
Il concetto di seno fu introdotto dal matematico e astronomo indiano Aryabhata I (in devanāgarī: आर्यभट) nella sua opera "Aryabhatiya" (499).
Il seno è per definizione la metà di una "corda", cioè un segmento che unisce due punti (detti "estremi") di una circonferenza. In sanscrito, "metà corda" è reso con "jya-ardha", a volte sostituito con "ardha-jya" e abbreviato in "jya" "corda". Questo termine fu importato nella lingua araba come "jiba", un termine senza significato prima di allora, ma che rifletteva la pronuncia del nome "jya". Secondo le regole della lingua araba, questo nome venne scritto con le due consonanti "jb", senza vocali. Successivamente, quando i traduttori occidentali attinsero alle fonti arabe, interpretarono la parola "jb" come "jaib", il cui significato era "baia". Infine l'italiano Gherardo da Cremona (1114 - 1187) tradusse la parola in latino come "sinus", il cui significato era appunto "baia".
</text>
</doc>
<doc id="404205" url="https://it.wikipedia.org/wiki?curid=404205">
<title>Collinearità</title>
<text>
In geometria vettoriale, due vettori formula_1 e formula_2 si dicono collineari se e solo se esiste uno scalare "k" tale che sia formula_3 o, equivalentemente, formula_4.
Etimologicamente collineari significa "giacenti sulla stessa linea retta". In effetti, in geometria affine, due vettori si dicono collineari se esistono due rispettivi rappresentanti situati sopra una stessa retta, ossia se esistono tre punti A, B e C allineati tali che 
La collinearità è una nozione importante in geometria affine, in quanto permette di definire 
Si nota che il vettore nullo di uno spazio vettoriale è collineare con tutti gli altri vettori.
Sull'insieme dei vettori non nulli la relazione di collinearità è 
Queste tre proprietà consentono di affermare che la relazione di collinearità è una relazione d'equivalenza; le sue classi d'equivalenza costituiscono lo spazio proiettivo associato allo spazio vettoriale.
Tre punti di coordinate ("x,y"), ("x,y") e ("x,y")
si dicono collineari, vale a dire giacenti sulla stessa retta in un sistema di riferimento cartesiano a due assi, se e solo se per il determinante che segue, vale che:
</text>
</doc>
<doc id="369748" url="https://it.wikipedia.org/wiki?curid=369748">
<title>Sistema di riferimento cartesiano</title>
<text>
In matematica, un sistema di riferimento cartesiano è un sistema di riferimento formato da "n" rette ortogonali, intersecantesi tutte in un punto chiamato "origine", su ciascuna delle quali si fissa un orientamento (sono quindi "rette orientate") e per le quali si fissa anche un'unità di misura (cioè si fissa una metrica di solito euclidea) che consente di identificare qualsiasi punto dell'insieme mediante "n" numeri reali. In questo caso si dice che i punti di questo insieme sono in uno spazio di dimensione "n".
Un sistema di riferimento cartesiano in due dimensioni viene chiamato piano cartesiano.
Per identificare la posizione di punti nello spazio fisico viene solitamente utilizzato un sistema di riferimento cartesiano a tre dimensioni. Tuttavia per descrivere la posizione di oggetti più complicati vengono utilizzati altri sistemi di riferimento non necessariamente cartesiani e un differente numero di dimensioni, dette in questo contesto gradi di libertà.
Usando un sistema di riferimento cartesiano, è possibile descrivere tramite equazioni algebriche forme geometriche come curve o superfici: i punti dell'oggetto geometrico sono quelli che soddisfano l'equazione associata. Per esempio è possibile descrivere una circonferenza nel piano cartesiano, oppure una quadrica nello spazio tridimensionale.
L'uso delle coordinate geometriche venne introdotto per la prima volta da Nicola d'Oresme, matematico del XIV secolo operante a Parigi. L'aggettivo "cartesiano" è riferito al matematico e filosofo francese René Descartes (italianizzato in " Renato Cartesio", latinizzato in " Renatus Cartesius") il quale, tra le altre cose, riprendendo gli studi di Nicola d'Oresme, lavorò sulla fusione dell'algebra con la geometria euclidea. Questi studi furono influenti nello sviluppo della geometria analitica, del calcolo infinitesimale e della cartografia.
L'idea di questo sistema di riferimento fu sviluppato nel 1637 in due scritti da Cartesio e, indipendentemente, da Pierre de Fermat, anche se Fermat non pubblicò la sua scoperta. Nella seconda parte del suo Discorso sul metodo, Cartesio introduce la nuova idea di specificare la posizione di un punto o di un oggetto su una superficie usando due rette che si intersecano in un punto come strumenti di misura, idea ripresa in "La Geometria" .
Un sistema di coordinate cartesiane ortogonale in due dimensioni è semplicemente chiamato piano cartesiano, ed è costituito da:
Il piano cartesiano, che viene spesso chiamato "xy" dal nome degli assi, può essere immaginato, pensando che il piano sia immerso orizzontalmente nello spazio fisico (pavimento), e mettendosi in piedi in un punto con il braccio sinistro teso in avanti e il braccio destro teso di lato in modo da formare con le due braccia un angolo retto: il punto sul quale si rappresenta l'origine, la direzione del braccio destro rappresenta l'asse delle ascisse positive (dalla parte opposta le ascisse negative), la direzione del braccio sinistro rappresenta l'asse delle ordinate positive (alle spalle le ordinate negative).
Il sistema costituito dalla coppia dei due assi orientati (e implicitamente dall'origine) consente di individuare ogni punto del piano con una coppia di numeri reali chiamati rispettivamente "ascissa" e "ordinata" del punto, i cui valori assoluti rappresentano le distanze del punto rispettivamente dall'asse y (ordinata) e dall'asse x (ascissa). Le coordinate di un punto generico del piano o di un punto che si pensa variabile spesso si denotano con formula_1 e formula_2. I punti sull'asse "x" hanno quindi ordinata "y"=0, mentre i punti sull'asse "y" hanno ascissa "x"=0; di conseguenza l'origine ha coordinate "x"=0 e "y"=0. Talora il sistema dei due assi si denota con formula_3.
Un generico punto si può quindi esprimere scrivendo formula_4
oppure formula_5. Ad esempio, i punti formula_6 e formula_7 hanno la stessa ascissa (quindi si trovano su una retta parallela all'asse "y"), mentre i punti formula_8 e formula_9 hanno la stessa ordinata (quindi si trovano su una retta parallela all'asse "x"). In particolare: se due punti hanno la stessa ascissa ma ordinate opposte sono simmetrici rispetto all'asse "x"; se due punti hanno la stessa ordinata ma ascisse opposte sono simmetrici rispetto all'asse "y"; se due punti hanno coordinate opposte sono simmetrici rispetto all'origine.
Il piano cartesiano viene suddiviso in quattro regioni denominate quadranti, indicate mediante numeri romani progressivi in senso antiorario:
Il piano cartesiano permette di rappresentare graficamente funzioni di due variabili del tipo formula_10 in cui x è la variabile indipendente e y la variabile dipendente. Ciò permette di visualizzare la "forma" di curve e risolvere graficamente sistemi di più equazioni come intersezioni tra le curve corrispondenti.
Per definizione, esiste una corrispondenza biunivoca fra i punti del piano cartesiano e le coppie ordinate di numeri reali. L'insieme di tutte le coppie di numeri reali, formula_11 è un formula_12-spazio vettoriale. La base canonica di formula_11 è formula_14 ove formula_15 ed formula_16. Gli elementi di E hanno un importante significato geometrico: sono i versori fondamentali sul piano, rispettivamente formula_17 e formula_18. Ciò vuol dire, per la definizione stessa di base di uno spazio vettoriale, che il piano cartesiano è generato dai versori fondamentali e che ogni punto del piano è esprimibile, "in modo unico", come combinazione lineare dei versori fondamentali (ciò giustifica l'espressione dei punti del piano cartesiano).
Si noti inoltre che ogni asse cartesiano è sottospazio vettoriale del piano cartesiano.
Aggiungendo una terza dimensione al piano otteniamo lo spazio euclideo tridimensionale, che è la modellizzazione a noi più familiare dello spazio fisico, e quella usata in meccanica classica: un sistema di assi cartesiani può quindi essere usato come sistema di riferimento per localizzare degli oggetti nello spazio, attribuendogli delle coordinate.
Essendo una diretta generalizzazione del piano cartesiano, un sistema di riferimento cartesiano tridimensionale è formato da tre rette orientate perpendicolari tra loro e incidenti in un punto, denominato origine degli assi. I tre assi (chiamati solitamente "x", "y" e "z") identificano tre piani nello spazio ("xy", "xz" e "yz"), che dividono lo spazio in otto "ottanti", simili ai quattro quadranti formati dagli assi cartesiani in due dimensioni. Ogni punto è identificato da 3 coordinate, che rappresentano ognuna la distanza del punto al piano formato dagli altri due.
Come nel caso del piano, ogni punto dello spazio tridimensionale può essere individuato da un vettore nello spazio tridimensionale (indicato come formula_19 e viene espresso come combinazione lineare dei tre versori di base, indicati convenzionalmente con formula_20, formula_21 e formula_22:
dove "x", "y" e "z" rappresentano proprio le coordinate nel punto nel sistema di riferimento formato dalla base formula_24.
Il piano cartesiano (e più in generale il sistema di riferimento cartesiano a "n" dimensioni) ha permesso di conciliare la geometria e l'algebra in un'unica branca della matematica: la "geometria analitica" (chiamata così dall'analisi matematica). Per esempio nel piano cartesiano una retta può essere espressa mediante un'equazione di primo grado a due variabili che rappresenta una funzione lineare del tipo ax+by=c; l'intersezione di due (o più) rette rappresenta un sistema di equazioni lineari.
Le equazioni di cui si è detto prima possono essere espresse in due forme: la forma esplicita e la forma implicita.
Nel caso ad esempio di una retta, la prima consiste un'equazione del tipo "y = mx + q", mentre la seconda si presenta come "ax + by + c = 0". Per passare dalla forma implicita a quella esplicita basta portare tutti i termini escluso "by" nel secondo membro e poi dividere per "b" ("principio di equivalenza delle equazioni"). Si noti che, nella forma esplicita, il termine noto q, detto intercetta o ordinata all'origine, indica l'ordinata del punto di intersezione della retta con l'asse y, mentre il coefficiente dell'incognita x, m, è detto coefficiente angolare ed indica la "pendenza" della retta.
Naturalmente il passaggio dalla forma implicita a quella esplicita è possibile solo se il coefficiente b è diverso da zero, cioè solo se la retta non è parallela all'asse delle ordinate.
Dati due punti distinti formula_25 e formula_26, l'equazione della retta passante per quei punti è: formula_27 anche detta formula_28 dove formula_29 è il coefficiente angolare dato da formula_30
</text>
</doc>
<doc id="338671" url="https://it.wikipedia.org/wiki?curid=338671">
<title>Triangolo isoscele</title>
<text>
In geometria, si definisce triangolo isoscele un triangolo che possiede due lati congruenti.
Vale il seguente "teorema": "Un triangolo è isoscele se e solo se ha due angoli congruenti". Questo teorema costituisce la quinta proposizione del Libro I degli Elementi di Euclide ed è noto come pons asinorum.
In un triangolo isoscele la bisettrice relativa all'angolo al vertice coincide con la mediana, l'altezza e l'asse relativi alla base. 
Particolari triangoli isosceli sono i triangoli equilateri e i triangoli rettangoli isosceli.
Esistono anche triangoli isosceli acutangoli e ottusangoli. 
I triangoli isosceli rettangoli sono tutti simili tra di loro, come i triangoli equilateri.
Un triangolo isoscele che non sia equilatero è invariante solo per la riflessione rispetto alla bisettrice dell'angolo diverso dai due rimanenti. Il suo gruppo di simmetria, oltre alla trasformazione identità, comprende solo questa riflessione e quindi è isomorfo al gruppo di due elementi, ovvero al gruppo moltiplicativo sull'insieme formula_1.
Teorema 1: Condizione necessaria e sufficiente affinché un triangolo con la base parallela agli assi sia isoscele è che abbia i due lati di coefficiente angolare opposto.
Dimostrazione.
Date le tre rette
ne calcoliamo l'intersezione.
Ora calcoliamo la distanza dei segmenti formula_14 e formula_15.
Quindi il triangolo è isoscele sulla base formula_18. In modo analogo si dimostra il caso della base parallela all'asse formula_19.
Viceversa costruiamo un triangolo isoscele con la base parallela all'asse delle ascisse.
Dati i due punti:
poiché il vertice di un triangolo isoscele giace sulla stessa retta del punto medio della base, prima troviamo formula_22 e poi formula_23.
Quindi troviamo formula_23, che avrà la stessa ascissa di formula_22 e diversa ordinata.
Verifichiamo che il triangolo è isoscele:
Ora calcoliamo il coefficiente angolare dei due lati:
Teorema 2: Condizione necessaria e sufficiente affinché un triangolo con la base parallela alla bisettrice di due quadranti sia isoscele è che abbia i due lati di coefficiente angolare inverso.
Dimostrazione.
Date le tre rette
ne calcoliamo l'intersezione.
Ora calcoliamo la distanza dei segmenti formula_14 e formula_15.
Quindi il triangolo è isoscele sulla base formula_18. In modo analogo si dimostra il caso della base parallela all'asse formula_19.
Viceversa costruiamo un triangolo isoscele con la base parallela alla bisettrice del primo e terzo quadrante (lo stesso vale per quella parallela alla bisettrice del secondo e quarto quadrante).
Dati i due punti:
poiché il vertice di un triangolo isoscele giace sulla stessa retta del punto medio della base, prima troviamo formula_22 e poi formula_23.
Quindi troviamo formula_23, che si trova sulla retta di equazione formula_58 perpendicolare alla base e passante per formula_22.
dove formula_61 è un numero reale arbitrario diverso da formula_62.
Verifichiamo che il triangolo è isoscele:
Ora calcoliamo il coefficiente angolare dei due lati:
</text>
</doc>
<doc id="391805" url="https://it.wikipedia.org/wiki?curid=391805">
<title>Cono</title>
<text>
In geometria, il cono è un solido di rotazione che si ottiene ruotando un triangolo rettangolo intorno a uno dei suoi cateti.
L'asse del cono è il cateto intorno a cui il solido è costruito; la base del cono è altresì il cerchio ottenuto dalla rotazione dell'altro cateto.
Il vertice del cono è, infine, il punto dell'asse opposto a quello dell'intersezione con la sua base.
L'aggettivo che definisce gli oggetti di natura simile al cono è "conico"; da esso derivano anche le curve e le figure piane cosiddette "coniche", ovvero risultanti dall'intersezione di un piano con un cono.
In matematica un cono può essere considerato come una piramide di base circolare, avente quindi numero infinito di facce oblique.
Un cono il cui vertice è tagliato da un piano parallelo alla sua base è detto "tronco di cono". Il termine cono viene talvolta esteso a figure più generali:
Il termine "cono" senza ulteriori specificazioni indica generalmente un cono circolare retto.
Il volume formula_1 di un cono con altezza formula_2 e con base di raggio formula_3 è formula_4 del volume del cilindro che ha le stesse dimensioni. Quindi:
Se la base è ellittica di semiassi formula_6 e formula_7:
Si può calcolare il volume del cono per mezzo del calcolo integrale come il volume del solido ottenuto dalla rotazione di una retta formula_9 con coefficiente angolare positivo (per semplicità passante per l'origine degli assi) attorno all'asse delle ascisse. Si ha:
Essendo formula_12 l'angolo acuto formato dalla retta formula_9 con l'asse delle ascisse, da considerazioni trigonometriche si ha che:
e poiché il coefficiente angolare formula_15 è uguale alla tangente goniometrica di formula_12, elevando al quadrato ambo i membri della precedente equazione si ha:
da cui si ottiene:
L'area totale formula_19 di una superficie conica è data dalla somma dell'area della base formula_20 con l'area laterale formula_21:
dove:
avendo definito l'apotema formula_25 del cono come
Sostituendo nella formula, si ottiene infine:
</text>
</doc>
<doc id="328338" url="https://it.wikipedia.org/wiki?curid=328338">
<title>Ottagono</title>
<text>
L’ottagono, in geometria, è un poligono con otto lati.
Per ottagono regolare si intende un ottagono convesso avente i lati della stessa lunghezza e gli angoli della stessa ampiezza (pari a 135°).
L'area dell'ottagono regolare di lato "a" si ricava con la seguente formula:
L'apotema di un ottagono di lato "a" è dunque pari a 
formula_2
È possibile costruire un ottagono regolare con riga e compasso. Il procedimento è illustrato, in 18 passi, dalla seguente animazione. Da notare che l'apertura del compasso rimane fissa nei passaggi da 7 a 10.
Il numero 8 fu importante soprattutto nell'arte Cristiana per il significato di questo numero come si enuncia nelle parole di Sant'Ambrogio:
Sette sono i giorni della Creazione secondo la Genesi, sette i giorni della settimana e l'ottavo è il giorno in più, l'eternità. L'otto simboleggiato dall'ottagono, infatti, rappresenta l'equilibrio cosmico nonché lo spirito universale per gli islamici.
La forma ottagonale nell'architettura ebbe subito grande importanza anche se la diffusione ne è limitata. Edifici a pianta ottagonale si iniziano a vedere nell'architettura romanica dove ne sono esempi i battisteri. 
Sette sono i giorni della Creazione secondo la Genesi, sette i giorni della settimana e l'ottavo è l'eternità. Lo studioso Jacques Le Goff scrive:: " In un trattato edito nella "Patrologia del Migne", Ugo di San Vittore esponendo i dati numerici simbolici secondo le Scritture, spiega il significato delle ineguaglianze tra i numeri [...] 8 maggiore del 7 è l'eternità dopo la vita terrena - si ritrova l'8 dell'ottagono di Aquisgrana, di San Vitale a Ravenna, del Santo Sepolcro, della Gerusalemme Celeste ". Anche i battisteri di Parma, Cremona, Firenze, Pistoia, Ascoli Piceno e i battisteri di Ravenna (Battistero Neoniano, Battistero degli ariani) sono ottagoni. "Dei battisteri antichi che ancora si conservano, la maggior parte ha una struttura ottagonale, che si ispira soprattutto al Battistero di San Giovanni in Laterano, modello imitato per secoli. La forma rappresenta l'ottavo giorno della settimana, "il nuovo giorno", in cui inizia l'era del Cristo: dopo i sei giorni della creazione e dopo il settimo, cioè il sabato, l'ottavo rappresenta l'Eternità, la resurrezione di Gesù e quella dell'umanità". L'otto è dunque un numero escatologico e l'autore della "Lettera di Barnaba", nel commento alle Scritture, fa dire a Dio: "Mettendo fine all'universo, darò inizio all'ottavo giorno, vale a dire a un altro mondo. Per questo motivo celebriamo con gioia l'ottavo giorno, nel quale Gesù è risorto" (par. 16).
Questa simbologia è talvolta chiamata dai Padri della Chiesa "ogdoade", "gruppo di otto". Essa rappresenta il mondo nuovo, nato dalla Resurrezione di Cristo, e la dimora celeste dei beati. Origene parla del "mistero dell'ogdoade" e Ilario di Poitiers di "sacramentum ogdoadis": è attraverso il battesimo che si resuscita con Cristo e si diventa cittadini del regno di Dio.
Nell'architettura moderna una "casa ottagonale" è una casa costruita con la forma di un ottagono la "Poplar Forest" di Thomas Jefferson, vicino a Lynchburg (Virginia) e la "Tayloe House" a Washington, DC, progettata da William Thornton per il colonnello John Tayloe.
In Italia Castel del Monte (vicino ad Andria, in Puglia), rappresentato anche sulle monete da un centesimo di Euro, è un edificio di pianta ottagonale, come ottagonali sono anche le torrette presenti ad ogni vertice.
Torre Santa Sabina invece ha la forma di ottagono concavo.
In architettura islamica all'interno delle moschee o in certi edifici civili del passato sono presenti gli howz, delle vasche per le abluzioni o per mera funzione decorativa che hanno forma ottagonale.
 Come trovare l'area di un ottagono
</text>
</doc>
<doc id="400021" url="https://it.wikipedia.org/wiki?curid=400021">
<title>Rotazione (matematica)</title>
<text>
In matematica, e in particolare in geometria, una rotazione è una trasformazione del piano o dello spazio euclideo che sposta gli oggetti in modo rigido e che lascia fisso almeno un punto, nel caso del piano, o una retta, nel caso dello spazio. I punti che restano fissi nella trasformazione formano più in generale un sottospazio: quando questo insieme è un punto o una retta, si chiama rispettivamente il "centro" e l"'asse" della rotazione. 
Più precisamente, una rotazione è una isometria di uno spazio euclideo che ne preserva l'orientazione, ed è descritta da una matrice ortogonale speciale.
Qualunque sia il numero delle dimensioni dello spazio di rotazione, gli elementi della rotazione sono:
In due dimensioni, una rotazione è una trasformazione formula_1, la quale supposta antioraria dipende da un angolo formula_2, e che trasforma il vettore formula_3 in
Usando la moltiplicazione di matrici la rotazione antioraria può essere descritta così:
La matrice quadrata presente in questa espressione è una matrice ortogonale speciale di rango formula_6. Questa trasformazione è chiamata rotazione antioraria di angolo formula_2 intorno all'origine.
La matrice formula_8 che descrive la rotazione è spesso chiamata matrice di rotazione di angolo formula_2.
Le formule di rotazione possono essere ottenute ragionando nel modo seguente. 
Sia formula_10 un punto qualsiasi e siano formula_11 e formula_12 le sue coordinate polari.
Si ha
il punto formula_14, immagine di formula_15 in una rotazione antioraria di un angolo formula_2, ha coordinate polari formula_17. Le sue coordinate cartesiane sono perciò date dal sistema precedente, ove si ponga formula_18 al posto di formula_12:
applicando le formule di addizione di seno e coseno e tenendo conto anche delle formule iniziali, si ottengono le formule di rotazione, infatti:
Una rotazione si esprime in modo più conciso interpretando il piano come piano complesso: una rotazione equivale al prodotto per un numero complesso di modulo unitario. 
In questo modo, ad esempio, la rotazione di angolo formula_2, con centro nell'origine, si scrive come
L'insieme dei numeri complessi con modulo unitario è algebricamente chiuso rispetto al prodotto, formando così un gruppo abeliano, chiamato il gruppo circolare: l'interpretazione complessa delle rotazioni del piano può essere allora espressa come il fatto che il gruppo circolare e il gruppo ortogonale speciale formula_24 sono isomorfi.
In tre dimensioni, una rotazione è determinata da un "asse", dato da una retta formula_25 passante per l'origine, e da un "angolo" formula_2 di rotazione. Per evitare ambiguità, si fissa una direzione dell'asse, e si considera la rotazione di angolo formula_2 effettuata in senso antiorario rispetto all'asse orientato. La rotazione è descritta nel modo più sintetico scrivendo i vettori dello spazio in coordinate rispetto ad una base ortonormale formula_28, dove formula_29 è il vettore di lunghezza uno contenuto in formula_25 e avente direzione giusta. La rotazione intorno all'asse formula_31 trasforma il vettore di coordinate formula_32 in:
Una rotazione generale in 3 dimensioni può essere espressa come una composizione di 3 rotazioni intorno a tre assi indipendenti, come ad esempio gli assi formula_34. Quindi dati tre angoli formula_35, che indicano rispettivamente di quanto si deve ruotare intorno a ognuno degli assi, la matrice di rotazione risulta:
formula_36
Senza cambiare base, la rotazione di un angolo formula_2 intorno ad un asse determinato dal versore formula_38 (ossia un vettore di modulo unitario) è descritta dalla matrice seguente:
Ponendo formula_40 oppure formula_41 oppure formula_42 si ottiene rispettivamente la rotazione attorno all'asse formula_43 all'asse formula_44 e all'asse formula_45 
Tale matrice è stata ottenuta scrivendo la matrice associata alla trasformazione lineare (rispetto alle basi canoniche nel dominio e codominio) della formula di Rodrigues.
In molte applicazioni risulta conveniente usare l'algebra dei quaternioni per effettuare rotazioni nello spazio tridimensionale.
In uno spazio euclideo di dimensione arbitraria, una rotazione è una trasformazione lineare dello spazio in sé che è anche una isometria, e che mantiene l'orientazione dello spazio. Le matrici formula_46 che realizzano queste trasformazioni sono le matrici ortogonali speciali.
</text>
</doc>
<doc id="401720" url="https://it.wikipedia.org/wiki?curid=401720">
<title>Riflessione (geometria)</title>
<text>
In matematica, e più precisamente in geometria, una riflessione è una trasformazione della retta, del piano o dello spazio che "specchia" tutti i punti rispetto a (rispettivamente) un punto, una retta, o un piano (detti rispettivamente centro, asse o piano di riflessione).
Sia "π" un iperpiano in uno spazio euclideo di dimensione "n" passante per l'origine. In altre parole, "π" è un sottospazio vettoriale di dimensione "n" − 1. 
Una riflessione rispetto a "π" è la trasformazione lineare data da
dove "a" è un qualsiasi vettore ortogonale a "π", e "v"·"a" è il prodotto scalare fra "v" ed "a".
Sia "p" un punto nello spazio euclideo. Una riflessione rispetto a "p" è la trasformazione lineare data da
Nel piano euclideo, due punti A e A' si dicono simmetrici rispetto a una retta "r" 
(cui non appartengono) quando "r" è l'asse del segmento [AA']. Il punto A' è il "simmetrico" di A rispetto ad "r" e viceversa. 
La corrispondenza biunivoca che associa ad ogni punto A che non appartiene ad "r" il punto A' suo simmetrico, e ad ogni punto C in "r" associa il punto C stesso, è detta simmetria assiale di "asse r" nel piano considerato.
La simmetria assiale è un"'isometria del piano", cioè conserva la lunghezza dei segmenti. 
Alcuni autori utilizzano la notazione formula_3 per indicare la simmetria assiale di asse "r"; il simmetrico di A si scrive quindi formula_4.
La simmetria assiale è involutoria, cioè coincide con la propria inversa e composta con se stessa dà l'identità. 
Infine, la simmetria assiale è un"'isometria di tipo inverso", cioè inverte l'orientazione degli oggetti (ad esempio, una coppia di assi ortogonali, il senso di percorrenza dei lati di un triangolo, etc.)
Si dice simmetria assiale di asse r la trasformazione geometrica che lascia invariata la retta r e che associa ad ogni punto P del piano non appartenente ad r il punto Q in modo tale che il segmento PQ sia perpendicolare alla retta r e abbia come punto medio H, piede della perpendicolare condotta da P a r.
Data l'equazione dell'asse di simmetria formula_5 e il segmento di estremi formula_6 e formula_7, la retta passante per P e Q è perpendicolare all'asse di simmetria (pertanto formula_8) e lo interseca nel punto medio H di coordinate
formula_9
Poiché H appartiene all'asse, vale la seguente equazione:
formula_10
Il coefficiente angolare della retta passante per P e Q si può scrivere come
formula_11
Pertanto,
formula_12
Per determinare le coordinate del punto Q, simmetrico di P, si ricorre al sistema di equazioni
formula_13
Da cui si ricava
formula_14
formula_16
formula_18
formula_20
formula_22
formula_24
formula_26
La riflessione è un tipo di corrispondenza biunivoca detta affinità che può essere ortogonale, quando il piano di riflesso (specchio) è ortogonale al piano della figura oggettiva, altrimenti obliqua.
</text>
</doc>
<doc id="480117" url="https://it.wikipedia.org/wiki?curid=480117">
<title>Perpendicolarità</title>
<text>
La perpendicolarità è un concetto geometrico che indica la presenza di un angolo retto tra due entità geometriche. Queste possono essere ad esempio due rette in un piano, oppure una retta ed un piano o due piani incidenti nello spazio.
Il significato fondamentale del termine si riferisce alla posizione di due linee rette.
Nel piano due rette si dicono perpendicolari, o equivalentemente ortogonali, se si incontrano formando angoli uguali (che si dicono retti). Due segmenti si dicono perpendicolari se tali sono le rette cui essi appartengono. Nel caso di rette nello spazio, si osservi che se esse sono incidenti esiste un piano (unico) che le contiene entrambe, e quindi si può applicare la definizione precedente considerando gli angoli da esse formati proprio su questo piano. 
Molti teoremi geometrici e trigonometrici riguardano proprietà strettamente collegate alla perpendicolarità.
In un sistema di coordinate cartesiane ortogonali gli assi di riferimento (in tre dimensioni gli assi x, y e z) sono mutuamente perpendicolari. Ogni triangolo rettangolo è definito da due segmenti perpendicolari, i suoi cateti: i cateti di triangoli rettangoli servono a definire le funzioni angolari che sono alla base della trigonometria.
Una retta nel piano cartesiano può essere descritta in vari modi, e per ciascuno di questi esistono delle condizioni per determinare se due rette sono perpendicolari. Ad esempio, due rette descritte nella forma
sono perpendicolari se e solo se
Avendo definito la perpendicolarità fra rette, è facile estendere la definizione ai piani. In particolare, una retta ed un piano incidenti si dicono perpendicolari se la retta è perpendicolare a qualsiasi retta del piano passante per il punto in comune con la retta data.
Affinché la suddetta condizione sia soddisfatta, è sufficiente che la retta data sia perpendicolare a due di queste.
La perpendicolare a tutte le rette appartenenti ad un piano si dice normale al piano.
Due piani nello spazio si dicono perpendicolari se esiste una retta in uno dei due piani perpendicolare all'altro piano.
La nozione di perpendicolarità fra rette e piani può essere estesa a linee e superfici curve, purché per esse siano definiti rette e piani tangenti. In questo caso, ogni punto su una curva planare o superficie ha un vettore perpendicolare, detto normale alla curva o normale alla superficie, che è quello passante per il punto e perpendicolare alla retta o al piano tangenti. Due curve o due superfici si dicono perpendicolari se tali sono le normali in un determinato punto.
La motivazione intuitiva di questa definizione è che, se consideriamo curve e superfici sufficientemente "regolari", esse ci appaiono tanto più simili a una retta o a un piano quanto più le "ingrandiamo", e quindi possiamo approssimarle localmente con questi due enti. La retta tangente in un punto a una curva, ad esempio, è proprio quella retta che approssima meglio, nelle vicinanze di quel punto, la curva stessa.
</text>
</doc>
<doc id="427952" url="https://it.wikipedia.org/wiki?curid=427952">
<title>Lunghezza di un arco</title>
<text>
In matematica, la lunghezza di un arco è un numero reale positivo che misura intuitivamente l"'estensione" di un arco o di una curva.
Benché la definizione di lunghezza di un segmento o di un "cammino poligonale" sia chiara da tempo, una definizione generale soddisfacente di "lunghezza d'arco" è relativamente recente. Questo problema, chiamato anche rettificazione, è stato prima affrontato per curve specifiche, e quindi risolto grazie al calcolo infinitesimale. La definizione risultante, accettata adesso da tutti i matematici, funziona per un insieme molto vasto di curve, dette rettificabili.
Scelto un numero finito di punti lungo la curva e connettendo ogni punto al successivo con un segmento, la somma delle lunghezze dei segmenti è la lunghezza del "cammino poligonale". La lunghezza del segmento sarà definita come la distanza tra i due estremi.
La lunghezza della curva è il più piccolo numero che la lunghezza del cammino poligonale non può superare, ovvero è l'estremo superiore della lunghezza del cammino della poligonale, al variare delle poligonali.
In termini matematici sia formula_1 la curva e formula_2 uno spazio metrico con la distanza "d". Per definire la poligonale bisogna scegliere i punti sulla curva. Sia quindi formula_3 una partizione dell'intervallo formula_4
La lunghezza della poligonale, è:
e la lunghezza della curva è l'estremo superiore di questa quantità al variare della partizione:
Se tale valore non è infinito, la curva si dice rettificabile. Le curve di Peano e di Koch non sono rettificabili. La lunghezza di una curva non dipende dalla sua parametrizzazione.
Se una curva è derivabile con continuità allora è rettificabile: per ogni punto "t" dell'intervallo è definita una velocità e si può dimostrare che la lunghezza definita come sopra è uguale all'integrale di questa velocità su "I" (quando la curva è in forma parametrica):
dove formula_9 è la norma indotta dalla distanza usata nella definizione sopra. Usando la nozione di integrale di linea si può scrivere anche:
A volte è utile conoscere la lunghezza del grafico di una funzione formula_11. In questo caso il grafico si può scrivere come curva formula_12:
Usando la definizione integrale di lunghezza di arco si perviene al risultato (quando la curva è in forma cartesiana):
dove formula_15 è la lunghezza dell'"i"-esima congiungente, data (per teorema di Pitagora) da formula_16, con formula_17. La lunghezza della curva tra "a" e "b" è allora data da
Osservando che, per "N" tendente a infinito, accade che
l'eguaglianza si riduce a
formula_22
Q.E.D.
se la curva bidimensionale è parametrica con x=f(t), y=g(t) la lunghezza dell'arco è:
se la curva è tridimensionale, con x=f(t), y=g(t), z=k(t) la lunghezza dell'arco è:
Durante la storia della matematica, per molto tempo, anche le menti migliori considerarono impossibile calcolare la lunghezza di un arco irregolare. Nonostante Archimede fosse stato pioniere con l'approssimazione rettangolare per trovare l'area sotto una curva con il suo metodo di esaustione, pochi credevano che fosse possibile che le curve avessero una lunghezza definita come le linee diritte. Come spesso avviene nel calcolo i primi risultati furono ottenuti come approssimazioni. Molti iniziarono a inscrivere i poligoni con delle curve e a calcolare la lunghezza dei lati. Usando sempre più lati e usando lati più piccoli, furono in grado di ottenere approssimazioni via via più accurate.
Nel 1600, il metodo d'esaustione consentì di rettificare con metodi geometrici molte curve trascendenti: la spirale logaritmica da parte di Evangelista Torricelli nel 1645 (alcune fonti dicono John Wallis negli anni 1650), la cicloide da parte di Christopher Wren nel 1658, e la catenaria da parte di Gottfried Leibniz nel 1691.
Prima del completo sviluppo del calcolo, le basi per la moderna lunghezza degli archi sotto forma di integrale furono scoperte indipendentemente da Hendrik van Heuraet e Pierre Fermat.
Nel 1659 van Heuraet pubblicò una costruzione mostrando che la lunghezza di un arco poteva essere interpretata come l'area sotto una curva, e lo applicò alla parabola. Nel 1660, Fermat pubblico una teoria più generale che conteneva gli stessi risultati nel suo "De linearum curvarum cum lineis rectis comparatione dissertatio geometrica".
Partendo dai suoi lavori sulle tangenti, Fermat usò la curva
la cui tangente in "x" = "a" ha pendenza:
quindi la retta tangente ha equazione
Successivamente, incrementò "a" di una piccola quantità a "a" + "ε", facendo diventare il segmento "AC" una buona approssimazione per la lunghezza della curva da "A" a "D". Per trovare la lunghezza del segmento "AC" usò il teorema di Pitagora:
che porta a
Per approssimare la lunghezza, Fermat sommò in una sommatoria di segmenti piccoli. Notare che il risultato moderno è:
</text>
</doc>
<doc id="225057" url="https://it.wikipedia.org/wiki?curid=225057">
<title>Congruenza (geometria)</title>
<text>
In geometria, due figure si dicono congruenti (dal latino "congruens": concordante, appropriato), quando hanno la stessa forma e dimensioni, quindi quando sono perfettamente sovrapponibili. Formalmente, sono congruenti quando è possibile trasformare l'una nell'altra per mezzo di una isometria, ovvero per mezzo di una combinazione di traslazioni, rotazioni e riflessioni.
La congruenza di due figure piane si può interpretare visivamente in questo modo: tagliando una figura con le forbici è possibile sovrapporla all'altra in modo che entrambe combacino perfettamente.
Nel suo Grundlagen der Geometrie, Hilbert descrive la congruenza come una delle tre relazioni binarie primitive della geometria euclidea e ne delinea le proprietà transitiva, riflessiva e simmetrica. Pertanto, la congruenza è una relazione d'equivalenza.
Le prime due figure sono congruenti. La terza ha sì la stessa forma, ma è più piccola: essa è perciò simile alle prime due, ma non congruente. L'ultima figura non è né congruente, né simile alle altre tre.
Il simbolo più comunemente usato per la congruenza è il simbolo uguale con una tilde sopra, ≅, che corrisponde al carattere Unicode 'circa uguale' (U+2245). Nel Regno Unito viene a volte usato il simbolo uguale a tre lineette, "≡", (U+2261).
</text>
</doc>
<doc id="225194" url="https://it.wikipedia.org/wiki?curid=225194">
<title>Traslazione (geometria)</title>
<text>
Nella geometria euclidea, una traslazione è una trasformazione affine dello spazio euclideo, che sposta tutti i punti di una distanza fissa nella stessa direzione. La si può anche interpretare come addizione di un vettore costante a ogni punto, o come spostamento dell'origine del sistema di coordinate. In altri termini, se formula_1 è un vettore fisso, la traslazione formula_2 è definita dall'operazione
Sia formula_4 una traslazione, allora l'immagine di un sottoinsieme di punti formula_5 relativo alla funzione formula_4 si chiama «formula_5 traslato di formula_4». L'insieme formula_5 traslato di formula_2 viene indicato spesso con la notazione formula_11.
Tutte le traslazioni sono isometrie.
La traslazione può anche essere vista come il risultato di una rotazione eseguita da un centro di rotazione che si trova all'infinito nella direzione ortogonale alla direzione di traslazione.
La traslazione nel piano è un'operazione utile in geometria analitica per spostare curve come rette e coniche: questo viene fatto modificando le equazioni che le descrivono.
La formula generale per ottenere un'equazione traslata è la seguente:
dove formula_13 sono le coordinate da ottenere; formula_14 sono quelle dell'equazione originale; formula_15 sono le componenti del vettore associato alla traslazione, utile per traslare le coniche nel piano cartesiano in due dimensioni. Pertanto, alla traslazione di equazioni formula_16 e formula_17 è associato il vettore formula_18 e viceversa.
Dati una funzione formula_19 e le componenti formula_20 e formula_21 del vettore associato a una specifica traslazione, si ottiene una "funzione traslata" formula_22, la cui espressione può essere scritta così:
Dal momento che la traslazione è una trasformazione affine ma non lineare, per rappresentarla con le matrici ci si serve generalmente di "coordinate omogenee". La trasformazione da coordinate cartesiane a coordinate omogenee è definita in questo modo:
La traslazione di un punto in coordinate omogenee lungo il vettore formula_25 si effettua allora per mezzo della "matrice di traslazione":
Moltiplicando la matrice di traslazione per il vettore in coordinate omogenee si ottiene il risultato aspettato:
L'inverso della matrice di traslazione si ottiene invertendo il segno del vettore associato:
Analogamente, il prodotto di matrici di traslazione si ottiene sommando i vettori associati:
Poiché l'addizione vettoriale è un'operazione commutativa, lo è anche la moltiplicazione di matrici di traslazione, a differenza della moltiplicazione fra matrici generiche.
Le traslazioni formano un gruppo. In particolare, la composizione di due traslazioni è una traslazione.
</text>
</doc>
<doc id="221987" url="https://it.wikipedia.org/wiki?curid=221987">
<title>Raggio (geometria)</title>
<text>
Secondo la definizione moderna della geometria, il raggio di un cerchio o di una sfera è un segmento di retta avente un estremo sulla circonferenza o superficie sferica e l'altro estremo nel centro della figura. Per estensione si definisce raggio di un cerchio o di una sfera anche la lunghezza di un tale segmento. Il raggio misura la metà del diametro.
Più generalmente — in geometria, ingegneria, teoria dei grafi, e in molti altri settori — il raggio di qualcosa (per esempio di un cilindro, di un grafo, o di un componente meccanico) è la distanza dei suoi punti più esterni dal centro o asse.
La definizione di raggio data per i cerchi e per le sfere si lascia estendere naturalmente al caso di iperspazi con più di tre dimensioni. Generalmente, un segmento che congiunge un punto di un'ipersfera al suo centro è un raggio dell'ipersfera.
In una spirale il raggio è una funzione dell'angolo. Tutte le circonferenze sono assimilabili a spirali con raggio costante.
Il raggio formula_1 di un cerchio avente diametro formula_2 è
Il raggio formula_1 di un cerchio avente circonferenza formula_5 è
Il raggio formula_1 di un cerchio avente area formula_8 è
Il raggio formula_1 della circonferenza che attraversa tre punti non collineari formula_11 è dato da
dove formula_13 è l'angolo formula_14 La formula è calcolata utilizzando il teorema dei seni.
Con riferimento alla figura a destra, lo stesso raggio formula_1 può anche essere espresso nel modo seguente:
dove formula_17 indica la lunghezza del segmento di estremi formula_18 e formula_19 mentre formula_20 è l'angolo formula_21
Pertanto, se consideriamo tre punti di coordinate formula_22 e formula_23 il raggio della circonferenza che li attraversa è dato da:
Il raggio medio formula_1 di un'ellisse è definito come il raggio di un cerchio di area (superficie) uguale a quella dell'ellisse.
È uguale alla radice quadrata del prodotto dei due semiassi dell'ellisse:
Si definisce "cerchio principale" di un'ellisse, il cerchio con centro nel centro dell'ellisse e di raggio formula_27 uguale al semiasse maggiore dell'ellisse.
Si definisce "cerchio secondario" di un'ellisse, il cerchio con centro nel centro dell'ellisse e di raggio formula_28 uguale al semiasse minore dell'ellisse.
Il raggio di un poligono regolare è il segmento che unisce il centro a uno dei suoi vertici. Pertanto, la lunghezza di tale segmento è uguale al raggio della circonferenza circoscritta al poligono.
Il raggio formula_1 di un poligono di formula_30 lati di lunghezza formula_31 ciascuno, è dato da:
Il raggio in funzione della lunghezza dell'apotema formula_33, è dato da:
Raccogliendo tutte le costanti (nella prima delle due formule), si può scrivere che il raggio formula_1 del poligono è formula_36, ed è dato da formula_37 con formula_38
Si arriva così alla tabella dei seguenti numeri fissi:
che, noti la lunghezza e il numero di lati, permette di calcolare il raggio del poligono.
Il raggio formula_1 di un ipercubo formula_2-dimensionale e lato formula_31, è:
</text>
</doc>
<doc id="248862" url="https://it.wikipedia.org/wiki?curid=248862">
<title>Tangente (geometria)</title>
<text>
Si possono dare varie definizioni intuitive di retta tangente a una curva nel piano.
La parola "tangente" viene da "tangere" cioè toccare. L'idea intuitiva di una retta tangente a una curva è quella di una retta che "tocca" la curva senza "tagliarla" o "secarla" (immaginando la curva come se fosse un oggetto fisico non penetrabile). Una retta che attraversa la curva "tagliandola" è invece chiamata "secante".
Data inoltre una "secante" che passa per due punti distinti P e Q di una curva, si può pensare la tangente in P come la retta cui tende (eventualmente) la secante quando il punto Q si avvicina a P lungo la curva.
Un ulteriore modo di vedere il concetto di tangenza si ha pensando che la tangente in un punto P a una curva γ, è la retta che "approssima" meglio γ nei dintorni di P.
Anche da queste definizioni informali ci si rende conto che possono esistere casi in cui la retta tangente non è definita. Ad esempio, se la curva è costituita dal perimetro di un triangolo e P è un vertice, è ovvio che nessuna delle due definizioni precedenti corrisponde univocamente a una retta passante per P.
Nell'ambito della geometria sintetica si possono dare definizioni rigorose alternative di retta tangente a curve specifiche che funzionano solo per tali curve. Ad esempio la tangente ad una circonferenza di centro O e raggio r in un suo punto P può essere definita come la retta passante per P e avente distanza r da O, o come l'unica retta del piano avente in comune con la circonferenza il solo punto P.
In una geometria a più dimensioni, si può definire il piano tangente ad una superficie in modo simile e, generalizzando, lo spazio tangente.
Per definire la tangente nel caso di una curva generica in genere si ricorre agli strumenti del calcolo infinitesimale.
Posto che una curva sia il grafico di una funzione formula_1, e che siamo interessati al suo punto (x,y), dove y=f(x), diremo che la curva ha una tangente non verticale nel punto (x,y) se e solo se la funzione è derivabile in x. In questo caso il coefficiente angolare della tangente è dato da f'(x). Si ha una tangente verticale se e solo se la curva raggiunge l'altezza di più o meno infinito.
L'equazione della tangente alla curva nel punto (x,y) è data dalla formula:
La stessa formula può essere scritta nella seguente notazione, dove la variabile m rappresenta il coefficiente angolare della funzione.
Se la tangente tocca la curva in un punto e la derivata seconda della funzione nel punto è nulla, mentre non lo è la derivata terza, la tangente è una tangente d'inflessione, ossia una tangente in un punto di flesso della funzione. Esiste in questo caso un intorno finito del punto di tangenza nel quale la curva attraversa la tangente e permane ai due lati opposti della stessa.
Ciò avviene anche quando tutte le derivate sono nulle fino a una derivata dispari non nulla.
</text>
</doc>
<doc id="265456" url="https://it.wikipedia.org/wiki?curid=265456">
<title>Arco (geometria)</title>
<text>
In geometria si definisce arco la parte di una curva regolare compresa fra due suoi punti, detti estremi dell'arco. Curve regolari sono le curve continue e dotate di tangente unica in ogni punto, come ad esempio quelle delle coniche.
Un arco può essere approssimato da una linea spezzata poligonale composta da un numero limitato di segmenti con i vertici disposti a intervalli regolari lungo l'arco stesso. Quando la lunghezza dei segmenti tende a zero, la lunghezza e l'andatura della poligonale tende a quella dell'arco.
Il segmento di retta delimitato dagli estremi di un arco si dice corda sottesa dall'arco. L'asse di tale segmento passa per il centro dell'arco.
Le misure degli archi di circonferenza sono espresse frequentemente in gradi o radianti, assumendo la circonferenza stessa, ovvero parte di essa, come unità di misura.
Per trovare la lunghezza formula_1 di un arco di circonferenza sotteso da due raggi formula_2 che formano un angolo formula_3 (espresso in radianti) fra loro si mettono in relazione, tramite proporzione, la lunghezza dell'arco con quella di tutta la circonferenza e l'angolo sotteso dai raggi con l'angolo giro:
e quindi la lunghezza dell'arco è
</text>
</doc>
<doc id="226642" url="https://it.wikipedia.org/wiki?curid=226642">
<title>Bidimensionalità</title>
<text>
La bidimensionalità è la pertinenza di un oggetto o di un'immagine al campo delle due tradizionali dimensioni spaziali. Larghezza e lunghezza oppure X e Y, nel caso di notazione matematica. È spesso contrapposta e confrontata con la tridimensionalità (3D).
Viene indicata anche con l'abbreviazione 2D o 2-D che letteralmente sta per "due dimensioni".
Un oggetto bidimensionale manca della "terza dimensione", la profondità, e si sviluppa dunque solo su superficie piana. Sono esempi di rappresentazioni 2D, ossia bidimensionali, le fotografie o i disegni.
</text>
</doc>
<doc id="267681" url="https://it.wikipedia.org/wiki?curid=267681">
<title>Spigolo</title>
<text>
La parola spigolo (dal latino "spiculum", diminutivo di "spica", punta) è utilizzata nella geometria solida per indicare i segmenti comuni a due facce di un poliedro, ovvero i lati di tali facce.
Secondo una relazione scoperta da Eulero, il numero di spigoli in un poliedro è pari alla somma fra il numero di facce e il numero di vertici del poliedro stesso, diminuita di due.
Talora è detta spigolo anche la retta di intersezione fra due piani non paralleli.
</text>
</doc>
<doc id="268070" url="https://it.wikipedia.org/wiki?curid=268070">
<title>Parallelismo (geometria)</title>
<text>
Nella geometria euclidea due o più enti sono mutuamente paralleli se tutti i punti dell'uno hanno la stessa distanza minima dall'altro, o dal prolungamento di questo. Inoltre ogni ente geometrico si considera parallelo a sé stesso. La relazione così definita si dice parallelismo ed è una relazione di equivalenza.
La relazione di parallelismo si nota generalmente con una doppia barra verticale o obliqua. Le espressioni formula_1 e formula_2 si leggono ""a" è parallelo a "b"".
Due o più rette distinte nello stesso piano euclideo sono parallele se e solo se non hanno alcun punto in comune, cioè se non si incontrano mai. Due o più segmenti sono paralleli se lo sono le rette che li contengono.
Nel piano cartesiano due rette (distinte o no) di equazioni implicite ax+by+c= 0 e a'x+b'y+c'=0 sono parallele se e solo se ab'=a'b.
Quindi sono parallele se e solo se hanno lo stesso coefficiente angolare (m=m' relativamente alle loro equazioni esplicite y=mx+q e y=m'x+q') o sono verticali (e quindi hanno equazioni x=a e x=b).
Date due rette tagliate da una trasversale, se gli angoli alterni interni sono congruenti, le due rette sono parallele.
In uno spazio euclideo tridimensionale due o più piani distinti sono paralleli se e solo se non hanno alcun punto in comune. Altrettanto vale per una retta ed un piano, che non la contiene, paralleli. È anche vero che due rette distinte parallele non hanno alcun punto in comune, ma è possibile per due rette distinte nello spazio non incontrarsi mai pur senza essere parallele. Si parla in questo caso di rette "sghembe".
Il "postulato delle parallele", meglio noto come quinto postulato di Euclide sostiene che per un punto P si può condurre una ed una sola retta parallela ad una data retta "r" non passante per P. È oggi dimostrato che tale assioma è indipendente dagli altri postulati di Euclide e la sua negazione conduce alle geometrie non euclidee, dove le proprietà del parallelismo classico non sono applicabili.
Due rette parallele proiettate su un piano restano parallele, ma anche due rette sghembe possono avere proiezioni parallele su un piano. Nello spazio a tre dimensioni due rette sono parallele se e solo se questo è vero per due piani non paralleli.
Un piano è parallelo a una retta se e solo se contiene una retta ad essa parallela. (Se e solo se il loro prodotto scalare è uguale a zero).
Due rette, o due piani, sono paralleli se e solo se hanno la stessa fuga; una retta è parallela a un piano se e solo se la sua fuga è contenuta in quella del piano.
</text>
</doc>
<doc id="25259" url="https://it.wikipedia.org/wiki?curid=25259">
<title>Diametro</title>
<text>
In geometria il diametro (indicato con D, d o ⌀) è il segmento che unisce due punti della circonferenza passando per il centro; tali punti sono detti opposti. Misura il doppio del raggio rappresentando la corda massima e la sua relazione con la circonferenza è π.
Per analogia lo stesso concetto può essere esteso alla sfera, intesa come circonferenza in rotazione su sé stessa; per ciò molte considerazioni fatte sulla figura bidimensionale sono valide anche per quella tridimensionale. I punti della sfera uniti dal diametro sono detti antipodali. 
In topologia il diametro di un insieme in uno spazio metrico è definito come la massima distanza tra due punti appartenenti al sottoinsieme stesso.
Considerando il segmento corrispondente valgono le seguenti proprietà:
</text>
</doc>
<doc id="2035" url="https://it.wikipedia.org/wiki?curid=2035">
<title>Geometria</title>
<text>
La geometria (dal greco antico ""γεωμετρία"", composto dal prefisso "geo" che rimanda alla parola "γή" = "terra" e "μετρία", "metria" = "misura", tradotto quindi letteralmente come "misurazione della terra") è quella parte della scienza matematica che si occupa delle forme nel piano e nello spazio e delle loro mutue relazioni.
La geometria coincide fino all'inizio del XIX secolo con la geometria euclidea. Questa definisce come concetti primitivi il punto, la retta e il piano, e assume la veridicità di alcuni assiomi, gli assiomi di Euclide. Da questi assiomi vengono quindi dedotti dei teoremi anche complessi, come il teorema di Pitagora ed i teoremi della geometria proiettiva.
La scelta dei concetti primitivi e degli assiomi è motivata dal desiderio di rappresentare la realtà, e in particolare gli oggetti nello spazio tridimensionale in cui viviamo. Concetti primitivi come la retta ed il piano vengono descritti informalmente come "fili e fogli di carta senza spessore", e d'altro canto molti oggetti della vita reale vengono idealizzati tramite enti geometrici come il triangolo o la piramide. In questo modo, i teoremi forniscono fin dall'antichità degli strumenti utili per le discipline che riguardano lo spazio in cui viviamo: meccanica, architettura, geografia, navigazione, astronomia.
La geometria piana si occupa delle figure geometriche nel piano. A partire dal concetto primitivo di retta, vengono costruiti i segmenti, e quindi i poligoni come il triangolo, il quadrato, il pentagono, l'esagono, ecc.
Le quantità numeriche importanti nella geometria piana sono la lunghezza, l'angolo e l'area. Ogni segmento ha una lunghezza, e due segmenti che si incontrano in un estremo formano un angolo. Ogni poligono ha un'area. Molti teoremi della geometria piana mettono in relazione le lunghezze, angoli e aree presenti in alcune figure geometriche. Ad esempio, la somma degli angoli interni di un triangolo risulta essere un angolo piatto, e l'area di un rettangolo si esprime come prodotto delle lunghezze dei segmenti di "base" e "altezza". La trigonometria studia le relazioni fra gli angoli e le lunghezze.
La geometria solida (o stereometria) studia le costruzioni geometriche nello spazio. Con segmenti e poligoni si costruiscono i poliedri, come il tetraedro, il cubo e la piramide.
I poliedri hanno vertici, spigoli e facce. Ogni spigolo ha una lunghezza, ed ogni faccia ha un'area. In più, il poliedro ha un volume. Si parla inoltre di angoli diedrali per esprimere l'angolo formato da due facce adiacenti in uno spigolo. Molti teoremi mettono in relazione queste quantità: ad esempio il volume della piramide può essere espresso tramite l'area della figura di base e la lunghezza dell'altezza.
La geometria euclidea considera anche alcune figure curve. Le figure "base" sono la circonferenza nel piano e la sfera nello spazio, definite come luogo dei punti equidistanti da un punto fissato. Partendo da queste figure, ne vengono definite altre come il cono. A queste figure vengono associate grandezze analoghe ai poliedri: si parla quindi di lunghezza della circonferenza, di area del cerchio e di volume della sfera.
L'intersezione nello spazio di un cono con un piano forma una nuova figura curvilinea: a seconda dell'inclinazione del piano, questa è una ellisse, una parabola, un'iperbole o una circonferenza. Queste sezioni coniche sono le curve più semplici realizzabili nel piano.
Ruotando una figura intorno ad una retta, si ottengono altre figure curve. Ad esempio, ruotando un'ellisse o una parabola si ottengono l'ellissoide ed il paraboloide. Anche in questo caso, il volume dell'oggetto può essere messo in relazione con altre quantità.
La geometria euclidea non fornisce però sufficienti strumenti per dare una corretta definizione di lunghezza e area per molte figure curve.
La geometria cartesiana (o analitica) ingloba le figure ed i teoremi della geometria euclidea, introducendone di nuovi grazie a due altre importanti discipline della matematica: l'algebra e l'analisi. Lo spazio (ed il piano) sono rappresentati con delle coordinate cartesiane. In questo modo ogni figura geometrica è descrivibile tramite una o più equazioni (o disequazioni).
Rette e piani sono oggetti risultanti da equazioni di primo grado, mentre le coniche sono definite tramite equazioni di secondo grado. Equazioni polinomiali di grado superiore definiscono nuovi oggetti curvi.
Il calcolo infinitesimale permette di estendere con precisione i concetti di lunghezza e area a queste nuove figure. L'integrale è un utile strumento analitico per determinare queste quantità. Si parla in generale quindi di curve e superfici nel piano e nello spazio.
Retta (passante per l'origine), piano (contenente l'origine) e spazio sono esempi di spazi vettoriali di dimensione rispettivamente 1, 2 e 3: infatti ogni punto è esprimile rispettivamente con 1, 2 o 3 coordinate. La geometria cartesiana è facilmente estendibile alle dimensioni superiori: in questo modo si definiscono spazi di dimensione 4 e oltre, come insiemi di punti aventi 4 o più coordinate.
Grazie all'algebra lineare, lo studio delle rette e dei piani nello spazio può essere esteso allo studio dei sottospazi di uno spazio vettoriale, di dimensione arbitraria. Lo studio di questi oggetti è strettamente collegato a quello dei sistemi lineari e delle loro soluzioni.
In dimensione più alta, alcuni risultati possono contrastare con l'intuizione geometrica tridimensionale a cui siamo abituati. Ad esempio, in uno spazio di dimensione 4, due piani possono intersecarsi in un punto solo.
In uno spazio vettoriale l'origine (cioè il punto da cui partono gli assi, di coordinate tutte nulle) gioca un ruolo fondamentale: per poter usare in modo efficace l'algebra lineare, si considerano infatti solo sottospazi passanti per l'origine. In questo modo si ottengono delle relazioni eleganti fra i sottospazi, come la formula di Grassmann.
Nella geometria affine il ruolo predominante dell'origine è abbandonato. I sottospazi non sono vincolati, e possono quindi essere paralleli: questo crea una quantità considerevole di casistiche in più. In particolare, la formula di Grassmann non è più valida. Lo spazio affine è considerato (fino alla scoperta della relatività ristretta) come lo strumento migliore per creare modelli dell'universo, con 3 dimensioni spaziali ed eventualmente 1 dimensione temporale, senza "origini" o punti privilegiati.
Dal XIX secolo in poi l'algebra diventa uno strumento preponderante per lo studio della geometria. Nel tentativo di "abbellire" il quadro, e di ricondurre molte proprietà e teoremi ad un numero sempre minore di proprietà fondamentali, la geometria analitica viene progressivamente inglobata in un concetto più ampio di geometria: si aggiungono i "punti all'infinito" (creando così la geometria proiettiva), e si fanno variare le coordinate di un punto non solo nei numeri reali, ma anche in quelli complessi.
La geometria proiettiva nasce come strumento legato al disegno in prospettiva, e viene formalizzata nel XIX secolo come un arricchimento della geometria cartesiana. La geometria proiettiva include i "punti all'infinito" ed elimina quindi alcune casistiche considerate fastidiose, come la presenza di rette parallele.
In questa geometria molte situazioni si semplificano: due piani distinti si intersecano sempre in una retta, e oggetti differenti della geometria analitica (come le coniche ellisse, parabola e iperbole) risultano essere equivalenti in questo nuovo contesto.
La geometria proiettiva è anche un esempio di compattificazione: similmente a quanto accade con la proiezione stereografica, aggiungendo i punti all'infinito lo spazio diventa compatto, cioè "limitato", "finito".
La geometria algebrica verte essenzialmente sullo studio dei polinomi e delle loro radici: gli oggetti che tratta, chiamati varietà algebriche, sono gli insiemi dello spazio proiettivo, affine o euclideo definiti come luoghi di zeri di polinomi.
Nel XX secolo il concetto di varietà algebrica assume un'importanza sempre maggiore. Rette, piani, coniche, ellissoidi, sono tutti esempi di varietà algebriche. Lo studio di questi oggetti raggiunge risultati impressionanti quando le coordinate dello spazio vengono fatte variare nel campo dei numeri complessi: in questo caso, grazie al teorema fondamentale dell'algebra, un polinomio ha sempre delle radici.
Questo fatto algebrico di grande importanza (esprimibile dicendo che i numeri complessi formano un campo algebricamente chiuso) ha come conseguenza la validità di alcuni teoremi potenti di carattere molto generale. Ad esempio, il teorema di Bézout asserisce che due curve di grado formula_1 e formula_2 nel piano che non hanno componenti in comune si intersecano "sempre" in formula_3 punti, contanti con un'opportuna molteplicità. Questo risultato necessita che il "piano" sia proiettivo e complesso. In particolare, è certamente falso nell'ambito classico della geometria analitica: due circonferenze non devono intersecarsi necessariamente in 4 punti, possono anche essere disgiunte.
Lo studio della geometria nello spazio proiettivo complesso aiuta anche a capire la geometria analitica classica. Le curve nel piano cartesiano reale possono ad esempio essere viste come "sezioni" di oggetti più grandi, contenuti nel piano proiettivo complesso, ed i teoremi generali validi in questo "mondo più vasto e perfetto" si riflettono nel piano cartesiano, pur in modo meno elegante.
Come lo studio della geometria affine fa largo uso dell'algebra lineare, quello delle varietà algebriche attinge a piene mani dall'algebra commutativa.
La geometria differenziale è lo studio di oggetti geometrici tramite l'analisi. Gli oggetti geometrici non sono necessariamente definiti da polinomi (come nella geometria algebrica), ma sono ad esempio curve e superfici, cioè oggetti che, visti localmente con una lente di ingrandimento, sembrano quasi rettilinei o piatti. Oggetti cioè "senza spessore", e magari un po' curvi. Come la superficie terrestre, che all'uomo sembra piatta, benché non lo sia.
Questo concetto di "spazio curvo" è espresso tramite la nozione di varietà differenziabile. La sua definizione non necessita neppure di "vivere" in uno spazio ambiente, ed è quindi usata ad esempio nella relatività generale per descrivere intrinsecamente la forma dell'universo. Una varietà può essere dotata di una proprietà fondamentale, la curvatura, che viene misurata tramite oggetti matematici molto complessi, come il tensore di Riemann. Nel caso in cui lo spazio sia una curva o una superficie, questi oggetti matematici risultano più semplici: si parla ad esempio di curvatura gaussiana per le superfici.
Su una varietà dotata di curvatura, detta varietà riemanniana, sono definite una distanza fra punti, e le geodetiche: queste sono curve che modellizzano i percorsi localmente più brevi, come le rette nel piano, o i meridiani sulla superficie terrestre.
Con la geometria differenziale è possibile costruire un "piano" in cui valgono tutti i postulati di Euclide, tranne il quinto, quello "delle parallele". Questo postulato ha avuto un'importanza storica fondamentale, perché ci sono voluti 2000 anni per dimostrare la sua effettiva indipendenza dai precedenti. Asserisce che, fissati una retta formula_4 ed un punto formula_5 non contenuto in formula_4, esiste un'unica retta formula_7 parallela a formula_4 e passante per formula_5.
Una geometria non euclidea è una geometria in cui valgono tutti gli assiomi di Euclide, tranne quello delle parallele. La sfera, con le geodetiche che giocano il ruolo delle rette, fornisce un esempio semplice di geometria non euclidea: due geodetiche si intersecano "sempre" in due punti antipodali, e quindi non ci sono rette parallele. Un tale esempio di geometria è detta ellittica. Esistono anche esempi opposti, in cui ci sono "così tante" rette parallele, che le rette formula_7 parallele a formula_4 e passanti per formula_5 sono infinite (e non una). Questo tipo di geometria è detta iperbolica, ed è più difficile da descrivere concretamente.
La topologia è infine lo studio delle forme, e di tutte quelle proprietà degli enti geometrici che non cambiano quando questi vengono deformati in modo continuo, senza strappi. La topologia studia tutti gli oggetti geometrici (definiti in modo algebrico, differenziale, o quant'altro) guardando solo la loro forma. Distingue ad esempio la sfera dal toro, perché quest'ultimo ha "un buco in mezzo". Studia le proprietà di connessione (spazi "fatti di un pezzo solo") e di compattezza (spazi "limitati"), e le funzioni continue fra questi.
Le forme degli oggetti vengono codificate tramite oggetti algebrici, come il gruppo fondamentale: un gruppo che codifica in modo raffinato la presenza di "buchi" in uno spazio topologico.
Nel 1872 Felix Klein elaborò un programma di ricerca, l"'Erlanger Programm", in grado di produrre una grande sintesi delle conoscenze geometriche e integrarle con altri settori della matematica, quali la teoria dei gruppi.
Nella prospettiva di Klein una "geometria" consiste nello studio di proprietà di uno spazio che sono invarianti rispetto ad un gruppo di trasformazioni (geometria delle trasformazioni):
La geometria analitica e l'algebra lineare forniscono importanti collegamenti tra l'intuizione geometrica e il calcolo algebrico che sono diventati ormai una parte costitutiva di tutta la matematica moderna e delle sue applicazioni in tutte le scienze.
La geometria differenziale ha trovato importanti applicazioni nella costruzione di modelli per la fisica e per la cosmologia.
La geometria piana e dello spazio fornisce inoltre degli strumenti per modellizzare, progettare e costruire oggetti reali nello spazio tridimensionale: è quindi di fondamentale importanza in architettura e in ingegneria come anche nel disegno e nella computer grafica.
La geometria descrittiva è una disciplina che permette, attraverso determinate costruzioni grafiche, di rappresentare oggetti tridimensionali già esistenti (rilievo) e/o da costruire (progettazione).
L'applicazione informatizzata della geometria descrittiva permette oggi la creazione di superfici e solidi, anche ad alta complessità tridimensionale. Inoltre, e soprattutto, ne permette il controllo in modo inequivocabile di ogni loro forma e dimensione.
I maggiori campi d'impiego della geometria descrittiva sono quelli dell'architettura, dell'ingegneria e quelli del design industriale.
La nascita della Geometria si fa risalire all'epoca degli antichi egizi. Erodoto racconta che a causa dei fenomeni di erosione e di deposito dovuti alle piene del Nilo, l'estensione delle proprietà terriere egiziane variavano ogni anno e dovevano quindi essere ricalcolate a fini fiscali. Nacque così il bisogno di inventare tecniche di "misura della terra" ("geometria" nel significato originario del termine).
Lo sviluppo della Geometria pratica è molto antico, per le numerose applicazioni che consente e per le quali è stata sviluppata, e in epoche remote fu a volte riservata a una categoria di sapienti con attribuzioni sacerdotali.
Presso l'Antica Grecia, , si diffuse massicciamente l'uso della riga e del compasso (sebbene pare che questi strumenti fossero già stati inventati altrove) e, soprattutto, nacque l'idea nuova di usare tecniche dimostrative. La geometria greca servì da base per lo sviluppo della geografia, dell'astronomia, dell'ottica, della meccanica e di altre scienze, nonché di varie tecniche, come quelle per la navigazione. 
Nella civiltà greca, oltre alla geometria euclidea che si studia ancora a scuola, e alla teoria delle coniche, nacquero anche la geometria sferica e la trigonometria (piana e sferica).
</text>
</doc>
<doc id="26274" url="https://it.wikipedia.org/wiki?curid=26274">
<title>Retta</title>
<text>
La retta o linea retta è uno dei tre enti geometrici fondamentali della geometria euclidea. Viene definita da Euclide nei suoi "Elementi" come un concetto primitivo. Un filo di cotone o di spago ben teso tra due punti è un modello materiale che ci può aiutare a capire cosa sia la retta, un ente geometrico immateriale senza spessore e con una sola dimensione.
La retta è illimitata in entrambe le direzioni, e inoltre contiene infiniti punti, cioè è infinita. Viene generalmente contrassegnata con una lettera minuscola dell'alfabeto latino (solitamente con la "r").
La retta è il secondo ente fondamentale della geometria; geometricamente priva di alcuno spessore ha una sola dimensione: la lunghezza.
Una retta può "giacere" (cioè essere contenuta) nel piano o nello spazio tridimensionale.
Due rette nel piano possono essere:
Due rette nello spazio possono essere:
Date due rette sghembe, per ognuna di esse passa un unico piano parallelo all'altra retta. La distanza tra questi due piani equivale alla distanza tra le due rette.
Una retta nel piano cartesiano è descritta da un'equazione lineare
dove i coefficienti formula_2, formula_3 e formula_4 sono dei numeri reali fissati, con formula_2 e formula_3 non contemporaneamente nulli.
Se formula_7 oppure formula_8, è possibile descrivere la stessa retta in "forma esplicita" rispettivamente in una delle due forme seguenti:
dove formula_11 si chiama "coefficiente angolare" e quantifica la pendenza della retta. Nella prima delle equazioni di cui sopra il termine noto formula_12 rappresenta l'ordinata del punto di intersezione della retta con l'asse delle formula_13 ("ordinata all'origine" o "intercetta"), nella seconda il termine noto formula_14 rappresenta l'ascissa del punto di intersezione della retta con l'asse delle formula_15.
Nello spazio euclideo tridimensionale, una retta può essere descritta tramite "equazioni cartesiane" come luogo di intersezione di due piani non paralleli:
In questo caso le soluzioni del sistema dipendono da un solo parametro formula_17 ed è sempre possibile ricavare un insieme di "equazioni parametriche" per la retta:
dove il vettore formula_19 è un vettore parallelo alla retta e il punto formula_20 è un punto appartenente alla retta.
Se formula_21 sono tutti diversi da zero è possibile ricavare le cosiddette "equazioni simmetriche" della retta:
Sia le equazioni cartesiane che le equazioni parametriche della retta non sono univocamente determinate, e sono in effetti infinite.
Nello spazio euclideo formula_23-dimensionale formula_24, una "retta" è un insieme dei punti del tipo
dove formula_26 e formula_27 sono due vettori fissati in formula_24 con formula_27 diverso da zero. Il vettore formula_27 descrive la direzione della retta, mentre formula_26 è un qualsiasi punto della retta. Scelte differenti dei vettori formula_26 e formula_27 possono descrivere la stessa retta.
Questa definizione di retta nello spazio di dimensione formula_23 è una estensione della rappresentazione in "forma esplicita" nel piano descritta sopra. Descrivere invece una retta in "forma implicita" come insieme di vettori che soddisfano delle equazioni lineari è più complicato, perché per il teorema di Rouché-Capelli sono necessarie formula_35 equazioni.
Si definisce come distanza tra due rette formula_36 e formula_37 la distanza minima tra due punti formula_38 e formula_39.
Tale distanza è ovviamente nulla nel caso di due rette che si intersecano. Per esaminare i restanti due casi (rette parallele e sghembe) verrà utilizzata la rappresentazione parametrica, che permette una trattazione unitaria per tutte le dimensioni. Siano dunque date due rette formula_40 e formula_41 di equazioni parametriche rispettivamente:
dove formula_43 e formula_44 sono i loro vettori direzionali e formula_45 e formula_46 i vettori associati al punto formula_47 della retta formula_40 e al punto formula_49 della retta formula_41, relativamente alla terna cartesiana formula_51.
Dato che le rette sono parallele possiamo misurare la distanza a partire da un punto qualsiasi della prima retta. Scegliamo il punto di formula_40 segnato dal vettore formula_45. Ogni punto della retta formula_41 può essere espresso nella forma formula_55. Se chiamo formula_56 il vettore ortogonale a formula_43 che segna la distanza dall'altra retta, allora per le proprietà del prodotto scalare
Ottenuto formula_56 risolvendo la precedente equazione (incognita in formula_60) è sufficiente calcolare la norma di formula_56 quindi con riferimento all'equazione parametrica la distanza formula_62 fra due rette parallele formula_36 e formula_64 si può scrivere come:
dove il vettore formula_27 è un vettore parallelo alle rette e il vettore formula_67 è il vettore che congiunge un punto formula_68 della retta formula_36 e un punto formula_70 della retta formula_64 ovvero la distanza fra due rette parallele è data dalla proiezione del vettore formula_67 nel verso ortogonale alle stesse.
Dimostrazione: dalle formule del prodotto vettoriale, i moduli dei versori sono unitari, resta:
formula_73
Se definiamo formula_56 come il vettore ortogonale a formula_75, la cui norma è la distanza tra le due rette, il nostro problema si riduce a trovare la norma di formula_56. I tre vettori formula_56, formula_43 e formula_44 sono una base, e possiamo quindi facilmente scomporre il vettore formula_80 lungo le tre componenti. Quindi
Molto semplicemente si ricava che
con riferimento all'equazione parametrica la distanza formula_62 fra due rette sghembe formula_36 e formula_64 si può scrivere come:
dove il vettore formula_67 è il vettore che congiunge un punto formula_68 della retta formula_36 che ha vettore parallelo formula_90 e un punto formula_70 della retta formula_64 che ha vettore parallelo formula_93, il vettore formula_94 è il vettore ortogonale formula_95 ovvero la distanza fra due rette sghembe è data dalla proiezione del vettore formula_67 nel verso del vettore formula_94.
Dimostrazione: dalle formule del prodotto scalare il modulo del versore è unitario, resta : formula_98
</text>
</doc>
<doc id="19898" url="https://it.wikipedia.org/wiki?curid=19898">
<title>Circonferenza</title>
<text>
In geometria una circonferenza è il luogo geometrico di punti del piano equidistanti da un punto fisso detto "centro". La distanza da qualsiasi punto della circonferenza dal centro si definisce raggio.
Le circonferenze sono curve chiuse semplici che dividono il piano in una superficie interna ed una esterna (infinita).
La superficie del piano contenuta in una circonferenza, insieme alla circonferenza stessa, prende il nome di cerchio, per cui: 
Per le altre superfici del piano geometrico, la lingua italiana non distingue l'area e il perimetro con due parole differenti. In inglese, oltre alle corrispondenti "circumference" e "circle", la parola "disk" indica una regione del piano con alcune importanti proprietà, che può essere chiusa, oppure aperta, se "non" contiene il cerchio che essa delimita. Nota la circonferenza di un cerchio, per qualsiasi superficie (chiusa) del piano geometrico si può disegnare una circonferenza inscritta ed una circonferenza circoscritta.
La circonferenza è il caso particolare di una ellisse, in cui i due fuochi coincidono in uno stesso punto, che è il centro della circonferenza: l'ellisse ha due centri (detti fuochi), la circonferenza ha invece un solo centro. Si dice quindi che la circonferenza ha eccentricità nulla. 
Ugualmente, la formula di calcolo per l'area del cerchio è un caso particolare della formula per l'area di un'ellisse.
Mediante il calcolo delle variazioni si dimostra che la circonferenza è la Figura piana che delimita la massima area per unità di perimetro quadrato.
Una circonferenza è inoltre un particolare caso di simmetria centrale, dal momento che tutti i punti della circonferenza sono equidistanti dal centro della stessa.
La formula per trovare la lunghezza della circonferenza è:
oppure:
Dove:
In geometria analitica una circonferenza in un piano può essere descritta utilmente sia mediante le coordinate cartesiane, sia mediante le coordinate polari, sia in forma parametrica.
In un sistema di riferimento cartesiano formula_8, la circonferenza di centro formula_9 e raggio formula_6 è il luogo dei punti caratterizzati dall'equazione:
cioè è l'insieme di tutti e soli i punti che distano formula_6 da formula_9.
All'equazione più generale si dà spesso la forma canonica:
collegata alla precedente dalle seguenti eguaglianze:
Da ciò segue che se formula_21 la circonferenza degenera in un solo punto, formula_22, se formula_23 il luogo geometrico (nel piano cartesiano reale) descritto dall'equazione non è una circonferenza, ma l'insieme vuoto.
Se il centro della circonferenza è l'origine formula_24, l'equazione diventa:
Se la circonferenza passa per l'origine formula_24, formula_27 e l'equazione diventa:
Se la circonferenza ha centro sull'asse x, formula_29 e l'equazione diventa:
Se la circonferenza ha centro sull'asse y, formula_31 e l'equazione diventa:
Nelle coordinate polari formula_33 e formula_34 l'equazione della circonferenza con centro nell'origine e raggio formula_6 è evidentemente data dall'equazione:
Una circonferenza formula_37 il cui centro ha coordinate formula_38 e raggio formula_39 viene descritta con la seguente forma parametrica:
formula_40
Basta usare l'equazione formula_41.
A questo problema sono riconducibili anche i seguenti
Basta ricordare che l'asse di una corda passa sempre per il centro della circonferenza. La procedura risolutiva è la seguente:
Il problema ha tre incognite: i coefficienti formula_42 dell'equazione canonica della circonferenza formula_43.
Si impone il passaggio per i tre punti dati dal problema e si ottiene un sistema lineare in tre equazioni e tre incognite formula_42.
Basta ricordare che la distanza della retta tangente dal centro è pari al raggio della circonferenza stessa. La procedura risolutiva è la seguente:
Basta ricordare che in un sistema di secondo grado (retta-circonferenza) la condizione di tangenza si ha quando il sistema ammette due soluzioni reali e coincidenti, cioè quando l"'equazione associata" di secondo grado al sistema ha formula_45.
Questo problema è risolto ricordando che la retta tangente alla circonferenza è perpendicolare al raggio nel suo punto di tangenza. Quindi, salvo casi particolari in cui la tangente è parallela all'asse y, la procedura risolutiva è la seguente:
Alternativamente è sufficiente usare la "formula di sdoppiamento" della circonferenza, così l'equazione della retta tangente alla circonferenza formula_43 nel punto formula_47 è semplicemente l'equazione
dove formula_49 sono dati.
Nel piano complesso una circonferenza di centro l'origine e raggio formula_39 può essere espressa dall'equazione parametrica
per formula_52.
Per rendersi conto che tale formula descrive una circonferenza è sufficiente considerare le equazioni parametriche descritte sopra e confrontarle con la formula di Eulero.
È possibile descrivere una circonferenza nello spazio come intersezione di una sfera S con un piano formula_4. Per calcolare il raggio di una circonferenza descritta nel seguente modo si può utilizzare il teorema di Pitagora:
formula_57.
La circonferenza
formula_58
è l'intersezione del piano
con la sfera "S" di centro origine e raggio 2.
La distanza del centro della sfera dal piano vale formula_60.
La distanza del centro della sfera dal piano è minore del raggio della sfera. Quindi il piano formula_4 interseca la sfera "S". A questo punto il raggio formula_56 della circonferenza si calcola utilizzando il teorema di Pitagora:
Tutte le circonferenze sono simili; di conseguenza, la circonferenza è proporzionale al raggio:
Una retta che incontra una circonferenza in due punti si chiama secante, mentre una che tocca la circonferenza in un solo punto, chiamato punto di tangenza, si chiama tangente. Il raggio che congiunge il centro della circonferenza con il punto di tangenza è sempre perpendicolare alla tangente. Presi due punti sulla circonferenza, questi dividono la circonferenza in due archi. Se i due archi sono della stessa lunghezza si chiamano semicirconferenze. Il segmento che congiunge due punti sulla circonferenza si chiama corda. La corda di lunghezza massima, che passa per il centro, si chiama diametro, ed equivale al doppio del raggio.
Per due punti passano infinite circonferenze, ed il luogo dei loro centri è l'asse del segmento che congiunge i due punti. La perpendicolare condotta dal centro di una circonferenza a una sua corda la divide a metà. Due corde congruenti hanno la stessa distanza dal centro. Se da un punto formula_65, esterno a una circonferenza di centro formula_66 si tracciano le rette formula_6 e formula_68 a essa tangenti, i segmenti di tangente compresi tra formula_65 e i punti di contatto con la circonferenza sono congruenti e il segmento formula_70 è bisettrice dell'angolo formula_71 di vertice formula_65.
Per tre punti non allineati passa una ed una sola circonferenza, il cui centro coincide con l'intersezione degli assi dei segmenti che congiungono i punti. L'equazione della circonferenza passante per i punti formula_73, formula_74, formula_75 si può esprimere nel seguente modo:
dove l'espressione a sinistra è il determinante della matrice.
Date due circonferenze che si intersecano, si definisce asse radicale la retta passante per i due punti in comune ("punti base"). Con semplici calcoli, partendo dall'equazione canonica e indicando con apici i coefficienti della seconda circonferenza, si ottiene che questa retta ha equazione formula_77 ed è perpendicolare alla retta che congiunge i centri delle circonferenze. La definizione si estende facilmente al caso di circonferenze tangenti, chiamando asse radicale la retta tangente alle due circonferenze nel punto comune. Questo concetto può essere ulteriormente generalizzato considerando i fasci di circonferenze. Approccio che, tra l'altro, permette di trattare unitariamente i suddetti casi.
Una "circonferenza topologica" si ottiene considerando un intervallo chiuso sulla retta reale e dotandolo della topologia quoziente che si ha identificando gli estremi.
La circonferenza è dotata di una naturale struttura di varietà differenziabile di dimensione 1, è uno spazio compatto e connesso ma non semplicemente connesso, infatti il suo gruppo fondamentale è il gruppo formula_78 dei numeri interi.
La circonferenza è naturalmente dotata della struttura algebrica di gruppo: possiamo identificare ogni punto della circonferenza con l'angolo che esso forma rispetto ad una semiretta prefissata (in genere l'asse delle ascisse in un sistema di riferimento cartesiano) e definire la "somma" di due punti individuati dagli angoli formula_79 e formula_80 come il punto individuato dall'angolo formula_81. È immediato verificare che la circonferenza dotata di questa operazione verifica le proprietà di un gruppo e che come "gruppo" è isomorfo al gruppo quoziente formula_82.
La circonferenza è un esempio di gruppo di Lie.
</text>
</doc>
<doc id="14564" url="https://it.wikipedia.org/wiki?curid=14564">
<title>Pentagono</title>
<text>
In geometria, un pentagono è un poligono di cinque lati e cinque angoli, congruenti o meno, regolare o irregolare, che può essere concavo o convesso, semplice o complesso (intrecciato). Un caso particolare di pentagono intrecciato è il pentagramma, la cui forma più nota può essere ottenuta da un pentagono regolare estendendone i lati, oppure disegnandone le diagonali: è la cosiddetta stella a cinque punte che si può ripetere un'infinità di volte dentro un pentagono.
La somma degli angoli interni è di 540°.
Per definizione, un pentagono regolare: è
Da queste definizioni si può dedurre che tutte le diagonali del pentagono sono congruenti, in quanto lati omologhi dei triangoli ABC, BCD, CDE, DEA e EAB, che sono a loro volta congruenti: hanno infatti due lati che coincidono con i lati del pentagono, i quali definiscono gli angoli interni del pentagono stesso (lati e angoli interni del pentagono regolare che, come detto, sono congruenti per definizione).
La definizione di pentagono regolare non implica automaticamente che tale poligono sia circoscrivibile o inscrivibile in una circonferenza, ma tale fatto può essere facilmente dimostrato. Bisecando ogni angolo interno del pentagono si ottiene la serie di triangoli AOB, BOC, COD... che sono tutti isosceli, in quanto gli angoli che giacciono sulle loro basi AB, BC, CD... sono ciascuno metà degli angoli interni del pentagono. I segmenti AO e BO sono quindi congruenti; ma lo sono anche BO e CO, CO e DO... di conseguenza:
Stabilito il fatto che un pentagono regolare può essere inscritto in una circonferenza, si può quantificare l'ampiezza degli angoli al centro, ovvero degli angoli che dal centro O della circonferenza sottendono ciascuno dei lati del pentagono:
Il punto E giace sulla circonferenza circoscritta al pentagono, quindi gli angoli AEB, BEC e CED, che sottendono rispettivamente gli archi (e le relative corde / lati) AB, BC e CD, hanno ampiezza ciascuno metà dell'angolo al centro:
Di conseguenza l'angolo interno del pentagono vale:
Esaminiamo ora la relazione fra lati e diagonali. Ogni diagonale del pentagono è parallela al lato opposto (ovvero a quello fra i lati del pentagono che non tocca una delle estremità della diagonale presa in esame). Verificando un caso particolare, si può vedere che gli angoli BEC e ECD sono alterni interni delle rette BE e CD tagliate dalla trasversale CE; essendo tali angoli congruenti (entrambi di ampiezza pari a β), il lato CD e la diagonale BE risultano appunto essere paralleli. Lo stesso vale per ogni altra coppia lato / diagonale del pentagono.
Ulteriore caratteristica dell'angolo β è di comparire un totale di 5 volte in ciascuno dei triangoli costituiti da due diagonali e un lato del pentagono (es. il triangolo BDE). Tali triangoli sono ovviamente isosceli, in quanto è già stato dimostrato che le diagonali del pentagono sono congruenti; in più, l'angolo in B, compreso fra le due diagonali, è metà di ciascuno degli altri due angoli: questo tipo di triangolo, in cui due angoli sono ciascuno il doppio del terzo, viene chiamato Triangolo Aureo, ed è fondamentale per procedere alla costruzione del pentagono regolare secondo il metodo descritto da Euclide.
Osservazione preliminare: il triangolo ABG è isoscele in quanto gli angoli in A e in B sono congruenti: ne consegue che sono congruenti tutti i segmenti come BG e CJ (che serviranno fra poco) che partono dai vertici del pentagono ABCDE per congiungersi ai vertici del pentagono interno GHJKL, costruito dalle diagonali.
Si osservi ora che i triangoli BAE e CJD sono simili, in quanto i lati omologhi sono tutti paralleli fra loro. Vale dunque la proporzione:
Osserviamo ora la diagonale BE, tagliata in G dalla diagonale AC. Il segmento GE ha ovviamente la stessa lunghezza sia di CD (CDEG è un parallelogramma) che di BA, mentre abbiamo già dimostrato che BG e CJ sono congruenti. Quindi possiamo riscrivere la precedente proporzione con i segmenti seguenti:
La proporzione qui sopra ha la forma classica formula_6, quella che definisce la sezione aurea. Ne consegue che la lunghezza s del lato del pentagono rispetto alla sua diagonale d è:
Viceversa:
Nel triangolo BDE si tracci l'altezza dal vertice B al piede S, e si prolunghi il segmento fino ad incontrare la circonferenza circoscritta al pentagono in T. Per costruzione, l'angolo BSE è retto, quindi si può applicare il teorema di Pitagora per calcolare la lunghezza del segmento BS:
Infine si può calcolare la lunghezza del segmento BT, che è un diametro della circonferenza circoscritta, e quindi vale due volte il raggio r della medesima: considerando il fatto che i triangoli BSE e BTE sono simili (sono entrambi rettangoli, e hanno il vertice B in comune), si imposta la proporzione:
da cui
Invertendo quest'ultima espressione possiamo ricavare la lunghezza della diagonale rispetto al raggio:
Il rapporto già calcolato fra lunghezze del diametro e del lato ci consente di ricavare la lunghezza del lato rispetto al raggio:
Infine, per completezza, si può calcolare la lunghezza del segmento ET, che è il lato del decagono inscritto nella stessa circonferenza del pentagono (questo dato sarà utile per descrivere la costruzione del pentagono regolare secondo Tolomeo). Come già detto, il triangolo BTE è rettangolo, quindi si può di nuovo applicare il teorema di Pitagora:
Come già mostrato, il lato e la diagonale del pentagono regolare stanno fra loro come il rapporto aureo. Quella che segue è la dimostrazione che tale rapporto è incommensurabile, ovvero che il rapporto fra dette lunghezze non può essere espresso da un numero razionale.
La dimostrazione che segue prende le mosse dalla Proposizione 2 del libro X degli Elementi di Euclide: "Se di due grandezze disuguali veniamo a sottrarre [...] la minore dalla maggiore quante volte sia possibile, e quella restante non misura mai la grandezza ad essa precedente, le grandezze sono incummensurabili".
Prendiamo quindi in esame (vedi Fig. 3) il pentagono ABCDE, la sua diagonale BE e il lato BA, minore di BE. Occorre ricavare la differenza fra queste due lunghezze: si traccia quindi l'arco AH, centrato in B e di raggio BA, fino a intersecare BE nel punto H; e dato che BH e BA sono congruenti, il segmento HE è la differenza cercata.
H divide il segmento BE in "media ed estrema ragione": ciò significa che BH e HE stanno anch'essi fra loro come diagonale e lato di un pentagono regolare, che può essere facilmente costruito. Tracciando l'arco EAJ, centrato in H e con raggio HE, si trova il punto J sul segmento BA. A questo punto BH è la diagonale del pentagono BFGHJ, in cui i lati BJ e JH sono congruenti con HE (non occorre in questa sede descrivere in che modo si determinano i punti F e G).
Ricapitoliamo: dati i segmenti BE (diagonale) e BA = BH (lato del pentagono) si trova la loro differenza HE = BJ; ma BH e BJ sono rispettivamente diagonale e lato del nuovo pentagono BFGHJ. Per proseguire secondo quanto disposto dalla citata proposizione di Euclide, occorre adesso trovare la differenza fra BH e BJ: ripetendo il meccanismo descritto sopra si trova il punto M, che fa parte del pentagono BKLMN. Ripetendo ancora il procedimento si ottiene il pentagono BOPQR, e così via: per quante volte si ripeta la costruzione, si troveranno sempre coppie di segmenti che stanno fra loro come il rapporto aureo; non arrivando mai ad ottenere due segmenti "misurabili" (secondo la proposizione di Euclide) ne risulterà che lato e diagonale del pentagono regolare sono incommensurabili.
L'apotema può essere calcolato sottraendo la lunghezza di un raggio dal segmento h (si vedano per chiarezza in Fig. 2 i segmenti BT e AF, congruenti):
Un ultimo valore, che serve per calcolare l'area del pentagono regolare, è il cosiddetto Numero Fisso, definito come rapporto fra apotema e lato:
L'uso della trigonometria consente la determinazione delle lunghezze significative del pentagono regolare in modo più semplice rispetto a quello descritto sopra, anche se presenta alcune controindicazioni:
Dalla Fig. 4 si può ricavare il modo più semplice per determinare le varie lunghezze in basa al raggio della circonferenza circoscritta. Dati l'angolo α che coincide con l'angolo al centro del pentagono; e β, metà di tale valore, si ricava facilmente che il lato del pentagono CD vale:
L'apotema OF:
La diagonale BE:
L'area del pentagono è la somma delle aree di 5 triangoli con base pari al lato e altezza pari all'apotema. Di seguito le formule per il calcolo dell'area in base alle lunghezze del lato e del raggio della circonferenza circoscritta:
Nella tabella che segue si danno i valori esatti ed approssimati delle lunghezze del raggio r della circonferenza circoscritta, del lato s, della diagonale d, dell'apotema a, e dell'area A, in base a ciascuno degli stessi elementi lineari:
Nel corso della storia sono stati proposti vari modi per disegnare un Pentagono Regolare con riga e compasso. Di seguito ne vengono mostrati alcuni esempi notevoli.
Nei suoi Elementi, Euclide prende in considerazione il triangolo aureo costituito da due diagonali e un lato del pentagono regolare, del quale sfrutta le seguenti caratteristiche:
In Fig. 5 è mostrata una sintesi delle proposizioni descritte da Euclide, con la sola differenza per cui, per semplicità, invece di inscrivere il pentagono in una circonferenza data, se ne costruisce uno a partire da una sua diagonale:
La costruzione più nota del Pentagono Regolare non è quella proposta da Euclide, ma deriva da un abbozzo che compare nell'Almagesto di Tolomeo. In effetti Tolomeo non si propone di costruire un pentagono regolare, bensì di determinare la lunghezza della corda sottesa da un angolo al centro di 72°, proprio la lunghezza del lato del pentagono (da questo valore ricaverà lo scheletro della sua Tavola delle Corde, comprendente la lunghezza delle corde di tutti gli angoli multipli di 12°).
Vediamo quindi i passaggi proposti da Tolomeo, integrati da quelli che servono all'effettiva costruzione del pentagono regolare (vedi Fig. 6):
Il fatto che i lati del decagono, esagono e pentagono regolari inscritti in circonferenze di pari raggio costituiscono i lati di un triangolo rettangolo era già stato dimostrato da Euclide. Egli usa però questa dimostrazione non per la costruzione del pentagono regolare, ma dell'Icosaedro inscritto in una sfera.
È noto che i vertici di un pentagono regolare, inscritto in un cerchio di raggio unitario, possono essere determinati risolvendo l'equazione ciclotomica
le cui radici sono date dall'espressione
per "n" compreso fra 0 e 4. Dato che l'equazione ciclotomica non ha termini di grado 1, sommando tutte le soluzioni si ottiene 0. Pertanto, se dal totale togliamo ξ=1, la somma delle rimanenti radici è -1. Inoltre, dalla formula di Eulero segue che:
da cui si possono ricavare le seguenti relazioni:
Queste espressioni danno luogo a un'equazione di secondo grado, che può facilmente essere risolta tramite un cerchio di Carlyle:
Le intersezioni della circonferenza con l'asse delle ascisse sono i punti V e W. La bisezione dei segmenti OV e OW determina i punti E ed F, che sono le ascisse dei vertici del pentagono.
</text>
</doc>
<doc id="15910" url="https://it.wikipedia.org/wiki?curid=15910">
<title>Angolo</title>
<text>
In matematica il termine angolo (dal latino "angulus", dal greco ἀγκύλος ("ankýlos"), derivazione dalla radice indoeuropea "ank", piegare, curvare) riguarda nozioni di larghissimo uso, innanzitutto nella geometria e nell'analisi infinitesimale.
Si definisce angolo ciascuna delle due porzioni di piano comprese tra due semirette aventi la stessa origine. Si chiama "angolo concavo" l'angolo che contiene i prolungamenti (lati) di queste semirette.
L"'angolo convesso" è la porzione di piano che non contiene i prolungamenti delle semirette che dividono il piano. La definizione di angolo convesso consente di sviluppare le basi della geometria piana euclidea e le prime nozioni di trigonometria.
A ogni angolo si associa un"'ampiezza", la misura correlata alla posizione di una semiretta rispetto all'altra e pertanto alla conformazione della porzione di piano costituente l'angolo: essa si esprime in gradi "sessagesimali" o "sessadecimali" oppure in gradi centesimali o infine in radianti, sempre con valori reali. Gli angoli convessi hanno ampiezza compresa tra 0 e 180 gradi sessagesimali, da 0 a 200 gradi centesimali, da 0 a "π radianti;" mentre l'ampiezza degli angoli concavi misura tra 180 e 360 gradi, ovvero da 200 a 400 gradi centesimali, ovvero da "π" a 2"π" radianti, quindi le ampiezze sono sempre positive.
Associando all'angolo un verso si introducono le ampiezze degli angoli con segno, entità meno intuitive che consentono di definire funzioni trigonometriche con argomenti reali qualsiasi fatte salve eventuali singolarità. Le ampiezze con segno sono da considerare insieme con il problema della rettificazione degli archi di circonferenza dotati di verso, alla natura del numero π e alle questioni relative alle aree con segno; tutti questi elementi forniscono contributi essenziali alle possibilità del calcolo infinitesimale e alle applicazioni alla fisica classica e alle conseguenti discipline quantitative.
Il termine angolo convesso riguarda una parte di piano definita da due semirette aventi l'origine in comune; le semirette "S" e "T" vengono dette "lati dell'angolo" e la loro origine "V" "vertice dell'angolo".
Se le semirette sono diverse, ma appartengono alla stessa retta "R", ciascuno dei due semipiani definiti da "R" muniti del vertice (che distingue le semirette) si dice angolo piatto.
A parte il caso particolare dell'angolo piatto, il piano si tripartisce in tre insiemi: l'insieme dei punti appartenenti alle due semirette, "S" e "T" (tra i quali il vertice) chiamata frontiera dell'angolo, e due insiemi connessi "K" e "K" e separati dai punti della frontiera. Uno solo di questi due insiemi "K" è costituito da punti che appartengono a segmenti con un estremo su una semiretta e l'altro sull'altra; in altre parole solo "K" è un insieme convesso. Il terzo insieme "K" non è convesso. Si definisce angolo convesso determinato da "S" e "T" l'unione di questo insieme convesso e della frontiera, formula_1. Si definisce angolo concavo determinato da "S" e "T" l'unione del terzo insieme non convesso e della frontiera, formula_2. I due angoli definiti dalle due semirette si dicono angoli esplementari.
Angoli convessi e concavi sono sottoinsiemi infiniti del piano, e quindi insiemi non misurabili attraverso la loro area che ha valore infinito. Spesso con angolo (convesso) si indica anche la parte di piano delimitata da due segmenti con un estremo in comune (vertice). Si può ricondurre questa definizione alla precedente prolungando i due segmenti dalla parte del loro estremo diverso dal vertice per ottenere le due semirette. Comunque questa estensione della definizione rende lecito assegnare a ogni triangolo tre angoli (convessi) associati biunivocamente ai suoi tre vertici.
Tuttavia il triangolo, essendo una parte del piano chiusa e limitata, ha area finita, infatti esso è l'intersezione degli angoli corrispondenti ai suoi tre vertici.
È naturale porsi il problema di "misurare un angolo": gli angoli possono servire per tante costruzioni e se a essi si associano misure numeriche ci si aspetta che per molte costruzioni possano essere utili calcoli numerici su queste misure.
Il problema della misura di un angolo non può essere risolto attraverso una misura della sua superficie che non è finita e che comunque non sarebbe significativa nemmeno nel caso di angoli sottesi da segmenti come nel caso del triangolo: si considerino per esempio triangoli simili.
Se si hanno due angoli convessi o concavi "A" e "B" con lo stesso vertice e "B" è sottoinsieme di "A" (situazione che si determina solo se i lati di "B" sono sottoinsiemi di "A") è ragionevole chiedere che la misura di "A" sia maggiore della misura di "B".
Dato un angolo convesso "A," si dice semiretta bisettrice dell'angolo la semiretta avente il vertice di "A" come estremo e i cui punti sono equidistanti dai lati di "A". Si può costruire facilmente la bisettrice con un compasso. La semiretta bisettrice di un angolo concavo si definisce come la semiretta avente come estremo il vertice dell'angolo allineata con la bisettrice del suo angolo (convesso) esplementare.
La semiretta bisettrice "B" di un angolo "A" convesso o concavo e ciascuno dei suoi due lati determinano due angoli convessi. La riflessione rispetto alla retta contenente la "B" scambia i due lati di "A" e trasforma uno dei due angoli nell'altro. È quindi ragionevole attribuire ai due angoli determinati dalla bisettrice una misura che sia la metà della misura di "A". È altrettanto ragionevole considerare che le misure dei due angoli determinati dalla semiretta bisettrice siano la metà della misura dell'angolo di partenza. Il processo di dimezzamento di un angolo può essere ripetuto N volte con N grande a piacere.
Un angolo convesso si dice angolo retto se i suoi due lati sono ortogonali, ovvero un angolo retto è la metà di un angolo piatto.
Un angolo convesso contenuto in un angolo retto avente il suo stesso vertice si dice angolo acuto. Un angolo convesso contenente un angolo retto avente lo stesso vertice si dice angolo ottuso.
Due angoli "A" e "B" che hanno in comune solo una semiretta si dicono angoli consecutivi. Se due angoli consecutivi hanno le semirette non in comune opposte (cioè la loro unione è una retta) allora si dicono angoli adiacenti. Per quanto riguarda gli angoli consecutivi, se questi sono angoli convessi la loro unione è un angolo che potrebbe essere convesso o concavo: si tratta dell'angolo definito dalle due semirette che sono i lati di uno solo dei due angoli. A questo angolo unione è ragionevole assegnare come misura la somma delle misure degli angoli adiacenti. L'angolo unione si dice "somma" dei due angoli "A" e "B".
In base alle considerazioni precedenti è lecito attribuire agli angoli misure costituite da numeri reali.
Due angoli trasformabili l'uno nell'altro mediante isometrie si dicono congruenti. Evidentemente una misura degli angoli invariante per le isometrie costituisce uno strumento con molti vantaggi: in particolare consente di individuare le classi di congruenza degli angoli. Quindi si chiede una misura degli angoli a valori reali e invariante per congruenza.
Se l'angolo è definito come la porzione del piano tra due semirette, la sua unità di misura dovrebbe essere una lunghezza al quadrato, ma questa misura non ha né significato né utilità pratica. Si è quindi pensato di considerare non la misura dell'angolo in sé, ma quella dell'ampiezza "del movimento" che porta una delle semirette a sovrapporsi all'altra.
Come giungere a determinare l'ampiezza di un angolo ha certamente chiesto maggiori sforzi all'intelletto umano di quanti ne abbia richiesti la misurazione di lunghezze e superfici. Misurare significa esprimere una grandezza "A" in rapporto a un'altra grandezza data, a essa omogenea, che funge da unità di misura. Se questo processo sorge abbastanza spontaneo per le grandezze spaziali, per cui basta ripetere un segmento o affiancare un quadrato U per n volte fino all'esaurimento della lunghezza o della superficie (A=n*U), lo stesso diventa meno intuitivo per le grandezze angolari, dove pure la stessa elaborazione mentale di un'unità di misura adatta richiede un maggior grado di astrazione.
Si prendano in considerazione i 4 angoli di ampiezza α della figura. Volendoli quantificare con l'area delimitata dai lati in verde, prolungando i lati a infinito nel caso A si ottiene un'area infinita e nei restanti casi B, C e D, considerando solo le superfici entro le linee tratteggiate, 3 aree determinate e quindi misurabili, ma visibilmente diverse fra loro, seppur originate dal medesimo angolo. Si presuma inoltre di dividere α esattamente in due angoli uguali, in modo che sia esprimibile in rapporto a questi ultimi, come formula_3= 2 formula_4; formula_4. Per quanto detto sopra, formula_4 può quindi essere considerato un"'unità di misura" e, se ora se ne considera l'area, l'uguaglianza sarà soddisfatta soltanto dai casi C e D, ma non da B, dove i due triangoli hanno aree diverse, pur trattandosi di due angoli formula_4 perfettamente sovrapponibili. Ne discende che "l'angolo non può essere misurato idoneamente in termini di area".
Si immagini quindi una semiretta che partendo dalla posizione verticale giri attorno al proprio estremo fino a diventare orizzontale; la semiretta ha compiuto un angolo α e nel suo movimento ha coperto la superficie compresa tra le due semirette. Sovrapponendo idealmente le immagini C e D si nota che, come in un compasso, allontanandosi dal centro di rotazione ogni punto traccia sul piano un arco più lungo, pur mantenendo immutato il rapporto fra lunghezza di quest'ultimo e il raggio. Inoltre se la semiretta compisse soltanto l'angolo formula_4 la lunghezza degli archi prodotti sarebbe invariabilmente la metà della lunghezza degli archi loro omologhi in formula_3.
Si consideri ora una rotazione completa che riporta la semiretta alla posizione di partenza, ovvero un angolo di massima ampiezza. In questo caso la semiretta copre l'intera superficie del piano tracciando infinite circonferenze; prendendo una qualunque di queste e segmentandola in "n" parti uguali, si possono individuare per ogni arco altrettante porzioni di piano equipollenti, in pratica una generica unità di misura per l'angolo. Dunque soltanto capendo che la misurazione dell'angolo non può essere avvenire quantificando un'area si comprende che bisogna astrarre il concetto di angolo come parte del piano e considerarlo invece cinematicamente come una porzione di superficie coperta da una semiretta in rotazione sul proprio estremo. Solo in questo modo è possibile misurarlo.
Sebbene questa nozione non sia immediata, deve comunque trattarsi di una conquista concettuale antica, se il sistema per la misurazione degli angoli comunemente più utilizzato ancora oggi, il sistema sessagesimale, è giunto sino noi dall'antica civiltà babilonese invariato nei secoli.
Nel sistema sessagesimale l'angolo completo o "angolo giro" è suddiviso in 360 spicchi, equivalenti all'unità di misura convenzionale denominata grado sessagesimale, indicata col simbolo °. La ragione della divisione in 360 parti dell'angolo giro è riconducibile all'uso astronomico che i babilonesi facevano di questa misura: dato che il Sole compie un giro completo sulla volta celeste nell'arco di un anno, a quel tempo stimato di circa 360 giorni, un grado corrisponde pressappoco allo spostamento del Sole sull'eclittica in un giorno.
Il nome "grado sessagesimale" deriva dal fatto che le sottounità del grado, il "minuto" e il "secondo", sono divise in sessantesimi; perciò, come nell'orologio, ogni grado è diviso in 60 minuti primi indicati col simbolo "'" e chiamati semplicemente minuti, e ogni minuto è diviso in 60 minuti secondi indicati col simbolo """ e chiamati semplicemente secondi. Ulteriori suddivisioni del secondo seguono invece il comune sistema decimale. Questa suddivisione deriva dal fatto che nell'antica Babilonia era in auge un sistema numerico su base sessagesimale, giunto sino a noi quale retaggio storico nell'orologio e sui goniometri.
L'ampiezza di un angolo potrebbe quindi essere espresso in una forma tipo:
Nel tempo sono poi stati adottati altri sistemi di misurazione nel tentativo di rendere più agevole la misura dell'ampiezza dell'angolo. Alla fine del Settecento non sfuggì ai tentativi di razionalizzazione neppure il sistema sessagesimale: venne proposto un sistema centesimale, basato appunto sul grado centesimale quale centesima parte nell'angolo retto, eletto ad angolo fondamentale per sostituire il 90 col più tondo e comodo 100, anche se trovò utilizzo pratico soltanto attorno al 1850 quando Ignazio Porro lo usò per costruire i suoi primi strumenti a divisione centesimale. Con questo sistema l'angolo giro viene diviso in 400 spicchi uguali con sottomultipli a frazioni decimali. Si tratta ancora di una unita di misura convenzionale non motivata da alcuna ragione matematica.
Dallo sviluppo dell'analisi infinitesimale guadagnò sempre più importanza un'altra unita di misura, per certi aspetti più "motivata" o "naturale:" il radiante, inteso come rapporto tra la lunghezza di un arco di circonferenza e il raggio della circonferenza stessa in quanto questo rapporto non dipende dal raggio, ma solo dall'angolo compreso. In questo modo l'angolo giro misura 2π, cioè il rapporto tra la lunghezza della circonferenza e il suo raggio.
Riepilogando, per misurare l'ampiezza dell'angolo i sistemi di misura più attestati sono:
Il primo viene più che altro usato in ambito strettamente topografico, mentre gli ultimi sono quelli maggiormente usati, il secondo per consuetudine il terzo per una maggiore semplicità dei calcoli nelle formule matematiche. La relazione che lega il sistema radiante e il sistema sessagesimale e permette il passaggio da uno all'altro è
dove formula_3 è la misura dell'ampiezza dell'angolo espresso in gradi, e formula_11 è la misura espressa in radianti.
Indicando l'ampiezza di un angolo con:
indicando con formula_17 la parte intera di un numero reale, e ricordando che vale la proporzione generale
valgono le seguenti formule di conversione da un sistema di misura all'altro
Nella nomenclatura degli angoli di ampiezza compresa tra 0 e formula_21 si è soliti usare aggettivi particolari per gli angoli associati a un angolo dato in quanto suoi "angoli di complemento" rispetto agli angoli fondamentali retto, piatto e giro.
Si dice complementare di un angolo di ampiezza α ogni angolo avente come ampiezza la β "mancante" per ottenere un angolo retto, cioè tale che sia formula_22. Da questa definizione segue che due angoli complementari devono essere entrambi acuti e che ha senso attribuire un complementare solo a un angolo acuto.
Si dice supplementare di un angolo di ampiezza α ogni angolo avente come ampiezza la β "mancante" per ottenere un angolo piatto, cioè tale che sia formula_23. Da questa definizione segue che ogni supplementare di un angolo acuto è un angolo ottuso e viceversa, mentre ogni supplementare di un angolo retto è anch'esso un angolo retto.
Quando due angoli supplementari sono anche "consecutivi", cioè hanno in comune solo una semiretta, vengono detti anche angoli adiacenti.
Si dice esplementare di un angolo di ampiezza α ogni angolo avente come ampiezza la β "mancante" per ottenere un angolo giro, cioè tale che sia formula_24. Ne segue che ogni esplementare di un angolo concavo è un angolo convesso e viceversa, mentre ogni esplementare di un angolo piatto è anch'esso piatto.
Due rette che si intersecano dividono il piano in 4 angoli; considerato uno qualsiasi di questi angoli: due degli altri gli sono adiacenti mentre il terzo, con il quale condivide solo il vertice, è detto angolo opposto al vertice. Due angoli sono tra loro opposti al vertice se i prolungamenti dei lati di uno risultano essere i lati dell'altro.
Sono adiacenti gli angoli delle coppie , , e .
Sono invece opposti al vertice gli angoli delle coppie e .
Quando sul piano due rette qualsiasi "r" e "s" vengono tagliate da un trasversale "t", si originano 8 angoli ognuno dei quali è posto in relazione con quelli a esso "non contigui".
Con riferimento ai due semipiani separati dalla trasversale t, sono definiti coniugati due angoli non contigui disposti sullo stesso semipiano, mentre sono considerati alterni due angoli non contigui situati sui due semipiani diversi. Rispetto alle rette r e s, invece sono definiti esterni due angoli non contigui avente in comune ai vertici solo uno dei semipiani originati dalla trasversale, mentre sono considerati interni due Sono inoltre definiti corrispondenti due angoli coniugati in comune ai vertici i semipiani originati dalle rette ma non reciprocamente; il che significa che solo uno degli angoli sarà contemporaneamente intersezione dei tre semi piani.
formula_25
formula_26
formula_27
formula_28
formula_29
Nel caso in cui le due rette "r" e "s" siano parallele gli angoli corrispondenti e gli angoli alterni, dello stesso tipo, sono congruenti. Invece gli angoli coniugati, anch'essi dello stesso tipo, sono supplementari.
Nella geometria euclidea la somma degli angoli interni di un triangolo è sempre di 180 gradi. Più in generale, data una qualunque figura geometrica convessa di formula_30 lati, la somma di tutti i suoi angoli interni è uguale a formula_31 gradi. Quindi, per esempio, la somma totale di tutti gli angoli interni di un quadrilatero è uguale a formula_32 gradi. Un caso particolare è dato dal quadrato, che ha quattro angoli retti, la cui somma è infatti 360 gradi. Analogamente, la somma di tutti gli angoli interni di un pentagono, regolare o meno, è uguale a 540 gradi.
In altre geometrie, dette non euclidee, la somma degli angoli interni di un triangolo può assumere sia valori maggiori sia valori minori di 180 gradi.
Molti problemi portano ad ampliare la nozione di angolo in modo da disporre di un'entità a cui si possa attribuire un'ampiezza data da un numero reale e quindi anche superiore a 360 gradi e negativa. Per questo occorre abbandonare l'associazione angolo - sottoinsieme del piano. Si dice che un angolo α è maggiore di un angolo β quando una parte di angolo α è congruente all'angolo β. Un angolo convesso o concavo può essere descritto cinematicamente come la parte di piano "spazzata" da una semiretta mobile che ruota mantenendo fisso il suo estremo; questo è il vertice dell'angolo e le posizioni iniziale e finale della semiretta sono i lati dell'angolo. Questa descrizione porta a distinguere due versi del movimento rotatorio. Si definisce verso negativo o verso orario il verso della rotazione che, osservata dal di sopra del piano, corrisponde al movimento delle lancette di un orologio tradizionale; si definisce verso positivo o verso antiorario il verso opposto (es. formula_33).
Per sviluppare considerazioni quantitative si considera una circonferenza Γ il cui centro ha il ruolo del vertice "V" per gli angoli che si prendono in considerazione. Il raggio "r" di questa circonferenza può essere scelto ad arbitrio e talora risulta comodo avere "r" = 1; quando si riferisce il piano a una coppia di assi cartesiani risulta comodo porre il vertice degli angoli nell'origine, in modo che la circonferenza corrisponda all'equazione formula_34.
Ogni angolo di vertice "V" determina un arco sulla circonferenza. Si consideri ora un movimento di una semiretta con estremo in "V" in un verso o nell'altro da una posizione iniziale "S" fino a una posizione finale "T": esso determina sulla Γ un arco orientato che ha come estremo iniziale il punto in cui Γ viene intersecata dalla "S" e come estremo finale il punto in cui viene intersecato dalla "T". Si può pensare l'arco orientato come se fosse "tracciato" dalla penna di un compasso avente l'altro braccio nel punto "V". Gli archi orientati con verso positivo si possono chiamare semplicemente archi (di circonferenza) positivi, quelli con verso negativo archi negativi.
Si può estendere la nozione di arco orientato pensando che il compasso possa compiere più di un giro, in verso positivo o negativo.
Si possono identificare gli angoli convessi con gli angoli relativi agli archi positivi interamente contenuti in una semicirconferenza; gli angoli concavi con gli archi positivi che contengono una semicirconferenza e sono contenuti in una circonferenza.
A questo punto si possono definire come angoli con segno di vertice "V" le entità che generalizzano gli angoli convessi e concavi con vertice in "V" e sono associate biunivocamente agli archi orientati sulla circonferenza Γ.
Gli angoli con segno possono essere sommati senza le restrizioni degli angoli associati a parti di piano e gli archi relativi risultano essere giustapposti; angolo opposto a un angolo dato corrisponde all'arco considerato con il verso opposto. Di conseguenza agli angoli con segno si attribuisce un'ampiezza rappresentata da un numero reale tale che alla somma di due angoli con segno corrisponda la somma algebrica delle ampiezze.
A questo punto si è indotti naturalmente ad associare all'ampiezza di un angolo con segno la lunghezza con segno del corrispondente arco. Questo richiede di precisare cosa si intenda per lunghezza di un arco e più in particolare richiede di definire la lunghezza di una circonferenza
Le considerazioni sulla rettificazione di una circonferenza portano alla definizione del numero π e, sul piano computazionale, alle valutazioni del suo valore.
</text>
</doc>
<doc id="8735" url="https://it.wikipedia.org/wiki?curid=8735">
<title>Distanza (matematica)</title>
<text>
L'accezione matematica del termine distanza ha un significato analogo a quello dell'uso comune, cioè quello della misura della "lontananza" tra due punti di un insieme al quale si possa attribuire qualche carattere spaziale. In matematica però questa nozione assume caratteri astratti e si basa solo su proprietà formali che ne fanno perdere l'univocità: esistono esempi di insiemi anche comuni come formula_1 in cui possono essere date infinite definizioni di distanza, tutte soddisfacenti le proprietà generali. Si può dire che in matematica il termine distanza caratterizza strumenti computazionali con alcune caratteristiche comuni, ma utilizzabili per scopi diversificati.
Il concetto di distanza e quello collegato di lunghezza vengono generalizzati mediante la definizione della geodetica come il più breve percorso tra due punti di uno "spazio curvo".
Una distanza (o metrica) su un insieme formula_2 è una funzione 
che soddisfa le seguenti proprietà per ogni scelta di formula_4 in formula_2:
La coppia formula_10 è chiamata spazio metrico.
In realtà solo le proprietà 2,3,4 sono indipendenti tra loro. Questo significa che si possono definire delle funzioni che soddisfano alcune tra 2,3,4 ma non altre.
Per esempio, se formula_11 allora la funzione formula_12 per questi particolari valori soddisfa le 2,4 ma non la 3 e quindi in generale non soddisfa la 3.
La dimostrazione che le 3,4 implicano la 1 è molto semplice.
Infatti, sfruttando la 4 si ha
formula_13 e formula_14. Sommando ora membro a membro otteniamo
formula_15 infine (sfruttando la 3) l’espressione si semplifica in 
formula_16 che è appunto la 1, dopo aver diviso per 2 (e scambiato i membri).
Data una norma formula_17 è possibile definire una distanza formula_18 definendo
Si verifica che la funzione così definita è una distanza, infatti:
Si osserva che ogni distanza indotta da una norma è invariante per traslazioni (ovvero, per ogni tripletta di vettori formula_24).
La distanza normalmente considerata in formula_25 è quella euclidea, pari alla radice quadrata del quadrato della differenza orizzontale (tra i due punti) più il quadrato della differenza verticale:
Se si elimina la seconda dimensione questa funzione si riduce al modulo della differenza tra i due numeri: formula_27.
Più in generale nello spazio euclideo formula_28 si può definire la distanza tra due punti formula_29 e formula_30 nei seguenti modi:
La "2-distanza" in uno spazio a "n" dimensioni corrisponde al teorema di Pitagora applicato "n-1" volte: è la distanza di uno spazio euclideo, normalmente usata nel piano o nello spazio e viene detta anche distanza pitagorica. La "1-distanza", detta anche "distanza L1" o distanza Manhattan, genera invece una geometria diversa, detta geometria del taxi. La "∞-distanza (o distanza L∞)" è la cosiddetta distanza di Chebyshev.
Nel caso di uno spazio di Hilbert formula_45, il teorema della proiezione afferma che per ogni punto formula_46 e per ogni insieme convesso chiuso formula_47 esiste un unico formula_48 tale per cui formula_49 assume il valore minimo su formula_50. In particolare, questo è vero per ogni sottospazio chiuso formula_51 di formula_45: in tal caso una condizione necessaria e sufficiente per formula_53 è che il vettore formula_54 sia ortogonale a formula_51.
Data una distanza su un insieme si può definire come palla, o bolla, o disco, centrata in un punto formula_56 di un certo raggio formula_57 positivo l'insieme dei punti dell'insieme che distano da formula_56 meno di formula_57:
Solitamente la definizione si intende con il &lt;; se però c'è bisogno di specificare, si dirà "disco aperto" l'insieme definito dalla relazione " &lt; " e "disco chiuso" l'insieme definito dalla relazione " ≤ ".
Si definisce anche "bordo" del disco l'insieme
L'insieme dei dischi aperti centrati nei vari punti dello spazio soddisfa la definizione topologica di base: la topologia sull'insieme formula_62 determinata da questa base si dice "topologia generata" (o "indotta") dalla distanza formula_63.
È importante notare come il disco chiuso non coincida sempre con la chiusura del disco aperto, ma in generale ne sia solo un soprainsieme; in particolare nello spazio euclideo comunque le due nozioni coincidono.
Due distanze formula_63 e formula_65 si dicono equivalenti se l'applicazione identità
è un omeomorfismo.
Equivalentemente, esse si possono dire equivalenti se ogni disco della prima metrica contiene un qualche disco della seconda metrica e viceversa. Ad esempio una distanza "d" è equivalente a quella data dalla funzione formula_67 ed a quella data dalla funzione formula_68.
Due distanze equivalenti generano la stessa topologia.
Se si indeboliscono le richieste su formula_63 si ottengono spazi con proprietà più deboli e più poveri come possibilità algoritmiche:
Al contrario, rinforzando la disuguaglianza triangolare e imponendo che
si ottiene una cosiddetta ultrametrica.
</text>
</doc>
<doc id="20467" url="https://it.wikipedia.org/wiki?curid=20467">
<title>Quadrato</title>
<text>
In geometria, il quadrato è un quadrilatero regolare, cioè un poligono con quattro lati e quattro angoli congruenti (tutti retti).
Il quadrato è un caso particolare di rombo (in quanto ha tutti e quattro i lati congruenti) e di rettangolo (in quanto ha quattro angoli congruenti) quindi è un caso particolare di parallelogramma (in quanto ha i lati a due a due paralleli).
Le diagonali di un quadrato sono congruenti e perpendicolari, il loro punto di intersezione le divide a metà e misurano come il lato moltiplicato per la radice quadrata di 2:
Questa formula si dimostra con il teorema di Pitagora. Ciascuna diagonale, infatti, divide il quadrato in due triangoli rettangoli per i quali vale che la somma dei quadrati costruiti sui cateti è equivalente al quadrato costruito sull'ipotenusa (che è la diagonale).
Il perimetro di un quadrato, visto che ha tutti i lati congruenti, misura:
L'area di un quadrato, visto che l'altezza e la base sono congruenti, misura:
ma si può calcolare anche come
Da ciò si deduce che la diagonale di un quadrato di area a è il lato del quadrato con Area 2a.
Il quadrato possiede 4 assi di simmetria: 2 passanti per una coppia di vertici opposti e 2 passanti per una coppia di punti medi dei lati.
Il punto di intersezione delle due diagonali è detto centro del quadrato ed è centro di simmetria di rotazione e di simmetria centrale per il quadrato. L'ordine della simmetria di rotazione del quadrato è 4; in altre parole, il quadrato è invariante per le rotazioni intorno al suo centro relative agli angoli formula_6; naturalmente la rotazione di formula_7 radianti è la simmetria centrale.
Il quadrato formula_8 di lato 2 e centro l'origine può essere descritto in vari modi. Ad esempio:
Il suo bordo è quindi
Questo può essere anche descritto come
Più in generale, l'equazione cartesiana di un quadrato avente centro nell'origine degli assi è: formula_12
Se si considera invece il centro del quadrato nel punto di coordinate formula_13 l'equazione diventa: 
formula_14
da cui:
formula_15
ovvero nella forma più generale possibile:
formula_16
Il cui bordo è quindi:
formula_17
Una dimostrazione costruttiva dell'esistenza del quadrato è data da Euclide nella proposizione 46 del I libro degli Elementi, subito prima di usare questa figura nell'enunciare e dimostrare il teorema di Pitagora. Nella tradizione didattica moderna l'esistenza dei quadrati è invece in genere data per scontata. Bisogna notare che la dimostrazione euclidea usa indirettamente il V postulato e l'esistenza di quadrati non è garantita nelle geometrie non euclidee.
Ad esempio, in geometria iperbolica non esistono poligoni con quattro lati uguali e quattro angoli retti: la somma degli angoli interni di un quadrilatero iperbolico è infatti sempre strettamente minore di un angolo giro. Esistono comunque "quadrati" nel piano iperbolico se si richiede solamente che i quattro angoli siano congruenti (ma non retti): per ogni numero reale formula_18 strettamente minore di formula_19 esiste infatti un poligono con quattro lati congruenti e quattro angoli congruenti pari a formula_18.
Un quadrato può essere inscritto in una circonferenza con riga e compasso. Qui sotto ne è mostrata un'animazione:
</text>
</doc>
<doc id="20472" url="https://it.wikipedia.org/wiki?curid=20472">
<title>Diagonale</title>
<text>
In geometria, si chiama diagonale il segmento che congiunge due vertici non consecutivi di un poligono o di un poliedro. Le diagonali possono essere interne o esterne al perimetro del poligono o al volume del poliedro, in particolare sono tutte interne se la figura è convessa.
Per sapere quante diagonali partono da un vertice di un poligono di formula_1 vertici si contano tutti i vertici tranne il vertice considerato e i due consecutivi ad esso, in quanto i segmenti ottenuti costituirebbero due lati (e quindi non sarebbero "diagonali" secondo la definizione sopra riportata), quindi si hanno formula_2 diagonali.
Il numero totale delle diagonali di un poligono di formula_1 vertici è dato dalla formula
Si dimentichi per un attimo il poligono e lo si sostituisca con l'insieme degli formula_5 punti corrispettivi dei vertici. Si tracci da ciascun punto le diagonali verso ognuno dei restanti formula_6 (per semplicità ora non si farà distinzioni fra lati e diagonali); si ha quindi che da ogni vertice del poligono partono in totale formula_6 diagonali, se però le si vogliono contare correttamente occorre fare il seguente ragionamento:
Il numero totale delle diagonali è quindi la sommatoria di una progressione aritmetica
da cui però bisogna togliere gli formula_5 lati, che inizialmente sono stati considerati per semplicità delle diagonali, quindi
Come si può verificare dalla formula, il triangolo con i suoi 3 lati è l'unico poligono a non avere diagonali.
Iniziamo dicendo che per formare una diagonale occorrono due vertici. Inoltre il segmento formula_22 e il segmento formula_23 rappresentano la stessa diagonale, quindi l'ordine con cui si prendono i vertici non è importante. Si tratta allora di contare quante configurazioni ordinate posso formare con formula_5 oggetti presi 2 alla volta. Per contare queste configurazioni ci viene in aiuto il calcolo combinatorio, infatti le configurazioni possibili sono le combinazioni semplici di formula_5 oggetti di classe 2
A queste configurazioni vanno poi tolte quelle ottenute prendendo due vertici consecutivi, quindi il numero formula_5 dei vertici del poligono
da cui
formula_29
Provando a tracciare le diagonali di diversi poligoni si ottiene la seguente tabella:
Si osserva che mentre i lati si susseguono linearmente, il numero delle rispettive diagonali aumenta in modo parabolico (per convincersene basta immettere i dati in un piano cartesiano), quindi la formula risolutiva deve essere un'equazione di secondo grado. Per trovarla si impieghi un sistema di 3 equazioni
formula_30
dove formula_31 è il numero dei lati e formula_32 è il numero delle diagonali corrispondenti. Poiché sia formula_31 che formula_32 sono noti (almeno per un numero finito di casi), le incognite sono formula_35. Sostituendo, ad esempio, formula_36 e i corrispondenti valori di formula_32 si ha
formula_38
e risolvendo il sistema si ottiene formula_39.
Quindi la formula risolutiva è formula_40 che sul piano cartesiano assume la forma della conica con formula_41 e formula_42.
</text>
</doc>
<doc id="35086" url="https://it.wikipedia.org/wiki?curid=35086">
<title>Angolo retto</title>
<text>
L'angolo retto è un angolo definito nel seguente modo: se da un punto di una retta si alza un'altra retta e gli angoli formati tra questa e la retta data da una parte e dall'altra sono congruenti, allora essi sono retti.
Le definizioni di grado, radiante e angolo giro implicano che l'angolo retto è un angolo di 90 gradi, ovvero di π/2 radianti o 1/4 dell'angolo giro.
Il seno di un angolo retto vale 1, il coseno 0, la tangente non è definita (ma tende a formula_1), la cotangente 0.
</text>
</doc>
<doc id="26011" url="https://it.wikipedia.org/wiki?curid=26011">
<title>Poliedro</title>
<text>
In matematica, e in particolare in geometria solida e in teoria dei grafi, un poliedro è un solido delimitato da un numero finito di facce piane poligonali. Come primi poliedri da prendere in considerazione, per la loro semplicità, vi sono i cubi, i parallelepipedi, le piramidi e i prismi. Fra i poliedri più complessi occupano un ruolo centrale i cinque solidi platonici, noti fin dall'antica Grecia.
Il termine "poliedro" deriva dal greco πολύεδρον (πολύς, "polys" = "molti" e ἔδρον, "édron" = "faccia"). Molti oggetti microscopici naturali (molecole, protozoi, virus, etc.) hanno forme o simmetrie poliedrali. I cristalli si possono presentare in questa forma anche a livello macroscopico.
In matematica non esiste una definizione univoca di poliedro. Nei casi più studiati, i poligoni formano una superficie che delimita una zona solida dello spazio: in questo caso per "poliedro" si intende questo solido, e non solo i poligoni che ne delimitano la superficie. Nei libri di testo la definizione formale di "poliedro" è spesso accompagnata da ipotesi tecniche aggiuntive, volte ad escludere alcuni casi considerati "patologici". Ad esempio, in Dedò (1999, p. 65) si definisce una "superficie poliedrale" come un numero finito di poligoni nello spazio tali che:
La scelta di ipotesi tecniche aggiuntive non è però univoca e dipende fortemente dalla scelta dell'autore. Le ipotesi qui elencate sono volte rispettivamente ad impedire che:
I poligoni sono le "facce" del poliedro. Le facce possono avere le forme più svariate: possono essere tutte congruenti come nel cubo, avere sempre lo stesso numero di lati senza però essere congruenti come in un più generico parallelepipedo, oppure avere numero di lati variabile come in un prisma o una piramide.
I lati delle facce sono gli "spigoli" del poliedro; per definizione, uno spigolo appartiene contemporaneamente a due facce distinte e la sua quantità più rappresentativa è la lunghezza. Un poliedro può possedere spigoli di lunghezza costante (come nel cubo) o variabile.
Le due facce che toccano uno spigolo formano un angolo, detto angolo diedrale, che varia in generale da spigolo a spigolo, ma che può assumere un valore costante in alcuni poliedri: per esempio, nel cubo è sempre un angolo retto, mentre nel tetraedro è approssimativamente di 70° 32'.
I vertici delle facce (cioè le estremità degli spigoli) sono i "vertici" del poliedro. Ogni vertice appartiene almeno a 3 facce distinte. Il numero formula_12 di facce cui appartiene è anche uguale al numero di spigoli che tocca: questo numero è la valenza del vertice.
La cuspide di un vertice è la struttura locale del poliedro intorno a questo.
Si dicono "vertici adiacenti" del poliedro due vertici che sono estremità comuni di uno spigolo; si dicono "spigoli adiacenti" del poliedro due spigoli che hanno un vertice in comune; si dicono "facce adiacenti" del poliedro due facce che hanno uno spigolo in comune. Ciascuna delle tre relazioni di adiacenza fra vertici, spigoli e facce di un poliedro chiaramente è una relazione simmetrica.
Ad esempio, in un tetraedro due vertici e due facce sono sempre adiacenti, mentre uno spigolo è adiacente a tutti gli altri spigoli tranne uno (detto "opposto").
Per fissare una terminologia, si introducono anche tre "relazioni di incidenza"; si dicono "incidenti": un vertice e uno spigolo di cui il vertice è estremità; un vertice e una faccia di cui il vertice fa parte; uno spigolo e una faccia della quale lo spigolo è lato.
Per ogni formula_13 maggiore o uguale a 4 esiste un poliedro con formula_13 facce.
Una prima classificazione dei poliedri riguarda il numero delle loro facce: un poliedro di 4, 5, 6, 7, 8, ...12, ...20, ... facce può essere chiamato rispettivamente tetraedro, pentaedro, esaedro, ettaedro, ottaedro... dodecaedro... icosaedro... secondo i prefissi numerici greci usati anche per i poligoni. I nomi ottaedro, dodecaedro e icosaedro sono però generalmente riservati a tre poliedri ben specifici, e non ad un generico poliedro con 8, 12 e 20 facce: si tratta di tre dei 5 solidi platonici.
Il numero di facce, spigoli e vertici di un poliedro forma una terna di numeri indicati con formula_15. Il tetraedro è il più piccolo poliedro, nel senso che esso presenta 4 facce, 6 spigoli e 4 vertici, mentre per tutti i poliedri rimanenti valgono le relazioni formula_16, formula_17, formula_18
Un "poliedro convesso" è un poliedro che individua un solido convesso. Questa condizione può essere espressa in vari modi equivalenti. 
Un poliedro non convesso è a volte detto "concavo".
I poliedri più noti sono convessi. Mentre la definizione generale di poliedro varia a seconda dell'autore, esiste una definizione di poliedro convesso universalmente accettata dai matematici, descritta sotto. Un "semispazio" è una delle due porzioni di spazio delimitate da un piano.
A partire da questa definizione, è possibile definire le "facce" come i poligoni ottenuti intersecando il poliedro con i piani che delimitano questi semispazi. Ad esempio, il cubo è un poliedro convesso con 6 facce e può essere ottenuto come intersezione di 6 semispazi, delimitati dai piani contenenti le facce.
Vertici e spigoli di un poliedro formano un grafo, detto "scheletro" del poliedro. Ad esempio, lo scheletro del tetraedro è un grafo completo con 4 vertici.
Lo scheletro di un poliedro convesso è un grafo planare: infatti è possibile proiettare da un punto interno del poliedro il grafo su una qualsiasi sfera centrata nel punto, e quindi successivamente nel piano tramite una proiezione stereografica. Il grafo di un poliedro più complesso può non essere planare.
Lo sviluppo piano di un poliedro è una figura piana che consiste in un certo numero di poligoni accoppiati lungo alcuni lati, che possono essere piegati nello spazio in modo da chiudersi e formare un poliedro. Lo sviluppo piano è uno strumento concreto utile per costruire poliedri di carta.
Qualsiasi poliedro convesso può essere costruito a partire da uno sviluppo piano. Lo stesso poliedro può essere costruito a partire da sviluppi piani differenti: l'insieme dei poligoni presenti è sempre lo stesso (sono le facce del poliedro), ma il percorso che queste formano può essere differente.
La "struttura combinatoria" di un poliedro è l'insieme dei suoi vertici, spigoli e facce e le relazioni di incidenza fra questi. La "struttura metrica" di un poliedro è invece la struttura del poliedro come spazio metrico, cioè come spazio dotato di distanza fra punti.
Una rotazione intorno ad un asse o una traslazione lasciano immutate le strutture metriche e combinatorie del poliedro. Una omotetia trasforma la struttura metrica (poiché modifica ad esempio le distanze fra i vertici) ma lascia invariata la struttura combinatoria. Più in generale, la struttura combinatoria è più flessibile: ad esempio, due parallelepipedi hanno sempre la stessa struttura combinatoria, ma non necessariamente metrica.
La terna formula_19 dà una buona descrizione della struttura combinatoria del poliedro: la descrizione non è però completa, perché non contiene tutte le informazioni sulle adiacenze. Un'ulteriore informazione è la valenza dei vertici.
Le proprietà topologiche di un poliedro sono quelle che ne descrivono solamente la forma globale. I poliedri più studiati (ad esempio, i convessi) hanno tutti la stessa forma topologica: sono "topologicamente equivalenti" ad una palla; tali poliedri sono detti "semplici". Per i poliedri semplici vale una formula importante, detta relazione di Eulero.
La superficie di un poliedro è l'unione delle sue facce. In alcuni casi, come nel grande icosaedro mostrato in figura, queste facce possono intersecarsi, e formare quindi una figura complicata. Quando questo accade, può non essere chiaro quale sia la porzione solida di spazio da considerare effettivamente "compresa" dalle facce. Questo fenomeno è simile a quello che si presenta in dimensione 2 nei poligoni stellati.
Nei casi più studiati però le facce non si intersecano e formano effettivamente una superficie che può essere studiata da un punto di vista topologico: se ne descrive cioè la forma globale, disinteressandosi degli angoli formati localmente dai vari spigoli e vertici. Una superficie delimita sempre una porzione di spazio.
Da un punto di vista topologico, una superficie nello spazio è caratterizzata soprattutto dal "numero di pezzi disgiunti" e dal "numero di buchi". Quando questa è formata da un pezzo solo e non ha buchi, è equivalente ad una sfera. I pezzi disgiunti ed il numero di buchi in matematica sono formalizzati rispettivamente con le nozioni di componente connessa e genere. Un poliedro con un buco ha una superficie avente la forma di un toro.
Un poliedro le cui facce formano una superficie con un pezzo solo e senza buchi è detto semplice. I poliedri convessi, e la maggior parte dei poliedri studiati, sono semplici.
La relazione di Eulero collega i numeri formula_20, formula_21 e formula_11 delle facce, degli spigoli e dei vertici di un poliedro semplice nel modo seguente:
Ad esempio:
La relazione di Eulero può essere usata per dimostrare ad esempio che non è possibile costruire un pallone da calcio simile a quello in figura ma con facce tutte esagonali.
La relazione di Eulero può non valere nei poliedri non semplici. Ad esempio, in un poliedro a forma di toro come quello mostrato in figura vale la relazione formula_40. La quantità formula_41 dipende effettivamente soltanto dalla topologia del poliedro ed è chiamata caratteristica di Eulero-Poincaré: si tratta di una quantità molto importante in topologia.
I poliedri più studiati sono quelli che presentano numerose simmetrie. Una simmetria di un poliedro è una isometria dello spazio che trasforma il poliedro in se stesso. Le simmetrie di un poliedro formano un gruppo, detto gruppo di simmetria.
Esistono due classi di isometria dello spazio euclideo tridimensionale: quelle che preservano l'orientazione dello spazio (trasformano cioè una mano destra in una mano destra) e quelle che la invertono (trasformano una mano destra in una sinistra). La stessa classificazione si riflette sulle simmetrie di un poliedro.
Ad esempio, il tetraedro ha 7 assi di simmetria: quattro per ogni vertice e tre per ogni coppia di spigoli opposti (si veda la figura). Ha inoltre 6 piani di simmetria (uno per ogni spigolo). Le simmetrie sono però in realtà 24. Di queste, 12 mantengono l'orientazione, e sono: l'identità, 2 rotazioni intorno ad ogni asse del primo tipo (di 120° e 240°) e una rotazione di 180° intorno ad un asse del secondo tipo (quindi formula_47). Ci sono inoltre 12 simmetrie che non mantengono l'orientazione: 6 sono riflessioni lungo piani come in figura, e altre 6 sono composizioni di riflessioni e rotazioni.
Piani e assi di simmetria sono il risultato della presenza di simmetrie riflessive e rotatorie. L'intersezione di tutti gli assi e di tutti i piani di simmetria può essere un piano, una retta, un punto, o vuota. L'intersezione può essere vuota solo se non vi sono simmetrie. Se l'intersezione è un punto, questo è chiamato "centro" del poliedro. Se l'intersezione è una retta, questa è l"'asse" del poliedro.
Ad esempio, il tetraedro ed il cubo hanno un centro. Una piramide quadrata non ha un centro, ma ha un asse.
Un poliedro è "chirale" se non è equivalente alla sua immagine riflessa. Più precisamente, un poliedro è chirale se tutte le sue simmetrie sono rotatorie: non ha cioè simmetrie che invertono l'orientazione. Più concretamente, un poliedro chirale si comporta come una mano: si presenta in due forme (una "sinistra" e una "destra") che sono una lo specchio dell'altra.
Una simmetria sposta un vertice su un vertice, che può essere uguale o diverso da quello di partenza. Analogamente, sposta uno spigolo su uno spigolo, ed una faccia su di una faccia. La simmetria determina quindi una permutazione dei vertici, degli spigoli e delle facce.
Le simmetrie di un poliedro inducono una relazione di equivalenza sull'insieme dei suoi vertici (e analogamente sull'insieme dei suoi spigoli e delle sue facce): due vertici (o spigoli o facce) sono "equivalenti" se esiste una simmetria che sposta il primo nel secondo. Due vertici (o spigoli o facce) equivalenti devono avere necessariamente lo stesso aspetto: ad esempio, due vertici equivalenti devono avere lo stesso tipo di cuspide (in particolare, la stessa valenza), due spigoli la stessa lunghezza e lo stesso angolo diedrale e due facce equivalenti devono essere congruenti. Tutte queste condizioni necessarie non sono generalmente sufficienti: possono esistere facce congruenti non equivalenti, spigoli della stessa lunghezza e con lo stesso angolo diedrale non equivalenti, ecc.
Se i vertici di un poliedro sono tutti equivalenti, questo è detto "regolare sui vertici". Analogamente, se sono equivalenti tutti gli spigoli o tutte le facce, è detto "regolare sugli spigoli" o "sulle facce". I termini "omogeneo" e "transitivo" possono essere usati come sinonimo di "regolare".
Un poliedro che è regolare sui vertici, sugli spigoli e sulle facce è detto "regolare". Esistono solo 5 poliedri semplici regolari: questi sono i solidi platonici.
Le simmetrie di un poliedro formano un gruppo con l'operazione di composizione. Questo gruppo è sempre un gruppo finito.
Ad esempio, il gruppo di simmetria del tetraedro è il gruppo di permutazioni formula_48 di 4 elementi: infatti ogni permutazione dei 4 vertici è realizzata esattamente da una simmetria. Le simmetrie sono effettivamente 4! = 24.
Le simmetrie che preservano l'orientazione formano un sottogruppo, detto "gruppo delle rotazioni". Questo può coincidere con tutto il gruppo (se il poliedro è chirale) oppure avere indice 2 (se non è chirale). Il tetraedro non è chirale: il gruppo delle rotazioni è il gruppo alternante formula_49, avente 12 elementi.
Nonostante la grande varietà di poliedri esistenti, vi sono poche classi di gruppi di simmetrie possibili. Ad esempio, gli unici gruppi che possono essere gruppi di rotazioni di qualche poliedro sono
dove formula_51 è il gruppo ciclico di ordine formula_12, formula_53 è il gruppo diedrale di ordine formula_54, formula_55 è il gruppo simmetrico di ordine formula_56 e formula_57 è il gruppo alternante di ordine formula_58. I gruppi formula_59 e formula_53 sono ottenuti anche come gruppi di rotazione e di simmetria di un poligono regolare con formula_12 lati: sono quindi gruppi che si ottengono anche nel piano, e la loro presenza non è quindi sorprendente.
I gruppi formula_62 e formula_63 sono quindi gli unici gruppi di rotazioni essenzialmente tridimensionali. Sono i gruppi di rotazioni dei 5 solidi platonici: formula_49 per il tetraedro, formula_48 per cubo e ottaedro, formula_66 per icosaedro e dodecaedro. I solidi platonici giocano qui (come in molti altri contesti) un ruolo centrale.
Due poliedri formula_67 e formula_68 sono "duali" se hanno i ruoli di vertici e facce scambiati. Più precisamente, ad ogni vertice, spigolo o faccia di formula_67 corrisponde rispettivamente una faccia, spigolo o vertice di formula_68, in modo che siano preservate adiacenze e incidenze. Ad esempio, se un vertice di formula_67 è adiacente ad uno spigolo di formula_67, la corrispondente faccia di formula_68 è adiacente al corrispondente spigolo di formula_68.
In particolare, le terne di numeri formula_75 dei due poliedri sono l'una l'opposta dall'altra. Ad esempio, il cubo, avente formula_76 è duale dell'ottaedro, avente formula_77. In molti casi (come questo) la dualità è realizzata in modo che i vertici di formula_68 siano punti interni delle corrispondenti facce di formula_67 (si veda un esempio in figura).
Ogni poliedro convesso ha un poliedro duale, che può essere definito come il risultato di una inversione rispetto ad una sfera. Quando il poliedro ha un centro, è naturale prendere come sfera una sfera centrata in questo punto. La costruzione del duale di poliedri non convessi è più problematica.
Un prismatoide è un poliedro i cui vertici giacciono in due piani paralleli. 
Tranne rare eccezioni, i prismatoidi hanno generalmente al più un asse di simmetria (ortogonale ai piani paralleli), ed il loro gruppo di simmetrie è ciclico (formula_59) o diedrale (formula_53), simile cioè al gruppo di simmetrie di un poligono nel piano.
Esistono varie famiglie infinite di prismatoidi. Qui sono elencate le più usate.
Esistono esattamente 5 poliedri semplici regolari sulle facce, sugli spigoli e sui vertici. Questi sono i solidi platonici. Questi poliedri sono detti anche "regolari".
La tabella indica per ogni solido platonico la terna formula_19 ed una coppia formula_83, con formula_84 pari al numero di lati di ogni faccia e formula_85 pari al numero di spigoli su ogni vertice (cioè la sua valenza). Cubo e ottaedro sono duali, dodecaedro e icosaedro sono duali. Il tetraedro è duale a se stesso (la dualità inverte le terne e le coppie di numeri nelle tabelle).
Il centro di ogni solido platonico è anche centro di una sfera inscritta (interna e tangente a tutte le facce) e di una sfera circoscritta (esterna e contenente tutti i vertici).
I poliedri, per essere "regolari", oltre ad avere come facce poligoni regolari tutti uguali, devono anche avere tutti gli spigoli e i vertici equivalenti.
I solidi platonici giocano un ruolo centrale nella geometria solida: sono i solidi che presentano la maggiore regolarità possibile e il maggior numero di simmetrie. I loro gruppi di simmetrie hanno collegamenti con le sezioni più disparate della matematica. Hanno inoltre un posto di rilievo nella storia del pensiero greco, arabo e rinascimentale. Platone, nel Timeo, associò ad ognuno di essi un elemento: al tetraedro il fuoco, al cubo la terra, all'ottaedro l'aria, all'icosaedro l'acqua, mentre ritenne che il dodecaedro fosse la forma dell'universo.
Oltre ai 5 poliedri platonici, esistono altri 4 poliedri regolari non semplici. Le facce di questi poliedri si intersecano vicendevolmente: due di questi, scoperti da Keplero, hanno come facce poligoni regolari stellati; altri due, scoperti da Louis Poinsot, hanno facce regolari non stellate, ma comunque intrecciate.
I due poliedri stellati hanno come facce 12 pentagoni stellati (pentagrammi). I valori formula_19 e formula_83 hanno lo stesso significato descritto precedentemente. I primi due poliedri sono uno duale dell'altro, come lo sono anche gli ultimi due.
Un poliedro uniforme è un poliedro 
Il poliedro non è necessariamente semplice. I poliedri uniformi sono catalogati nel modo seguente:
I poliedri duali dei poliedri uniformi sono regolari sulle facce e hanno cuspidi regolari. Tra questi, i 13 solidi duali dei solidi archimedei sono detti poliedri di Catalan dal nome del matematico belga Eugène Charles Catalan.
Un solido di Johnson è un poliedro convesso 
In altre parole, i solidi di Johnson sono i solidi convessi con facce regolari che non sono uniformi.
I solidi di Johnson sono 92, e vengono generalmente indicati con una sigla che va da formula_88 fino a formula_89.
I solidi convessi aventi facce regolari sono quindi: prismi e antiprismi regolari (in quantità infinita), i solidi platonici (5), i solidi archimedei (13) e quelli di Johnson (92).
Un poliedro composto è un poliedro ottenuto come unione di più poliedri distinti aventi lo stesso centro. Un poliedro di questo tipo generalmente non è convesso.
Ad esempio, la stella octangula mostrata precedentemente può essere descritta come poliedro composto, formato da due tetraedri, aventi lo stesso centro ma posizionati in modo differente.
I poliedri composti più importanti sono quelli che presentano molte simmetrie. Ad esempio il poliedro mostrato accanto in figura, denominato cinque tetraedri nel dodecaedro, è effettivamente l'unione di 5 tetraedri concentrici: ha formula_90 vertici, i quali sono anche i vertici di un dodecaedro regolare.
Alcune operazioni permettono di trasformare un poliedro in un altro, o di affiancare più poliedri in modo da ricoprire lo spazio.
Il "troncamento" di un vertice formula_1 di un poliedro consiste nell'eliminazione di una porzione di poliedro (una cuspide) tramite un taglio vicino a formula_1. Il pezzo da rimuovere è una piramide con vertice in formula_1 e base determinata dal piano lungo cui viene fatto il taglio. La base è un poligono con formula_12 lati, dove formula_12 è la valenza di formula_1.
Il nuovo poliedro ha una faccia in più del precedente. Troncando un vertice alla volta, è quindi possibile, partendo dal tetraedro, costruire poliedri con un numero arbitrario 4, 5, 6... di facce.
Molti solidi archimedei sono ottenuti troncando opportunamente tutti i vertici di un solido platonico. Una troncatura variabile può essere usata in alcuni casi per passare da un poliedro al suo duale, come in questa sequenza che collega il cubo all'ottaedro:
La "stellazione" è un'operazione definita da Keplero nel 1619: consiste nell'estendere alcune facce del poliedro fino ad un punto in cui queste si incontrano nuovamente. Con questa operazione Keplero costruì, partendo dal dodecaedro regolare, due dei quattro poliedri noti oggi come solidi di Keplero-Poinsot. La stella octangula è una stellazione dell'ottaedro regolare.
Qui sotto sono elencate alcune stellazioni: una dell'ottaedro regolare (la stella octangula), tre del dodecaedro regolare (le prime due sono i solidi di Keplero), e una dell'icosaedro.
Alcuni poliedri possono essere usati come mattoni per riempire lo spazio senza lasciare buchi, similmente a quanto accade nelle arnie: una tale operazione è detta tassellazione dello spazio (o "pavimentazione" dello spazio). I poliedri in una tassellazione sono adiacenti lungo le loro facce. Fra i solidi platonici, l'unico in grado di tassellare lo spazio è il cubo; fra i solidi archimedei, vi sono il dodecaedro rombico e l'ottaedro troncato. Ottaedri e tetraedri regolari possono essere usati a coppie per tassellare lo spazio.
Molti minerali cristallizzano con una forma, detta "abito", che corrisponde ad un poliedro. La pirite si può presentare con tre abiti diversi: con cristalli cubici, ottaedrici o aventi la forma di un dodecaedro non regolare (detto "pentadodecaedro" o "piritoedro"). Nessun minerale ha però la forma di un icosaedro o dodecaedro "regolare".
La leucite può avere la forma di un icositetraedro trapezoidale (un solido di Catalan). Il piropo può avere la forma di un dodecaedro rombico (un poliedro di Catalan) e l'aragonite la forma di un prisma esagonale.
Molti organismi microscopici hanno forme o simmetrie poliedrali. Tra questi, i radiolari possono avere la forma di un icosaedro regolare o di un geode. Nella forma geodale, è possibile verificare una delle conseguenze della relazione di Eulero descritta sopra: non è possibile costruire un solido i cui vertici hanno valenza 3 e le cui facce sono tutte esagonali. Nella figura sono infatti presenti alcuni pentagoni. Le regolari geometrie degli scheletri di questi microorganismi hanno affascinato dalla fine del secolo XIX molti naturalisti, tra cui Ernst Haeckel e Thompson D'Arcy, che hanno anche cercato una interpretazione integrata fra biologia e geometria sul significato di queste strutture:
Il solido artificiale più antico di cui è rimasta traccia è sicuramente la piramide.
Il dado da gioco classico ha la forma di un cubo. Alcuni giochi di ruolo fanno però uso di tutti e 5 i solidi platonici: la regolarità del solido infatti garantisce che ogni faccia abbia la stessa probabilità di uscire dopo un lancio (naturalmente la densità del solido dev'essere uniforme).
Per mantenere la stessa probabilità è sufficiente che il solido sia regolare sulle facce: per questo motivo vengono usati anche i trapezoedri. Ad esempio, due trapezoedri con 10 facce come in figura usati simultaneamente permettono di sorteggiare un numero da 0 a 99.
Una cupola geodetica è un solido con molte facce, la cui forma è molto simile a quella di una sfera (o di una porzione di questa). Come nei radiolari mostrati sopra, nelle cupole geodetiche è spesso facile verificare gli effetti della relazione di Eulero.
</text>
</doc>
<doc id="26036" url="https://it.wikipedia.org/wiki?curid=26036">
<title>Funzione trigonometrica</title>
<text>
In matematica, le funzioni trigonometriche o funzioni goniometriche o funzioni circolari sono funzioni di un angolo; esse sono importanti nello studio dei triangoli e nella modellizzazione dei fenomeni periodici, oltre a un gran numero di altre applicazioni.
Sono spesso definite come rapporti fra i lati di un triangolo rettangolo contenenti l'angolo e, equivalentemente, possono essere definite come le lunghezze di diversi segmenti costruiti dal cerchio unitario. Definizioni più moderne li esprimono come serie infinite o come soluzioni di certe equazioni differenziali, ottenendo la loro estensione a valori positivi o negativi e anche ai numeri complessi. Tutti questi differenti approcci sono presentati di seguito.
Lo studio delle funzioni trigonometriche risale ai tempi dei babilonesi, e una quantità considerevole del lavoro fondamentale fu svolto dai matematici greci, indiani e persiani.
Nell'uso corrente, vi sono sei funzioni trigonometriche di base, che sono elencate sotto insieme alle identità che le mettono in relazione. Specialmente per le ultime quattro, queste relazioni sono spesso prese come "definizioni" di quelle funzioni, sebbene sia ugualmente possibile definirle geometricamente o per altre vie, e solo in seguito derivare queste relazioni. Poche altre funzioni erano comuni in passato (e figuravano nelle vecchie tabelle) ma sono oggi poco usate, come il senoverso (1 − cos θ) e l'exsecante (sec θ − 1). Molte altre relazioni notevoli fra queste funzioni sono elencate nella voce sulle identità trigonometriche.
Il più antico riferimento alla funzione seno risale a "Sulba Sutras", scritto nell'antica India dall'VIII al VI secolo a.C. Più tardi, le funzioni trigonometriche furono studiate da Ipparco di Nicea (180-125 a.C.), Aryabhata (476 – 550), Varāhamihira, Brahmagupta, Muḥammad ibn Mūsā al-Ḵwārizmī, Abu'l-Wafa, Omar Khayyam, Bhaskara II, Nasir al-Din Tusi, Regiomontano (1464), Ghiyath al-Kashi (XIV secolo), Ulugh Beg (XIV secolo), Madhava (1400 circa), Retico, il suo discepolo Valentin Otho. All"'Introductio in analysin infinitorum" (1748) di Leonardo Eulero si riconosce il merito di aver stabilito la trattazione analitica delle funzioni trigonometriche in Europa, definendole come serie infinite e presentando la "formula di Eulero".
La nozione secondo cui deve esserci una corrispondenza fra le lunghezze dei lati di un triangolo e gli angoli del triangolo sorge non appena si intuisce che i triangoli simili mantengono gli stessi rapporti fra i lati corrispondenti. In altri termini, per qualsiasi triangolo simile il rapporto fra l'ipotenusa (per esempio) e un altro dei lati rimane lo stesso. Se l'ipotenusa è il doppio, anche i lati sono lunghi il doppio. Le funzioni trigonometriche esprimono proprio questi rapporti.
Al fine di definire le funzioni trigonometriche di un angolo "A", si consideri un arbitrario triangolo rettangolo che contiene l'angolo "A":
Usiamo i seguenti nomi per i lati del triangolo:
Tutti i triangoli vengono considerati appartenenti al piano euclideo in modo che la somma degli angoli interni è π radianti (o 180°); di conseguenza, per un triangolo rettangolo, i due angoli non retti sono compresi fra 0 e π/2 radianti. A rigore, le definizioni che seguono consentono di definire le funzioni trigonometriche solo per gli angoli in questo intervallo. Si può tuttavia estendere le definizioni all'insieme dei numeri reali utilizzando la circonferenza unitaria, o imponendo che tali funzioni posseggano certe simmetrie o siano periodiche.
1) Il seno di un angolo è il rapporto fra la lunghezza del lato opposto e la lunghezza dell'ipotenusa. Nel nostro caso
È importante notare che questo rapporto non dipende dal particolare triangolo rettangolo scelto, purché contenga l'angolo A, dal momento che tutti questi triangoli sono simili.
L'insieme degli zeri del seno è
2) Il coseno di un angolo è il rapporto fra la lunghezza del lato adiacente e la lunghezza dell'ipotenusa. Nel nostro caso
L'insieme degli zeri del coseno è
3) La tangente di un angolo è il rapporto fra la lunghezza del lato opposto e la lunghezza del lato adiacente. Nel nostro caso
L'insieme degli zeri della tangente è
Esso coincide con l'insieme degli zeri del seno poiché
Le funzioni rimanenti sono definite convenientemente utilizzando le tre definizioni già fornite.
4) La cosecante csc("A") è l'inverso moltiplicativo di sin("A"), ossia il rapporto fra la lunghezza dell'ipotenusa e quella del lato opposto:
5) La secante sec("A") è l'inverso moltiplicativo di cos("A"), ossia il rapporto fra la lunghezza dell'ipotenusa e quella del lato adiacente:
6) La cotangente cot("A") è l'inverso moltiplicativo di tan("A"), ossia il rapporto fra la lunghezza del lato adiacente e quella del lato opposto:
È possibile definire le sei funzioni trigonometriche a partire dalla circonferenza unitaria o circonferenza goniometrica, centrata nell'origine e con il raggio pari ad 1. La definizione attraverso la circonferenza goniometrica non aiuta nel calcolo pratico dei valori delle funzioni; infatti essa si basa sui triangoli rettangoli per molti angoli. Essa consente, tuttavia, la definizione delle funzioni trigonometriche per tutti gli argomenti reali, positivi e negativi, non solo quelli limitati all'intervallo fra 0 e π/2. Essa consente inoltre di visualizzare graficamente in una sola figura tutte le funzioni trigonometriche. L'equazione della circonferenza goniometrica è:
Nell'immagine sono indicati alcuni angoli comuni, misurati in radianti. Le misure in verso antiorario sono angoli positivi, quelli in verso orario sono negativi. Consideriamo l'intersezione con la circonferenza goniometrica di una retta che forma un angolo θ con la metà positiva dell'asse "x". L'ascissa "x" e l'ordinata "y" di questo punto sono uguali rispettivamente a cos θ e sin θ. Il triangolo nel disegno dimostra l'equivalenza con la definizione precedente: il raggio della circonferenza è l'ipotenusa del triangolo ed ha una lunghezza pari ad 1, pertanto sin θ = "y"/1 e cos θ = "x"/1. Si può pensare alla circonferenza goniometrica come ad un modo per considerare un numero infinito di triangoli rettangoli in cui varia la lunghezza dei cateti, mentre l'ipotenusa si mantiene uguale ad 1.
Per angoli maggiori di 2π o minori di −2π, si può semplicemente immaginare di compiere più giri intorno al cerchio. In questo modo, il seno ed il coseno diventano funzioni periodiche di periodo 2π.
per ogni angolo θ e ogni intero "k".
Il "più piccolo" periodo positivo di una funzione periodica è detto "periodo primitivo" della funzione. Il periodo primitivo del seno, del coseno, della secante e della cosecante è l'intera circonferenza, ossia 2π radianti o 360 gradi; il periodo primitivo della tangente e della cotangente è solo metà circonferenza, ossia π radianti o 180 gradi.
Sopra sono state definite sulla circonferenza unitaria soltanto le funzioni seno e coseno, ma le altre quattro funzioni trigonometriche possono essere definite da:
L'immagine sulla destra mostra il grafico sul piano cartesiano della funzione f(θ) = tan(θ), considerevolmente diverso da quelli visti prima per il seno e il coseno. I punti di intersezione con l'asse "x" coincidono corrispondenti di sin(θ), mentre la funzione non è definita in corrispondenza delle intersezioni della funzione cos(θ) con l'asse "x". I valori della funzione cambiano lentamente in prossimità di angoli pari a "k"π, mentre cambiano rapidamente per gli angoli in prossimità di ("k"/2) π. Il grafico della tangente ha anche un asintoto verticale per θ = "k"π/2: infatti la funzione tende ad infinito se l'angolo θ tende ad "k"/π da sinistra e meno infinito se θ tende ad "k"/π da destra.
In alternativa, è possibile definire "tutte" le funzioni trigonometriche di base a partire dalla circonferenza goniometrica (mostrata a destra); tali definizioni venivano usate storicamente. In particolare, data una corda "AB" della circonferenza, dove θ è la metà dell'angolo sotteso, sin(θ) è "AC" (metà della lunghezza della corda), ovvero il cateto del triangolo rettangolo avente la corda come ipotenusa e un cateto giacente sul raggio, una definizione introdotta in India (vedi sopra). cos(θ) è la distanza orizzontale "OC", ovvero l'altro cateto, e versin(θ) = 1 − cos(θ) è "CD, ovvero la distanza tra il coseno e il punto della circonferenza che tocca il raggio su cui giace". tan(θ) è la lunghezza del segmento "AE" sulla retta tangente per "A", da cui il nome "tangente, data dall'incrocio di questa con il prolungamento del raggio". cot(θ) è un altro segmento tangente, "AF, ovvero il prolungamento della tangente che incontra quello del raggio perpendicolare a quello su cui giace il coseno". sec(θ) = "OE" e csc(θ) = "OF" sono segmenti di rette secanti (che intersecano la circonferenza in due punti), ovvero la somma di seno, cosenoverso e secante esterna"OA" aglitale . "DE" è chiamata exsec(θ) = sec(θ) − 1 (la porzione della secante fuori che interseca la cotangente dal cerchio). Da queste costruzioni, è facile vedere che le funzioni secante e tangente divergono se θ tende a π/2 (90 gradi) e che la cosecante e la cotangente divergono se θ tende a zero. (È possibile effettuare molte costruzioni simili, e le identità trigonometriche di base si possono dimostrare graficamente.)
Utilizzando solo le nozioni di geometria e le proprietà dei limiti, si può dimostrare che la derivata del seno è il coseno e la derivata del coseno è l'opposto del seno. (Qui, e in genere nel calcolo infinitesimale, tutti gli angoli sono misurati in radianti; vedi anche l'importanza dei radianti sotto.) Si può usare la teoria delle serie di Taylor per dimostrare che le seguenti identità sono valide per ogni numero reale formula_16:
Queste identità sono spesso prese come "definizioni" delle funzioni seno e coseno. Esse sono spesso usate come punto di partenza per una trattazione rigorosa delle funzioni trigonometriche e delle loro applicazioni, dal momento che la teoria delle serie infinite può essere sviluppata a partire dai fondamenti del sistema dei numeri reali, prescindendo da ogni considerazione geometrica. La derivabilità e la continuità di queste funzioni sono quindi ottenute a partire dalla sola definizione in serie di potenze.
Si possono ottenere altri sviluppi in serie:
dove
Il coefficiente formula_25 a numeratore ha una interpretazione combinatoria: esso indica il numero di permutazioni alternate di insiemi finiti di cardinalità dispari.
Il coefficiente formula_26 a numeratore può essere interpretato, dal punto di vista combinatorio, come il numero di permutazioni alternate di insiemi finiti di cardinalità pari.
Per un teorema dell'analisi complessa, vi è un'unica estensione analitica di questa funzione ai numeri complessi. Le funzioni trigonometriche sono definite sui numeri complessi usando le serie di Taylor viste sopra.
Si può dimostrare, dalle definizioni in serie, che le funzioni seno e coseno sono rispettivamente la parte immaginaria e quella reale della funzione esponenziale complessa quando il suo argomento è un numero immaginario:
Questa relazione fu notata per la prima volta da Eulero e, per questo motivo, l'identità è conosciuta come formula di Eulero. In questo modo, le funzioni trigonometriche diventano essenziali nell'interpretazione geometrica dell'analisi complessa. Per esempio, se si considera la circonferenza unitaria nel piano complesso (dal punto di vista algebrico, con l'ordinaria moltiplicazione tra numeri complessi, si tratta di un gruppo moltiplicativo abeliano, noto come gruppo circolare), definita da formula_28, si può parametrizzare questa circonferenza in termini di coseni e seni, rendendo evidente la relazione fra le funzioni trigonometriche e l'esponenziale complesso.
Questo, inoltre, permette di estendere la definizione delle funzioni trigonometriche ad un argomento complesso formula_29:
dove formula_32. Inoltre, per formula_16 reale,
È noto anche che i processi esponenziali sono strettamente collegati al moto circolare e ai comportamenti periodici.
Entrambe le funzioni seno e coseno soddisfano l'equazione differenziale
Ossia, ognuna è l'opposto della sua derivata seconda. Nello spazio vettoriale bidimensionale "V" formato da tutte le soluzioni di questa equazione, la funzione seno è l'unica soluzione che soddisfa le condizioni iniziali "y"(0) = 0 e "y"′(0) = 1, e la funzione coseno è l'unica che soddisfa le condizioni iniziali "y"(0) = 1 e "y"′(0) = 0. Poiché le funzioni seno e coseno sono linearmente indipendenti, insieme formano una base di "V". Questo metodo per definire le funzioni seno e coseno è essenzialmente equivalente all'uso della formula di Eulero. (Vedi equazione differenziale lineare) Questa equazione differenziale può essere usata non solo per definire le funzioni seno e coseno, ma anche per dimostrare le identità trigonometriche per le funzioni seno e coseno.
La tangente è l'unica soluzione dell'equazione differenziale non lineare
che soddisfa la condizione iniziale "y"(0) = 0. Vi è una dimostrazione grafica molto interessante del fatto che la funzione tangente soddisfa questa equazione differenziale; vedi "Visual Complex Analysis" di Needham.
I radianti specificano un angolo misurando la lunghezza dell'arco corrispondente della circonferenza goniometrica. Esistono altre unità di misura per gli angoli, come i comuni gradi sessagesimali. Tuttavia, solo se l'angolo è misurato in radianti le funzioni seno e coseno soddisfano l'equazione differenziale che viene classicamente utilizzata per descriverle. Se l'argomento del seno o del coseno è moltiplicato per una opportuna costante di conversione,
allora la derivata sarà uguale a
Se "x" è in gradi, allora 
Ciò significa che la derivata seconda del seno in gradi non soddisfa l'equazione differenziale
ma
e un discorso analogo vale per il coseno.
Ciò significa che queste funzioni seno e coseno hanno un comportamento diverso, e, per esempio, la derivata quarta del seno è uguale al seno solo se l'argomento è misurato in radianti.
Vi sono molte identità che mettono in relazione le varie funzioni trigonometriche. Fra quelle usate più di frequente vi è l'identità fondamentale della trigonometria, altresì chiamata identità pitagorica, che afferma che, per ogni angolo, la somma tra il quadrato del seno ed il quadrato del coseno vale formula_43. Ciò si verifica facilmente applicando il teorema di Pitagora ad un triangolo rettangolo di ipotenusa formula_43. Simbolicamente si può scrivere
Altre relazioni di primaria importanza sono le formule di addizione e sottrazione, che forniscono il seno ed il coseno della somma e della differenza di due angoli in funzione del seno e del coseno degli angoli stessi.
Se i due angoli sono uguali, le formule di addizione si riducono ad identità più semplici note come formule di duplicazione.
Per gli integrali e le derivate delle funzioni trigonometriche, consultare le sezioni rilevanti delle tavola delle derivate, tavola degli integrali più comuni, e tavola degli integrali indefiniti di funzioni trigonometriche.
In analisi matematica, è possibile definire le funzioni trigonometriche ricorrendo ad equazioni funzionali basate su proprietà come le formule di addizione e sottrazione. Imponendo la validità di queste formule e dell'identità fondamentale, per esempio, si può dimostrare che esistono solo due funzioni reali che soddisfano queste condizioni. In simboli, esiste solo una coppia di funzioni reali formula_48 e formula_49 tali che, per tutti i numeri reali formula_16 e formula_51, siano soddisfatte le seguenti uguaglianze:
con, in aggiunta, la condizione che
Quindi:
Per determinare le funzioni trigonometriche degli angoli di π/3 radianti (60 gradi) e π/6 radianti (30 gradi), iniziamo con un triangolo equilatero di lato 1. Tutti i suoi angoli sono pari a π/3 radianti. Dividendo il triangolo in due parti tramite un'altezza, otteniamo un triangolo rettangolo con angoli di π/6 e π/3 radianti, ossia 30 e 60 gradi. Per questo triangolo, i due cateti valgono 1/2 e (√3)/2 e l'ipotenusa 1. Ciò implica:
Nel triangolo possono essere calcolate esattamente le funzioni trigonometriche degli angoli, mediante i lati, a titolo esemplificativo si usa soltanto l'angolo α relativo al vertici A e opposto al lato "a"
dove Δ rappresenta l'area del triangolo.
Poiché le funzioni trigonometriche sono periodiche, è necessario restringere il loro dominio per non avere ambiguità nella definizione dell'inverso. Di seguito presentiamo le definizioni usuali per le funzioni inverse:
Per le funzioni trigonometriche inverse, viene spesso usata anche la notazione sin, cos, ecc. in luogo di arcsin e arccos. Con questa notazione, però, corre il rischio di confondere le funzioni inverse con l'inverso moltiplicativo delle funzioni.
Analogamente al seno e al coseno, anche le funzioni trigonometriche inverse si possono definire tramite serie infinite. Ad esempio,
Queste funzioni si possono definire anche dimostrando che sono gli integrali indefiniti di altre funzioni. L'arcoseno, per esempio, si può scrivere tramite il seguente integrale:
Formule analoghe per le altre funzioni si possono trovare nella voce sulle funzioni trigonometriche inverse. Usando il logaritmo complesso, è possibile generalizzare tutte queste funzioni ad argomenti complessi:
Le funzioni trigonometriche, come dice il nome, sono di importanza cruciale nella trigonometria, principalmente per i due seguenti risultati.
Il teorema dei seni afferma che per ogni triangolo vale:
scritto spesso come:
Questo teorema si può dimostrare dividendo il triangolo in due triangoli rettangoli (tracciando l'altezza) e usando la definizione di seno. Il numero comune "a"/(sin"A") è uguale al diametro della circonferenza circoscritta al triangolo, ossia quella passante per i tre punti "A", "B" e "C". Il teorema dei seni è utile per calcolare la lunghezza di lati ignoti di un triangolo se sono noti due angoli e un lato. Questa situazione è comune nella "triangolazione", una tecnica per determinare le distanze misurando due angoli e la distanza fra i due punti in cui è effettuata la misurazione.
Il teorema del coseno o di Carnot è una generalizzazione a qualunque triangolo del teorema di Pitagora:
ossia:
Anche questo teorema si può dimostrare dividendo il triangolo in due triangoli rettangoli. Il teorema di Carnot è utile per la risoluzione di un triangolo di cui siano noti due lati e l'angolo compreso fra di essi.
Se l'angolo noto non è quello compreso fra i due lati, il triangolo potrebbe non essere unico. È necessario prestare la dovuta attenzione a questo caso ambiguo del teorema.
Esiste anche un teorema delle tangenti o teorema di Nepero:
La funzione seno (e di conseguenza la funzione coseno, che altro non è se non la funzione seno sfasata di π/2) è essenziale per la descrizione del moto armonico semplice, un concetto molto importante in fisica. In questo contesto, il seno e il coseno sono usati per descrivere la proiezione in una dimensione del moto circolare uniforme, il moto di una massa soggetta ad una forza elastica o piccole oscillazioni di un pendolo. Esse sono funzioni periodiche il cui grafico è il tipico schema di un'onda, e sono utili per la modellizzazione di fenomeni periodici come le onde acustiche o elettromagnetiche. Qualsiasi segnale si può rappresentare come una somma (tipicamente infinita) di funzioni seno e coseno di frequenza differente; questa è l'idea di base dell'analisi di Fourier, in cui le serie trigonometriche sono utilizzate per risolvere molti problemi con condizioni al contorno nelle equazioni differenziali alle derivate parziali. Per esempio, l'onda quadra si può scrivere attraverso la serie di Fourier
</text>
</doc>
<doc id="28079" url="https://it.wikipedia.org/wiki?curid=28079">
<title>Secante (geometria)</title>
<text>
Per retta secante di una curva si intende una retta che interseca la curva in due o più dei suoi punti. 
Questo termine deriva dal latino "secare", per "tagliare".
Le secanti possono essere usate per approssimare le tangenti alla curva.
Per individuare la tangente ad una curva "C" in un suo punto "P" si possono considerare le secanti della "C" definite da due punti, uno dei quali "P" e l'altro "Q" variabile. Se si avvicina "Q" a "P" sulla curva e se questa è "abbastanza regolare", la relativa secante si avvicina alla tangente in "P" (che la regolarità della "C" garantisce essere unica).
Di conseguenza si può dire che il limite della pendenza (o direzione) della secante in un punto è la pendenza (direzione) della tangente in questo punto.
Si consideri la curva "C" definita da "y" = "f"("x") in un sistema di coordinate cartesiane, un suo punto "P"
di coordinate ("c", "f"("c")) e un altro punto "Q" con coordinate ("c" + Δ"x", "f"("c" + Δ"x")).
Allora la pendenza "m" della retta secante di "C" in "P" e "Q", è data da:
e la sua equazione da:
Il rapporto incrementale che costituisce l'ultimo membro della precedente relazione, quando Δ"x" si avvicina a zero 
si avvicina alla derivata di "f"("c"), ammesso che esista.
</text>
</doc>
<doc id="2748058" url="https://it.wikipedia.org/wiki?curid=2748058">
<title>Auto similarità</title>
<text>
In matematica, un oggetto auto-simile è esattamente o approssimativamente simile a una sua parte (cioè una o più delle sue parti è internamente omotetica al tutto). Molti oggetti nel mondo reale, come ad esempio le coste, sono statisticamente auto-simili: parti di questi oggetti mostrano le stesse proprietà statistiche a molte scale. L'auto-similarità è una proprietà tipica dei frattali.
L'invarianza di scala è una forma esatta di auto-similarità, dove ad ogni ingrandimento c'è una parte dell'oggetto che è simile al tutto. Per esempio, un lato del fiocco di Koch è sia simmetrico che invariante di scala: può essere ingrandito di un fattore 3 senza cambiare forma.
Uno spazio topologico compatto "X" è auto-simile se esiste un insieme finito "S" di indici di un insieme di omeomorfismi non suriettivi formula_1 per cui
Se formula_3, diciamo che "X" è auto-simile se è il solo sottoinsieme non-vuoto di "Y" tale che l'equazione precedente vale per formula_1. Una struttura auto-simile è formata dalla terna 
Gli omeomorfismi possono essere iterati, dando origine a un sistema di funzioni iterate. La composizione di funzioni crea la struttura algebrica di monoide. Quando l'insieme "S" ha solo due elementi, il monoide è detto monoide diadico. Il monoide diadico può essere rappresentato come un albero binario infinito; più in generale, se l'insieme "S" ha "p" elementi, allora il monoide può essere rappresentato come un albero con numero di coordinazione "p".
Gli automorfismi di un monoide diadico formano il gruppo modulare. Gli automorfismi possono essere rappresentati come le rotazioni iperboliche dell'albero binario.
L'insieme di Mandelbrot è autosimile attorno ai punti di Misiurewicz.
L'autosimilarità ha importanti conseguenze nel progetto di reti di computer, poiché il tipico traffico sulla rete ha proprietà di autosimilarità. Ad esempio, nell'ingegneria delle telecomunicazioni, le sequenze di dati in reti a commutazione di pacchetto sembrano essere auto-simili. Questa proprietà significa che semplici modelli che utilizzano una distribuzione di Poisson sono inadeguati e le reti progettate senza considerare l'auto-similarità hanno grandi probabilità di funzionare in modo inaspettato.
Le piante sono oggetti auto-simili presenti in natura. L'immagine a fianco è un esempio, benché generato al computer, di autosimilarità. Le felci reali, tuttavia, sono estremamente vicine all'auto-similarità reale. 
</text>
</doc>
<doc id="1140445" url="https://it.wikipedia.org/wiki?curid=1140445">
<title>Decagono</title>
<text>
In geometria, un decagono è un poligono con dieci lati e dieci angoli. In un decagono regolare tutti i lati hanno lunghezza uguale e tutti gli angoli sono di 144º. L'area di un decagono regolare con lato lungo formula_1 è data da:
Un decagono regolare può essere costruito con riga e compasso. Qui sotto ne è mostrata un'animazione:
Il perimetro di un decagono regolare si trova moltiplicando un suo lato per 10
P= a • 10
</text>
</doc>
<doc id="1244945" url="https://it.wikipedia.org/wiki?curid=1244945">
<title>Poligono</title>
<text>
In geometria un poligono è una figura geometrica piana delimitata da una linea spezzata chiusa. I segmenti che compongono la spezzata chiusa si chiamano lati del poligono e i punti in comune a due lati consecutivi si dicono vertici del poligono.
La parola "poligono" deriva dal greco πολύς (polys, "molti") e γωνία (gōnia, "angolo").
Una definizione di poligono è la seguente.
Ricordiamo che una linea spezzata è l'insieme finito e totalmente ordinato di segmenti, detti lati, che sono ordinatamente consecutivi e ordinatamente non adiacenti. Una linea spezzata è "chiusa" quando il secondo estremo dell'ultimo segmento coincide con il primo estremo del primo. Una linea spezzata è "semplice" (o "non intrecciata") se due lati non successivi, secondo l'ordinamento assegnato, non si intersecano (a parte il primo e l'ultimo lato che possono avere in comune rispettivamente il primo e il secondo estremo).
Il punto in comune a due lati consecutivi è detto "vertice".
Il fatto che una linea spezzata chiusa non intrecciata delimiti effettivamente una porzione di piano è, per quanto intuitivo, un risultato non banale della geometria piana: si tratta di una conseguenza del teorema della curva di Jordan. 
Una definizione costruttiva è la seguente: un punto formula_1 del piano appartiene al poligono se (con al più un numero finito di eccezioni) tutte le semirette uscenti da formula_1 intersecano la spezzata in un numero finito e dispari di punti distinti.
Una prima classificazione di un poligono riguarda il suo numero di lati (vedi i nomi di poligono).
Un poligono è:
Un poligono semplice è:
In base alla simmetria, un poligono è:
La somma degli angoli interni di un poligono è pari a tanti angoli piatti quanti sono i suoi lati (formula_4), meno due
Ad esempio, il poligono in figura ha cinque lati, e quindi:
La dimostrazione può essere svolta per induzione: in un triangolo la somma degli angoli è formula_3, e preso un qualunque poligono una sua diagonale lo divide in due altri poligoni con un numero minore di lati, per cui si può far valere l'ipotesi induttiva.
La somma degli angoli esterni di un poligono convesso con formula_4 lati è uguale a 
In quanto la somma di tutti gli angoli esterni ed interni è, evidentemente, uguale a formula_4 volte un angolo giro: sottraendo al totale la somma di quelli interni, avremo la somma di quelli esterni.
Con la formula dell'area di Gauss è possibile calcolare l'area di un poligono con formula_4 vertici aventi coordinate cartesiane formula_12 nel modo seguente: 
con la convenzione che formula_14.
Con questa formula possiamo ricavare una superficie di una qualsiasi figura piana attraverso le coordinate dei suoi vertici. È una formula molto utilizzata nella topografia e nella trigonometria.
Distinzione in base al numero di lati e, quindi, di angoli:
</text>
</doc>
<doc id="620304" url="https://it.wikipedia.org/wiki?curid=620304">
<title>Funzione trigonometrica inversa</title>
<text>
In matematica, le funzioni trigonometriche inverse sono un insieme di funzioni strettamente collegate alle funzioni trigonometriche. Le funzioni inverse principali sono elencate nella seguente tabella.
Talvolta vengono utilizzate le notazioni formula_1, formula_2, etc in luogo di arcsin, arccos, etc, ma questa notazione ha lo svantaggio di creare confusione, per esempio, fra formula_3 e formula_4, sebbene il contesto sia generalmente sufficiente a chiarire l'ambiguità.
Nei linguaggi di programmazione al computer le funzioni arcsin, arccos, arctan sono generalmente chiamate asin, acos, atan. Molti linguaggi di programmazione forniscono anche la funzione con due argomenti atan2, che calcola l'arcotangente di "y"/"x" dati "y" ed "x", ma in un intervallo di [-π,π].
Analogamente al seno ed al coseno, le funzioni trigonometriche inverse si possono in alternativa definire in termini di serie infinite.
Queste funzioni si possono anche definire dimostrando che sono integrali di altre funzioni.
È possibile esprimere queste funzioni usando i logaritmi naturali. Ciò permette di estendere in modo naturale il loro dominio all'intero piano complesso.
Queste relazioni si possono dimostrare elementarmente tramite l'espansione delle funzioni trigonometriche alla forma esponenziale.
Sia formula_25
Le derivate delle funzioni trigonometriche inverse valgono:
Questi risultati si ottengono facilmente derivando la forma logaritmica mostrata sopra.
Tutti questi integrali si ricavano integrazione per parti e le derivate elencate al paragrafo precedente.
È possibile combinare la somma o differenza di due funzioni trigonometriche inverse in un'espressione dove la funzione trigonometrica compare una sola volta:
</text>
</doc>
<doc id="48169" url="https://it.wikipedia.org/wiki?curid=48169">
<title>Piano (geometria)</title>
<text>
Il piano è un concetto primitivo della geometria, ovvero un concetto per il quale non esiste una definizione formale e che si suppone intuitivamente comprensibile e/o esperianzialmente acquisito, pertanto un'idea universalmente accettata ed unica rappresentabile con oggetti concreti che fungono da esempio ma che per la loro sussistenza stessa non risolvono pienamente il concetto (gli altri concetti primitivi della geometria sono il punto e la retta). Nel caso del piano per rappresentarlo idealmente si pensi ad un foglio di carta di dimensioni infinite: il piano è l'idea, il concetto astratto, ma non è il foglio di carta sia perché questo ha uno spessore ed un piano ideale non ne ha e sia perché non è possibile produrre o ritrovare un foglio di carta di dimensioni infinite.
In definitiva, esso:
Le relazioni che intercorrono tra un piano e i punti e le rette che esso contiene sono espresse dagli assiomi di Euclide e dagli assiomi di Hilbert.
L'equazione canonica del piano nello spazio tridimensionale formula_1 è del tipo: 
con formula_3 e formula_4 non tutti nulli.
Siano formula_5 tre punti dello spazio non allineati. Per questi tre punti passa uno ed un solo piano formula_6.
Un punto formula_7 appartiene al piano formula_6 solo se il vettore formula_9 è combinazione lineare dei vettori formula_10 e formula_11, ovvero se
Sviluppando il determinante con la regola di Laplace rispetto alla prima riga si ottiene:
Dove
Infine, per ottenere l'equazione canonica del piano, si definisce formula_15 come segue:
Dove formula_17 è un punto che appartiene al piano, pertanto in questo caso si possono utilizzare le coordinate di un punto qualsiasi fra formula_18, formula_19 e formula_20.
Si può studiare la posizione reciproca di due piani mettendo a sistema le loro equazioni. Quando la matrice dei coefficienti ha Rango due il sistema è compatibile e risulta ammettere una semplice infinità (formula_21) soluzioni, che rappresentano tutti i punti della retta di intersezione tra i due piani.
Quando la matrice dei coefficienti ha rango 1, le soluzioni ammesse sono una doppia infinità (formula_22), e i piani risultano essere paralleli e coincidenti (Parallelismo Improprio).
Se infine la matrice dei coefficienti ha rango 0, il sistema risulta essere incompatibile, e i piani sono paralleli e distinti (Parallelismo Proprio).
È possibile calcolare la distanza di un punto formula_23 da un piano formula_6 utilizzando la seguente formula:
In particolare, se formula_26, allora il punto formula_27 appartiene al piano formula_6.
</text>
</doc>
<doc id="48513" url="https://it.wikipedia.org/wiki?curid=48513">
<title>Punto (geometria)</title>
<text>
In geometria il punto è un concetto primitivo. Intuitivamente equivale ad un'entità adimensionale spaziale, per cui può essere considerato semplicemente come una posizione, cioè come una coordinata.
In topologia ed analisi matematica, viene spesso chiamato punto un elemento qualunque di uno spazio topologico e, in particolare, di uno spazio funzionale.
Negli "Elementi" di Euclide, al punto è riservata la prima delle definizioni del I libro, dove si indica che punto è ciò che non ha parti. Il punto è l'ente fondamentale della geometria ed è privo di una qualsiasi dimensione. Tale definizione è di tipo "ostensivo" cioè non ha una valenza logica ma che serve ad indicare ciò di cui ci si vuole occupare.
Con l'assiomatizzazione rigorosa della geometria effettuata da David Hilbert nei Grundlagen der Geometrie il punto, assieme alla retta ed al piano, diventa una delle nozioni primitive della geometria e quindi non è definito. Da notare che sarebbe possibile anche fondare la geometria assumendo come primitiva la nozione di regione e definendo i punti tramite opportune classi di "regioni sempre più piccole". Ricerche in tale direzione, che iniziano con alcune analisi di Alfred North Whitehead, vanno sotto il nome di "Geometria senza punti".
Molti preferiscono dare una definizione di questi tre enti fondamentali della geometria e definiscono il punto l'ente che, pur essendo reale, non ha dimensioni.
Un punto nella geometria euclidea non ha grandezze di alcun tipo (volume, area, lunghezza), e nessuna caratteristica in generale tranne la sua posizione. I postulati di Euclide asseriscono in alcuni casi l'esistenza di punti; un esempio: se due linee in un piano non sono parallele, c'è esattamente un punto che appartiene ad entrambe.
Tre o più punti nello spazio si dicono allineati se sono contenuti in una retta. Quattro o più punti nello spazio si dicono complanari se sono contenuti in un piano.
Nella geometria euclidea il punto è in relazione con gli altri enti geometrici fondamentali, quali la retta e il piano. Ad esempio:
Nella geometria cartesiana del piano e dello spazio euclideo un punto è identificato da un insieme ordinato di coordinate. Quindi un punto nello spazio tridimensionale è rappresentato da una terna ordinata di numeri, ad esempio: formula_1
In generale, un punto in uno spazio euclideo di dimensione formula_2 è una successione di formula_2 numeri. In questo contesto i punti possono essere identificati con i vettori (applicati nell'origine).
Le proprietà elencate sopra possono essere estese ad uno spazio euclideo di dimensione arbitraria nel modo seguente:
Oppure possono essere estese a oggetti "curvi", quali curve e superfici, ad esempio nel modo seguente:
</text>
</doc>
<doc id="62127" url="https://it.wikipedia.org/wiki?curid=62127">
<title>Segmento</title>
<text>
In geometria un segmento è una parte di retta compresa tra due punti, detti estremi del segmento. 
Quando i due estremi si trovano su una curva, il segmento è detto corda. Quando i due estremi sono vertici di un poligono, il segmento è detto lato se i due vertici sono adiacenti, altrimenti è detto diagonale.
Il segmento, generalmente, si indica con due lettere maiuscole dell'alfabeto italiano, poste agli estremi (indicati da 2 punti). Essendo una linea, si può indicare anche con una lettera minuscola posta fra i due estremi, anche se questa notazione non è propriamente corretta.
In algebra lineare si può introdurre un concetto primitivo di segmento, costruito in un ambiente generico e astratto quale uno spazio vettoriale: si definisce "segmento" un sottoinsieme "L" di uno spazio vettoriale reale "V" che può essere descritto come 
Gli estremi di "L" sono poi definiti come i vettori formula_2 e formula_3. 
Un segmento (chiuso) di estremi formula_4 e formula_5 può essere indicato con la scrittura formula_6, analoga alla notazione utilizzata nel caso 1-dimensionale per gli intervalli di formula_7.
</text>
</doc>
<doc id="86605" url="https://it.wikipedia.org/wiki?curid=86605">
<title>Quadrilatero</title>
<text>
In geometria il quadrilatero è un poligono con quattro lati e quattro vertici.
Tutti i quadrilateri hanno quattro vertici e quattro angoli interni (cioè sono quadrangoli).
La somma delle ampiezze degli angoli interni di un quadrilatero semplice ABCD è uguale a 360°:
formula_1
Le due "diagonali" di un quadrilatero convesso sono segmenti che uniscono vertici opposti.
Si distinguono vari tipi di quadrilateri; in altre parole nell'insieme dei quadrilateri vengono individuati vari sottoinsiemi. I diversi tipi di quadrilateri hanno diverse applicazioni, spesso importanti; presentano interesse, anche operativo, le relazioni di inclusione che sussistono tra i sottoinsiemi notevoli dell'insieme dei quadrilateri.
"Quadrilatero convesso", come dice il termine, è un quadrilatero e una "figura piana convessa", cioè una figura piana che per ogni coppia di punti interni contiene tutti i punti del segmento di cui essi sono le estremità. Tutti gli angoli interni di un quadrilatero convesso hanno ampiezza inferiore a 180°.
"Quadrilatero non convesso", anche questo termine è autoesplicativo, è un quadrilatero e una "figura piana non convessa", cioè una figura piana che contiene due punti tali che il segmento che li congiunge possiede punti che non appartengono alla figura stessa. Almeno un angolo interno di un quadrilatero non convesso ha ampiezza maggiore di π (in realtà un solo angolo possiede questa proprietà).
Dunque l'insieme dei quadrilateri si ripartisce nel sottoinsieme dei quadrilateri convessi e nel sottoinsieme dei quadrilateri non convessi (complementare del precedente). I quadrilateri non convessi sono caratterizzati anche dal fatto che prolungando due loro lati si ottengono punti interni della figura.
Il trapezio è un quadrilatero convesso che ha due lati opposti paralleli. Il parallelogramma è un caso particolare del trapezio. Le diagonali dividono in due parti congruenti ogni singolo angolo. Il trapezio può essere di tre tipi diversi:
Il parallelogramma è un quadrilatero che presenta i lati a due a due paralleli. Si tratta di un caso particolare di trapezio. Ha i lati opposti e uguali a due a due, gli angoli adiacenti sono supplementari, sommati formano un angolo di 180°.
Il rombo o losanga è un quadrilatero con i lati congruenti e gli angoli due a due uguali. Esso risulta essere un particolare parallelogramma e presenta le due diagonali ortogonali e intersecantesi nel loro punto di mezzo, dette diagonale maggiore e diagonale minore.
Il rettangolo è un parallelogramma i cui angoli interni uguali e tutti retti (90°). Ogni angolo è composto da due rette perpendicolari. Il rettangolo è un quadrilatero ciclico.
Il quadrato è un rettangolo con tutti i lati e gli angoli congruenti, cioè uguali. È anche un rombo con tutti gli angoli retti. Dall'unione di quadrati e rettangoli si possono ottenere figure interessanti come il rettangolo armonico, il rettangolo aureo (utilizzato soprattutto per le sculture antiche) e il tatami, che è un altro tipo di rettangolo di origine giapponese usato per costruire dei tappeti che compongono il "pavimento" delle loro case.
</text>
</doc>
<doc id="86944" url="https://it.wikipedia.org/wiki?curid=86944">
<title>Trapezio</title>
<text>
In geometria un trapezio è un quadrilatero con almeno due lati paralleli.
Facendo riferimento alla figura a fianco del teorema, i due lati paralleli formula_1 e formula_2 sono detti basi del trapezio, rispettivamente "base maggiore" e "base minore", mentre gli altri due lati formula_3 e formula_4 sono detti lati obliqui del trapezio.
La distanza formula_5 fra i due lati paralleli, lunghezza di ogni segmento ortogonale che collega le basi o i loro prolungamenti, fornisce l'altezza del trapezio.
Nel caso particolare in cui anche i due lati obliqui siano paralleli si ha un parallelogramma. Se poi ha pure gli angoli retti si ha un rettangolo; se invece ha tutti i lati lunghi uguali si ha il rombo; se ha entrambe queste caratteristiche si ha il quadrato. Tutte queste figure sono trapezi, poiché hanno una coppia di lati paralleli.
Se i lati obliqui non sono paralleli, essi possono essere prolungati fino ad incontrarsi in un punto, in modo da formare un triangolo che contiene il trapezio: questo è il più piccolo triangolo circoscritto al trapezio che contiene il trapezio stesso ed è unico.
L'area formula_6 del trapezio si può calcolare facendo la somma delle basi per l'altezza il tutto diviso due.
Tale formula può essere spiegata se si fa riferimento alla figura a fianco: se al trapezio originario si affianca un altro trapezio ad esso congruente ottenuto tramite una rotazione di un angolo piatto, si nota che la figura così ottenuta è un parallelogramma la cui area è data dal prodotto della somma delle basi per l'altezza. Poiché essa è il doppio di quella voluta, ossia di quella del trapezio, ne va presa la metà.
Si definisce trapezio rettangolo un trapezio nel quale i due angoli adiacenti ad un lato obliquo sono angoli congruenti e quindi retti, poiché sono supplementari. Un trapezio, dunque, è rettangolo se e solo se ha un lato obliquo perpendicolare alle basi. 
Si definisce trapezio isoscele un trapezio nel quale i due angoli adiacenti ad una base siano congruenti. Di conseguenza i lati obliqui sono anch'essi congruenti. 
Si definisce trapezio ottusangolo un trapezio che presenta un angolo ottuso adiacente alla base di lunghezza maggiore. Un trapezio è ottusangolo se e solo se il corrispondente triangolo circoscritto è un triangolo ottusangolo.
Un trapezio ottusangolo non può essere isoscele.
Si definisce trapezio scaleno un trapezio con i lati di diversa lunghezza e gli angoli di diversa ampiezza: può essere fatto derivare dall'intersezione di un rettangolo con un triangolo scaleno. Alcune fonti definiscono il trapezio scaleno richiedendo solamente che i lati obliqui siano diversi fra loro.
Talvolta viene impropriamente usato il termine trapezoide al posto di trapezio: tale uso improprio sembra derivare dal fatto che in negli Stati Uniti e in Canada il trapezio viene chiamato "trapezoid" (a differenza della Gran Bretagna dove viene chiamato "trapezium").
Il termine appropriato è invece trapezio: infatti in italiano con "trapezoide" si intende, più genericamente, un semplice quadrilatero.
Con il termine trapezoide si intende anche un trapezio il cui lato obliquo è una curva; è utilizzato nelle funzioni.
</text>
</doc>
<doc id="87039" url="https://it.wikipedia.org/wiki?curid=87039">
<title>Aquilone (geometria)</title>
<text>
In geometria un aquilone (o deltoide) è un quadrilatero che presenta due coppie di lati consecutivi che sono congruenti (mentre un parallelogramma presenta due coppie di lati congruenti che sono opposti). 
In un deltoide le due diagonali si intersecano ortogonalmente, è sempre presente almeno una coppia di angoli opposti congruenti e la diagonale tra i due vertici opposti comuni alle due coppie di lati congruenti è asse di simmetria per la figura.
L'insieme dei deltoidi si bipartisce nei due sottoinsiemi degli aquiloni convessi e dei concavi: il punto di intersezione delle due diagonali è punto interno per gli aquiloni convessi, mentre è punto esterno per gli aquiloni concavi. Tra queste due classi di aquiloni si possono collocare i triangoli isosceli visti come caso limite degli aquiloni quando due dei lati consecutivi congruenti formano tra loro un angolo piatto (in questo caso, però, non si ha un quadrilatero e si parla di "aquilone degenere").
Un deltoide convesso è sempre circoscrivibile ad una circonferenza, ossia esiste sempre una circonferenza alla quale tutti i suoi lati sono tangenti.
La diagonale che è asse di simmetria divide ogni aquilone in due triangoli l'uno simmetrico dell'altro. L'altra diagonale divide ogni aquilone convesso in due triangoli isosceli e individua per ogni aquilone concavo due triangoli isosceli per i quali l'aquilone costituisce la chiusura topologica della loro differenza insiemistica.
Un aquilone (convesso) che ha tutti i quattro lati congruenti è un rombo. 
Ad ogni aquilone si possono associare i due triangoli isosceli caratterizzati rispettivamente da una coppia di lati congruenti. Fissati i tre vertici di uno dei due suddetti triangoli isosceli, facendo muovere il quarto vertice sul suo asse di simmetria si individua una famiglia di aquiloni che comprende anche i due aquiloni degeneri costituiti dal triangolo fissato e da due dei suoi lati.
Un quadrilatero è un aquilone se e solo se le sue diagonali sono ortogonali e una delle due viene divisa in due segmenti congruenti dal loro punto di intersezione. 
Un quadrilatero è un aquilone se e solo se sono congruenti due suoi lati adiacenti e sono congruenti i due angoli delimitati dalle due coppie di lati per i quali non si è richiesta la congruenza.
Un aquilone è un quadrilatero ciclico, cioè ha tutti i vertici appartenenti al suo circumcerchio, se e solo se due suoi angoli opposti sono angoli retti; in tale caso la diagonale di simmetria dell'aquilone è diagonale del circumcerchio e i due triangoli tra loro simmetrici che lo compongono sono rettangoli.
L'area si può ottenere con la seguente formula che calcola l'area di un qualsiasi parallelogramma con le diagonali perpendicolari:
formula_1
dove formula_2 è la diagonale maggiore e formula_3 è quella minore.
</text>
</doc>
<doc id="87064" url="https://it.wikipedia.org/wiki?curid=87064">
<title>Parallelogramma</title>
<text>
 la geometria euclidea, un parallelogramma (o parallelogrammo) è un quadrilatero con i lati opposti paralleli. I lati e gli angoli opposti di un parallelogramma sono congruenti.
La congruenza dei lati e degli angoli opposti è una diretta conseguenza del V postulato di Euclide, relativo agli angoli interni determinati da una retta che ne taglia due, e nessuna delle caratteristiche del quadrilatero può essere dimostrata senza ricorrere al postulato di Euclide o a una delle sue formulazioni equivalenti.
L'etimologia, dal greco παραλληλ-όγραμμον, una forma "di linee parallele", riflette la definizione.
Il parallelogramma è un caso particolare di trapezio. Il parallelogramma ha una sola altezza (le possibili altezze sono due) che dipende da quale lato viene considerato come base.
Dalle definizioni illustrate precedentemente è affermabile che:
La legge del parallelogramma caratterizza gli spazi di Hilbert nell'ambito degli spazi di Banach.
Ogni parallelogramma consente di costruire una tassellazione del piano.
La figura solida corrispondente tridimensionale del parallelogramma è il parallelepipedo.
Un parallelogramma con base formula_1 e altezza formula_2 può essere diviso in un trapezoide e un triangolo retto, per essere ricombinato in un rettangolo, come mostrato nella figura a destra. Questo significa che l'area di un parallelogramma è la stessa di quella di un rettangolo con identica base e altezza: 
</text>
</doc>
<doc id="87117" url="https://it.wikipedia.org/wiki?curid=87117">
<title>Rettangolo</title>
<text>
In geometria, il rettangolo è un quadrilatero che ha tutti gli angoli interni congruenti tra loro (e, di conseguenza, retti).
Da questa definizione si evince che in un rettangolo ciascuna delle due coppie di lati opposti è costituita da lati congruenti; in altre parole i rettangoli sono particolari parallelogrammi. I rettangoli sono anche particolari quadrilateri ciclici: si possono definire come i quadrilateri ciclici aventi come diagonali due diametri del cerchio circoscritto. 
Il quadrato è un tipo particolare di rettangolo, caratterizzato dall'avere tutti i quattro lati congruenti. Equivalentemente si dice che l'insieme dei quadrati è l'intersezione dell'insieme dei rettangoli con l'insieme dei rombi.
Nel parlare colloquiale per sottolineare che un rettangolo non ha tutti i lati congruenti come un quadrato, si dice che un rettangolo è una "figura oblunga". Quando si presenta un rettangolo nel piano cartesiano e questo ha due lati sensibilmente più lunghi degli altri due e disposti orizzontalmente, si parla di "rettangolo largo"; se invece i lati più lunghi sono disposti verticalmente si parla di "rettangolo alto" o addirittura di "rettangolo sottile". La lunghezza dei due lati opposti più lunghi viene chiamata "lunghezza" o "base" del rettangolo, mentre la lunghezza dei due lati più corti viene chiamata "larghezza" o "altezza". 
Un quadrilatero convesso è un rettangolo se e solo se possiede una di queste caratteristiche equivalenti:
Il poligono duale del rettangolo è un rombo, come illustrato nella tabella sottostante.
L'area del rettangolo è il prodotto della sua lunghezza per la sua larghezza, ovvero della sua base per la sua altezza. Per esempio, il rettangolo nella prima figura ha una base di 5 e un'altezza di 4: la sua area è quindi 20, risultato della moltiplicazione 5 × 4. 
Se invece la base e l'altezza di un rettangolo si indicano rispettivamente con formula_1 ed formula_2 per la sua area formula_3 e per il suo perimetro formula_4 si ha:
Nel calcolo infinitesimale l'integrale di Riemann viene definito come limite delle somme delle aree di rettangoli via via più sottili.
Il termine, inteso come aggettivo, può specificare altre figure geometriche. 
</text>
</doc>
<doc id="42226" url="https://it.wikipedia.org/wiki?curid=42226">
<title>Sistema di coordinate</title>
<text>
finisce sistema di coordinate un sistema di riferimento basato su coordinate, le quali individuano la posizione di un oggetto in qualche spazio. A seconda del numero di coordinate usate, si può parlare di:
Il sistema di riferimento unidimensionale ideato da Cartesio è costituito da una retta, sulla quale un oggetto, di solito un punto, è vincolato a muoversi. Su questa retta si fissa un'origine, che è consuetudine indicare con formula_1, un verso di percorrenza ed un'unità di misura delle lunghezze.
È possibile individuare un punto sulla retta in base ad un numero reale, che individua la distanza dall'origine nell'unità di misura scelta, positiva se concorde con il verso di percorrenza scelto e negativa altrimenti, del punto. Tale numero è detto coordinata, e per indicare genericamente tale coordinata si usa la lettera formula_2. La retta su cui si è fissato origine, verso di percorrenza e unità di misura è detta ascissa.
Quando un punto, anziché su una retta, è vincolato a muoversi su una curva è possibile scegliere anche su quest'ultima un'origine, un verso di percorrenza ed un'unità di misura, ma in tal caso si parlerà di ascissa curvilinea. La distanza con segno del punto dall'origine è la coordinata curvilinea del punto.
Uno dei sistemi di riferimento bidimensionale è costituito da una coppia di rette incidenti. Tali rette sono indicate, in genere, con formula_3 e formula_4, ed il loro punto di intersezione è l'origine per entrambe le rette. Su ciascuna retta si fissa un verso di percorrenza ed un'unità di misura che in genere è uguale per entrambe le rette, ma per esigenze particolari può benissimo essere diversa per ciascuna retta. La posizione di un punto vincolato a muoversi su un piano può essere individuata da una coppia di valori reali, genericamente indicati con le lettere formula_2 e formula_6. Si indica con formula_2 il numero reale che individua la distanza dall'asse formula_4 del punto, misurata parallelamente all'asse formula_3 nell'unità di misura scelta per quest'ultimo; con formula_6 il numero reale che individua la distanza dall'asse formula_3 del punto, misurata parallelamente all'asse formula_4 nell'unità di misura scelta per quest'ultimo. La coppia di coordinate che individua il punto si indica scrivendo formula_13 oppure formula_14.
Quando gli assi formula_3 e formula_4 sono fra loro ortogonali tale sistema di riferimento si dice ortogonale. Se gli assi sono tra loro ortogonali e l'unità di misura di entrambi è la stessa tale sistema di riferimento si dice ortonormale o cartesiano, in onore del matematico francese Cartesio che lo riprese in età moderna, dopo che era già stato introdotto, nel Medioevo, da Nicola d'Oresme. In tal caso l'asse formula_3, orizzontale, prende il nome di asse delle ascisse, e l'asse formula_4, verticale, prende il nome di asse delle ordinate. Nelle opere di Oresme, erano, rispettivamente, "longitudo" e "latitudo".
Un sistema di riferimento polare è formato da due coordinate indicate con le lettere formula_19 e formula_20. Con formula_19 si indica la distanza del punto considerato dall'origine del sistema; in pratica se consideriamo il vettore formula_22 che congiunge l'origine degli assi con il nostro punto, formula_19 ne indica il modulo. Con formula_20, invece, ci si riferisce all'angolo o anomalia che si forma tra il vettore formula_22 considerato prima, e il verso positivo dell'asse formula_3 di un normale sistema ortogonale. Dunque, formula_19 è il raggio e formula_20 un angolo orientato.
Per passare dalle coordinate polari alle cartesiane si usano le seguenti formule:
e per passare da quelle cartesiane a quelle polari
Si può trovare in molti casi la coordinata formula_19 denotata con la lettera formula_34. Questo passaggio di coordinate è molto utile in alcune applicazioni della matematica come nella risoluzione degli integrali multipli su domini costituiti da corone circolari.
Il sistema di riferimento tridimensionale è costituito da tre rette non parallele, in genere indicate con formula_3, formula_4 e formula_37, passanti per un punto che è l'origine del sistema di riferimento. Per ciascuna di tali rette si sceglie un'unità di misura ed un verso di percorrenza. Le coordinate generiche di un punto nello spazio sono indicate con le lettere formula_2, formula_6 e formula_40. Si indica con formula_2 il numero reale che individua la distanza di un punto dal piano individuato dalle rette formula_4 e formula_37 misurata parallelamente all'asse formula_3 nell'unità di misura scelta per quest'ultimo asse. Si definiscono analogamente formula_6 e formula_40. Le tre coordinate che individuano un punto nello spazio sono indicate con la simbologia formula_47. Quando i tre assi sono fra loro ortogonali il sistema di riferimento si dice "ortogonale" o "rettangolare".
Ciascuna delle tre rette è un "asse cartesiano", e insieme formano la "terna cartesiana".
Il sistema cilindrico è la naturale espansione del sistema polare nelle tre dimensioni. In questo caso le coordinate sono formula_19, formula_20 e formula_40. Considerando un generico punto formula_51, e la sua proiezione formula_52 sul piano formula_53, la coordinata formula_40 indica la distanza formula_55. Con formula_19 si denota la distanza dall'origine del punto formula_52, mentre formula_20 individua l'angolo che si forma tra il vettore formula_22 e l'asse formula_2.
Per passare dal sistema cilindrico a quello rettangolare:
e per passare alle coordinate cilindriche:
Molto spesso la coordinata formula_19 viene indicata con formula_64.
Un altro sistema che si può usare per orientarsi nello spazio è il sistema sferico. È formato da tre coordinate: formula_19, formula_66 e formula_20. Si considera sempre un generico punto formula_51 e la sua proiezione sul piano formula_69 chiamata formula_52. Con formula_19 questa volta si indica la distanza di formula_51 dall'origine e formula_66 è l'angolo che formula_22 forma con il semiasse positivo delle formula_37, chiamato angolo di inclinazione. Indichiando invece con formula_76 il vettore che collega l'origine con il punto formula_52, si ha che formula_20 individua l'angolo che quest'ultimo vettore forma con l'asse formula_3, detto azimut.
Per passare da un sistema sferico ad uno rettangolare si usano le seguenti uguaglianze:
Per passare da coordinate cartesiane a sferiche:
Anche con questo sistema spesso si usa la lettera formula_34 al posto della lettera formula_19.
A partire dal sistema di coordinate sferiche si può definire una nuova base vettoriale in ogni punto dello spazio mediante i vettori tangenti alle linee coordinate. Sia
allora la base naturale dello spazio tangente (isomorfo a formula_89) è data dai tre vettori:
Definendo inoltre
la matrice del cambio di coordinate da formula_92 a formula_93, si ha che un vettore di formula_89può essere scritto nei due sistemi di coordinate come
Poiché formula_96 manda un sistema di coordinate ortonormali levogiro in un altro, si ha formula_97
Esprimendo in modo esplicito le relazioni tra i versori di base si ottiene:
e inversamente
In fisica delle particelle in alcuni casi si preferisce usare in luogo dell'angolo polare formula_66 la pseudorapidità definita come
</text>
</doc>
<doc id="50465" url="https://it.wikipedia.org/wiki?curid=50465">
<title>Terna pitagorica</title>
<text>
Una terna pitagorica è una terna di numeri naturali formula_1, formula_2, formula_3 tali che formula_4. 
Il nome viene dal teorema di Pitagora, da cui discende che ad ogni triangolo rettangolo con lati interi corrisponde una terna pitagorica e viceversa.
Se formula_5 è una terna pitagorica, lo è anche formula_6, dove formula_7 è un numero naturale qualsiasi. Il numero formula_7 è quindi un divisore comune dei tre numeri formula_9, formula_10, formula_11. Una terna pitagorica si dice "primitiva" se formula_1, formula_2 e formula_3 non hanno divisori comuni. I triangoli descritti da terne pitagoriche non primitive sono sempre simili a quelli descritti dalla corrispondente terna primitiva.
Esiste una formula capace di generare tutte le terne pitagoriche primitive; tali formule sono citate da Euclide (Ευκλείδης) nei suoi Elementi (τα Στοιχεία):
Le formule di Euclide generano una terna pitagorica primitiva se e solo se formula_16 e formula_17 sono coprimi ed uno di loro è pari e l'altro dispari (se sia formula_17 che formula_16 sono dispari formula_1, formula_2 e formula_3 sono pari, e quindi quella terna pitagorica non può essere primitiva). Tutte le terne primitive si possono ottenere in questo modo da un'unica coppia di numeri coprimi formula_23, mentre le restanti (non primitive) si possono ottenere moltiplicando i termini di una terna primitiva per un opportuno fattore. Le formule così modificate sono quindi in grado di generare tutte le terne possibili, anche se in modo non univoco:
Una conseguenza immediata di queste formule è che le terne pitagoriche sono infinite, in quanto sono infinite le possibili scelte di formula_16 e formula_17.
Inoltre è facile dimostrare che il prodotto di formula_1 per formula_2 (dei due cateti) è sempre divisibile per formula_29, mentre il prodotto formula_30 (di tutti e tre i lati del triangolo pitagorico) è sempre divisibile per formula_31 (formula_32).
Esistono solo 16 terne pitagoriche primitive con formula_33:
Eccone altre:
È interessante notare che ci possono essere più terne pitagoriche primitive con lo stesso intero minore. Il primo esempio è con il 20, che è il più piccolo intero di due terne primitive: 20, 21, 29 e 20, 99, 101.
Al contrario, il numero 1 229 779 565 176 982 820 è l'intero minore in esattamente 15 386 terne primitive; la più piccola e la più grande fra queste sono:
1 229 779 565 176 982 820
1 230 126 649 417 435 981
1 739 416 382 736 996 181
e
1 229 779 565 176 982 820
378 089 444 731 722 233 953 867 379 643 788 099
378 089 444 731 722 233 953 867 379 643 788 101.
Per i curiosi, si consideri la fattorizzazione:
L'ultimo teorema di Fermat afferma che non esistono terne non banali analoghe a quelle pitagoriche ma con esponenti maggiori di formula_35 (cioè che l'equazione formula_36 non ammette soluzioni se formula_37; a parte, come detto, i casi banali in cui almeno uno dei numeri è uguale a zero).
Un legame tra terne pitagoriche e primi gemelli può essere stabilito tramite la derivata aritmetica. Infatti un semiprimo i cui fattori primi siano due primi gemelli può essere espresso come formula_38, la sua derivata aritmetica come formula_39 e formula_40. Questi numeri sono fra loro coprimi e perciò costituiscono una terna pitagorica primitiva.
Ciascun numero naturale maggiore di 2 appartiene almeno a una terna pitagorica e ogni numero primo può appartenere al più a 2 terne (in quest'ultima situazione una volta come cateto e una volta come ipotenusa del triangolo rettangolo cui si riferisce).
</text>
</doc>
<doc id="65939" url="https://it.wikipedia.org/wiki?curid=65939">
<title>Coefficiente angolare</title>
<text>
In geometria analitica il coefficiente angolare (in lingua inglese "slope", "pendenza") di una retta "non verticale" nel piano cartesiano è il coefficiente formula_1 che compare nella sua equazione, scritta nella forma:
Partendo dai coefficienti dell'equazione generale
con formula_4 (retta non verticale), il coefficiente angolare è espresso dal rapporto
Due rette (non verticali) sono parallele esattamente quando hanno lo stesso coefficiente angolare; in particolare, il coefficiente angolare della retta passante per l'origine,
è la tangente degli angoli formati dalla retta con l'asse delle ascisse: la retta infatti passa per il punto di coordinate formula_7, quindi
Il coefficiente angolare di una retta (non verticale) è il rapporto tra la differenza delle ordinate e la differenza delle ascisse fra due punti distinti della retta, formula_9 e formula_10:
Per una retta verticale, di equazione formula_12, questa espressione è priva di significato: due distinti punti della retta hanno diverse coordinate formula_13 ma uguali coordinate formula_14, quindi per calcolare il rapporto bisognerebbe dividere per zero (al contrario, in geometria proiettiva il simbolo formula_15 è ben definito).
Considerando la retta come grafico di una funzione formula_16, il suo coefficiente angolare è la derivata della funzione: formula_17. (La retta "tangente" è la retta stessa.)
Poiché due rette in forma generale, formula_3 e formula_19, sono perpendicolari esattamente quando formula_20, ne segue che due rette (non verticali) formula_21 e formula_22 sono perpendicolari esattamente quando il prodotto dei loro coefficienti angolari è 
Questa condizione può essere riscritta come formula_24, ed espressa dicendo che formula_25 è l"'antireciproco" (opposto del reciproco) di formula_1.
</text>
</doc>
<doc id="92291" url="https://it.wikipedia.org/wiki?curid=92291">
<title>Piramide (geometria)</title>
<text>
In geometria si definisce piramide un poliedro individuato da una faccia poligonale chiamata "base" e da un vertice che non giace sul piano della base e che talora viene chiamato "apice della piramide". 
Sono suoi spigoli i lati del poligono di base e i segmenti delimitati dall'apice e da ciascuno dei vertici della base. Sono facce della piramide la sua base e le facce triangolari (chiamate facce laterali) che hanno come vertice il suo apice.
Una piramide avente come base un poligono di formula_1 lati (formula_2) si dice "piramide formula_1-gonale" ed ha formula_4 facce, formula_5 spigoli ed formula_4 vertici.
Si dice altezza di una piramide il segmento che ha una estremità nell'apice e cade ortogonalmente sul piano contenente la base.
Le piramidi possono essere rette: nella base può essere inscritto un cerchio e il piede dell'altezza risiede nel centro di quel cerchio.
In una piramide retta si dice apotema ogni segmento che congiunge perpendicolarmente il suo apice con un suo lato di base, ovvero la loro lunghezza comune. Si dice apotema di base il raggio del cerchio inscritto nel poligono di base.
Una piramide è convessa se e solo se il poligono di base è convesso. Talora viene chiamata piramide obliqua una piramide la cui altezza cade al di fuori del poligono di base (o del suo inviluppo convesso).
Le piramidi più considerate sono quelle che hanno per base un poligono regolare e la cui altezza cade nel centro di tale poligono. Una tale piramide talora viene detta piramide simmetrica(o piramide regolare); essa in effetti ha la simmetria, elevata, del poligono di base. Spesso per piramide si intende, per antonomasia, una piramide simmetrica a base quadrata.
Le uniche piramidi che sono anche poliedri regolari sono i tetraedri che hanno base e facce laterali triangolari e tutte uguali.
Secando una piramide formula_7 con un piano parallelo alla base di formula_7, e conservando la parte compresa tra il piano della base e quello della sezione, si ha la cosiddetta piramide tronca (o tronco di piramide). In questo modo, tra il piano della base e quello della sezione intercorre una corrispondenza biunivoca, detta omotetia.
Le facce laterali della piramide tronca sono dei trapezi.
L'area laterale per le piramidi rette è
dove formula_10 è il perimetro di base e formula_11 è l'apotema della piramide.
L'area totale si calcola come: Area di base + Area laterale.
Il volume di una piramide generica è uguale ad un terzo del prodotto dell'area della base per la misura dell'altezza. Detto formula_12 il volume, formula_13 l'area di base e formula_14 l'altezza, si calcola come:
Cioè il volume della piramide è 1/3 di quello di un prisma con uguale altezza e base. Questa formula costituiva il teorema 7 del libro XII degli "Elementi" di Euclide ed era dimostrata con il metodo di esaustione (oggi diremmo con il calcolo infinitesimale).
È possibile visualizzare una dimostrazione grafica del fatto che una piramide occupa un terzo del volume del prisma che la contiene. La cosa è particolarmente semplice partendo da un cubo e dividendolo in tre piramidi uguali, come si vede nella figura a lato.
Da un vertice del cubo si tracciano le quattro diagonali che uniscono il vertice alle tre facce opposte.
Nel caso della figura, si vede il vertice in alto in primo piano che va a congiungersi con la faccia di fondo, la faccia sul retro e la faccia laterale.
Si formano tre piramidi, ciascuna con base (quadrata) coincidente con una faccia (nascosta) del cubo, con due delle facce laterali (ciascuna coincidente con mezza faccia del cubo) costituite da triangoli rettangoli ortogonali alla base, e con le altre due facce (interne al cubo), delimitate dalle diagonali di faccia e dalla diagonale principale del cubo.
L'altezza di ciascuna piramide coincide con un lato del cubo.
Si vede dunque che le tre piramidi sono esattamente uguali e insieme costituiscono il cubo di partenza.
Quindi hanno un volume pari ad 1/3 di quello del cubo.
Per estendere il risultato ad una piramide di forma qualsiasi, ed anche al volume del cono rispetto al cilindro che lo contiene, si può far ricorso al principio di Cavalieri.
</text>
</doc>
<doc id="52138" url="https://it.wikipedia.org/wiki?curid=52138">
<title>Cilindro (geometria)</title>
<text>
In matematica un cilindro ellittico è una quadrica (cioè una superficie nello spazio tridimensionale definita da un'equazione polinomiale di secondo grado in formula_1, formula_2, formula_3), che soddisfa la seguente equazione in coordinate cartesiane:
Questa è l'equazione di un cilindro ellittico. Un cilindro può essere anche considerato un prisma a base circolare, dove il numero dei rettangoli è quindi infinito.
Se formula_5 si ha la superficie di un "cilindro circolare". Il cilindro è una "quadrica degenere" in quanto una delle coordinate dello spazio non compare nella sua equazione (nel caso precedente la coordinata formula_3). Secondo certe terminologie i cilindri non sono considerati casi particolari di quadriche.
Nell'uso comune, con la parola "cilindro" si intende l'insieme limitato dei punti delimitati da un "cilindro circolare retto" e da due piani ortogonali al suo asse; alle sue due estremità piane esso presenta due superfici circolari, come nella figura a destra. Se questo cilindro ha raggio formula_7 e altezza formula_8, il suo volume è dato da
e la sua superficie laterale
mentre la sua superficie totale è data dalla somma della superficie laterale e del doppio della superficie di base.
Superficie di base:
Superficie totale:
Si può calcolare il volume del cilindro per mezzo del calcolo integrale come il volume del solido ottenuto dalla rotazione di una retta parallela all'asse delle ordinate (del tipo formula_13, con formula_14 costante) attorno all'asse delle ascisse. Si ha:
Essendo formula_14 proprio il raggio formula_7 del cilindro.
Per un dato volume, il cilindro con la minima area superficiale, ha formula_18. Per una data area superficiale, il cilindro con più esteso volume ha formula_18. Un cilindro di questo tipo è detto "cilindro equilatero".
Qualora si dovesse calcolare il volume di un cilindro circolare troncato si deve utilizzare la seguente formula:
dove formula_8 indica la lunghezza del lato corto, mentre formula_22 indica la lunghezza del lato lungo.
Nel caso si dovesse calcolare la superficie laterale di un cilindro circolare troncato la formula è:
Un cilindro ellittico è invariante per le rotazioni di formula_24 intorno al suo asse di simmetria, l'asse delle formula_3 nel caso della equazione di partenza; esso è anche invariante per tutte le traslazioni dirette come il suo asse. Un cilindro circolare è anche invariante per tutte le rotazioni intorno al suo asse.
Ci sono altri tipi di cilindro meno usuali. Quello caratterizzato dall'equazione che segue viene detto "cilindro ellittico immaginario":
il "cilindro iperbolico" ha equazione:
mentre il "cilindro parabolico" ha equazione:
Più in generale, data una curva ed una retta, un cilindro è la superficie rigata costituita dalle rette parallele alla retta data ed incidenti con la curva.
Un problema ricorrente è il calcolo del volume di liquido posto all'interno di un cilindro in orizzontale di lunghezza formula_29 e raggio formula_7, in funzione dell'altezza formula_31 raggiunta dal liquido. Il problema è di facile risoluzione: il volume è pari all'area sottesa tra la corda di altezza formula_8 e la circonferenza, moltiplicata per la lunghezza formula_29 del cilindro.
Sia formula_34 l'angolo al centro (misurato in radianti) che insiste sulla corda; stabiliamo sia formula_35 se formula_36 e formula_37 se invece formula_38; sarà:
La formula sarà allora data dall'area della sezione circolare individuata dall'angolo formula_34 meno il prodotto
(che rappresenta l'area del triangolo che ha come vertici il centro del cerchio e le intersezioni fra i lati dell'angolo e la circonferenza, o il suo opposto, a seconda del segno di formula_43), il tutto moltiplicato per la lunghezza formula_29 del cilindro, e quindi:
Dall'applicazione del calcolo integrale si ottiene invece la seguente formula, valida per qualsiasi altezza di liquido (formula_8) sia per un livello al di sotto della metà del serbatoio, sia per un livello al di sopra della metà stessa, applicabile misurando direttamente la sola altezza h del liquido e conoscendo la lunghezza del cilindro e il suo raggio. In tale formula, per formula_8 si intende l'altezza di liquido misurata con l'asticella graduata dal fondo del serbatoio.
Va tuttavia tenuto presente che generalmente i serbatoi non sono perfettamente cilindrici. Vengono pertanto predisposte apposite tabelle di ragguaglio per la determinazione del volume di liquido, contenuto in un serbatoio posto orizzontalmente, in base al livello del liquido stesso.
</text>
</doc>
<doc id="1281356" url="https://it.wikipedia.org/wiki?curid=1281356">
<title>Dodecagono</title>
<text>
Un dodecagono è un poligono con 12 lati e 12 angoli. In un dodecagono regolare tutti i lati hanno lunghezza uguale e tutti gli angoli sono di 150º. L'area del dodecagono regolare con lato lungo formula_1 è data da:
Un dodecagono regolare può essere costruito con riga e compasso. Qui sotto ne è mostrata un'animazione:
Ci sono 3 tassellazioni periodiche del piano che usano dodecagoni:
</text>
</doc>
<doc id="1440714" url="https://it.wikipedia.org/wiki?curid=1440714">
<title>Faccia (geometria)</title>
<text>
In geometria, una faccia di un poliedro è uno dei poligoni che compongono il suo bordo o più semplicemente i poligoni che delimitano il poliedro. Ad esempio, il cubo ha sei facce: queste sono i sei quadrati che compongono il suo bordo. 
Assieme ai vertici e agli spigoli, le facce sono una componente fondamentale di un poliedro: il suffisso "-edro" è infatti derivato dal greco "hedra" che vuol dire proprio "faccia".
Il numero di facce, spigoli e vertici di un poliedro convesso formano tre quantità formula_1 e formula_2 che sono in relazione tramite la formula di Eulero
Ad esempio, il cubo ha 8 vertici, 12 spigoli e 6 facce. Infatti 8-12+6 = 2.
Il concetto di faccia può essere esteso ad un politopo di dimensione arbitraria formula_4. Un qualsiasi politopo di dimensione inferiore formula_5 che compone il bordo è una faccia: si tratta di una faccia formula_6-dimensionale. In questa ottica, vertici e spigoli sono rispettivamente le facce 0-dimensionali e 1-dimensionali.
Se il politopo è convesso, le facce sono esattamente le intersezioni del politopo con gli iperpiani che intersecano il politopo solo nel suo bordo.
</text>
</doc>
<doc id="1430203" url="https://it.wikipedia.org/wiki?curid=1430203">
<title>Poligono stellato</title>
<text>
In geometria piana, un poligono stellato è un poligono avente una forma stellata a causa dell'intersezione di più lati.
Un poligono stellato è una linea spezzata chiusa che delimita un insieme stellato del piano. A differenza degli ordinari poligoni, la linea spezzata può autointersecarsi: coppie di spigoli distinti possono cioè intersecarsi in un punto interno.
Un poligono stellato è regolare se 
La seconda affermazione va intesa nel modo seguente: i vertici sono ordinati ciclicamente lungo la circonferenza lungo cui giacciono, ed il numero formula_3 è da interpretare nell'aritmetica modulare modulo formula_1: cioè, se formula_7, il numero formula_3 va interpretato in realtà come formula_9.
Per formula_10 si ottiene l'usuale poligono regolare con formula_1 lati.
Compreso il caso di formula_10, un "n"-poligono stellato può assumere formula_13 forme. Ad esempio, nel caso in cui formula_14, sono possibili il pentagono regolare o il pentagramma, mentre per formula_15 esistono quattro poligoni differenti (per formula_16). Tuttavia, nel caso in cui "n" e "k" non siano coprimi, ossia quando formula_17, questi poligoni sono in realtà composti da "sotto-stelle", equivalenti a dei formula_18-poligoni stellati il cui "k" è formula_19; il numero di questi sotto-poligoni è formula_20; questo si spiega ricordando che i vertici del poligono sono numerati in modo modulare, e quindi se "n" equivale a un multiplo (in senso modulare) di "k", il poligono si chiuderà in sotto-poligoni. 
Se, per esempio, consideriamo un 14-gono con "k=6", la frazione formula_21 si può ridurre: formula_22. Ciò significa che il poligono considerato è formato da 2 (cioè formula_23) 7-goni stellati con "k=3", centro comune e ruotati l'uno rispetto all'altro di formula_24. Poligoni stellati di questo tipo sono detti "composti".
Un poligono stellato regolare ha spigoli tutti di eguale lunghezza, e angoli ai vertici di eguale ampiezza. In particolare, se formula_25 è il lato del poligono regolare stellato e formula_26 la distanza tra due vertici adiacenti dello stesso, vale la relazione:
dove "n" è il numero di vertici del poligono e "k" la distanza modulare tra due vertici connessi dal lato del poligono stellato.
Sono mostrati qui sotto i poligoni stellati regolari per i primi valori di formula_28.
La "parte interna" di un poligono stellato può essere interpretata in vari modi diversi, come mostrato nella figura seguente.
Tali interpretazioni si riflettono anche in alcuni poliedri definiti aventi facce stellate. Ad esempio, il prisma archimedeo stellato è definito come un prisma, ma con facce regolari stellate alle due basi. 
</text>
</doc>
<doc id="2214272" url="https://it.wikipedia.org/wiki?curid=2214272">
<title>Teorema di Pitagora</title>
<text>
Il teorema di Pitagora è un teorema della geometria euclidea che stabilisce una relazione fondamentale tra i lati di un triangolo rettangolo.
Si può considerare un caso speciale, per i triangoli rettangoli, del teorema del coseno.
Quello che modernamente conosciamo come "teorema di Pitagora" viene solitamente attribuito al filosofo e matematico Pitagora. In realtà il suo enunciato (ma non la sua dimostrazione) era già noto ai Babilonesi. Viene a volte affermato che il teorema di Pitagora fosse noto agli antichi Egizi: Carl Boyer esclude questa ipotesi, basandosi sull'assenza del teorema dai papiri matematici rinvenuti. Era conosciuto anche in Cina e sicuramente in India, come dimostrano molte scritture fra cui lo "Yuktibhāṣā" e gli "Śulbasūtra". La dimostrazione del teorema è invece con ogni probabilità successiva a Pitagora.
oppure:
L'uso dell'aggettivo "uguale" invece di "equivalente" richiede di riferirsi o alle aree dei quadrati "costruiti" sui cateti e sull'ipotenusa (area intesa come misura dell'estensione di una superficie), oppure ai quadrati delle lunghezze dei cateti/quadrato della lunghezza dell'ipotenusa. La possibile ambivalenza della lingua italiana deriva dal fatto che, in assenza del termine "costruito", la parola "quadrato" può definire sia la "superficie" della figura geometrica in quanto tale, sia la generica operazione di elevamento alla seconda potenza.
In altre lingue, segnatamente in inglese, francese e spagnolo, nell'enunciato del teorema di Pitagora si preferisce parlare di quadrati (delle lunghezze) dei cateti e dell'ipotenusa, il che consente il semplice utilizzo dei termini "equal" (in inglese), "égal" (in francese), "igual" (in spagnolo). 
Per esempio, in spagnolo, "el cuadrado de la hipotenusa" significa, senza ambiguità, il "quadrato (della misura) dell'ipotenusa".
Dato un triangolo rettangolo di lati formula_1, formula_2 e formula_3, ed indicando con formula_3 la sua ipotenusa e con formula_1 e formula_2 i suoi cateti, il teorema è espresso dall'equazione:
o, esplicitando formula_3:
Le misure dei cateti sono perciò:
e
Una terna di numeri interi positivi formula_12 che soddisfi il teorema di Pitagora si dice terna pitagorica.
Viceversa, ogni triangolo in cui i tre lati verifichino questa proprietà è rettangolo: questo teorema, con la sua dimostrazione, appare nell'ultimo enunciato del primo libro di "Elementi".
La dimostrazione classica del teorema di Pitagora completa il primo libro degli "Elementi" di Euclide, e ne costituisce il filo conduttore. Dato che richiede il postulato delle parallele, esso non vale nelle geometrie non-euclidee e nella geometria neutrale. Nel testo di Euclide la dimostrazione del teorema è immediatamente preceduta dalla dimostrazione della costruibilità dei quadrati. L'esistenza stessa dei quadrati dipende infatti dal postulato delle parallele e viene meno nelle geometrie non euclidee. Questo aspetto del problema è in genere trascurato nella didattica contemporanea, che tende spesso ad assumere come ovvia l'esistenza dei quadrati.
La dimostrazione del teorema di Pitagora consiste nel riempire uno stesso quadrato di lato uguale alla somma dei cateti prima con quattro copie del triangolo rettangolo più il quadrato costruito sull'ipotenusa e poi con quattro copie del triangolo rettangolo più i quadrati costruiti sui cateti, come in figura.
Essendo il teorema uno dei più noti della storia della matematica, ne esistono moltissime dimostrazioni, in totale alcune centinaia, opera di matematici, astronomi, agenti di cambio, per esempio un presidente statunitense James A. Garfield e Leonardo da Vinci. Per questo teorema sono state classificate dallo scienziato statunitense Elisha Scott Loomis 371 differenti dimostrazioni, che sono state pubblicate nel 1927 nel suo libro The Pythagorean Proposition.
La dimostrazione attribuita al matematico e astronomo persiano Abu'l-Wafa verso la fine del X secolo d.C. e riscoperta dall'agente di cambio Henry Perigal (trovata nel 1835-1840, pubblicata nel 1872 e successivamente nel 1891) si basa sulla scomposizione del quadrato costruito sul cateto maggiore, in giallo nell'immagine: tagliandolo infatti con due rette passanti per il suo centro, una perpendicolare ed una parallela all'ipotenusa, si può ricomporre in maniera da incorporare l'altro quadrato, e formando il quadrato sull'ipotenusa, come nella figura. Questo procedimento è legato al problema della trisezione del quadrato.
Esiste anche una dimostrazione poetica, dell'astronomo Sir George Airy, in inglese:
di cui una traduzione letterale è
I versi si riferiscono alla parte bianca: i primi due triangoli sono quelli rossi, i secondi quelli blu.
Sia quella di Perigal sia quest'ultima sono puramente geometriche, ossia non richiedono alcuna definizione di operazioni aritmetiche, ma solo congruenze di aree e di segmenti.
Dimostrazione geometrica basata su due quadrati concentrici, di lati rispettivamente pari all'ipotenusa (formula_3) e alla somma dei due cateti (formula_14).
Come si vede dalla figura, tolti i quattro triangoli rettangoli (in giallo di area formula_15) al quadrato più grande, che corrisponde all'area formula_16, si ottiene il quadrato più piccolo, rappresentato in bianco, che equivale invece all'area formula_17.
Quindi
da cui risolvendo si ottiene
Questa dimostrazione, utilizzando il passaggio algebrico del quadrato della somma di due numeri, ha una rappresentazione visiva semplice e diretta, che non richiede lo spostamento e sovrapposizione di forme come le altre dimostrazioni geometriche formulate.
Un'altra dimostrazione geometrica, nella cui costruzione non compare alcun quadrato, fu trovata nel 1876 da Garfield, che in seguito divenne il ventesimo Presidente degli Stati Uniti d'America. Allora nell'esercito, Garfield commentò il suo risultato: ""Questo è qualcosa su cui i due rami del parlamento potranno essere d'accordo"".
La dimostrazione segue sostanzialmente il metodo usato nella dimostrazione di Pomi, applicato però a metà figura, considerando cioè il trapezio anziché il quadrato:
In formule, detto formula_1 il cateto rosso, formula_2 il blu e formula_3 l'ipotenusa, e ricordando la potenza del binomio
Un'altra dimostrazione utilizza il primo teorema di Euclide. Si traccia l'altezza sull'ipotenusa. Questa spezza l'ipotenusa in due segmenti, di lunghezza formula_24 e formula_25. Il teorema di Euclide fornisce le relazioni
da cui
e quindi 
Vale anche l'inverso del Teorema di Pitagora (proposizione 48 del primo libro degli "Elementi" di Euclide):
"Se in un triangolo di lati formula_1, formula_2 e formula_3 vale la relazione formula_32, allora il triangolo è rettangolo".
"Dimostrazione". Sia formula_33 un triangolo di lati formula_1, formula_2 e formula_3 tale che formula_32. Consideriamo un secondo triangolo rettangolo formula_38 che abbia i cateti pari ad formula_1 e formula_2 (è sempre possibile costruire un triangolo rettangolo dati i due cateti). Per il Teorema di Pitagora (diretto) l'ipotenusa del triangolo formula_38 sarà pari a formula_42, ossia sarà uguale al lato formula_3 del triangolo formula_33. I due triangoli formula_33 e formula_38 risulteranno dunque congruenti per il terzo criterio di congruenza, avendo tutti e tre i lati ordinatamente uguali. Ma allora anche il triangolo formula_33 sarà rettangolo "(CVD)".
Un corollario del teorema di Pitagora consente di determinare se un triangolo sia o meno rettangolo, acutangolo o ottusangolo. Là dove c è scelto come ipotenusa, il lato più lungo dei tre, e formula_48 (altrimenti non avremo un triangolo), valgono le seguenti relazioni:
L'enunciato inverso fornisce anche un sistema per costruire un angolo retto (o per controllare la quadratura di un angolo già esistente) in situazioni pratiche, come la topografia o l'agrimensura.
A titolo di esempio, con una fune di lunghezza pari alla somma di una terna pitagorica (diciamo 12, somma di 5, 4 e 3, in una qualche unità di misura) sarebbe sufficiente disporre le due porzioni minori della corda (quelle di misura 4 e 3) ad un certo angolo fra loro; se gli estremi della fune, disposta infine in forma triangolare, si chiudono, si saprà che l'angolo compreso fra le due porzioni minori della corda (a questo punto i due cateti) è certamente retto.
Il teorema di Pitagora può essere generalizzato in vari modi. Solitamente, una generalizzazione è una relazione che si applica a tutti i triangoli, e che applicata ai triangoli rettangoli risulta essere equivalente al teorema di Pitagora.
Una generalizzazione del teorema di Pitagora è il teorema del coseno, che si applica ad un triangolo qualsiasi (non necessariamente retto). In un triangolo con vertici e angoli indicati come in figura, vale l'uguaglianza:
Nel caso in cui formula_53 sia retto, vale formula_54 e quindi l'enunciato è equivalente al teorema di Pitagora. Il termine aggiuntivo può essere interpretato come il prodotto scalare dei vettori formula_55 e formula_56.
Il teorema dei seni mette in relazione le lunghezze dei lati di un triangolo e i seni degli angoli opposti. Anche questa relazione si applica a qualsiasi triangolo e, nel caso in cui questo sia rettangolo, può essere ritenuta equivalente al teorema di Pitagora (benché in modo meno immediato rispetto al teorema del coseno).
Il teorema dei seni asserisce che in un triangolo qualsiasi, con le notazioni come in figura, valgono le relazioni seguenti:
Elevando al quadrato:
Sommando i termini si ottiene:
Quando formula_60 è un angolo retto, si ottiene formula_61 e quindi 
Si ottiene quindi in questo caso il teorema di Pitagora
È possibile estendere il teorema di Pitagora ad un triangolo qualsiasi senza fare uso di funzioni trigonometriche quali il seno ed il coseno.
Dato un triangolo formula_64 come in figura, si tracciano due segmenti che collegano il vertice formula_65 con due punti formula_66 e formula_67 contenuti nel segmento opposto formula_68 (oppure in un suo prolungamento), in modo tale che gli angoli formula_69 e formula_70 siano entrambi uguali all'angolo formula_60 del vertice formula_65. La figura mostra un caso in cui l'angolo formula_60 è ottuso: se è acuto, i due punti formula_74 e formula_67 sono in ordine inverso (il primo a destra e il secondo a sinistra) e possono uscire dal segmento formula_68.
Vale la relazione seguente:
Quando formula_60 è un angolo retto, i punti formula_74 e formula_67 coincidono e si ottiene il teorema di Pitagora
La relazione generale può essere dimostrata sfruttando la similitudine fra i triangoli formula_82, formula_83 e formula_84, che porta alle relazioni
Si ottiene quindi
Sommando le due eguaglianze si ottiene la relazione iniziale.
Il Teorema di Pitagora continua a valere quando su ogni lato di un triangolo rettangolo si costruiscono figure simili tra loro anche non regolari.
Il suo enunciato diventa:
In ogni triangolo rettangolo, l'area di un qualunque poligono, anche curvilineo, costruito sull'ipotenusa è uguale alla somma delle aree dei poligoni, simili a quello costruito sull'ipotenusa, costruiti sui cateti.
"Dimostrazione"
Siano:
e
Per il teorema di Pitagora in forma classica risulta:
e quindi
Ricordando che se due poligoni simili hanno 2 lati corrispondenti in rapporto formula_101 allora le loro superfici sono in rapporto formula_102, avendo definito formula_103 , risulta 
Avendo inoltre definito uguale a
formula_105
il rapporto tra i due cateti, per il motivo precedente risulta uguale a
formula_106 il rapporto tra le aree dei poligoni simili costruiti su di essi, ovvero:
Quindi:
E quindi
Come volevasi dimostrare.
Il teorema di Pitagora può essere "generalizzato" a spazi vettoriali di qualsiasi dimensione, come lo spazio euclideo di dimensione 3 o superiore, o uno spazio vettoriale su corpo complesso, continuando a valere anche su funzioni viste come somme infinite di vettori come nell'analisi funzionale, finché sia possibile definire un prodotto scalare, rendendo lo spazio vettoriale uno spazio prehilbertiano.
La dimostrazione è semplice, e l'enunciato del teorema è: "La somma dei quadrati delle norme di due vettori ortogonali è uguale al quadrato della norma del loro vettore somma", ovvero formula_110 .
Per definizione di ortogonalità, il prodotto scalare fra formula_111 e formula_112 è nullo e commuta. L'enunciato del teorema è equivalente, per la definizione di norma euclidea, a formula_113. Svolgendo il prodotto scalare al secondo membro sfruttando la sua bilinearità e ricordando che il prodotto scalare fra formula_112 e formula_111 è nullo, il teorema risulta provato.
Una leggenda racconta che Pitagora abbia formulato il suo teorema mentre stava aspettando un'udienza da Policrate. Seduto in un grande salone del palazzo di Samo, Pitagora si mise ad osservare le piastrelle quadrate del pavimento, si pensa che ne abbia vista una rotta perfettamente su una diagonale, così da formare due triangoli rettangoli uguali, ma oltre ad essere 2 triangoli rettangoli erano anche isosceli, avendo i due lati uguali. Pitagora immaginò un quadrato costruito sulla diagonale di rottura della piastrella, un quadrato avente come lati le diagonali delle piastrelle circostanti.
La dimostrazione è la seguente: 
Il teorema di Pitagora, in realtà, non era considerato solo una legge matematica del pensiero, ma anche una legge ontologica, cioè dell'essere, riguardante la realtà dell'uomo e del mondo. Il significato filosofico ed esoterico che esso pertanto assumeva costituiva parte integrante delle conoscenze iniziatiche riservate agli adepti della scuola pitagorica. 
Una delle possibili interpretazioni del teorema che si è tramandata fino ad oggi all'interno della massoneria, dove l'emblema dei quadrati costruiti sui lati di un triangolo rettangolo compare nel gioiello indossato dagli anziani Maestri ex-Venerabili (detti in inglese "Past Masters"), mette in relazione la volontà del Cielo, ossia la Provvidenza, rappresentata dal cateto verticale, con la volontà della singola individualità umana, simboleggiata dal cateto orizzontale. Quanto più l'individuo, anziché estendere arbitrariamente la propria volontà, cerca di adeguarla a quella del Cielo, tanto meno subirà il peso del Destino, cioè dell'ipotenusa, e quindi tanto più riuscirà a dominare le forze della necessità che gravano sulla sua vita.
</text>
</doc>
<doc id="2258032" url="https://it.wikipedia.org/wiki?curid=2258032">
<title>Sfera</title>
<text>
La sfera (dal greco σφαῖρα, "sphaîra") è il solido geometrico costituito da tutti i punti che sono a distanza minore o uguale a una distanza fissata formula_1, detta "raggio della sfera", da un punto formula_2 detto "centro della sfera".
L'insieme dei punti la cui distanza è eguale a formula_1 è detto "superficie sferica" di centro formula_2 e raggio formula_1.
È detta "semisfera" ciascuna delle metà di un solido sferico diviso in due da un piano passante per il centro o anche ciascuna delle due superfici di una sfera divisa da una sua circonferenza massima.
In geometria cartesiana, una superficie sferica con centro formula_6 e di raggio formula_1 è rappresentata dall'insieme di punti formula_8 tali che
I punti della superficie sferica possono essere parametrizzati in coordinate sferiche nel modo seguente
dove formula_11 e formula_12 rappresentano la latitudine e la longitudine del punto, variando negli intervalli
Ogni punto della superficie sferica è descritto da una sola coppia formula_14 di questo tipo, tranne i poli: la coppia formula_15 descrive sempre il polo nord, e formula_16 sempre il polo sud (per qualsiasi valore di formula_12).
Alternativamente si può utilizzare l'equazione cartesiana della superficie sferica:
con formula_19, formula_20, formula_21, formula_22, numeri reali tali che formula_23. Dall'equazione cartesiana si possono ricavare le coordinate del centro:
L'area della superficie di una sfera di raggio formula_25 è data dall'equazione:
La sfera può essere pensata come un solido di rotazione ottenuto ruotando attorno all'asse formula_27 il grafico della funzione
che rappresenta una semicirconferenza di raggio formula_25. Pertanto, per il primo teorema di Guldino, la superficie laterale è data da:
La superficie totale della sfera si può ottenere, per il primo teorema di Guldino, tramite il seguente integrale:
Il volume della sfera di raggio formula_25 è dato dall'equazione (integrale in formula_33 della superficie):
La dimostrazione di questa formula può essere ottenuta in modo immediato usando il metodo degli indivisibili oppure con gli strumenti nell'analisi matematica.
Si pensi di sommare tutte le aree dei cerchi che si ottengono sezionando la sfera con dei piani orizzontali. Il raggio di questi cerchi varierà con una funzione formula_35 della distanza del piano orizzontale dal centro della sfera e dato che l'area di un cerchio equivale a formula_36 per il raggio al quadrato:
dove formula_38 appunto è la distanza del piano dal centro della sfera.
Raggio alla distanza formula_27
Dunque, dal teorema di Pitagora, formula_35 vale:
che, sostituita nell'equazione del volume, si trova:
Allo stesso modo si può calcolare il volume formula_44 di un segmento di sfera di altezza formula_45
La sfera può anche essere intesa come l'insieme di numerose piramidi infinitesime, tutte con il vertice nel centro della sfera e con i poligoni di base delle piramidi che poggiano sulla superficie della sfera: queste infinite piramidi elementari riempiranno tutto e solo il volume della sfera. Il volume di ogni piramide è:
dal quale si desume il significato della formula per il volume della sfera.
La sfera è la figura tridimensionale con il minimo rapporto superficie/volume: ciò spiega perché a tale forma tendono molti oggetti fisici, dalle gocce di liquido ai corpi celesti. Ad esempio, le bolle sono sferiche perché la tensione superficiale tende a minimizzare l'area a parità di volume.
Il cilindro circoscritto ha un volume che è formula_50 quello della sfera, ed una superficie laterale che è la stessa di quella della sfera. Questo fatto, e le formule scritte sopra, erano già noti ad Archimede.
Con l'aumentare del raggio, il volume della sfera cresce più della superficie. Infatti il rapporto fra queste due quantità è formula_51.
Una sfera può anche essere definita come formata da un semicerchio che ruota intorno al suo diametro. Se si usa una ellisse, si ottiene un ellissoide di rotazione.
Due punti della superficie sferica che stanno sulla stessa retta passante per l'origine sono detti antipodali, e una tale retta è detta "asse", poiché è un asse di simmetria della sfera.
Un cerchio massimo è una circonferenza avente lo stesso centro della sfera, ottenuta quindi intersecando la superficie sferica con un piano passante per l'origine.
Se un punto della superficie sferica è identificato come polo nord, il suo antipodale è il polo sud e l'equatore è il cerchio massimo equidistante dai due poli. I cerchi massimi passanti per i poli sono i meridiani, mentre la linea retta passante per l'origine ed i due poli è l"'asse". Questa terminologia è usata anche per i corpi celesti come la terra, anche se non perfettamente sferici.
La sfera può essere generalizzata in altre dimensioni. Per ogni numero naturale formula_52, una sfera formula_52-dimensionale è l'insieme dei punti nello spazio euclideo formula_54-dimensionale formula_55 che hanno una distanza fissata formula_56 da un certo punto dello spazio.
Ad esempio:
Le sfere di dimensione &gt; 2 sono chiamate anche ipersfere. La sfera formula_52-dimensionale di raggio unitario, centrata nell'origine, viene indicata con formula_61.
Più in generale, in uno spazio metrico formula_62, la sfera di centro formula_27 e raggio formula_56 è l'insieme 
Una sfera in uno spazio metrico può essere un oggetto molto diverso dalla sfera usuale. Ad esempio, può essere vuota: se consideriamo formula_66 con la metrica euclidea, una sfera di raggio formula_1 è vuota se e solo se formula_68 non può essere scritto come somma di formula_52 quadrati.
Dove con formula_1 si intende il raggio della sfera, con formula_45 l'altezza del segmento di sfera o della calotta sferica, con formula_72 l'ampiezza in steradianti della calotta.
Per quanto si sia avvicinato, l'uomo non è ancora riuscito a produrre alcun oggetto dalla sfericità matematicamente perfetta. Finora il miglior risultato è stato raggiunto dall"'Australian Centre for Precision Optics", di Lindfield (Australia). La sfera è stata ottenuta attraverso una levigazione ad altissima precisione di una barra di silicio 28 (un isotopo del silicio) ed è frutto del Progetto Avogadro, che si propone di arrivare alla definizione del chilogrammo perfetto, basata sulla conoscenza dell'esatto numero di atomi che compongono tale sfera. Il suo diametro è di 9,36 centimetri e come uniche imperfezioni presenta una rugosità di 0,3 nanometri e piccole deviazioni di sfericità di circa 60-70 nanometri. In precedenza, il miglior risultato era stato ottenuto dalla NASA, che per la sonda Gravity Probe B, costruita per degli studi gravitazionali in orbita, ha creato dei giroscopi con deviazioni inferiori ai 100 nanometri.
Parmenide paragona l'Essere a una sfera perfetta, sempre uguale a se stessa nello spazio e nel tempo, chiusa e finita (per gli antichi greci il finito era sinonimo di perfezione). La sfera è infatti l'unico solido geometrico che non ha differenze al suo interno, ed è uguale dovunque la si guardi; l'ipotesi collima suggestivamente con la teoria della relatività di Albert Einstein che nel 1900 dirà: «Se prendessimo un binocolo e lo puntassimo nello spazio, vedremmo una linea curva chiusa all'infinito» in tutte le direzioni dello spazio, ovvero, complessivamente, una sfera (per lo scienziato infatti l'universo è finito sebbene illimitato, fatto di uno spazio tondo ripiegato su se stesso).
</text>
</doc>
<doc id="869486" url="https://it.wikipedia.org/wiki?curid=869486">
<title>Ennagono</title>
<text>
In geometria, un ennagono (o nonagono) è un poligono con nove lati. Il nome "ennagono" deriva dal greco "enneagonon", (εννεα, "nove" + γωνον, "angolo").
Per ennagono regolare si intende un ennagono semplice, convesso, avente i lati di uguale lunghezza e con gli angoli interni di uguale ampiezza, pari a 140°. L'area di un ennagono regolare di lato "a" è data da
In molti contesti il termine ennagono viene usato per indicare un ennagono regolare.
Tra gli ennagoni non semplici, con lati che si intersecano in punti non estremità, si trovano due ennagoni stellati regolari.
Il grafo completo K è spesso disegnato come un "ennagono regolare" con tutti e 36 gli spigoli connessi. Questo grafo rappresenta anche una proiezione ortogonale dei 9 vertici e 36 spigoli di un simplesso 8-dimensionale.
Un ennagono regolare "non" può essere costruito in modo esatto con riga e compasso. Qui sotto ne è mostrata una costruzione che fornisce un'ottima approssimazione (circa un millesimo di grado sull'angolo al centro):
Per trovare il perimetro di un ennagono regolare basta moltiplicare un suo lato per 9
P= a • 9
</text>
</doc>
<doc id="909410" url="https://it.wikipedia.org/wiki?curid=909410">
<title>Punto medio</title>
<text>
In geometria, il punto medio è il punto equidistante da due altri punti presi a riferimento e allineato con essi; solitamente lo si associa a un segmento, i cui punti di riferimento sono gli "estremi", che divide in due parti congruenti (o isometriche). 
Relativamente alla sua geometria euclidea, la sua "unicità" è assunta per assioma - o come conseguenza dell'assioma della divisibilità dei segmenti. 
Il concetto di punto medio è spesso frequente nella geometria elementare.
Nei poligoni regolari il punto medio di un lato è il punto in cui l'apotema tocca il lato; sempre nei poligoni e specialmente nei triangoli, la mediana è il segmento che congiunge un vertice al punto medio del lato opposto. In un cerchio, il punto medio del diametro è il centro del cerchio.
Nella geometria euclidea
Sulla retta formula_1, il punto medio del segmento [AB] di estremi A("x") 
e B("x") corrisponde all'ascissa 
Nel piano cartesiano formula_3, il punto medio del segmento [AB] di estremi formula_4 e formula_5 è il punto M di coordinate: 
Il procedimento si estende immediatamente agli spazi di dimensione formula_8, come 
formula_9.
</text>
</doc>
<doc id="909863" url="https://it.wikipedia.org/wiki?curid=909863">
<title>Tassellatura</title>
<text>
In geometria piana, si dicono tassellature (talvolta tassellazioni o pavimentazioni) i modi di ricoprire il piano con una o più figure geometriche ripetute all'infinito senza sovrapposizioni.
Tali figure geometriche, (dette appunto "tasselli"), sono spesso poligoni, regolari o no, ma possono anche avere lati curvilinei, o non avere alcun vertice. L'unica condizione che solitamente si pone è che siano connessi, anzi semplicemente connessi (ovvero che siano un pezzo unico e non abbiano buchi).
In matematica sono state molto studiate anche le tassellazioni dello spazio, dove i tasselli sono solidi.
Si dicono regolari (o "periodiche") quelle tassellature che rispettano la seguente regola: "esistono due traslazioni indipendenti che mandano la tassellatura in sé stessa" (con ""indipendenti"" si intende che le due traslazioni non devono avere la stessa direzione).
Tale condizione viene solitamente detta regola del parallelogramma perché se chiamiamo formula_1 e formula_2 i vettori associati alle due più piccole traslazioni che mandano la tassellatura in sé ci accorgiamo che il parallelogramma avente come lati formula_1 e formula_2 (e che viene detto "parallelogramma di base") "genera" la tassellatura mediante le due traslazioni (in altre parole, possiamo ridisegnare tutta la tassellatura replicando il parallelogramma di base e senza mai doverlo ruotare o "rovesciare").
Sebbene tale condizione possa sembrare molto restrittiva, è rispettata da quasi tutte le pavimentazioni a cui si possa pensare. Il motivo per cui risulta utile è che permette di confrontare tra di loro tassellature all'apparenza totalmente diverse.
La sagoma del parallelogramma di base non è però il modo più completo per classificare le tassellature regolari; conoscere le misure dei suoi angoli e dei suoi lati infatti non ci permette di stabilire con certezza le caratteristiche geometriche della nostra tassellatura: potrebbe accadere che ci sia una porzione di piano "più piccola" del parallelogramma (più precisamente, una proporzione del parallelogramma) con la quale sia possibile ricostruire tutta la tassellatura (non più con sole traslazioni, ma utilizzando anche altre isometrie): il disegno minimo. Diremo quindi che due tassellature appartengono alla stessa classe se:
Ad esempio, nell'immagine a fianco vediamo una tassellatura con accanto il suo parallelogramma di base (un quadrato) e il suo disegno minimo (un triangolo rettangolo). La tassellatura si può ottenere traslando il quadrato, ma anche traslando e riflettendo il solo triangolo rettangolo. Invece non esiste nessuna porzione di piano più piccola del triangolo con cui si possa ricreare tutta la tassellatura.
Si dimostra che le classi di tassellature regolari sono esattamente 17. Per catalogare una qualsiasi tassellazione è sufficiente conoscere le trasformazioni necessarie per generarla a partire dal disegno minimo, come schematizzato nella seguente tabella:
Esiste una grandissima varietà di tassellature regolari aventi come tasselli dei poligoni regolari. Si dimostra però che ne esistono "solo 11" che rispettano le seguenti due condizioni:
Quando diciamo che ne esistono esattamente 11, non ci riferiamo più alle classi, ma proprio alla forma degli spigoli: stiamo dicendo che date 12 tali tassellature ce ne saranno sempre almeno 2 tali che, scalandone e colorandone opportunamente una, essa diventi identica all'altra.
In particolare, è piuttosto facile osservare che se imponiamo l'utilizzo di "un solo poligono regolare" per tutta la tassellatura, abbiamo 3 configurazioni possibili; infatti la misura degli angoli del tassello dovrà essere un divisore intero di 360, e quindi andranno bene solo il triangolo equilatero (formula_5), il quadrato (formula_6) e l'esagono regolare (formula_7):
Con due o più poligoni regolari abbiamo invece le seguenti configurazioni (sotto ogni immagine sta la "descrizione dei vertici", che - ricordiamo - sono tutti uguali: ogni numero indica il tipo di poligono adiacente, girando in senso orario):
Come già detto, molte delle tassellature a cui viene da pensare sono regolari. Altre tassellature, pur non essendo regolari, vengono mandate in sé stesse da particolari traslazioni (è il caso ad esempio di tassellature composte da bande di lunghezza infinita una accanto all'altra che siano ricoperte ognuna da una stessa tassellatura regolare ma disposte sfalsate tra di loro).
È possibile però realizzare, ed è un risultato a cui i matematici sono arrivati in tempi relativamente recenti, anche tassellature "aperiodiche", ovvero tali che "nessuna" traslazione le mandi in sé. È il caso ad esempio dei domino di Wang, delle piastrelle di Robinson e della famosa tassellatura di Penrose.
Abbiamo visto che l'unico requisito richiesto a una forma geometrica per essere un "buon" tassello è essere connessa, anzi semplicemente connessa. Il motivo è semplice: supporre che un tassello non abbia tale caratteristica non aumenta sostanzialmente le possibili configurazioni, quindi non è geometricamente interessante.
Infatti se un tassello non sarà connesso sarà diviso in due parti, che potranno essere considerate come due tasselli separati.
Se un tassello invece è un pezzo unico ma presenta un buco dovrà essere riempito con uno o più tasselli, ma questo riempimento diventa un problema completamente indipendente dalla tassellatura intorno.
Le tassellature nell'arte figurativa, astratta e nell'architettura sono da sempre un modo di unire estetica, eleganza e semplicità, e sono state utilizzate in miriadi di contesti; riportiamo alcuni esempi significativi:
Non è un caso che le tassellature vengano chiamate anche "pavimentazioni": in effetti ogni possibile modo di coprire un pavimento con delle mattonelle di forma data non è altro che una tassellatura. È per questo che le tassellature sono necessariamente presenti in grandissima parte degli edifici realizzati nel corso della storia. In particolare tassellature colorate sono state spesso viste come un espediente per vivacizzare un pavimento, o una parete.
Famosissime sono le tassellature che ricoprono molte pareti del complesso de l'Alhambra, a Granada, frutto dell'arte e dei gusti arabi della dinastia nasride: gli arabi sono sempre stati grandi studiosi di matematica e geometria, e tali conoscenze pervadono anche la loro arte, tanto che è tuttora comunemente usato, per indicare motivi decorativi geometrici, il termine "arabesco".
Moltissime delle opere dell'artista olandese Maurits Cornelis Escher sono tassellature, i cui tasselli rappresentano solitamente pesci, uccelli, cavalli, pipistrelli, ma anche figure antropomorfe. Escher non solo dedicò moltissima attenzione alla realizzazione di tasselli che assomigliassero effettivamente agli animali che desiderava rappresentare, ma anche allo studio matematico e alla catalogazione delle tassellature, confrontandosi anche con matematici del suo tempo.
Dal punto di vista matematico le sue opere più ardite sono probabilmente quelle in cui raffigura tassellature disposte non su un ordinario piano euclideo ma trasferendo sul piano geometrie non euclidee. Sebbene queste non siano formalmente delle tassellature (dato che i tasselli non vengono solo ripetuti ma anche scalati), il ragionamento geometrico di base è lo stesso, adattato al modello di geometria non euclidea scelto. Ad esempio, nella famosa serie Limite del cerchio si può riconoscere i postulati del piano iperbolico studiato da Henri Poincaré.
Notevole è anche la serie Metamorphosis, in cui Escher concatena in una lunga striscia diverse tassellature alternate ad altri motivi geometrici o disegnati a mano libera, dando così anche l'idea che le semplici regole geometriche alla base delle tassellature siano presenti ovunque e alla base della natura stessa.
Molti materiali, sia naturali che artificiali, sono caratterizzati da una struttura microscopica che si ripete sempre più o meno uguale (fino alla regolarità estrema dei cristalli).
Ci sono svariati casi in cui è però possibile trovare tassellature di una regolarità talvolta sorprendente anche di dimensioni macroscopiche e quindi visibili a occhio nudo:
Nella computer grafica, in particolar modo nel rendering di ambienti 3D, questa tecnica permette di suddividere ulteriormente i poligoni, i quali grazie ad una displacement mapping o mappatura spostamenti potranno creare una forma tridimensionale più dettagliata.
Con la tassellazione dinamica si ha una diversa incisione di questo effetto, il quale sarà più marcato per oggetti vicini, mentre sarà ridotto per oggetti distanti, evitando così un inutile spreco di risorse.
</text>
</doc>
<doc id="1016060" url="https://it.wikipedia.org/wiki?curid=1016060">
<title>Spazio (matematica)</title>
<text>
In matematica il termine spazio è ampiamente utilizzato e si collega ad un concetto estremamente importante e generale. Il termine spazio compare nei nomi di svariate strutture algebriche e/o topologiche (in genere continue e di interesse per la geometria, ma anche discrete) le quali hanno in comune il fatto di costituire l'ambiente entro il quale si costruiscono o si definiscono strutture più specifiche (figure, forme, politopi, superfici, ecc.). 
In matematica dunque si incontrano strutture e specie di "strutture spaziali" che presentano un insieme sostegno ai cui elementi si danno spesso nomi come punti o vettori. Su tali elementi si individuano sottoinsiemi, funzioni, operazioni e relazioni che devono soddisfare determinate richieste. 
Il primo spazio, dal punto di vista storico e dell'uso, è lo spazio euclideo tridimensionale, la struttura che fornisce il modello per l'ambiente nel quale si colloca la nostra vita quotidiana e che è servito allo sviluppo della meccanica classica (newtoniana). Nella teoria della relatività ristretta di Einstein esso viene sostituito dalla struttura ottenuta con l'aggiunta del "tempo", lo spazio a 4 dimensioni detto spazio di Minkowski. Una specie di strutture tra le più generali è quella di spazio topologico; in particolari spazi topologici si studiano le geometrie non euclidee e la relatività generale. Altri spazi sono utilizzati dall'analisi funzionale e dalla teoria della probabilità.
Molti degli spazi che sono stati via via introdotti sono stati indicati con il nome del loro scopritore; tale consuetudine, però, ha ostacolato l'uso di nomi più adatti a manifestare le caratteristiche delle strutture.
</text>
</doc>
<doc id="874812" url="https://it.wikipedia.org/wiki?curid=874812">
<title>Ettagono</title>
<text>
Un ettagono, o eptagono, è un poligono che ha sette lati e sette angoli. Si dice ettagono regolare un ettagono convesso con tutti i lati della stessa lunghezza e con gli angoli interni della stessa ampiezza (la somma degli angoli interni è sempre 900°), pari a formula_1 radianti, circa 128,571 gradi. L'area di un ettagono regolare di lato formula_2 è data da
Un ettagono regolare non è costruibile con riga e compasso ma è costruibile con un righello graduato e compasso. Questo tipo di costruzione è chiamata costruzione di Neusi. È inoltre costruibile con il compasso, la riga ed il trisettore di angolo. L'impossibilità della costruzione tramite riga e compasso segue dall'osservazione che formula_4 è uno zero del polinomio irriducibile cubico formula_5. Di conseguenza questo polinomio è il polinomio minimo di formula_6, mentre il grado del polinomio minimo per un numero costruibile deve essere una potenza di formula_7.
Una costruzione esatta, per quanto non ottenuta con l'uso classico di riga e compasso, può essere ottenuta grazie a una costruzione di Neusi (vedi figura accanto). formula_8 è un quadrato di lato unitario; la retta verticale identificata dal punto formula_9 è l'asse del segmento formula_10 mentre l'arco formula_11 è tracciato con centro in formula_12. La costruzione di Neusi comporta la ricerca di un segmento di lunghezza unitaria passante per formula_13 e i cui estremi cadano sull'asse di formula_10 e sull'arco formula_11: l'angolo formula_16 è l'angolo interno dell'ettagono.
Con riga e compasso classici un ettagono regolare non può essere costruito in modo esatto.
</text>
</doc>
<doc id="585772" url="https://it.wikipedia.org/wiki?curid=585772">
<title>Asse di un segmento</title>
<text>
In geometria euclidea l'asse di un segmento o asse di un lato è la retta perpendicolare al segmento che passa per il suo punto medio.
L'asse di un segmento si può caratterizzare come il luogo dei punti che hanno uguale distanza dai due estremi del segmento.
In un triangolo, tutti gli assi dei lati sono concorrenti nel circocentro "O", e oltre a questi sono sempre su di essi anche i punti medi "M" e i centri "J" dei cerchi di Johnson
Il circocentro è interno al triangolo acutangolo, sul punto medio dell'ipotenusa del triangolo rettangolo e esterno all'ottusangolo.
</text>
</doc>
<doc id="610907" url="https://it.wikipedia.org/wiki?curid=610907">
<title>Rombo (geometria)</title>
<text>
Il rombo o losanga è un poligono di quattro lati che ha tutti i lati della stessa lunghezza (congruenti); è un caso particolare di parallelogramma.
Il quadrato è un particolare tipo di rombo che, oltre ad avere tutti i lati congruenti, ha anche tutte le diagonali congruenti e gli angoli anch'essi congruenti.
I lati opposti di un rombo sono paralleli; esso quindi appartiene alla famiglia dei parallelogrammi. Inoltre è un poligono equilatero perché ha tutti i lati uguali.
Essendo un quadrilatero, anche il rombo ha due diagonali; esse hanno la caratteristica di essere perpendicolari fra loro e di intersecarsi nel loro punto medio. Ciascuna diagonale divide il rombo in due triangoli isosceli, che sono congruenti. Le due diagonali costituiscono anche le bisettrici degli angoli.
Gli angoli opposti sono congruenti, vale a dire hanno uguale ampiezza: quindi
Due angoli adiacenti a ciascun lato sono supplementari, con somma quindi pari a 180°:
Un caso particolare di rombo, avente tutti gli angoli uguali e pari a 90°, è il quadrato.
La somma degli angoli interni è sempre 360°
Le altezze di un rombo sono congruenti.
L'altezza formula_4 del rombo è pari al diametro della circonferenza inscritta al rombo o al rapporto tra l'area e lato base:
Se formula_6 è il lato del rombo, il suo perimetro formula_7 è dato da:
L'area del rombo si può calcolare in quattro modi:
</text>
</doc>
<doc id="596510" url="https://it.wikipedia.org/wiki?curid=596510">
<title>Vertice (geometria)</title>
<text>
Il vertice, nella geometria piana è:
Il vertice, nella geometria solida è:
</text>
</doc>
<doc id="513690" url="https://it.wikipedia.org/wiki?curid=513690">
<title>Teorema della bisettrice</title>
<text>
Il teorema della bisettrice dell'angolo interno di un triangolo è un teorema della geometria elementare che è una particolare conseguenza del teorema di Talete. 
"In un triangolo due lati stanno fra loro come le parti in cui resta diviso il terzo lato dalla bisettrice dell'angolo interno ad esso opposto."
In altri termini: dato il triangolo ABC sia AL la bisettrice dell'angolo interno in A; sussiste allora la proporzione
formula_1
Si conduca dal vertice C la parallela alla retta AL fino a incontrare il prolungamento del lato BA dalla parte di A nel punto D. Il triangolo ACD è isoscele perché i suoi angoli in C e in D sono congruenti. Infatti:
formula_2 perché alterni interni rispetto alle rette parallele AL e DC tagliate dalla trasversale AC;
formula_3 perché corrispondenti rispetto alle rette parallele AL e DC tagliate dalla trasversale AD;
formula_4 perché parti uguali dello stesso angolo.
Per la proprietà transitiva dell'uguaglianza è allora
formula_5. Si ha pertanto che i segmenti AC e AD sono congruenti.
Per il teorema di Talete sussiste la proporzione
formula_6
e poiché AC e AD sono congruenti anche
formula_7
</text>
</doc>
<doc id="554580" url="https://it.wikipedia.org/wiki?curid=554580">
<title>Simmetria</title>
<text>
Il termine simmetria indica generalmente la presenza di alcune ripetizioni nella forma geometrica di un oggetto. L'oggetto può essere ad esempio una figura bidimensionale (un dipinto, un poligòno, una tassellazione, ...) oppure una figura tridimensionale (una statua, un poliedro, ...). Molte simmetrie sono osservabili in natura.
Il concetto di simmetria è ampiamente studiato in geometria ed è usato in matematica e fisica con un'accezione più generale.
Un oggetto ha una "simmetria" quando la sua forma presenta delle ripetizioni regolari. Ci sono vari tipi di simmetria.
La simmetria è una tecnica compositiva molto utilizzata nel campo artistico. Può essere di diversi tipi:
- Simmetria bilaterale (criterio principalmente medioevale che fa uso dei seguenti metodi: al centro la figura principale e ai lati due forme o figure che si equilibrano, con caratteristiche simili e di conseguenza ugual peso visivo, che forniscono staticità e equilibrio all'opera)
- Simmetria speculare (più statica della precedente, implica l'uguaglianza tra le figure laterali)
- Simmetria traslatoria (definita anche come variante del ritmo, consiste nella ripetizione di figure rispetto a un asse)
- Simmetria rotatoria, centrale o radiale (le forme hanno caratteristiche simili e si sviluppano attorno a un punto, il centro. È tipica dei formati circolari e della pittura in generale.
La simmetria permette di dare equilibrio e armonia all'opera in cui viene utilizzata.
</text>
</doc>
<doc id="608290" url="https://it.wikipedia.org/wiki?curid=608290">
<title>Pendenza topografica</title>
<text>
Il termine pendenza è usato per indicare il grado di ripidità o di inclinazione di una strada o di un tratto di percorso. La pendenza di una strada è indicata dalla segnaletica verticale con cartelli di pericolo che indicano la pendenza con una percentuale. Il termine pendenza è un termine usato anche in geometria analitica.
La pendenza m in un particolare punto della strada è definita matematicamente come la tangente dell'angolo θ di inclinazione:
La pendenza in percentuale ("m%") si ottiene moltiplicando m per 100:
Se invece si vuole risalire al valore dell'angolo θ a partire dal valore della pendenza "m" o dalla pendenza percentuale "m%", basta applicare le seguenti formule di conversione:
Un valore maggiore della pendenza corrisponde a una maggiore ripidità del tratto di strada. Un tratto orizzontale ha una pendenza che vale 0% (tangente = 0), un tratto di strada in salita che forma un angolo di 45° con l'orizzontale ha una pendenza di 100% (tangente = 1). A un tratto verticale non corrisponde una vera pendenza, visto che il suo valore sarebbe infinito (la funzione tangente ha un asintoto verticale in corrispondenza all'angolo retto).
La stessa definizione di pendenza come tangente trigonometrica di un angolo dovrebbe far comprendere che la pendenza, in quanto funzione trigonometrica, non è una funzione di tipo lineare. In altre parole una strada che ha una pendenza del 10% non è 10 volte meno pendente di una strada con pendenza del 100%: l'angolo di inclinazione di un tratto al 10% è di 5.7°, quello di un tratto al 100% è di 45°.
La pendenza media formula_5 di una strada è definita dal rapporto tra il dislivello Δy tra il punto di partenza e quello di arrivo e la distanza orizzontale Δx:
La distanza orizzontale Δx è la proiezione sull'orizzontale del percorso, non la distanza effettivamente percorsa Δs. Il suddetto rapporto è dunque matematicamente coincidente alla tangente trigonometrica media dell'angolo di incidenza sull'intero percorso, considerando cioè esclusivamente punto di partenza e punto di arrivo.
La pendenza media espressa in percentuale formula_7 si ottiene poi moltiplicando la pendenza media formula_5 per 100:
Un valore maggiore della pendenza corrisponde a una maggiore ripidità del tratto di strada. Una strada che abbia il punto di partenza e di arrivo alla stessa altitudine ha una pendenza dello 0%; aver percorso una strada con una pendenza media del 100% significa essersi spostati in orizzontale della stessa distanza che si è percorso in verticale.
Per pendenze non molto elevate si può anche usare, invece di Δy/Δx, il rapporto Δy/Δs (matematicamente il seno dell'angolo in questione), metodo molto più intuitivo e di facile misurazione rispetto al tradizionale approccio matematico, in quanto più facile da misurare sul terreno: infatti Δs si può ottenere direttamente con un contachilometri; su una mappa invece si misura più facilmente Δx. Per piccoli angoli seno e tangente sono approssimabili tra loro con un errore trascurabile: nella seguente tabella sono mostrati i valori della pendenza calcolati nei due modi e l'errore commesso nell'approssimare Δy/Δx a Δy/Δs:
Poiché le strade molto raramente presentano pendenze superiori al 20%, l'errore commesso nell'approssimare Δy/Δx a Δy/Δs è minore del 2% (e minore dello 0,5% per pendenze inferiori al 10%), quindi agli effetti pratici si può approssimare Δy/Δx a Δy/Δs.
La pendenza massima ammissibile per una strada, così come gli altri elementi geometrici (raggi delle curve, distanze di visibilità, ecc.), è commisurata alla velocità di progetto, che costituisce un valore ideale corrispondente alla velocità più alta che può essere mantenuta con sicurezza, su un determinato tratto stradale, quando le condizioni meteorologiche, di traffico e di ambiente sono così favorevoli che la stessa velocità dei veicoli è limitata solo dalle caratteristiche geometriche della strada.
Per le strade di nuova costruzione le pendenze massime sono stabilite dal d.m. 5 novembre 2001 (Norme funzionali e geometriche per la costruzione delle strade), emanato dal Ministero delle Infrastrutture e dei Trasporti in attuazione dell'art. 13, comma 1, del decreto legislativo 30 aprile 1992, n. 285 (Nuovo codice della strada).
La tabella seguente mostra alcuni valori:
Prima dell'emanazione del d.m. 5 novembre 2001 non esistevano regole cogenti per la costruzione delle strade: ciò spiega perché è frequentissimo che le strade già esistenti alla data del 4 gennaio 2002 (pubblicazione del decreto in Gazzetta Ufficiale) riportino valori di pendenza anche di molto superiori rispetto a quelli stabiliti con il d.m. del 2001.
Sono note ad esempio le pendenze, spesso superiori al 20%, delle strade alpine sulle quali transita il Giro d'Italia, abitualmente aperte anche al traffico automobilistico. 
Le ferrovie hanno le pendenze espresse in "per mille" (‰); una pendenza dell'1‰ corrisponde ad 1 metro di dislivello che viene superato con inclinazione costante su una distanza – anche eventualmente con parti curve – di mille metri (i mille metri sono riferiti alla lunghezza della proiezione del tracciato sul piano orizzontale, non alla lunghezza del tracciato – dei binari – che sarà quindi di estensione superiore).
I limiti massimi – per le ferrovie ad aderenza naturale – variano in base alla loro tipologia; le linee principali e ad alto traffico arrivano a massimo 10-12‰, per le secondarie raramente si arriva al 35‰ ed oltre.
In Italia vi sono casi estremi che arrivano fino al 60‰, ma sono per lo più relativi a ferrovie dismesse (raro esempio in funzione: ramo Perugia Ponte San Giovanni-Perugia Sant'Anna della Ferrovia Centrale Umbra) o a scartamento ridotto (esempio: Ferrovia Domodossola-Locarno).
Riguardo al resto d'Europa, la linea ferroviaria ad aderenza naturale e scartamento normale più ripida è la ferrovia dell'Uetliberg, che arriva al 79‰.
Le ferrovie con aderenza a cremagliera raggiungono anche pendenze fino al 480‰.
La pendenza massima di percorsi per carrozzine è dell'8%, pari a 4,57°.
Per i tetti a falda è consigliata una pendenza tra il 30% e il 40%, corrispondente a circa 16°-22°.
Si ammette come pendenza minima per il deflusso dell'acqua una pendenza del 2%, pari a 1,15°. Per valori inferiori l'acqua non defluisce abbastanza velocemente o non defluisce affatto.
</text>
</doc>
<doc id="548967" url="https://it.wikipedia.org/wiki?curid=548967">
<title>Sezione (geometria descrittiva)</title>
<text>
In geometria descrittiva, il termine sezione indica una condizione d'incidenza tra due elementi geometrici. Per inciso indica un elemento geometrico che può essere ottenuto, rispettivamente:
Secondo la giacitura del piano α che seziona un solido K, la sezione che viene ottenuta può essere chiamata, rispettivamente:
Inoltre, la figura piana che si ottiene come sezione di un piano α con un solido K, può essere chiamata, rispettivamente: longitudinale, quando α passa per l'asse di K, e trasversale quando α è ortogonale all'asse di K.
La sezione in grafica rappresenta un oggetto (o un edificio, o un essere vivente, ecc.) come se uno o più piani regolari avessero tagliato una parte, che viene esclusa dalla rappresentazione. Le sezioni più comuni sono quelle orizzontali (tipica è la pianta) e verticali (spaccato). A volte più sezioni vengono combinate con un'assonometria: in quel caso si parla di spaccato assonometrico.
</text>
</doc>
<doc id="549329" url="https://it.wikipedia.org/wiki?curid=549329">
<title>Similitudine (geometria)</title>
<text>
La similitudine è una trasformazione geometrica, del piano o dello spazio, che conserva i rapporti tra le distanze. In altre parole, una trasformazione formula_1 del piano (o dello spazio) in sé è una similitudine se e solo se esiste un numero reale positivo formula_2 tale che:
per ogni coppia di punti formula_4
Ogni similitudine si può ottenere dalla composizione di una "omotetia" e una "isometria", o viceversa.
Queste trasformazioni mantengono la "forma" (non vengono modificati gli angoli) dell'oggetto, pur cambiandone la posizione, l'orientazione o la grandezza; quindi due oggetti simili hanno la stessa "forma".
Due circonferenze nel piano sono sempre simili. Tutti i quadrati sono simili: più in generale, tutti i poligoni regolari con un numero fissato di lati sono simili.
Tutte le parabole sono simili fra loro, mentre ellissi ed iperboli non lo sono necessariamente.
Quando due oggetti formula_5 e formula_6 sono simili, si scrive generalmente
In geometria affine, una similitudine del piano cartesiano è una particolare affinità
In questa notazione formula_9 indica un generico punto del piano formula_10, mentre formula_11 è una matrice 2x2
e formula_13 è un vettore colonna fissato formula_14. Nella notazione si fa uso della moltiplicazione fra matrici. 
Una affinità descritta in questo modo è una similitudine se e solo se:
Questo è equivalente a chiedere che i coefficienti formula_16 siano non tutti nulli e che una delle due seguenti condizioni sia verificata:
Nel primo caso, il determinante di formula_11 è positivo, la similitudine preserva l'orientazione e si dice "diretta". Nel secondo caso il determinante è negativo, l'orientazione è ribaltata e si dice "inversa".
Esistono alcuni criteri che permettono di determinare se due triangoli sono simili, il primo è il più noto:
Esistono criteri analoghi per due poligoni arbitrari nel piano. Il più importante è il seguente:
In verità, non è necessario effettuare la verifica su tutti gli angoli e tutti i lati: è possibile escludere 
Se il poligono non è un triangolo, "non" è vero che due poligoni aventi gli angoli interni uguali sono simili: ad esempio, due rettangoli hanno sempre gli stessi angoli interni, ma sono simili soltanto se hanno lo stesso rapporto fra i lati.
Ogni similitudine fra due oggetti nel piano può essere elegantemente espressa tramite l'uso dei numeri complessi. È sufficiente descrivere il piano come piano complesso: in questo modo, ogni similitudine è esprimibile tramite una trasformazione lineare del tipo
oppure
dove formula_22 e formula_23 sono due numeri complessi, e formula_24 è il complesso coniugato di formula_25.
Un frattale è un oggetto geometrico "autosimilare": ogni sua piccola parte contiene un oggetto simile all'oggetto grande. 
</text>
</doc>
<doc id="2904544" url="https://it.wikipedia.org/wiki?curid=2904544">
<title>Triangolo</title>
<text>
Il triangolo è un poligono con tre lati e tre angoli.
Il triangolo è caratterizzato dalle seguenti proprietà:
Due triangoli sono congruenti se soddisfano almeno uno dei criteri di congruenza.
Due triangoli si dicono simili se soddisfano almeno uno dei criteri di similitudine.
I triangoli possono essere classificati in base alla lunghezza relativa dei lati:
I triangoli possono essere classificati anche in base alle dimensioni del loro angolo interno più ampio; sono descritti di seguito usando i gradi d'arco.
Per i triangoli che non sono rettangoli vale una generalizzazione del teorema di Pitagora nota in trigonometria come teorema di Carnot. 
Si dice triangolo degenere un triangolo che presenta un angolo 
di 180°. Gli altri due angoli hanno necessariamente ampiezza zero, ed un lato misura quanto la somma degli altri due: tale triangolo, come insieme di punti (graficamente), costituisce un segmento.
Si usa il termine "triangolo degenere" anche per una figura ottenuta come limite di un triangolo nel quale alcuni dei suoi vertici vanno all'infinito; tale figura si chiama anche triangolo ideale. Questa costruzione è molto usata in geometria iperbolica.
Un triangolo ideale con un vertice all'infinito risulta essere una striscia delimitata da un segmento e da due semirette che si estendono illimitatamente nella stessa direzione, ciascuna delle quali ha come estremità una di quelle del segmento; in particolare le rette possono essere ortogonali al segmento.
Ad ogni triangolo sono associati vari punti, ciascuno dei quali svolge un ruolo che, per qualche aspetto, lo qualifica come centrale per il triangolo stesso. Definiamo concisamente questi punti riferendoci ad un triangolo formula_1 i cui vertici denotiamo con formula_2, formula_3 e formula_4 e i cui lati opposti denotiamo rispettivamente con formula_5, formula_6 e formula_7.
L'area di un triangolo può essere trovata per via trigonometrica. Usando le lettere della figura a destra, l'altezza formula_55. Sostituendo questo nella formula trovata precedentemente (per via geometrica), formula_56. L'area di un triangolo è quindi anche uguale al semiprodotto di due lati per il seno dell'angolo compreso.
Di conseguenza, per la nota identità formula_57, l'area di un triangolo qualsiasi con i due lati formula_5 e formula_6 e l'angolo compreso formula_60, è uguale all'area del triangolo con gli stessi lati formula_5 e formula_6 ma con angolo compreso supplementare formula_63
L'area di un parallelogramma con due lati adiacenti formula_5 e formula_6 e angolo compreso formula_60 è il doppio di quella del triangolo che ha gli stessi dati, cioè formula_67.
Per risolvere il triangolo, cioè determinare la misura di tutti i lati ed angoli, dati due lati e l'angolo compreso fra di essi, o un lato e i due angoli adiacenti, si usano il teorema dei seni e il teorema del coseno, quest'ultimo meglio noto col nome di Carnot.
L'area formula_2 del triangolo può essere misurata con la formula matematica:
dove formula_6 è la base e formula_71 l'altezza ad essa relativa, perché il triangolo va visto come la metà di un parallelogramma di base formula_6 e altezza formula_71.
Alternativamente l'area del triangolo può essere calcolata con
dove formula_5, formula_6 e formula_7 sono i lati e formula_78 il semiperimetro (Formula di Erone).
Consideriamo un triangolo formula_79 nel piano cartesiano individuato attraverso le coppie di coordinate dei vertici formula_80.
La sua area formula_2 è dato dall'espressione
oppure con un'espressione che non utilizza il concetto di matrice
oppure
dove
e il suo perimetro formula_89 è dato da
Il concetto di triangolo si estende ed è ampiamente usato in tutte le geometrie non euclidee. Un triangolo in una geometria non euclidea si differenzia generalmente per il fatto che la somma dei suoi angoli interni non è 180°: questa somma è inferiore a 180° per ogni triangolo nel caso di una geometria iperbolica, mentre è superiore per ogni triangolo nel caso di una geometria ellittica.
Se due triangoli hanno rispettivamente congruenti due lati e l'angolo tra essi compreso, essi sono congruenti.
Se due triangoli hanno rispettivamente congruenti due angoli e il lato ad essi adiacente, essi sono congruenti.
Se due triangoli hanno rispettivamente congruente i tre lati, essi sono congruenti.
Criterio generale di congruenza dei triangoli
Se due triangoli hanno rispettivamente congruenti un lato e due angoli qualsiasi, essi sono congruenti.
In un triangolo ogni angolo esterno è maggiore di ciascuno degli angoli interni non adiacenti ad esso.
In un triangolo l'angolo esterno di uno di essi è uguale alla somma degli altri due angoli interni.
In un triangolo ciascun lato è minore della somma degli altri due e maggiore della loro differenza.
Se due triangoli hanno due angoli rispettivamente congruenti, allora sono simili. 
Se due triangoli hanno rispettivamente due lati proporzionali e l'angolo tra essi compreso congruente, allora sono simili. 
Se due triangoli hanno i tre lati rispettivamente proporzionali, allora sono simili.
Criterio generale di similitudine dei triangoli
Se due triangoli hanno due angoli rispettivamente congruenti, essi sono simili.
In un triangolo rettangolo ogni cateto è medio proporzionale tra l'ipotenusa e la sua proiezione sull'ipotenusa. 
In un triangolo rettangolo l'altezza relativa all'ipotenusa è media proporzionale fra le proiezioni dei cateti sull'ipotenusa. 
In ogni triangolo rettangolo, il quadrato costruito sull'ipotenusa è uguale alla somma dei quadrati costruiti sui cateti.
</text>
</doc>
<doc id="161098" url="https://it.wikipedia.org/wiki?curid=161098">
<title>Rapporto segnale/rumore</title>
<text>
In telecomunicazioni ed elettronica il rapporto segnale-rumore, spesso abbreviato con la sigla inglese SNR ("Signal to Noise Ratio") o S/N anche nell'uso italiano, è una grandezza numerica che mette in relazione la potenza del segnale utile rispetto a quella del rumore in un qualsiasi sistema di acquisizione, elaborazione o trasmissione dell'informazione.
Il rapporto segnale-rumore è un numero puro o adimensionale dato dal rapporto fra due grandezze omogenee che esprime quanto il segnale sia più potente del rumore nel sistema considerato. È formalmente espresso dalla relazione:
dove formula_2 è la potenza del segnale utile e formula_3 la potenza totale del rumore presente nel sistema, grandezze queste solitamente espresse in watt o dBm.
Può essere applicato indifferentemente a sistemi di natura ottica, elettronica, ecc. e si tratta di una grandezza fondamentale nell'ambito del trattamento dei segnali e della teoria dell'informazione. Un qualsiasi sistema che debba trasportare o trattare informazioni è infatti affetto da rumore, primo fra tutti il rumore termico, che è un fattore non ideale della trasmissione ovvero del tutto indesiderato che corrompe il segnale utile spesso sommandovisi in maniera additiva: tanto maggiore è la potenza di rumore rispetto alla potenza del segnale utile tanto minore è la qualità della comunicazione. È logico dunque aspettarsi che l'SNR sia un parametro di qualità che si cerca o si tenda in qualche modo a massimizzare o preservare il più possibile.
Esso è dunque un parametro di merito molto importante per il dimensionamento e il corretto funzionamento del sistemi di telecomunicazioni in quanto strettamente collegato alla capacità del sistema ricevente di rilevare il flusso informativo originario senza incorrere in alterazioni dovute a distorsione ed errori dovuti essenzialmente all'azione disturbante del rumore. Più basso è l'SNR, più sarà difficoltosa la decodifica del segnale ovvero più alta sarà la probabilità di errore e quindi anche il BER nelle trasmissioni digitali.
Nel caso di trasmissioni analogiche una diminuzione di SNR determina un deperimento graduale della qualità del segnale ricevuto (si pensi ad esempio a una radio FM che riceve male o a un televisore in cui compaiono audio o video disturbati: tipici casi di SNR basso); per le trasmissioni analogiche tuttavia è l'utente finale a stabilire una soglia di fruibilità del sistema.
Nel caso di trasmissioni digitali, invece, esiste una soglia minima di SNR sotto la quale il sistema non è in grado di funzionare (si pensi alla televisione digitale satellitare: o si vede bene o non si vede del tutto); l'errore di decisione/decodifica può essere tale da far decidere per un simbolo piuttosto che un altro; tuttavia grazie alle moderne tecniche di modulazione e di protezione dei dati tramite codifica di canale tale soglia è piuttosto bassa, al di sotto di quella che servirebbe per ottenere prestazioni analoghe su sistemi analogici.
La soglia minima di SNR è determinata dalla tecnologia dell'apparato ricevente; in fase di progetto di un sistema di telecomunicazioni il primo obiettivo è quindi quello di far pervenire al ricevitore un SNR sufficientemente elevato.
In Alta Fedeltà il rapporto segnale-rumore è uno dei parametri di merito fondamentali, anche se non l'unico, per la valutazione delle prestazioni di un impianto per quello che concerne la pulizia del suono prodotto, tanto che i preamplificatori di fascia alta sono realizzati in due distinti telai, uno contenente il circuito amplificatore, l'altro il circuito alimentatore.
Tale rapporto è legato inoltre alla velocità di trasmissione sul canale tramite il Teorema di Shannon-Hartley.
Il rapporto segnale-rumore decade con la distanza percorsa dal segnale sul canale o mezzo trasmissivo in virtù dell'attenuazione della potenza del segnale utile così che a una certa distanza dal trasmettitore rimane solo rumore. Tale decadimento fa decadere a sua volta la velocità di trasmissione con la distanza sul canale stesso.
Esiste inoltre un parametro, il SINAD, concettualmente molto simile al SNR, e che insieme al rumore include anche la distorsione generata dal circuito: esso dà una valutazione più precisa della degradazione assunta da un segnale per effetto delle non idealità delle apparecchiature che attraversa, in particolare degli ADC e dei DAC, che possono in alcuni casi essere parecchio distorcenti.
In molti sistemi elettronici e di telecomunicazioni per ottenere una massimizzazione del rapporto segnale-rumore si usa comunemente un filtro adattato.
</text>
</doc>
<doc id="1745121" url="https://it.wikipedia.org/wiki?curid=1745121">
<title>Outlier</title>
<text>
 è un termine utilizzato in statistica per definire, in un insieme di osservazioni, un valore anomalo e aberrante. Un valore quindi chiaramente distante dalle altre osservazioni disponibili.
In statistica viene definito outlier un valore al di fuori dall'intervallo:
Dove formula_2 e formula_3 sono rispettivamente primo e terzo quartile. formula_4 è una costante che regola l'ampiezza dell'intervallo. Normalmente assume il valore unitario.
Gli outlier sono valori numericamente distanti dal resto dei dati raccolti (ad esempio, in un campionamento). Le statistiche che derivano da campioni contenenti outlier possono essere fuorvianti. Per esempio, se misurassimo la temperatura di dieci oggetti presenti in una stanza, la maggior parte dei quali risultasse avere una temperatura compresa fra 20 e 25 gradi Celsius, allora il forno acceso, avente una temperatura di 350 gradi, sarebbe un dato aberrante. La mediana dei valori sarebbe circa 23, mentre la temperatura media salirebbe a circa 55 gradi: un indice chiaramente non rappresentativo della maggioranza dei valori di temperatura riscontrati nella stanza. In questo caso, la mediana rifletterebbe meglio della media aritmetica le misure della temperatura degli oggetti. Gli outliers possono essere indicativi del fatto che, in un dato campione, alcuni dati appartengono ad una popolazione differente rispetto a quella del resto del campione.
Nella maggioranza dei grandi campioni, alcuni dati saranno più lontani dalla media del campione di quanto sarebbe logico aspettarsi. Ciò può essere dovuto ad un errore sistematico che si è verificato nella raccolta dei dati, oppure a una fallacia nella teoria che ha orientato l'assunzione di una data distribuzione campionaria di probabilità, ma potrebbe anche essere semplicemente dovuto al caso, che ha fatto sì che nella raccolta dei dati alcune osservazioni abbiano prodotto dati molto lontani dai valori medi del campione. Inoltre, gli outliers potrebbero essere indicativi di dati errati, procedure erronee o aree sperimentali in cui alcune teorie potrebbero non essere valide. Tuttavia, un piccolo numero di dati aberranti non dovuti a condizioni anomale è dato per scontato nei grandi campioni.
Stimatori poco influenzati dai dati aberranti sono detti robusti.
</text>
</doc>
<doc id="1844568" url="https://it.wikipedia.org/wiki?curid=1844568">
<title>Analisi della regressione</title>
<text>
L'analisi della regressione è una tecnica usata per analizzare una serie di dati che consistono in una variabile dipendente e una o più variabili indipendenti. Lo scopo è stimare un'eventuale relazione funzionale esistente tra la variabile dipendente e le variabili indipendenti. La variabile dipendente nell'"equazione di regressione" è una funzione delle variabili indipendenti più un "termine d'errore". Quest'ultimo è una variabile casuale e rappresenta una variazione non controllabile e imprevedibile nella variabile dipendente. I parametri sono stimati in modo da descrivere al meglio i dati. Il metodo più comunemente utilizzato per ottenere le migliori stime è il metodo dei "minimi quadrati" (OLS), ma sono utilizzati anche altri metodi.
Il "data modeling" può essere usato senza alcuna conoscenza dei processi sottostanti che hanno generato i dati; in questo caso il modello è un modello empirico. Inoltre, nella modellizzazione, non è richiesta la conoscenza della distribuzione di probabilità degli errori. L'analisi della regressione richiede ipotesi riguardanti la distribuzione di probabilità degli errori. Test statistici vengono effettuati sulla base di tali ipotesi. Nell'analisi della regressione il termine "modello" comprende sia la funzione usata per modellare i dati che le assunzioni concernenti la distribuzione di probabilità.
L'analisi della regressione può essere usata per effettuare previsioni (ad esempio per prevedere dati futuri di una serie temporale), inferenza statistica, per testare ipotesi o per modellare delle relazioni di dipendenza. Questi usi della regressione dipendono fortemente dal fatto che le assunzioni di partenza siano verificate. L'uso dell'analisi della regressione è stato criticato in diversi casi in cui le ipotesi di partenza non possono essere verificate. Un fattore che contribuisce all'uso improprio della regressione è che richiede più competenze per criticare un modello che per adattarlo.
La prima forma di regressione fu il metodo dei minimi quadrati, pubblicato da Legendre nel 1805, e da Gauss nel 1809. Il termine “minimi quadrati” deriva da quello usato da Legendre: "moindres carrés". Tuttavia, Gauss affermò di essere a conoscenza di questo metodo fin dal 1795.
Legendre e Gauss applicarono entrambi il metodo al problema di determinare, a partire da osservazioni astronomiche, l'orbita dei pianeti attorno al Sole. Eulero aveva lavorato sullo stesso problema intorno al 1748, ma senza successo. Gauss pubblicò un ulteriore sviluppo della teoria dei minimi quadrati nel 1821, includendo una versione del teorema di Gauss-Markov.
Il termine "regressione" venne coniato nel diciannovesimo secolo per descrivere un fenomeno biologico, ovvero che la progenie di individui eccezionali tende in genere ad essere meno eccezionale dei propri genitori e più simile ai loro avi più distanti. Francis Galton, un cugino di Charles Darwin, studiò questo fenomeno e applicò il termine vagamente fuorviante di "regressione verso il centro/regressione verso la media". Per Galton, la regressione aveva solo questo significato biologico, ma il suo lavoro venne in seguito esteso da Udny Yule e Karl Pearson in un contesto statistico più generale. Oggi il termine "regressione" è spesso sinonimo di "curva intercetta dei minimi quadrati".
Queste condizioni sono sufficienti (ma non tutte necessarie) perché lo stimatore dei minimi quadrati goda di buone proprietà. In particolare queste assunzioni implicano che lo stimatore sia non distorto, consistente ed efficiente nella classe degli stimatori lineari non distorti. Molte di queste assunzioni possono essere rilassate in analisi più avanzate.
Nella regressione lineare, il modello assume che la variabile dipendente, formula_1 sia una combinazione lineare dei "parametri" (ma non è necessario che sia lineare nella "variabile indipendente"). Ad esempio, nella regressione lineare semplice con formula_2 osservazioni ci sono una variabile indipendente: formula_3, e due parametri, formula_4 e formula_5:
Nella regressione lineare multipla, ci sono più variabili indipendenti o funzioni di variabili indipendenti. Ad esempio, aggiungendo un termine in formula_7 alla regressione precedente si ottiene:
Si tratta ancora di una regressione lineare: sebbene l'espressione sulla destra sia quadratica nella variabile indipendente formula_3, è comunque lineare nei parametri formula_4, formula_5 e formula_12
In entrambi i casi, formula_13 è un termine di errore e l'indice formula_14 identifica una particolare osservazione. Dato un campione casuale della popolazione, stimiamo i parametri della popolazione e otteniamo il modello di regressione lineare semplice:
Il termine formula_16 è il residuo, formula_17. Un metodo di stima è quello dei minimi quadrati ordinari. Questo metodo ottiene le stime dei parametri che minimizzano la somma dei quadrati dei residui, SSE:
La minimizzazione di questa funzione risulta essere un sistema di equazioni normali, un insieme di equazioni lineari simultanee nei parametri, che vengono risolte per trovare le stime dei parametri, formula_19. Vedi coefficienti di regressione per informazioni sulle proprietà statistiche di tali stimatori.
Nel caso della regressione semplice, le formule per le stime dei minimi quadrati sono 
dove formula_22 è la media (media) dei valori formula_23 e formula_24 è la media dei valori formula_25.
Sotto l'ipotesi che il termine di errore della popolazione abbia una varianza costante, la stima di quella varianza è data da:
formula_26
Questo è la radice dell'errore quadratico medio (RMSE) della regressione. 
Gli errori standard delle stime dei parametri sono dati da
Sotto l'ulteriore ipotesi che il termine di errore della popolazione abbia distribuzione normale, il ricercatore può usare questi errori standard stimati per creare intervalli di confidenza e condurre test d'ipotesi sui parametri della popolazione.
Nel più generale modello di regressione multipla, ci sono formula_29 variabili indipendenti:
Le stime dei parametri dei minimi quadrati sono ottenute da formula_29 equazioni normali. Il residuo può essere scritto come
Le equazioni normali sono
In notazione matriciale, le equazioni normali sono scritte come
Una volta costruito un modello di regressione, è importante confermare la bontà di adattamento del modello e la significatività statistica dei parametri stimati. I controlli della bontà di adattamento comunemente usati includono l'indice R-quadro, analisi dei residui e test di ipotesi. La significatività statistica è verificata con un test F dell'adattamento globale, seguito da t-test per ogni singolo parametro.
L'interpretazione di questi test dipende fortemente dalle assunzioni sul modello. Nonostante l'analisi dei residui sia usata per determinare la bontà di un modello, i risultati dei test-T e dei test-F sono difficili da interpretare nel caso in cui le assunzioni di partenza non siano soddisfatte. Ad esempio, se la distribuzione degli errori non è normale, può accadere che in campioni di numerosità ridotta le stime dei parametri non seguano una distribuzione normale, cosa che complica l'inferenza. Per grandi campioni, il teorema del limite centrale permette di effettuare i test usando un'approssimazione asintotica delle distribuzioni.
La variabile risposta può essere non continua. Per le variabili binarie (zero/uno), si può procedere con un particolare tipo di modello lineare linear probability model. Se si usa un modello non-lineare i modelli più utilizzati sono il probit e il modello logit. Il modello probit multivariato rende possibile stimare congiuntamente la relazione tra più variabili binarie dipendenti e alcune variabili indipendenti. Per variabili categoriche con più di due valori si utilizza il modello logit multinomiale. Per variabili ordinali con più di due valori, si utilizzano i modelli logit cumulativo e probit cumulativo. Un'alternativa a tali procedure è la regressione lineare basata su polychoric o polyserial correlazioni tra le variabili categoriche. Tali procedure differiscono nelle ipotesi fatte sulla distribuzione delle variabili nella popolazione. Se la variabile rappresenta una ripetizione di un evento nel tempo, è positiva e con poche realizzazioni ("eventi rari"), si possono utilizzare modelli di Poisson o binomiale negativa.
I modelli di regressione predicono una variabile formula_25 partendo dai valori di altre variabili formula_23. Se i valori della previsione sono compresi nell'intervallo dei valori delle variabili formula_23 utilizzate per la costruzione del modello si parla di interpolazione. Se i valori escono dal range delle variabili esplicative si parla di estrapolazione. In questo caso la previsione diventa più rischiosa.
Quando la funzione del modello non è lineare nei parametri la somma dei quadrati deve essere minimizzata da una procedura iterativa.
Sebbene i parametri di un modello di regressione siano di solito stimati usando il metodo dei minimi quadrati, altri metodi includono:
Tutti i principali pacchetti statistici eseguono i tipi comuni di analisi di regressione correttamente e in modo semplice. La regressione lineare semplice può essere fatta in alcuni fogli elettronici. C'è una quantità di programmi che esegue forme specializzate di regressione, e gli esperti possono scegliere di scrivere il loro proprio codice per usare o software per analisi numerica.
</text>
</doc>
<doc id="1845509" url="https://it.wikipedia.org/wiki?curid=1845509">
<title>Significatività</title>
<text>
In statistica la significatività è la possibilità rilevante che compaia un determinato valore. Ci si riferisce anche come "statisticamente differente da zero"; ciò non significa che la "significatività" sia rilevante, o vasta, come indurrebbe a pensare la parola. Ma solo che è diversa dal numero limite.
Il livello di significatività di un test è dato solitamente da una verifica del test d'ipotesi. Nel caso più semplice è definita come la probabilità di accettare o rigettare l'ipotesi nulla.
I livelli di significatività sono solitamente rappresentati con la lettera greca α (alfa). I livelli più usati sono 5% (α=0,05) e 1% (α=0,01); nel caso di ipotesi a carattere prevalentemente esplorativo è consuetudine adoperare un livello di significatività al 10% (α=0,1). Se il test di verifica d'ipotesi dà un valore p minore del livello α, l'ipotesi nulla è rifiutata.
Tali risultati sono informalmente riportati come 'statisticamente significativi'. Per esempio se si sostiene che "c'è solo una possibilità su mille che ciò possa accadere per coincidenza," viene usato un livello di significatività dello 0,1%. Più basso è il livello di significatività, maggiore è l'evidenza. In alcune situazioni conviene esprimere la significatività statistica con 1 − α. In generale, quando si interpreta una significatività stabilita, bisogna stare attenti nell'indicare che cosa, precisamente è stato testato statisticamente.
Differenti livelli di α hanno differenti vantaggi e svantaggi. α-livelli più bassi danno maggiore confidenza nella determinazione della significatività, ma corrono maggiori rischi di errore nel respingere una falsa ipotesi nulla (un errore di tipo II, o "falsa determinazione negativa"), e così hanno maggiore potenza statistica. La selezione di un α-livello inevitabilmente implica un compromesso fra significatività e potenza, e di conseguenza, fra errore tipo I ed errore tipo II.
In alcuni campi, per esempio nella fisica nucleare ed in quella delle particelle, si usa esprimere la significatività statistica in unità di "σ" (sigma), la deviazione standard di una distribuzione gaussiana. Una significatività statistica di "formula_1" può essere convertita in un valore di α usando la funzione errore:
L'uso di σ è motivato dalla onnipresenza della distribuzione gaussiana nella 
misura delle incertezze. Per esempio se una teoria prevede che un parametro abbia un valore, ad esempio 100, e ad una misurazione indica che il parametro è 100 ± 3, allora bisogna riportare la misura come una "deviazione 3σ" dalla previsione teorica. in termini di α, questa situazione è equivalente al dire che "supponendo vera la teoria, la possibilità di ottenere che il risultato sperimentale coincida è dello 0,27%" (poiché 1  − erf(3/√2) = 0.0027).
Fissati i livelli di significatività come quelli menzionati in seguito possono essere considerati come utili nelle analisi di dati esploratorie. Comunque, la moderna statistica è dell'avviso che, dove il risultato di un test è essenzialmente il risultato finale di un esperimento o di altro studio, il p-valore deve essere considerato esplicitamente. Inoltre, ed è importante, bisogna considerare se e come il p-valore è significativo o meno. Questo consente di accedere al massimo delle informazioni che devono essere trasferiti da un riassunto degli studi nelle meta-analisi.
Un errore comune è ritenere che un risultato statisticamente significativo sia sempre di significatività pratica, o dimostri un largo effetto nella popolazione. Sfortunatamente, questo problema si incontra diffusamente negli scritti scientifici. Dato un campione sufficientemente grande, per esempio, si può scoprire che differenze estremamente piccole e non visibili sono statisticamente significative, ma la significatività statistica non dice niente di una significatività pratica di una differenza.
Uno dei problemi più comuni nel testare la significatività è la tendenza delle comparazioni multiple a tendere a significative differenze spurie anche dove l'ipotesi nulla è vera. Per esempio, in uno studio di venti comparazioni, usando un 
α-livello del 5%, una comparazione può effettivamente riportare un risultato significativo nonostante sia vera l'ipotesi di nullità. in questi casi i p-valori sono corretti al fine di controllare o il valore falso o l'errore familiare.
Un problema addizionale è che si ritiene che le analisi frequentiste dei p-valori esagerino la ""significatività statistica"". Si veda il fattore di Bayes per i dettagli.
J. Scott Armstrong, negli articoli "Significance Tests Harm Progress in Forecasting," e "Statistical Significance Tests are Unnecessary Even When Properly Done," espone la sua posizione secondo cui in alcuni casi, seppure eseguiti correttamente, i test di significatività statistica non sarebbero utili. A suo parere, un certo numero di tentativi ha fallito nel trovare prove empiriche che sostenessero l'uso di test di significatività, ed i test di significatività statistica usati da soli potrebbero essere nocivi allo sviluppo della conoscenza scientifica perché distrarrebbero i ricercatori dall'uso di metodi statistici in alcuni casi più adatti. 
Armstrong suggerisce quindi che secondo lui i ricercatori dovrebbero evitare i test di significatività statistica, e dovrebbero piuttosto fare uso di strumenti di area di effetto, intervalli di fiducia, ripetizioni/estensioni, e meta-analisi.
La significatività statistica può essere considerata come la fiducia che si ha in un dato risultato. In uno studio di comparazione, essa dipende dalla differenza relativa tra i gruppi confrontati, la quantità delle misurazioni e il rumore associato alle misurazioni. In altre parole, la fiducia che si ha che un dato risultato sia non casuale (cioè non una conseguenza di un caso) dipende dal rapporto segnale-rumore (SNR) e misura campione. Esprimendosi matematicamente, la fiducia che un risultato non sia casuale è dato dalla seguente formula di Sackett:
Per chiarezza, la succitata formula è rappresentata tabularmente qui di seguito.
Dipendenza della fiducia con rumore, segnale e misura campione (forma tabulare)
In parole la dipendenza di una fiducia è maggiore se il rumore è basso o la misura campione è estesa o l'ampiezza effettiva (del segnale) è larga. La fiducia di un risultato (e l'associato intervallo di fiducia) "non" dipende dagli effetti della sola ampiezza effettiva del segnale. Se la misura campione è grande e il rumore e piccolo un'ampiezza effettiva di segnale può essere misurata con grande fiducia. Sebbene un'ampiezza effettiva viene considerata importante essa dipende nel contesto degli eventi comparati.
In medicina, piccole ampiezze effettive (riflesse da piccoli aumenti di rischio) sono spesso considerate clinicamente rilevanti e sono frequentemente usati per guidare decisioni di trattamento (se c'è una grande fiducia in essi). Sebbene un dato trattamento è considerato un giusto tentativo esso dipende dai rischi, dai benefici e dai costi.
</text>
</doc>
<doc id="1716607" url="https://it.wikipedia.org/wiki?curid=1716607">
<title>Test t</title>
<text>
Il test t (o, dall'inglese, t-test) è un test statistico di tipo parametrico con lo scopo di verificare se il valore medio di una distribuzione si discosta significativamente da un certo valore di riferimento. Differisce dal test z per il fatto che la varianza formula_1 è sconosciuta.
Se la varianza della popolazione non è nota, la verifica d'ipotesi sulla media della popolazione si effettua sostituendo alla varianza di universo la sua stima ottenuta a partire dallo stimatore varianza corretta del campione:
In questo modo la statistica test è:
la cui distribuzione è quella della formula_4 di Student con formula_5 gradi di libertà. Ad ogni modo, all'aumentare dei gradi di libertà, per il teorema del limite centrale, la variabile casuale formula_4 tende alla distribuzione normale e quindi alla formula_4 si può sostituire la formula_8 diciamo per una soglia campionaria formula_9 maggiore di 30. Se il test è bidirezionale, si rifiuterà l'ipotesi nulla se la formula_10 empirica è maggiore della formula_10 teorica di formula_12 con formula_5 gradi di libertà e si accetterà l'ipotesi alternativa formula_14 con un errore formula_15 di I specie.
In econometria la statistica formula_10 ha la seguente forma:
</text>
</doc>
<doc id="6539843" url="https://it.wikipedia.org/wiki?curid=6539843">
<title>Boosting</title>
<text>
Il boosting è una tecnica di machine learning che rientra nella categoria dell'Apprendimento ensemble. Nel boosting più modelli vengono generati consecutivamente dando sempre più peso agli errori effettuati nei modelli precedenti. In questo modo si creano modelli via via più "attenti" agli aspetti che hanno causato inesattezze nei modelli precedenti, ottenendo infine un modello aggregato avente migliore accuratezza di ciascun modello che lo costituisce.
In algoritmi come Adaboost, l'output del meta-classificatore è dato dalla somma pesata delle predizioni dei singoli modelli. Ogni qual volta un modello viene addestrato, ci sarà una fase di ripesaggio delle istanze. L'algoritmo di boosting tenderà a dare un peso maggiore alle istanze misclassificate, nella speranza che il successivo modello sia più esperto su quest'ultime.
In generale si ha che l'errore di predizione in un problema di apprendimento supervisionato è dato da:
formula_1
Il boosting mira a ridurre la varianza.
</text>
</doc>
<doc id="456307" url="https://it.wikipedia.org/wiki?curid=456307">
<title>Sistema non lineare</title>
<text>
In matematica un sistema non lineare (talvolta nonlineare) è un sistema di equazioni in cui almeno una di esse è non lineare, cioè non esprimibile come combinazione lineare delle incognite presenti e di una costante. Ad esempio potrebbe contenere equazioni algebriche con almeno un termine di grado diverso da uno, o più in generale dei termini non polinomiali. In pratica, ogni sistema di equazioni che non sia lineare è detto non lineare.
Un sistema è "polinomiale" se ogni equazione è un polinomio. In questo caso il grado del sistema è il prodotto dei gradi dei polinomi, ed il sistema è non lineare precisamente quando ha grado maggiore di uno. Ad esempio, il sistema seguente ha due equazioni e due incognite, e non è lineare perché ha grado due:
Il sistema seguente ha invece grado quattro:
Le soluzioni di un sistema polinomiale dipendono fortemente dal campo in cui vengono considerate. Generalmente determinare le soluzioni è impossibile. Teoremi di geometria algebrica (generalizzazioni del teorema di Bézout) garantiscono il fatto seguente: 
Nel caso in cui il sistema abbia due variabili, l'insieme delle soluzioni può essere visto geometricamente come il luogo di intersezione fra alcune curve, ciascuna determinata da un'equazione. Nei due esempi descritti, si tratta rispettivamente dell'intersezione di una conica ed una retta e di due coniche.
Questo sistema molto semplice è in realtà una sola equazione in una variabile.
Il sistema è lineare se e solo se la funzione "f" è lineare, ovvero della forma "f"("x") = a"x" + "b" con "a" e "b" nel dominio opportuno. In tutti gli altri casi il sistema non è lineare, come negli esempi seguenti:
In questo caso il sistema è una singola equazione con due variabili
ed è lineare se e solo se la funzione "f" è esprimibile come "f"("x", "y") = "ax" + "by" + "c", cioè se è l'equazione di un piano affine. In tutti gli altri casi il sistema è non lineare, ad esempio:
Un sistema generico con "n" incognite e "m" equazioni è esprimibile come:
o in notazione vettoriale compatta come: 
dove stavolta formula_9 è il vettore nullo dello spazio vettoriale K (dove K è il campo in cui sono studiate le soluzioni, ad esempio R o C) e "F" è una funzione da K in K.
Un sistema non lineare è, ad esempio:
La quasi totalità dei sistemi fisici è non lineare, questo rende la ricerca di soluzioni analitiche molto difficile e a volte impossibile. È solitamente possibile trasformare un problema non lineare in un problema localmente lineare, cioè trovare un sistema lineare che approssima, entro un certo raggio, il sistema non lineare originale.
A questo scopo si utilizzano vari tipi di espansione in serie, in particolare l'espansione in serie di Taylor (e l'analogo multidimensionale) e l'espansione in serie di Fourier. Nella figura a destra si vede l'espansione al primo ordine in serie di Taylor della funzione esponenziale.
</text>
</doc>
<doc id="361518" url="https://it.wikipedia.org/wiki?curid=361518">
<title>Metodo dei minimi quadrati</title>
<text>
Il metodo dei minimi quadrati (in inglese OLS: "Ordinary Least Squares") è una tecnica di ottimizzazione (o regressione) che permette di trovare una funzione, rappresentata da una curva ottima (o curva di regressione), che si avvicini il più possibile ad un insieme di dati (tipicamente punti del piano).
In particolare, la funzione trovata deve essere quella che minimizza la somma dei quadrati delle distanze tra i dati osservati e quelli della curva che rappresenta la funzione stessa.
In questo caso, possiamo distinguere parabola dei minimi quadrati e retta dei minimi quadrati.
Questo metodo converge solo nel suo caso limite a un'interpolazione, per cui di fatto si richiede che la curva ottima contenga tutti i punti dati.
L'utilizzo più frequente è la deduzione dell'andamento medio in base ai dati sperimentali per l'estrapolazione fuori dal campo di misurazione.
Anche altri problemi di ottimizzazione, come la minimizzazione dell'energia o la massimizzazione dell'entropia, possono essere riformulati in una ricerca dei minimi quadrati.
Gli stimatori OLS sono:
Le assunzioni OLS sono:
Le assunzioni OLS sono:
Da notare che l'ipotesi di media condizionale dell'errore nulla implica che:
Siano formula_17 con formula_18 i punti che rappresentano i dati in ingresso. Si vuole trovare una funzione formula_19 tale che approssimi la successione di punti dati. Questa può essere determinata minimizzando la distanza (euclidea) tra le due successioni formula_20 e formula_21, ovvero la quantità S :
da cui il nome "minimi quadrati".
Nei casi pratici in genere "f"("x") è parametrica: in questo modo il problema si riduce a determinare i parametri che minimizzano la distanza dei punti dalla curva. Naturalmente per ottenere un'unica curva ottimizzata e non un fascio, è necessario un numero di punti sperimentali maggiore del numero di parametri da cui dipende la curva (il problema in genere
si dice "sovradeterminato"). In genere dai dati sperimentali ottenuti ci si aspetta una distribuzione regolata da relazioni determinate per via analitica; risulta utile quindi parametrizzare la curva teorica e determinare i parametri in modo da minimizzare "S".
La funzione interpolante desiderata è una retta, i parametri sono due "a" e "b": per essere determinati univocamente servono almeno due punti da interpolare.
In tal caso è possibile scrivere in modo esplicito i valori dei parametri "a" e "b".
Si consideri di avere "N" coppie formula_17. Allora i coefficienti sono:
La funzione interpolante desiderata è una potenza e possiede un solo parametro; diversamente dall'esempio precedente la funzione non è lineare rispetto ai parametri.
Sia "f"("x") una funzione lineare rispetto ai parametri
dove "p" sono i "k" parametri, formula_29 e "n" è il numero di punti noti.
Si può riorganizzare la situazione attraverso il sistema lineare sovradimensionato
dove:
Da cui:
formula_32
Il problema di minimizzare "S" si riconduce dunque a minimizzare la norma del residuo
dove con formula_34 si intende l"'i"-esima componente del vettore prodotto fra "A" e "p".
Possiamo minimizzare formula_35 derivando formula_36 rispetto a ciascun "p" e ponendo le derivate pari a 0:
queste equazioni sono equivalenti al sistema:
Quindi il vettore "p" che minimizza "S" è la soluzione dell'equazione:
Quest'ultima equazione è detta equazione normale.
Se il rango di "A" è completo allora formula_40 è invertibile e dunque:
La matrice formula_42 è detta pseudo-inversa.
In molti casi la funzione formula_43 non è lineare, in questi casi non si può indicare un modo certo per ottenere i parametri. Nel tipico caso in cui la dimensione dello spazio dei parametri sia maggiore di 1, il problema diventa fortemente non lineare conviene ricorrere all'uso di programmi di analisi numerica specifici che minimizzi la variabile formula_44.
Una delle librerie più famose per questo compito è "MINUIT", inizialmente sviluppata al CERN in Fortran ed ora integrata nel più recente framework di analisi dati "ROOT". Si segnalano per questo compito anche altre librerie come le "Gnu Scientific Library".
Questo metodo si utilizza quando quello dei minimi quadrati ordinari fallisce, perché la stima ottenuta è correlata all'errore. In questo caso si opera una regressione della variabile che si vuole stimare su una variabile strumentale che sia correlata alla variabile dipendente stessa, ma non al termine di errore. Ottenuta questa stima, la si utilizza per girare una nuova regressione che non dovrebbe dare problemi. Ovviamente il problema più grosso è trovare una variabile strumentale con le caratteristiche adeguate.
È tipicamente utilizzato con le variabili strumentali.
Le assunzioni OLS sono:
</text>
</doc>
<doc id="434228" url="https://it.wikipedia.org/wiki?curid=434228">
<title>Information retrieval</title>
<text>
L'information retrieval (IR) (in italiano "recupero delle informazioni") è l'insieme delle tecniche utilizzate per gestire la rappresentazione, la memorizzazione, l'organizzazione e l'accesso ad oggetti contenenti informazioni quali documenti, pagine web, cataloghi online e oggetti multimediali. Il termine è stato coniato da Calvin Mooers alla fine degli anni quaranta del Novecento ed oggi è usato quasi esclusivamente in ambito informatico.
L'information retrieval è un campo interdisciplinare che nasce dall'incrocio di discipline diverse coinvolgendo la psicologia cognitiva, l'architettura informativa, la filosofia (vedi la voce ontologia), il "design", il comportamento umano sull'informazione, la linguistica, la semiotica, la scienza dell'informazione e l'informatica. Molte università e biblioteche pubbliche utilizzano sistemi di information retrieval per fornire accesso a pubblicazioni, libri ed altri documenti.
Lo scopo dell'information retrieval è di soddisfare il cosiddetto "bisogno informativo dell'utente", ovvero garantire a quest'ultimo, in seguito ad una sua ricerca, i documenti e le informazioni che rispondono alla sua richiesta.
Due concetti sono di fondamentale importanza per analizzare un sistema di information retrieval: query ed oggetto.
Comunemente, si definisce "task" di un sistema di "information retrieval" una situazione tipica che un sistema di questo genere deve risolvere.
Nel momento in cui un utente intende usare un qualsiasi sistema di reperimento dell'informazione (per esempio, un motore di ricerca) per acquisire informazioni su un determinato argomento, questi deve tradurre tale necessità in una query; il sistema di information retrieval ha il compito di restituire, a partire da essa, tutti i documenti rilevanti alla richiesta effettuata.
Ci sono molti modi per misurare quanto l'informazione intesa si associa bene all'informazione recuperata.
La precisione (in inglese "precision") è la proporzione di documenti pertinenti fra quelli recuperati:
Nella classificazione binaria la precisione è analoga al valore positivo di previsione. 
La precisione può anche essere valutata rispetto a un certo valore soglia, indicato con "P@n", piuttosto che relativamente a tutti i documenti recuperati: in questo modo, si può valutare quanti fra i primi "n" documenti recuperati sono rilevanti per la query.
Il significato e l'uso del termine "precisione" nel campo dell'information retrieval differiscono quindi dalla definizione di accuratezza e precisione tipiche di altre discipline scientifiche e tecnologiche.
Il recupero o richiamo (in inglese "recall") è la proporzione fra il numero di documenti rilevanti recuperati e il numero di tutti i documenti rilevanti disponibili nella collezione considerata:
Nella classificazione binaria, questo valore è chiamato sensitività.
La misura F (in inglese "F-measure") è la media armonica pesata fra precisione e recupero. La versione tradizionale, detta anche "bilanciata", è data da:
Questa misura è anche detta formula_2, perché sia la precisione che il recupero nella formula precedente hanno appunto il peso 1.
In generale, la formula è:
Altre due formule comuni sono formula_4, che assegna alla precisione un peso doppio rispetto al recupero, e la formula_5, che al contrario pesa il recupero al doppio della precisione.
Per concludere con successo una ricerca di informazioni, è necessario rappresentare i documenti in qualche modo. C'è un certo numero di modelli aventi tale scopo. Essi possono essere classificati secondo due criteri, come mostrato nella figura a destra: in base ad un criterio matematico e in base alle proprietà del modello (tradotto da fonte originale logos-verlag.de).
Sistemi di Information Retrieval in campo scientifico
Software di Information Retrieval Open Source
Principali gruppi di ricerca sull'Information Retrieval
Approfondimenti
</text>
</doc>
<doc id="480718" url="https://it.wikipedia.org/wiki?curid=480718">
<title>Discesa del gradiente</title>
<text>
In ottimizzazione e analisi numerica il metodo di discesa del gradiente (detto anche "metodo del gradiente", "metodo steepest descent" o "metodo di discesa più ripida") è una tecnica che consente di determinare i punti di massimo e minimo di una funzione di più variabili.
Il metodo è stato sviluppato - e pubblicato nel 1847 - dal matematico francese Augustin-Louis Cauchy nel tentativo di risolvere il problema di determinare l'orbita di un corpo celeste a partire dalle sue equazioni del moto.
Si supponga di voler minimizzare la funzioneformula_1 e si scelga come soluzione iniziale il vettore formula_2. Allora
e, muovendosi in un intorno di formula_4:
Questi calcoli mostrano che, per individuare dei punti - "vicini" a formula_4 - in corrispondenza dei quali la funzione assuma un valore minore di formula_7, conviene spostarsi lungo direzioni che abbiano la prima e la terza componente formula_8 più piccole o seconda componente formula_9 più grande. Inoltre esistono delle direzioni "preferenziali" lungo le quali la funzione formula_10 decresce più velocemente (ad esempio scegliere una coordinata formula_11 più piccola è preferibile, ad esempio, rispetto a far diminuire formula_12).
La procedura può essere iterata partendo da un nuovo punto, ad esempio formula_13, fino ad individuare un minimo per formula_10. L'esempio mostra che una procedura che aggiorni la soluzione in modo iterativo sulla base delle informazioni disponibili "localmente" può portare ad individuare un punto di minimo per la funzione assegnata.
Si voglia risolvere il seguente problema di ottimizzazione non vincolata nello spazio formula_15-dimensionale formula_16
La tecnica di discesa secondo gradiente si basa sul fatto che, per una data funzione formula_18, la direzione di massima discesa in un assegnato punto formula_19 corrisponde a quella determinata dall'opposto del suo gradiente in quel punto formula_20. Questa scelta per la direzione di discesa garantisce che la soluzione tenda a un punto di minimo di formula_10. Il metodo del gradiente prevede dunque di partire da una soluzione iniziale formula_4 scelta arbitrariamente e di procedere iterativamente aggiornandola come
dove formula_24 corrisponde alla lunghezza del passo di discesa, la cui scelta diventa cruciale nel determinare la velocità con cui l'algoritmo convergerà alla soluzione richiesta.
Si parla di metodo "stazionario" nel caso in cui si scelga un passo formula_25 costante per ogni formula_26, viceversa il metodo si definisce "dinamico". In quest'ultimo caso una scelta conveniente, ma computazionalmente più onerosa rispetto a un metodo stazionario, consiste nell'ottimizzare, una volta determinata la direzione di discesa formula_27, la funzione di una variabile formula_28 in maniera analitica o in maniera approssimata. Si noti che, a seconda della scelta del passo di discesa, l'algoritmo potrà convergere a uno qualsiasi dei minimi della funzione formula_10, sia esso locale o globale.
Lo schema generale per l'ottimizzazione di una funzione formula_18 mediante metodo del gradiente è il seguente:
Un caso particolare di applicazione del metodo del gradiente consiste nella risoluzione di sistemi lineari della forma
dove formula_33 è una matrice simmetrica e definita positiva.
Per le proprietà di formula_33 la soluzione di tale problema è equivalente alla procedura di minimizzazione della forma quadratica associata:
Infatti:
da cui
Per la funzione formula_38 si ha che la direzione di massima discesa nel punto formula_39 è:
coincidente con il residuo formula_41 del sistema lineare. Dunque la direzione di discesa scelta a ogni iterazione è formula_42.
Inoltre vale la seguente relazione:
che permette di calcolare analiticamente il passo formula_44 ottimale. Infatti, imponendo la condizione di stazionarietà
si ricava
L'algoritmo del metodo del gradiente per la risoluzione di sistemi lineari è dunque
In aritmetica floating point la condizione del ciclo while può essere valutata verificando che la norma del residuo formula_48 non sia più piccola di una tolleranza impostata dall'utente.
In molti casi è possibile accelerare la velocità di convergenza dell'algoritmo migliorando le proprietà di condizionamento della matrice formula_33. Si introduca a tal fine una matrice di precondizionamento formula_50 simmetrica e definita positiva.
Lo schema risolutivo in questo caso diventa:
Il metodo del gradiente coniugato costituisce una variante del metodo del gradiente in cui viene effettuata una scelta diversa, ma particolarmente conveniente nel caso di sistemi lineari simmetrici e definiti positivi, per le direzioni di discesa formula_27. Tale scelta garantisce la convergenza del metodo (in aritmetica esatta) in un numero di iterazioni pari al più alla dimensione del sistema da risolvere.
È possibile dimostrare che l'errore commesso alla formula_26-esima iterazione del metodo del gradiente soddisfa la seguente stima:
dove
formula_56 indica il numero di condizionamento in norma formula_57 di formula_33 e formula_59 è la norma indotta da formula_33.
Nel caso precondizionato vale la stessa stima con
Si riporta un esempio di possibile implementazione del metodo del gradiente nella versione precondizionata compatibile con i linguaggi di programmazione Octave e MATLAB.
Quando la funzione obiettivo è troppo costosa da calcolare ad ogni iterazione, ma può essere scomposta in una somma di molti addendi (ad esempio, la somma del costo calcolato su ogni singolo record in un dataset), il gradiente può essere approssimato stocasticamente restringendo la somma su un sottinsieme di addendi ad ogni iterazione, metodo noto come discesa stocastica del gradiente.
La discesa del gradiente è ampiamente utilizzata in statistica e apprendimento automatico per l'addestramento tramite apprendimento supervisionato di modelli come reti neurali artificiali e modelli grafici. Il principio è noto come regola delta, e consiste nel valutare il modello su un input il cui corrispondente output esatto sia noto, e correggere ciascun parametro del modello in una quantità proporzionale (ma di segno opposto) rispetto al suo contributo all'errore sul risultato. L'algoritmo usato nelle reti neurali per implementare questo principio è noto come retropropagazione dell'errore, che consiste in un'applicazione della discesa del gradiente, essendo il contributo di ciascun parametro all'errore del modello dato dalla derivata parziale della funzione di perdita rispetto al parametro stesso.
La regola, classificabile fra i metodi per l'apprendimento supervisionato, può essere applicata a reti neurali di tipo "in avanti" (cioè con propagazione unidirezionale dei segnali, in inglese: "feedforward") e permette di calcolare la differenza tra i valori di output che la rete ottiene e quelli che invece dovrebbe apprendere. La regola deve essere applicata a reti che usano unità di output ad attivazione continua e differenziabile ed è l'elemento fondamentale dell'algoritmo di retropropagazione dell'errore ("backpropagation"), alla base dell'approccio connessionista.
Data una rete "in avanti" con le proprietà sopra descritte, l'obiettivo che ci si prefigge è minimizzare la diversità tra i valori di attivazione delle unità di output formula_62 della rete (ottenuti sommando i segnali provenienti dalle diverse unità di input formula_63 moltiplicati per l'efficacia, o "pesi sinaptici" formula_64 delle connessioni in ingresso), e i valori formula_65 della risposta desiderata. Tale diversità viene quantificata attraverso una funzione di perdita. La funzione obiettivo che si vuole minimizzare è il valore atteso della perdita (in pratica la perdita media sui dati).
Per applicare il metodo del gradiente, la funzione di perdita deve essere derivabile rispetto ai valori di output formula_62. Una scelta adatta a problemi di regressione è lo scarto quadratico medio tra formula_62 e formula_65 (valutato per tutte le unità di output e per tutti i pattern d'apprendimento); per problemi di classificazione si può utilizzare la divergenza di Kullback-Leibler o equivalentemente l'entropia incrociata.
Nella fase di addestramento, variando i pesi sinaptici formula_64 (parametri del modello) si può aumentare o diminuire la funzione obiettivo; la "prestazione" della rete sarà funzione delle variabili formula_64, e sarà massima quando si raggiunge il minimo della funzione obiettivo, il che si ottiene applicando il metodo del gradiente e aggiornando iterativamente i valori dei pesi sinaptici.
Poiché nelle applicazioni pratiche le dimensioni dei modelli e dei relativi dataset usati nell'addestramento sono molto grandi, in pratica si fa generalmente uso della discesa stocastica del gradiente per l'addestramento delle reti neurali e di altri modelli statistici e di apprendimento automatico.
</text>
</doc>
<doc id="3216546" url="https://it.wikipedia.org/wiki?curid=3216546">
<title>Analisi dei dati</title>
<text>
Nell'ambito della scienza dei dati l'analisi dei dati è un processo di ispezione, pulizia, trasformazione e modellazione di dati con il fine di evidenziare informazioni che suggeriscano conclusioni e supportino le decisioni strategiche aziendali. L'analisi di dati ha molti approcci e sfaccettature, il che comprende tecniche diversissime tra loro che si riconoscono con una serie di definizioni varie nel commercio, le scienze naturali e sociali.
Il data mining è una tecnica particolare di analisi dei dati che si focalizza nella modellazione e scoperta di conoscenza per scopi predittivi piuttosto che descrittivi. Il business intelligence identifica l'analisi di dati che si basa fondamentalmente sull'aggregazione, focalizzandosi sulle informazioni aziendali. Nell'ambito dei big data si parla di big data analytics. Nelle applicazioni statistiche, gli studiosi dividono l'analisi dei dati in statistica descrittiva, analisi dei dati esplorativa (ADE) e analisi dei dati di conferma (ADC). L'ADE si concentra sullo scoprire nuove caratteristiche presenti nei dati, mentre l'ADC nel confermare o falsificare le ipotesi esistenti. L'analisi predittiva si concentra sull'applicazione di modelli statistici o strutturali per classificazione o il forecasting predittivo, mentre l'analisi testuale applica tecniche statistiche, linguistiche e strutturali per estrarre e classificare informazioni da fonti testuali, una categoria di dati non-strutturati.
L'integrazione di dati è un precursore dell'analisi dei dati, la quale è collegata alla visualizzazione di dati.
</text>
</doc>
<doc id="3112560" url="https://it.wikipedia.org/wiki?curid=3112560">
<title>Matrice di confusione</title>
<text>
Nell'ambito dell'Intelligenza artificiale, la matrice di confusione, detta anche tabella di errata classificazione, restituisce una rappresentazione dell'accuratezza di classificazione statistica.
Ogni colonna della matrice rappresenta i valori predetti, mentre ogni riga rappresenta i valori reali. L'elemento sulla riga i e sulla colonna j è il numero di casi in cui il classificatore ha classificato la classe "vera" i come classe j.
Attraverso questa matrice è osservabile se vi è "confusione" nella classificazione di diverse classi.
Attraverso l'uso della matrice di confusione è possibile calcolare il coefficiente kappa, anche conosciuto come coefficiente kappa di Cohen.
Esaminiamo il caso di una classificazione dove si distinguono tre classi: gatto, cane e coniglio. Nelle righe si scrivono i valori veri, reali. Mentre nelle colonne quelli predetti, stimati dal sistema. 
Nell'esempio si può notare che dei 7 gatti reali, il sistema ne ha classificati 2 come cani. Allo stesso modo si può notare come dei 12 conigli veri, solamente 1 è stato classificato erroneamente. Gli oggetti che sono stati classificati correttamente sono indicati sulla diagonale della matrice, per questo è immediato osservare dalla matrice se il classificatore ha commesso o no degli errori.
Inoltre, è possibile ottenere due valori di accuratezza significativi:
Nel caso della classe "gatto", questo ha i seguenti valori (vedi la matrice qui sopra):
Nell'apprendimento automatico questa tabella può anche essere utilizzata con i valori di "veri positivi"/"falsi positivi" e "falsi negativi"/"veri negativi".
Così facendo è possibile calcolare:
</text>
</doc>
<doc id="5340062" url="https://it.wikipedia.org/wiki?curid=5340062">
<title>Modellazione dei dati</title>
<text>
La modellazione dei dati o data modeling in Ingegneria del software è il processo di creazione di un modello dei dati per un sistema informativo applicando tecniche formali di modellazione dei dati.
La modellazione dei dati è un processo usato per definire e analizzare i requisiti dei dati di cui si ha bisogno per supportare i processi aziendali nell'ambito dei corrispondenti sistemi informativi nelle organizzazioni.
Quindi, il processo coinvolge professioni dei sistemi informativi esperti di modellazione dei dati che lavorano a stretto contatto con il lato business dell'impresa, come con i potenziali utenti del sistema informativo.
</text>
</doc>
<doc id="2354612" url="https://it.wikipedia.org/wiki?curid=2354612">
<title>Clustering gerarchico</title>
<text>
In statistica e apprendimento automatico, il clustering gerarchico è un approccio di clustering che mira a costruire una gerarchia di cluster. Le strategie per il clustering gerarchico sono tipicamente di due tipi:
Il risultato di un clustering gerarchico è rappresentato in un dendrogramma.
Per decidere quali cluster devono essere combinati (approccio agglomerativo) o quale cluster deve essere suddiviso (approccio divisivo) è necessario definire una misura di dissimilarità tra cluster. Nella maggior parte dei metodi di clustering gerarchico si fa uso di metriche specifiche che quantificano la distanza tra coppie di elementi e di un criterio di collegamento che specifica la dissimilarità di due insiemi di elementi (cluster) come funzione della distanza a coppie tra elementi nei due insiemi.
La scelta di una metrica appropriata influenza la forma dei cluster, poiché alcuni elementi possono essere più "vicini" utilizzando una distanza e più "lontani" utilizzandone un'altra. Per esempio, in uno spazio a 2 dimensioni, la distanza tra il punto (1, 1) e l'origine (0, 0) è 2, formula_1 or 1 se si utilizzando rispettivamente le norme 1, 2 o infinito.
Metriche comuni sono le seguenti:
Il criterio di collegamento ("linkage criterion") specifica la distanza tra insiemi di elementi come funzione di distanze tra gli elementi negli insiemi.
Dati due insiemi di elementi "A" e "B" alcuni criteri comunemente utilizzati sono:
dove "d" è la metrica prescelta per determinare la similarità tra coppie di elementi.
</text>
</doc>
<doc id="2496382" url="https://it.wikipedia.org/wiki?curid=2496382">
<title>Convalida incrociata</title>
<text>
La convalida incrociata ("cross-validation" in inglese) è una tecnica statistica utilizzabile in presenza di una buona numerosità del campione osservato o "training set". In particolare la k-fold cross-validation consiste nella suddivisione del dataset totale in k parti di uguale numerosità e, ad ogni passo, la k-esima parte del dataset viene ad essere il "validation dataset", mentre la restante parte costituisce il "training dataset". Così, per ognuna delle "k" parti si allena il modello, evitando quindi problemi di overfitting, ma anche di campionamento asimmetrico (e quindi affetto da bias) del "training dataset", tipico della suddivisione del "dataset" in due sole parti (ovvero "training" e "validation dataset"). In altre parole, si suddivide il campione osservato in gruppi di egual numerosità, si esclude iterativamente un gruppo alla volta e lo si cerca di predire con i gruppi non esclusi. Ciò al fine di verificare la bontà del modello di predizione utilizzato.
</text>
</doc>
<doc id="2398076" url="https://it.wikipedia.org/wiki?curid=2398076">
<title>Selezione naturale</title>
<text>
La selezione naturale, concetto introdotto da Charles Darwin nel 1859 nel libro "L'origine delle specie", e indipendentemente da Alfred Russel Wallace nel saggio "On the Tendency of Varieties to Depart Indefinitely From the Original Type," è un meccanismo chiave dell'evoluzione e secondo cui, nell'ambito della diversità genetica delle popolazioni, si ha un progressivo (e cumulativo) aumento degli individui con caratteristiche ottimali per l'ambiente in cui vivono.
In riferimento alla competizione tra individui, Darwin, ispirandosi a Thomas Malthus, descrisse il concetto di "lotta per l'esistenza", che si basava sull'osservazione che gli organismi, moltiplicandosi con un ritmo troppo elevato, producono una progenie quantitativamente superiore a quella che le limitate risorse naturali possono sostenere, e di conseguenza sono costretti a una dura competizione per raggiungere lo stato adulto e riprodursi.
Gli individui di una stessa specie si differenziano l'uno dall'altro per caratteristiche genetiche (genotipo) e quindi fenotipiche (cioè morfologiche e funzionali, frutto dell'interazione del genotipo con l'ambiente). La teoria della selezione naturale prevede che all'interno di tale variabilità, derivante da mutazioni genetiche casuali (nel senso che esse sono casuali rispetto a un adattamento), nel corso delle generazioni successive al manifestarsi della mutazione, vengano favorite ("selezionate") quelle mutazioni che portano gli individui ad avere caratteristiche più vantaggiose in date condizioni ambientali, determinandone, cioè, un vantaggio adattativo (migliore adattamento) in termini di sopravvivenza e riproduzione. La selezione naturale quindi, favorendo le mutazioni vantaggiose, funziona come meccanismo deterministico, opposto al caso. 
Gli individui meglio adattati a un certo habitat si procureranno più facilmente il cibo e si accoppieranno più facilmente degli altri individui della stessa specie che non presentano tali caratteristiche. In altre parole, è l'ambiente a selezionare le mutazioni secondo il criterio di vantaggiosità sopra descritto: i geni forieri di vantaggio adattativo potranno così essere trasmessi, attraverso la riproduzione, alle generazioni successive e con il susseguirsi delle generazioni si potrà avere una progressiva affermazione dei geni "buoni" a discapito dei geni inutili o dannosi. La specie potrà evolversi progressivamente grazie allo sviluppo di caratteristiche che la renderanno meglio adattata all'ambiente, sino a una situazione di equilibrio tra ambiente e popolazione che persisterà finché un cambiamento ambientale non innescherà un nuovo fenomeno evolutivo.
Esempio tipico è l'evoluzione del collo delle giraffe. Nel corso di milioni di anni le mutazioni genetiche che portarono alcuni individui ad avere un collo (sempre) più lungo si rivelarono vantaggiose; questi individui potevano raggiungere più facilmente le foglie di alberi alti, il che, in condizioni di scarsità di cibo, determinò un migliore adattamento all'ambiente rispetto agli individui col collo più corto: migliore capacità di procurarsi il cibo, quindi maggiore probabilità di sopravvivere, di raggiungere l'età della riproduzione e di riprodursi, dunque, maggiore probabilità di trasmettere il proprio patrimonio genetico (e quindi la lunghezza del collo) alle generazioni successive.
I principi fondamentali su cui si basa la selezione naturale sono:
Il concetto della selezione naturale sviluppato da Charles Darwin nel libro "The Origin of species", pubblicato nella sua prima edizione nel 1859, è stato successivamente integrato con la genetica mendeliana nel libro pubblicato da Ronald Fisher nel 1930, "The Genetical theory of natural selection", che è considerato uno dei più importanti documenti della Moderna sintesi dell’evoluzione.
Le variazioni del fenotipo all'interno di una popolazione derivano da variazioni del genotipo, ma possono a volte essere influenzate dall'ambiente e dalle interazioni gene/ambiente.
Un gene per un determinato carattere può esistere, all'interno di una popolazione, sotto forma di versioni diverse, denominate alleli.
Solo alcuni caratteri sono il risultato dell'espressione di un singolo locus, mentre la maggior parte delle caratteristiche fenotipiche, come il colore degli occhi e il colore della pelle, derivano dalla cooperazione dei prodotti di più coppie di geni. Una mutazione in uno di questi geni può determinare solo una piccola variazione del carattere, mentre mutazioni in più geni, che avvengono in maniera progressiva nel corso delle generazioni, hanno un effetto cumulativo producendo notevoli differenze nei fenotipi.
Inoltre, ci sono casi in cui la selezione di un carattere è correlata alla selezione di un altro carattere; questo si verifica perché i geni corrispondenti sono intimamente associati in "loci" molto vicini nello stesso cromosoma, oppure perché ci sono geni che possono influenzare più caratteri contemporaneamente.
La frequenza allelica, calcolata come il rapporto tra il numero degli alleli uguali, presenti all'interno di una popolazione, rispetto al numero totale di alleli per un determinato "locus" genico, definisce quanto una determinata versione di un gene è rappresentata all'interno della popolazione.
La mutazione e la ricombinazione sono i meccanismi principali con i quali sono prodotti nuovi alleli, mentre la selezione naturale rappresenta il meccanismo che influenza le frequenze relative dei vari alleli all'interno di una popolazione.
Bisogna distinguere tra il meccanismo con cui agisce la selezione naturale e i suoi effetti.
La selezione naturale agisce sui fenotipi, favorendo quelli più adatti e conferendo loro un vantaggio sia di sopravvivenza sia riproduttivo.
La selezione naturale agisce indistintamente sia sulla componente ereditaria sia su quella non ereditaria dei caratteri, ma è solo sulla prima, vale a dire sul genotipo, che si manifestano i suoi effetti, poiché è solo questa che è trasmessa alla progenie.
Fondamentale nella descrizione della selezione è il concetto di fitness, che misura la capacità di un genotipo di riprodursi e di trasmettersi alla generazione successiva, conferendo, se superiore a quella media, un vantaggio riproduttivo all'individuo che lo possiede.
Di conseguenza, genotipi con fitness elevata aumenteranno di frequenza nelle generazioni successive e diventeranno i più rappresentati, mentre genotipi con fitness bassa, diventeranno sempre meno frequenti, fino alla scomparsa.
La fitness si riferisce, quindi, alla capacità di produrre prole; poiché il numero di discendenti che un individuo può generare dipende sia dalla sua capacità di arrivare allo stato adulto, sia dalla sua fertilità, possiamo considerare la fitness come il prodotto di due componenti, la vitalità e la fertilità:
fenotipi che aumentano la capacità di sopravvivenza di un individuo, ma lo rendono sterile hanno fitness nulla.
La fitness è influenzata dall'ambiente in cui l'organismo vive, infatti, lo stesso fenotipo può avere fitness diverse in ambienti diversi.
La fitness può essere misurata in vari modi:
La "fitness assoluta" W, di un determinato genotipo, si riferisce al rapporto del numero d'individui con quel genotipo dopo un evento selettivo (N2) rispetto al loro numero prima dell'evento selettivo(N1):
formula_1
La fitness assoluta può essere calcolata all'interno della stessa generazione, immediatamente prima e dopo l'evento selettivo (ad esempio l'introduzione di un parassita, l'aumento di temperatura o l'esposizione a un antibiotico, per quanto riguarda le colonie batteriche); più in generale, la fitness si misura dopo una generazione, mettendo in rapporto il numero di discendenti con un determinato genotipo rispetto a quello della generazione precedente (in questo caso N2 rappresenta il numero di figli e N1 il numero dei genitori con lo stesso genotipo).
Un altro sistema di misura della fitness mette in relazione le frequenze geniche riscontrate nella progenie rispetto alle frequenze geniche attese secondo l'equazione di Hardy- Weinberg, misurando in questo caso la deviazione da quest'equilibrio.
Più usata è la "fitness relativa", ω, che si calcola assumendo che il genotipo più rappresentato all'interno della popolazione, con W = Wmax, abbia una fitness relativa ωmax = 1, mentre le fitness relative degli altri genotipi derivano dal rapporto della loro fitness assoluta rispetto a quella massima:
formula_2
Di conseguenza omega può assumere valori compresi tra zero e uno:
0&lt;ω&lt;1
Un altro parametro di misura della selezione naturale è il "coefficiente di selezione", s, misurato secondo l'equazione:
s = 1 – ω,
che ha un significato opposto a quello della fitness, infatti un suo aumento significa una diminuzione delle frequenze alleliche corrispondenti nella popolazione.
All'interno di una popolazione naturale, la variabilità tra gli individui, requisito essenziale della selezione, può esistere a vari livelli: morfologico, cellulare, subcellulare, biochimico, genico.
Occorre precisare che, anche se la fitness è spesso riferita al genotipo, il bersaglio della selezione naturale è l'organismo "in toto", e deriva dalle interazioni delle diverse fitness genotipiche. Ogni gene, infatti, può contribuire aumentando o diminuendo la capacità riproduttiva di un individuo, o la sua sopravvivenza, ma la fitness totale sarà il risultato delle diverse pressioni selettive.
Molto spesso, i fenotipi di determinati caratteri, ad esempio l'altezza degli individui o il loro peso corporeo, variano in maniera continua e le loro frequenze possono essere distribuite secondo una curva a campana, distribuzione normale, con un massimo di frequenza in corrispondenza del fenotipo medio. In altri casi, come la presenza/assenza di una determinata malattia ereditaria o la resistenza/sensibilità agli antibiotici, riscontrata nelle popolazioni batteriche, il fenotipo può presentarsi in maniera discontinua, sotto forma di due o poche varianti.
La selezione naturale può essere distinta in tre tipi:
Si parla inoltre di selezione negativa, intendendo la rimozione selettiva di alleli rari che sono dannosi. Viene detta anche selezione purificante e può portare al mantenimento delle sequenze geniche, conservate tra le specie, per lunghi periodi di tempo evolutivo. La progressiva epurazione di alleli dannosi, per il costante ripresentarsi di nuove mutazioni dannose, si ricollega alla selezione ambientale.
Darwin, nel capitolo 4 del suo libro "The Origin of Species" dedicò un paragrafo alla definizione di un altro tipo di selezione, distinta da quella ecologica, che veniva descritta come il processo attraverso il quale un individuo acquisisce vantaggio rispetto a un altro dello stesso sesso, grazie alla sua capacità di accoppiarsi con un maggior numero di partner e, di conseguenza, di avere un maggior numero di discendenti.
Questo tipo di selezione, nota come selezione sessuale, interessa generalmente gli individui di sesso maschile e si può manifestare con due tipi di comportamenti:
Questo tipo di selezione conduce all'evoluzione di armi speciali o di caratteri ornamentali che si definiscono caratteri sessuali secondari perché, a differenza di quelli primari, non sono coinvolti direttamente nella riproduzione ma offrono un vantaggio nell'accoppiamento. Le relative differenze tra maschio e femmina determinano il dimorfismo sessuale. Darwin attribuì una notevole importanza al dimorfismo sessuale, che descrisse in un gran numero di specie. La comparazione tra le specie dimostrava che tale dimorfismo era più diffuso nelle specie poligame, nelle quali il maschio poteva accoppiarsi con più femmine, rispetto a quelle monogame.
Sebbene la teoria classica della selezione naturale consideri l'individuo come il principale bersaglio delle forze selettive, esiste un altro approccio (benché molto controverso e minoritario) che prende in considerazione la presenza di una selezione multilivello, con forze selettive che agiscono sui vari livelli della gerarchia biologica, cioè i geni, la cellula, l'individuo, il gruppo e la specie.
Da questa considerazione deriva la possibilità dell'esistenza di forze selettive che agiscono su caratteristiche o comportamenti condivisi da gruppi d'individui o da intere popolazioni.
L'ipotesi della selezione di gruppo è stata introdotta nel 1962 da Wynne-Edwards, il quale sosteneva che alcuni individui, in determinate circostanze, possono sacrificare la loro riproduzione per offrire un vantaggio al gruppo a cui appartengono. In altre parole, ci sarebbero forze selettive che agiscono sulla competizione tra gruppi e non su quella tra individui e, di conseguenza, si osserverebbe una diminuzione della fitness individuale per la sopravvivenza del gruppo. È ciò che succede, ad esempio, nelle popolazioni di insetti sociali. In alcune occasioni, secondo Wynne-Edwards, gli individui possono limitare la loro riproduzione, ad esempio quando scarseggia il cibo, per evitare il sovraffollamento con successivo impoverimento delle risorse alimentari.
Esistono numerose controversie riguardo all'esistenza della selezione di gruppo e, secondo un'opinione abbastanza condivisa, la maggior parte dei casi di questo tipo di selezione si possono spiegare con una selezione individuale.
Uno dei maggiori problemi sollevati dagli oppositori di questa teoria è che, diminuendo la fitness individuale, i tratti legati a questo tipo di comportamento "altruistico" non possono essere trasmessi nelle generazioni.
Nel 1964 W.D.Hamilton, rielaborando un'ipotesi espressa da J.B.S.Haldane nel 1932, introdusse il concetto di selezione di parentela o "kin selection". 
Secondo questo zoologo, il comportamento altruistico è favorito solo se gli individui che ricevono i benefici sono imparentati con l'individuo altruista, cioè quando esiste una forte condivisione di geni tra l'altruista e il beneficiario e, di conseguenza, i geni possono essere trasmessi alle generazioni successive. In questi casi, la fitness individuale viene sostituita da una "fitness inclusive".
Hamilton espresse matematicamente questa sua tesi con la seguente equazione:
rb&gt;c
dove r rappresenta la frazione dei geni condivisi tra individui altruisti e beneficiari;
b è il beneficio del ricevente e c il costo pagato dall'individuo altruista.
Con questa teoria si possono spiegare le cure parentali dei genitori per i figli e anche i comportamenti altruistici riscontrati nelle popolazioni di insetti sociali, i cui individui condividono un'alta percentuale di geni.
La teoria della selezione di parentela è stata in seguito sostenuta anche da altri studiosi, tra cui Maynard-Smith.
La selezione naturale è alla base dei processi di adattamento e speciazione, e quindi dell'evoluzione delle specie.
L'adattamento è l'insieme delle caratteristiche, sia strutturali sia comportamentali, che sono state favorite dalla selezione naturale perché aumentano le possibilità di sopravvivenza e di riproduzione di un organismo nel suo habitat naturale.
L'adattamento è la conseguenza dei cambiamenti del pool genico che avvengono all'interno delle popolazioni in seguito alle pressioni selettive dell'ambiente, che favoriscono individui con fitness più elevata. Variazioni del pool genico possono risultare, oltre che dalla selezione naturale, anche dalla deriva genetica.
Tuttavia, si deve precisare che il concetto di adattamento è relativo, perché ciò che può essere adatto in un ambiente può non esserlo in un altro e, in seguito a variazioni ambientali, caratteristiche vantaggiose possono diventare svantaggiose e/o viceversa. Inoltre, in natura l'adattamento non è mai perfetto, ma risulta come compromesso tra le esigenze adattative delle diverse caratteristiche di un organismo.
Un classico esempio di adattamento, descritto da Darwin, è rappresentato dal becco del picchio, perfettamente adattato a estrarre gli insetti dalla corteccia degli alberi. Altro esempio tipico è il mimetismo criptico, con il quale il colore e la forma di alcuni animali si adatta perfettamente all'ambiente in cui vivono, proteggendoli dai predatori.
Esempi tipici di evoluzione adattativa si possono facilmente osservare nelle popolazioni di microrganismi, come batteri o virus, grazie ai loro tempi di generazioni molto brevi. Ad esempio in una popolazione di batteri esistono individui che contengono geni conferenti resistenza ad alcuni farmaci, come gli antibiotici. In assenza di tale farmaco, tutti gli individui hanno la stessa probabilità di sopravvivenza, ma se esposti all'antibiotico specifico, i batteri sensibili saranno inibiti, mentre quelli che contengono il gene della resistenza potranno moltiplicarsi indisturbati e, dopo poche generazioni, la popolazione sarà formata quasi esclusivamente da batteri resistenti; la popolazione si è dunque adattata. Alcuni virus, come il virus dell'influenza e il virus dell'immunodeficienza acquisita, subiscono una continua evoluzione, per la presenza di ceppi resistenti alle terapie e alle reazioni immunitarie dell'ospite.
Se, da una parte, è relativamente semplice spiegare l'evoluzione adattativa di caratteristiche fenotipiche semplici, che derivano da uno o pochi geni, come il mimetismo, la resistenza ai farmaci, ecc., risulta più complicato seguire l'evoluzione di un organo complesso, ad esempio l'occhio dei vertebrati.
Secondo la teoria evoluzionistica, questi caratteri complessi, che derivano da più geni, hanno subito adattamenti successivi, verificatisi in molte tappe, ciascuna delle quali ha coinvolto mutazioni in geni differenti che hanno conferito un vantaggio addizionale agli individui portatori. Si è verificata quindi una lenta e progressiva evoluzione che ha portato allo sviluppo di organi specializzati in determinate funzioni.
Poiché l'adattamento rappresenta una risposta degli organismi alle pressioni selettive dell'ambiente, che si realizza con lo sviluppo di organi che hanno una determinata funzione, esso riesce a spiegare anche altri fenomeni evolutivi, come la convergenza evolutiva, in base alla quale specie diverse, vivendo in ambienti simili e quindi sottoposte allo stesso tipo di selezione, hanno sviluppato organi o funzioni simili.
Gli organi vestigiali, a loro volta, sono strutture rappresentanti un retaggio del passato, che si erano selezionate per una specifica funzione e che, per cambiamenti ambientali avvenuti in seguito, hanno perso il loro significato.
La selezione naturale è alla base della speciazione, il processo evolutivo che conduce alla formazione di nuove specie. Generalmente la speciazione avviene quando popolazioni della stessa specie sono separate da barriere geografiche o comportamentali e sono quindi sottoposte a pressioni selettive differenti, che conducono alla divergenza delle loro strutture anatomiche, fino a quando le differenze accumulate producono popolazioni di individui nettamente distinte e incapaci di accoppiarsi. Esistono quattro tipi di speciazione: allopatrica, peripatrica, parapatrica e simpatrica. Un classico esempio di speciazione allopatrica è quello proposto da Darwin, che descrisse quattordici specie di fringuelli in diverse isole delle Galápagos, tutte derivanti da un'unica specie parentale.
</text>
</doc>
<doc id="263204" url="https://it.wikipedia.org/wiki?curid=263204">
<title>Macchine a vettori di supporto</title>
<text>
Le macchine a vettori di supporto (SVM, dall'inglese "Support-Vector Machines") sono dei modelli di apprendimento supervisionato associati ad algoritmi di apprendimento per la regressione e la classificazione. Dato un insieme di esempi per l'addestramento (training set), ognuno dei quali etichettato con la classe di appartenenza fra le due possibili classi, un algoritmo di addestramento per le SVM costruisce un modello che assegna i nuovi esempi ad una delle due classi, ottenendo quindi un classificatore lineare binario non probabilistico. Un modello SVM è una rappresentazione degli esempi come punti nello spazio, mappati in modo tale che gli esempi appartenenti alle due diverse categorie siano chiaramente separati da uno spazio il più possibile ampio. I nuovi esempi sono quindi mappati nello stesso spazio e la predizione della categoria alla quale appartengono viene fatta sulla base del lato nel quale ricade.
Oltre alla classificazione lineare è possibile fare uso delle SVM per svolgere efficacemente la classificazione non-lineare utilizzando il metodo kernel, mappando implicitamente i loro input in uno spazio delle feature multi-dimensionale.
Quando gli esempi non sono etichettati è impossibile addestrare in modo supervisionato e si rende necessario l'apprendimento non supervisionato, questo approccio cerca di identificare i naturali cluster in cui si raggruppano i dati, mappando successivamente i nuovi dati nei cluster ottenuti. L'algoritmo di clustering a vettori di supporto, creato da Hava Siegelmann e Vladimir N. Vapnik, applica le statistiche dei vettori di supporto, sviluppate negli algoritmi delle SVM, per classificare dati non etichettati, ed è uno degli algoritmi di clustering maggiormente utilizzato nelle applicazioni industriali.
Le macchine a vettori di supporto possono essere pensate come una tecnica alternativa per l'apprendimento di classificatori polinomiali, contrapposta alle tecniche classiche di addestramento delle reti neurali artificiali.
Le reti neurali ad un solo strato hanno un algoritmo di apprendimento efficiente, ma sono utili soltanto nel caso di dati linearmente separabili. Viceversa, le reti neurali multistrato possono rappresentare funzioni non lineari, ma sono difficili da addestrare a causa dell'alto numero di dimensioni dello spazio dei pesi e poiché le tecniche più diffuse, come la "back-propagation", permettono di ottenere i pesi della rete risolvendo un problema di ottimizzazione non convesso e non vincolato che, di conseguenza, presenta un numero indeterminato di minimi locali.
La tecnica di addestramento SVM risolve entrambi i problemi: presenta un algoritmo efficiente ed è in grado di rappresentare funzioni non lineari complesse. I parametri caratteristici della rete sono ottenuti mediante la soluzione di un problema di programmazione quadratica convesso con vincoli di uguaglianza o di tipo box (in cui il valore del parametro deve essere mantenuto all'interno di un intervallo), che prevede un unico minimo globale.
Formalmente, una macchina a vettori di supporto costruisce un iperpiano o un insieme di iperpiani in uno spazio a più dimensioni o a infinite dimensioni, il quale può essere usato per classificazione, regressione e altri scopi come il rilevamento delle anomalie. Intuitivamente una buona separazione si può ottenere dall'iperpiano che ha la distanza maggiore dal punto (del training set) più vicino di ognuna delle classi; in generale maggiore è il margine fra questi punti, minore è l'errore di generalizzazione commesso dal classificatore.
Mentre il problema originale può essere definito in uno spazio di finite dimensioni, spesso succede che gli insiemi da distinguere non siano linearmente separabili in quello spazio. Per questo motivo è stato proposto che lo spazio originale di dimensioni finite venisse mappato in uno spazio con un numero di dimensioni maggiore, rendendo presumibilmente più facile trovare una separazione in questo nuovo spazio. Per mantenere il carico computazionale accettabile, i mapping utilizzati dalle SVM sono fatti in modo tale che i prodotti scalari dei vettori delle coppie di punti in input siano calcolati facilmente in termini delle variabili dello spazio originale, attraverso la loro definizione in termini di una funzione kernel formula_1scelta in base al problema da risolvere. Gli iperpiani in uno spazio multidimensionale sono definiti come l'insieme di punti il cui prodotto scalare con un vettore in quello spazio è costante, dove tale insieme di vettori è un insieme ortogonale (e quindi minimale) di vettori che definiscono un iperpiano. I vettori che definiscono gli iperpiani possono essere scelti come combinazioni lineari con parametri formula_2delle immagini dei vettori delle feature formula_3. Con tale scelta dell'iperpiano, i punti formula_4 nello spazio delle feature che sono mappati nell'iperpiano sono definiti dalla relazione formula_5. Si noti che se formula_1 diventa più piccolo al crescere di formula_7 rispetto ad formula_4, ogni termine della somma misura il grado di vicinanza del punto di test formula_4 al corrispondente punto di base formula_3. Si noti che l'insieme di punti formula_4 mappato in un qualsiasi iperpiano può produrre un risultato piuttosto complicato, permettendo discriminazioni molto più complesse fra insiemi non completamente convessi nello spazio originario.
L'originale algoritmo SVM è stato inventato da Vladimir Vapnik e Aleksej Červonenkis nel 1963.
Nel 1992 Bernhard Boser, Isabelle Guyon e lo stesso Vapnik suggerirono un modo per creare un classificatore non lineare applicando il metodo kernel all'iperpiano con il massimo margine. Lo standard corrente che propone l'utilizzo di un margine leggero fu invece proposto da Corinna Cortes e Vapnik nel 1993 e pubblicato nel 1995.
Alcune applicazioni per cui le SVM sono state utilizzate con successo
sono:
I seguenti framework mettono a disposizione un'implementazione di SVM:
</text>
</doc>
<doc id="221079" url="https://it.wikipedia.org/wiki?curid=221079">
<title>Base di conoscenza</title>
<text>
Una base di conoscenza (individuata anche con il termine inglese knowledge base e con l'acronimo KB) è un tipo speciale di database per la gestione della conoscenza per scopi aziendali, culturali o didattici. Essa costituisce dunque un ambiente volto a facilitare la raccolta, l'organizzazione e la distribuzione della conoscenza.
Una base di conoscenza di interesse aziendale, normalmente, si propone l'esplicita conoscenza di una organizzazione, inclusa quella che può servire alla risoluzione dei problemi, e concerne articoli, rapporti, manuali per gli utenti ed altro. Una base di conoscenza dovrebbe rispettare una ben progettata struttura di classificazione, osservare (pochi) determinati formati per i contenuti e disporre di un motore di ricerca.
L'aspetto più importante di una base di conoscenza è il tipo di informazione che contiene. Una base di conoscenza che diventa un sito da dove scaricare informazioni irrilevanti vede il suo ruolo compromesso, proprio come un'informazione irrilevante. Assicurarsi che le informazioni più rilevanti ed aggiornate siano presenti in una base di conoscenza è essenziale per il suo successo, per non menzionare il fatto di avere un eccellente sistema di recupero delle informazioni (motore di ricerca).
Determinare il tipo delle informazioni e dove queste risiedono nella base di conoscenza è un'attività che viene determinata in base ai processi che supportano il sistema. Un robusto processo di creazione della struttura è la spina dorsale di una base di conoscenza di successo.
</text>
</doc>
<doc id="255044" url="https://it.wikipedia.org/wiki?curid=255044">
<title>Intervallo di confidenza</title>
<text>
statistica, quando si stima un parametro, la semplice individuazione di un singolo valore è spesso non sufficiente.
È opportuno allora accompagnare la stima di un parametro con un intervallo di valori plausibili per quel parametro, che viene definito intervallo di confidenza (o intervallo di fiducia).
Se formula_1 e formula_2 sono variabili casuali con distribuzioni di probabilità che dipendono da qualche parametro formula_3 e formula_4 (dove formula_5 è un numero tra 0 e 1), allora l'intervallo casuale formula_6 è un intervallo di confidenza al formula_7 per formula_8. I valori estremi dell'intervallo di confidenza si chiamano "limiti di confidenza".
Ad esso si associa quindi un valore di probabilità cumulativa che caratterizza, indirettamente in termini di probabilità, la sua ampiezza rispetto ai valori massimi assumibili dalla variabile aleatoria misurando cioè la probabilità che l'evento casuale descritto dalla variabile aleatoria in oggetto cada all'interno di tale intervallo, graficamente pari all'area sottesa dalla curva di distribuzione di probabilità della variabile aleatoria nell'intervallo considerato.
È bene non confondere l'intervallo di confidenza con la probabilità. Data l'espressione "vi è un livello di confidenza del 95% che formula_9 sia nell'intervallo", nulla si può dire sulla probabilità che l'intervallo ottenuto contenga formula_10
Si ipotizzi di voler calcolare l'età media degli abitanti di un luogo. Supponiamo che non si conosca l'età per ogni singolo abitante. Viene allora estratto un campione casuale di abitanti di cui è possibile sapere l'età, e dal campione si tenta di inferire ("predire") l'età media per tutta la popolazione residente e la variabilità di tale dato. Questo può essere fatto calcolando, ad esempio, l'età media delle persone presenti nel campione e ipotizzando che questo valore coincida con l'età media di tutta la popolazione inclusa quella non scelta nel campione. In questo caso si è fatta una "stima puntuale". Alternativamente, a partire dalle età delle persone nel campione, si può calcolare un intervallo di valori entro il quale si ritenga ci sia il valore della media di tutta la popolazione e, se la procedura è fatta in modo rigoroso e statisticamente corretto, è possibile stabilire un valore di "confidenza" di quanto sia "credibile" che l'intervallo ottenuto contenga effettivamente il valore cercato. In questo caso si è fatta una "stima per intervalli" e l'intervallo ottenuto è detto "intervallo di confidenza".
Riassumendo: la stima puntuale fornisce un valore singolo che varia a seconda del campione, e difficilmente coincide con il valore vero della popolazione; la stima per intervalli fornisce un insieme di valori (intervallo) che con una certa "confidenza" contiene il valore vero della popolazione.
Se formula_11 è una variabile aleatoria di media formula_9 e varianza formula_13 con formula_14 si indica la variabile campionaria corrispondente che ha media aritmetica degli formula_15 dati osservati nel campione
e deviazione standard
Il livello di confidenza è fissato dal ricercatore. Il valore scelto più di frequente è 95%. Tuttavia, meno di frequente, viene scelto anche un livello di confidenza del 90%, oppure del 99%.
Se il valore di formula_18 non differisce molto dalla variabilità formula_19 della popolazione, può essere assunto come suo stimatore (ad esempio con un numero di soggetti osservati e replicazioni complessivamente maggiore di 60; in alternativa si ipotizza una distribuzione t di Student caratterizzata da una maggiore dispersione rispetto alla normale standard). In questa prima ipotesi, l'intervallo di confidenza per la media formula_9 ("vera media", della popolazione) al 99% (al livello formula_21), è dato da:
Al 95% è dato da:
Prima della diffusione dei computer si cercava di utilizzare l’approssimazione normale ogni qualvolta possibile. Adesso non è più strettamente necessario, e nella formula possono essere utilizzati percentili di altre distribuzioni, facendo rifierimento a campioni di dimensione più ridotta).
Dalle formule risulta che i due intervalli di confidenza possono essere scritti in funzione dei "soli dati campionari" formula_24.
Oltre a diminuire con il livello di confidenza, l'ampiezza dell'intervallo dipende dall'errore della stima formula_25 e diminuisce se:
Qualora la popolazione non segua il modello gaussiano, se il campione è grande a sufficienza, la variabile campionaria tende a seguire comunque una legge normale (teorema centrale del limite). In altre parole, le due formule precedenti per l'intervallo di confidenza si possono usare anche nel caso in cui non è nota la sua legge di probabilità.
Il livello di confidenza o copertura è il complemento a uno del livello di significatività formula_27: ad esempio, un intervallo di confidenza al formula_28 corrisponde a un livello di significatività di formula_29.
Gli intervalli di confidenza sono spesso confusi con altri concetti della statistica, e talora oggetto di errate interpretazioni anche da parte di ricercatori professionisti. Alcuni errori comuni:
Gli intervalli di confidenza furono introdotti da Jerzy Neyman in un articolo pubblicato nel 1937.
C'è un metodo agevole per il calcolo degli intervalli di confidenza attraverso il test di verifica d'ipotesi (secondo l'impostazione di Neyman).
Un intervallo di confidenza al 95% si può quindi ricavare da un test di verifica d'ipotesi di significatività 5%.
</text>
</doc>
<doc id="281074" url="https://it.wikipedia.org/wiki?curid=281074">
<title>Linguaggio di interrogazione</title>
<text>
In informatica un linguaggio di interrogazione (o in inglese query language o data query language DQL) è un linguaggio usato per creare query sui database e sui sistemi informativi da parte degli utenti. Serve per rendere possibile l'estrazione di informazioni dal database, attraverso il relativo DBMS, interrogando la base dei dati e interfacciandosi dunque con l'utente e le sue richieste di servizio.
Alcuni esempi di linguaggi di interrogazione sono:
IL DQL ("data query language" – linguaggio di interrogazione dei dati) secondo lo standard SQL comprende i comandi per leggere ed elaborare i dati presenti in un database, strutturato secondo il modello relazionale. Questi dati devono essere stati precedentemente inseriti attraverso il Data Manipulation Language (DML) in strutture o tabelle create con il Data Definition Language (DDL), mentre il Data Control Language (DCL) stabilisce se l'utente può accedervi.
Col comando "select" abbiamo la possibilità di estrarre i dati, in modo mirato, dal database.
dove: 
Di default il comando "select" agisce con il metodo "all", ma specificando "distinct" è possibile eliminare dai risultati le righe duplicate.
La clausola "ORDER BY" serve per ordinare i risultati in base a uno o più campi.
"Limit" (o "top", a seconda delle implementazioni) limita il numero delle righe fornite: LIMIT 10 prende le prime 10 righe della mia tabella. È anche possibile scartare un certo numero di righe all'inizio dei risultati aggiungendo un parametro a "LIMIT" o la clausola "OFFSET".
L'SQL standard non prevede alcun ordinamento se non si specifica la clausola "ORDER BY", pertanto senza di essa anche "LIMIT" ha un effetto imprevedibile.
Un esempio è il seguente:
Questa "query" estrae l'elenco di tutti gli utenti maggiorenni ordinando l'output in base al cognome.
La definizione di "select" è comunque molto più ampia, prevede molte altre opzioni ma in linea di massima con queste opzioni si compongono la maggior parte delle interrogazioni.
l'asterisco permette di includere nella selezione tutte le colonne della tabella "utenti"
Con le select è possibile anche eseguire dei calcoli:
questo produce dati estratti ma anche dati calcolati.
La clausola AS serve per dare un nome alla nuova colonna nella nuova tabella che creerà la select.
Molti DBMS supportano la clausola non standard LIMIT, che deve essere posta per ultima e può avere tre forme:
"numero_risultati" è il numero delle righe da estrarre. "pos_primo_risultato" è l'indice della prima riga da estrarre. Insieme, possono essere utilizzati per suddividere i risultati in blocchi e leggerli un po' alla volta (per esempio per comodità del DBA, o per la paginazione dei risultati mostrati da una applicazione web).
Una forma di select composto tra più tabelle con uno o più campi comuni si ottiene attraverso la clausola Join.
Le subquery possono essere inserite ovunque il linguaggio SQL ammetta un'espressione che restituisce un singolo valore e nella clausola FROM. In questo secondo caso, le subquery sono chiamate anche "tabelle derivate" (derived table).
Le subquery propriamente dette possono restituire un singolo valore, oppure un insieme di risultati, a seconda dei casi. Un esempio piuttosto semplice è quello in cui si vogliono estrarre da una tabella i valori numerici superiori alla media. Una sola Select non può leggere la media e al contempo i valori che la superano. A questo scopo si avrà una select che legge la media:
Questa query verrà inserita nella clausola WHERE della query più esterna; la subquery viene eseguita per prima:
Come si vede, da un punto di vista sintattico è necessario porre le subquery tra parentesi.
Le tabelle derivate sono un caso particolare di subquery: una Select interna estrae i dati che verranno poi interrogati dalla Select esterna.
L'esempio seguente viene utilizzato a scopo didattico, ma non rappresenta un approccio ottimale. Si supponga comunque di voler estrarre i record per i quali il campo mail non è vuoto, scegliendoli tra gli utenti la cui registrazione è stata confermata. Ecco come ottenere questo risultato con una tabella derivata:
In realtà le tabelle derivate sono utili laddove non vi sono altri approcci possibili, il che accade quando la query esterna contiene una JOIN.
</text>
</doc>
<doc id="253357" url="https://it.wikipedia.org/wiki?curid=253357">
<title>Massimo e minimo di una funzione</title>
<text>
In matematica si dice che una funzione a valori reali:
ha in un punto formula_2 del proprio dominio formula_3 un massimo globale (o assoluto) se in formula_2 assume un valore maggiore o uguale a quello che assume negli altri punti di formula_3, ovvero
Viceversa formula_7 ha un minimo globale (o assoluto) in un punto formula_2 di formula_3 se
Si dice che una funzione formula_7 ha in formula_2 un massimo locale (o relativo) se formula_2 appartiene al dominio formula_3 di formula_7, è di accumulazione per formula_3, e inoltre formula_17 in un intorno di formula_2. 
formula_7 ha invece un minimo locale (o relativo) in formula_2 se formula_2 appartiene al dominio formula_3 di formula_7, è di accumulazione per formula_3, e inoltre formula_25 in un intorno di formula_2.
In tutti questi casi, si parla di formula_2 come di "punto di massimo" (o "di minimo") "assoluto" (o "relativo").
I punti di massimo e minimo vengono anche detti punti estremanti, e i valori assunti dalla funzione in questi punti sono detti estremi della funzione.
Nel caso di una funzione derivabile di una variabile reale la condizione necessaria, ma non sufficiente, affinché un punto possa, eventualmente, essere di massimo o di minimo locale è data dal teorema di Fermat, in base al quale la derivata prima di una funzione deve annullarsi se calcolata in corrispondenza di un punto di massimo o minimo locale:
Tale condizione permette di trovare un certo numero di punti ("x", "x", ...) che si chiamano punti critici o stazionari. Naturalmente questa condizione vale per tutti i punti interni al dominio di derivabilità, cioè nei punti interni di questo insieme, mentre negli estremi dell'insieme non è detto che la derivata esista e proprio per questo motivo la condizione vale per gli intervalli aperti. Questa condizione si può dimostrare: infatti se formula_2 è un punto di massimo locale, allora in un intorno formula_30 di "x" vale che il rapporto incrementale:
formula_31
per cui passando al limite di una funzione per formula_32 si deduce che necessariamente formula_33.
Geometricamente questa condizione significa che la retta tangente nel punto "x" è orizzontale. Tale condizione non è né necessaria né sufficiente per avere un massimo o un minimo locale: infatti da un lato ci possono essere punti di massimo o minimo locale anche laddove la funzione non è derivabile, e dall'altro ci possono essere punti (di flesso) dove la derivata si annulla ma la funzione non ha massimo o minimo locale.
Possiamo utilizzare la derivata prima per classificare i punti critici. Un punto formula_2 è di massimo locale per "f" se nei suoi intorni destro e sinistro:
Viceversa è di minimo locale se:
Se infine il valore della derivata non cambia attraversando il punto formula_2 allora questo è un punto di flesso ascendente o discendente a seconda che la derivata prima rimanga sempre positiva o sempre negativa.
Alternativamente se la funzione ammette la derivata seconda in un punto, un punto è di massimo o minimo relativo se la derivata prima della funzione si annulla (quindi formula_2 è un punto stazionario) e la derivata seconda non si annulla. Più precisamente, posto che la derivata prima si annulli, se la derivata seconda risulta essere maggiore di 0, allora significa che la concavità sarà rivolta verso l'alto, perciò il punto è di minimo. Mentre, se la derivata seconda è minore di zero, significa che la concavità è rivolta verso il basso e quindi si tratterà di un punto di massimo. Se invece la derivata seconda si annulla, nel caso in cui la derivata terza sia diversa da zero, avremo in quel punto un flesso a tangenza orizzontale ascendente o discendente e, per la definizione di flesso, la funzione cambierà concavità in tale punto.
Nel caso di funzioni in più variabili, il discorso fatto è analogo, ma ad annullarsi è il differenziale (e quindi il gradiente) della funzione. 
Nel caso di funzioni di 2 variabili, per verificare se il punto è di massimo o minimo, si guarda il segno del determinante della matrice hessiana e il primo termine della matrice: 
Nel caso di funzioni di 3 o più variabili, invece, si deve studiare il segno degli autovalori della matrice hessiana (nei punti critici, cioè dove si annulla il gradiente) e:
In caso di funzioni di due o più variabili, la ricerca dei punti di massimo e minimo non si esaurisce all'interno del dominio dove la funzione è derivabile, ma si devono cercare i massimi e i minimi anche sulla frontiera, in cui in generale la funzione non è differenziabile. In tal caso, nelle funzioni di due variabili si parametrizza la frontiera e si cercano i punti di massimo e di minimo come visto per una variabile reale.
Si consideri 
Calcoliamo la derivata prima:
Calcoliamo la derivata seconda: 
La derivata prima si annulla nei punti 
Nel punto formula_43 la derivata seconda è negativa, quindi è un punto di massimo, mentre nel punto formula_44 la derivata seconda è positiva, quindi è un punto di minimo.
Si consideri la funzione di 2 variabili
Calcoliamo le derivate parziali prime:
Quindi il gradiente di formula_48 è:
I punti critici sono dati dalla soluzione del sistema:
Quindi... 
formula_51 
oppure
formula_52 
Calcoliamo le derivate parziali seconde:
Quindi la matrice hessiana di z sarà:
Basandosi sul modello:
Calcoliamo la matrice hessiana nei punti critici (anche detti "punti stazionari"):
Questa matrice ha determinante negativo (-9), quindi è un punto di sella.
Questa seconda matrice ha invece determinante positivo (27) e primo termine (-6) negativo quindi è un punto di massimo relativo.
</text>
</doc>
<doc id="3606" url="https://it.wikipedia.org/wiki?curid=3606">
<title>Quartile</title>
<text>
In statistica, in particolare in statistica descrittiva, data una distribuzione di un carattere quantitativo oppure qualitativo ordinabile (ovvero le cui modalità possano essere ordinate in base a qualche criterio), i quartili sono quei valori/modalità che ripartiscono la popolazione in quattro parti di uguale numerosità.
I quartili sono indici di posizione e rientrano nell'insieme delle statistiche d'ordine.
La differenza tra il terzo ed il primo quartile è un indice di dispersione ed è detto scarto interquartile; i quartili vengono inoltre utilizzati per rappresentare un Box-plot.
Il quartile zero, il primo, il secondo, il terzo e il quarto quartile corrispondono con le prime modalità la cui frequenza cumulata percentuale è almeno 0, 25, 50, 75 e 100 rispettivamente. Cioè, ad esempio, il primo quartile corrisponde con la modalità "i"-esima se la frequenza cumulata percentuale formula_1 e formula_2.
Il primo, il secondo e il terzo quartile in una distribuzione ordinata sono "vicini" ai valori di posizione "[n/4]", "[n/2]" e "[3n/4]".
Il secondo quartile coincide con la mediana, e divide la popolazione in due parti di uguale numerosità, delle quali il primo ed il terzo quartile sono le mediane.
Il quartile zero coincide con il valore minimo della distribuzione. Il quarto quartile coincide con il valore massimo della distribuzione.
I quartili equivalgono ai quantili "q" (quartile zero), "q" (primo quartile), "q=q" (secondo quartile), "q" (terzo quartile) e "q" (quarto quartile).
In un sondaggio fatto all'interno di una facoltà composta da 250 studenti (la popolazione statistica), si intende rilevare il carattere "Gradimento dei professori", secondo le cinque modalità "molto deluso", "insoddisfatto", "parzialmente soddisfatto", "soddisfatto", "entusiasta". Risulta che 10 studenti si dicono entusiasti dell'operato dei professori, 51 si dicono soddisfatti, 63 mediamente soddisfatti, 90 insoddisfatti, 36 molto delusi.
La distribuzione di frequenza viene rappresentata con una tabella come la seguente:
Nel caso ipotizzato, il primo quartile e la mediana sono rappresentati dalla modalità "insoddisfatto", mentre il terzo quartile è rappresentato dalla modalità "parzialmente soddisfatto". Questo significa che "almeno" la metà degli studenti non è soddisfatto dei professori e "almeno" tre quarti degli studenti non è pienamente soddisfatto.
</text>
</doc>
<doc id="3653" url="https://it.wikipedia.org/wiki?curid=3653">
<title>Regressione lineare</title>
<text>
La regressione formalizza e risolve il problema di una relazione funzionale tra variabili misurate sulla base di dati campionari estratti da un'ipotetica popolazione infinita. Originariamente Galton utilizzava il termine come sinonimo di correlazione, tuttavia oggi in statistica l'analisi della regressione è associata alla risoluzione del modello lineare. Per la loro versatilità, le tecniche della regressione lineare trovano impiego nel campo delle scienze applicate: chimica, geologia, biologia, fisica, ingegneria, medicina, nonché nelle scienze sociali: economia, linguistica, psicologia e sociologia.
Più formalmente, in statistica la regressione lineare rappresenta un metodo di stima del valore atteso condizionato di una variabile "dipendente", o "endogena", formula_1, dati i valori di altre variabili "indipendenti", o "esogene", formula_2: formula_3. L'uso dei termini "endogeno"/"esogeno" è talvolta criticato, in quanto implicherebbe una nozione di causalità che l'esistenza di una regressione non prevede; in determinati contesti, provocherebbe inoltre confusione, essendo ad esempio il concetto di esogeneità in econometria formalmente definito tramite l'ipotesi di ortogonalità alla base delle proprietà statistiche della regressione lineare col metodo dei minimi quadrati.
La prima, e ancora popolare, forma di regressione lineare è quella basata sul metodo dei minimi quadrati (si veda oltre). La prima pubblicazione contenente un'applicazione del metodo nota è datata 1805, a nome di Adrien-Marie Legendre; Carl Friedrich Gauss elabora indipendentemente lo stesso metodo, pubblicando le sue ricerche nel 1809. Sebbene Gauss sostenne di avere sviluppato il metodo sin dal 1795, la paternità delle sue applicazioni in campo statistico è normalmente attribuita a Legendre; lo stesso termine "minimi quadrati" deriva dall'espressione francese, utilizzata da Legendre, "moindres carrés".
Sia Gauss che Legendre applicano il metodo al problema di determinare, sulla base di osservazioni astronomiche, le orbite di corpi celesti intorno al sole. Eulero aveva lavorato allo stesso problema, con scarso successo, nel 1748. Nel 1821 Gauss pubblica un ulteriore sviluppo del metodo dei minimi quadrati, proponendo una prima versione di quello che è oggi noto come teorema di Gauss-Markov.
L'origine del termine "regressione" è storicamente documentata. L'espressione "reversione" era usata nel XIX secolo per descrivere un fenomeno biologico, in base al quale la progenie di individui eccezionali tende in media a presentare caratteristiche meno notevoli di quelle dei genitori, e più simili a quelle degli antenati più remoti. Francis Galton studiò tale fenomeno, applicandovi il termine, forse improprio, di regressione verso la media (o la "mediocrità").
Per Galton l'espressione "regressione" ha solo tale significato, confinato all'ambito biologico. Il suo lavoro (1877, 1885) fu in seguito esteso da Karl Pearson e George Udny Yule a un contesto statistico più generale (1897, 1903); i lavori di Pearson e Yule ipotizzano che la distribuzione congiunta delle variabili dipendenti e indipendenti abbia natura gaussiana. Tale ipotesi è in seguito indebolita da Ronald Fisher, in lavori del 1922 e 1925. Fisher in particolare ipotizza che la distribuzione "condizionata" della variabile dipendente sia gaussiana, il che non implica necessariamente che così sia per quella congiunta di variabili dipendenti e indipendenti. Sotto tale profilo, la formulazione di Fisher è più vicina a quella di Gauss del 1821.
Il modello di regressione lineare è:
dove:
Possiede delle peculiari assunzioni OLS.
Per ogni osservazione campionaria
si dispone di una determinazione formula_1 e di formula_14 determinazioni non stocastiche formula_15. Si cerca quindi una relazione di tipo lineare tra la variabile formula_1 e le formula_14 variabili deterministiche. Una prima analisi può essere condotta considerando un modello semplice a due variabili (si suppone in pratica che formula_14 sia uguale a formula_19). Un tipico esempio è riscontrabile dall'esperienza economica considerando la relazione tra Consumi (formula_20) e Reddito (formula_1). Ricercando una relazione funzionale in cui i consumi siano "spiegati" dal reddito si può ricorrere alla relazione lineare:
dove formula_24 rappresenta l'intercetta e formula_25 la pendenza della retta interpolatrice.
Generalizzando il problema a due variabili formula_26 e formula_27, scriveremo:
formula_29 è una generica funzione di formula_26 e comunemente si assume formula_31. Ponendo, senza perdita di generalità, tale condizione la formula diviene:
Quindi la variabile dipendente formula_27 viene "spiegata" attraverso una relazione lineare della variabile indipendente formula_26 (cioè: formula_35) e da una quantità casuale formula_36.
Il problema della regressione si traduce nella determinazione di formula_24 e formula_25 in modo da esprimere al ‘meglio' la relazione funzionale tra formula_27 e formula_26.
Per avvalorare di un significato statistico la scelta dei coefficienti occorre realizzare alcune ipotesi sul modello lineare di regressione:
Date queste ipotesi si calcolano i coefficienti formula_46 e formula_47 secondo il metodo dei minimi quadrati (in inglese "Ordinary Least Squares", o OLS, da cui il riferimento agli stimatori di seguito ottenuti come agli "stimatori OLS") proposto da Gauss; detta:
le stime si ottengono risolvendo:
Le soluzioni si ricavano uguagliando a zero le derivate parziali di formula_50 rispetto ad formula_24 e formula_25:
Dove formula_55 denota il numero delle osservazioni; segue:
da cui si ricavano le soluzioni:
Essendo la varianza osservata data da:
e la covarianza osservata da:
dove formula_62 denotano le medie osservate, si possono scrivere i parametri nella forma:
Si consideri il seguente problema teorico: date due variabili casuali formula_65 e formula_1, quale è il "migliore" stimatore per il valore atteso di formula_1, ossia quale stimatore presenta lo scarto quadratico medio (o MSE, dall'inglese "Mean Squared Error") minimo?
Se si utilizza uno stimatore affine che sfrutta l'informazione relativa alla variabile casuale formula_68 allora formula_69, è possibile dimostrare che lo scarto quadratico medio formula_70 è minimizzato se:
Tale osservazione fornisce una giustificazione di tipo probabilistico alle espressioni proposte sopra; si veda oltre per un'analisi formale, nel caso multivariato.
Il metodo dei minimi quadrati è esaminato nel caso bivariato, deriva una retta che interpola uno scatter di punti minimizzando la somma dei quadrati delle distanze formula_72 dei punti stessi dalla retta; il grafico fornisce un'intuizione del procedimento.
La scelta di minimizzare i "quadrati" degli formula_72 non è, ovviamente, arbitraria. Facendo ad esempio riferimento alla semplice somma degli formula_72, distanze positive (verso l'alto) e negative (verso il basso) si compenserebbero, rendendo in generale peggiore la qualità dell'interpolazione; se per contro si adottasse una funzione criterio uguale alla somma dei valori assoluti degli formula_72, non essendo la funzione valore assoluto derivabile su tutto l'asse reale non si potrebbe ricorrere all'elegante metodo di minimizzazione sopra illustrato.
Si osservi inoltre che gli formula_72 rappresentano una distanza di un tipo alquanto particolare. In geometria la distanza di un punto da una retta è infatti data dalla lunghezza del segmento che unisce il punto alla retta, perpendicolare a quest'ultima; evidentemente non è questo il caso degli formula_72. La scelta operata trova giustificazione nelle proprietà statistiche delle stime, illustrate in seguito: la particolare forma degli stimatori dei minimi quadrati sopra ottenuti consente un più semplice trattamento delle loro proprietà statistiche.
Due parole infine sul significato di regressione "lineare". Il nome di questa tecnica statistica "non" significa che nella funzione stimata la variabile dipendente formula_78 è una funzione lineare della(e) variabile(i) esplicativa(e) formula_79, ma dei "parametri" oggetto di stima (formula_24 e formula_25 sopra). La stima di una funzione del tipo:
rientra nel raggio d'azione del modello lineare, dal momento che formula_78 è una funzione lineare dei parametri formula_84, formula_85, formula_86. Per ulteriori considerazioni al riguardo, si veda l'articolo Regressione nonlineare.
Il metodo sopra illustrato può essere esteso al caso in cui più variabili contribuiscono a spiegare la variabile dipendente formula_1:
dove:
Possiede delle peculiari assunzioni OLS.
Raggruppando le osservazioni delle variabili esplicative in una matrice formula_65 di dimensioni formula_107, che si ipotizza avere rango pieno e uguale a formula_108 (il termine costante, o intercetta, corrisponde ad avere una colonna di formula_19 nella formula_65), è possibile scrivere, in notazione matriciale:
Nella formulazione più elementare, si assume che formula_112, ossia: formula_113 formula_114 (omoschedasticità), formula_115 (assenza di correlazione nei disturbi). Si ipotizza inoltre che:
ossia che non vi sia correlazione tra i regressori e i disturbi casuali — quest'ipotesi riveste un'importanza cruciale, in quanto rende possibile considerare i regressori compresi nella matrice formula_65 come variabili "esogene" (da cui il nome con cui l'ipotesi è spesso indicata: ipotesi di "esogeneità"). Quest'ultima proprietà è tutt'altro che banale, in quanto soltanto laddove essa è valida è possibile garantire che il vettore di stime dei parametri del modello, formula_118, abbia per valore atteso il vero valore dei parametri formula_85 (godendo così della proprietà di correttezza; si veda oltre).
Sotto tali ipotesi, è possibile ottenere le stime del vettore di parametri formula_85 tramite il metodo dei minimi quadrati risolvendo il problema di minimo:
Le condizioni del primo ordine per un minimo definiscono il sistema (detto delle equazioni normali):
da cui:
Per le proprietà della forma quadratica minimizzanda, si è certi che la soluzione trovata corrisponde a un minimo, non solo locale ma globale.
Il vettore di stime OLS formula_124 consente di ottenere i valori previsti ("teorici") per la variabile dipendente:
Formalmente, l'espressione sopra corrisponde a una proiezione ortogonale del vettore delle osservazioni formula_27 sullo spazio generato dalle colonne della matrice formula_65; la figura a lato illustra questo risultato.
Per chiarire questo punto, sia formula_128 la proiezione di formula_27 sullo spazio generato dalle colonne matrice formula_65: 
Ciò significa che esisterà un vettore di pesi formula_86 tale per cui è possibile ottenere formula_128 come formula_134, ossia come combinazione lineare delle colonne di formula_65. A sua volta formula_27 sarà uguale a formula_128 più una componente formula_138 ortogonale allo spazio generato da formula_65:
Dunque formula_141; premoltiplicando per formula_142 si ha: formula_143; così che:
ossia l'espressione per il vettore di stime OLS formula_118 derivata in precedenza. Questa intuizione geometrica è formalizzata nel teorema di Frisch-Waugh-Lovell.
Gli stimatori formula_124 degli OLS godono di una serie di interessanti proprietà algebriche; tali proprietà dipendono dal metodo dei minimi quadrati adottato, e non dal particolare modello oggetto di stima.
Si osservi che le prime tre proprietà valgono solo se la matrice dei regressori include il termine costante, ossia se include un vettore di soli formula_19.
L'R², o coefficiente di determinazione, è una misura della bontà dell'adattamento (in inglese "fitting") della regressione lineare stimata ai dati osservati.
Al fine di definire l'R², sia formula_167; questa matrice trasforma i vettori in scarti dalla propria media, così che, ad esempio, formula_168. Si osservi che la matrice formula_169 è simmetrica (formula_170) e idempotente (formula_171). Dunque la somma degli scarti al quadrato delle formula_153 da formula_173 è semplicemente: formula_174.
L'R² è definito come:
Spesso le quantità al numeratore e al denominatore sono chiamate, rispettivamente, ESS (formula_176, dall'inglese "Explained Sum of Squares") e TSS (formula_174, dall'inglese "Total Sum of Squares"). Osservando che, per semplice sostituzione:
dove l'ultima uguaglianza segue dal fatto che la media dei residui è zero, si ha:
così che l'R² sarà un numero compreso tra formula_180 e formula_19 (alcuni pacchetti statistici trasformano tale numero in una percentuale); in analogia con quanto sopra, spesso la quantità formula_182 è indicata con la sigla RSS (dall'inglese "Residual Sum of Squares"), o SSR ("Sum of Squared Residuals", grammaticalmente più preciso, ma forse meno usato).
Euristicamente, l'R² misura la frazione della variabilità delle osservazioni formula_153 che siamo in grado di spiegare tramite il modello lineare. Due importanti "caveat" devono in ogni caso essere tenuti a mente:
È evidente che, al crescere del numero di regressori formula_14, formula_190 in generale decresce, correggendo l'artificiale incremento dell'R². Si può inoltre dimostrare che formula_190 aumenta, aggiungendo un regressore, soltanto se il valore della statistica formula_192 associata al coefficiente di tale regressore (si veda oltre) è maggiore di formula_19, così che il valore dell'R² corretto è legato alla significatività delle variabili aggiuntive.
È opportuno far emergere alcune credenze sbagliate riguardo l'R². Innanzitutto non può mai assumere valori negativi perché è il rapporto di due varianze; tuttavia i software statistici possono produrre un output di una regressione che presenta un R² negativo. Ciò è dovuto al fatto che in questi programmi l'R² si calcola come differenza tra varianza spiegata e quella dei residui. Tuttavia nel caso di mispecificazione del modello (si "dimenticano" variabili che il data generating process contiene, intercetta compresa), il valore atteso della stima dei residui è in genere diverso da zero, quindi la media dello stimatore di formula_1 è diverso dalla media di formula_1. Pertanto il calcolo del software risulta errato perché non tiene conto di ciò.
Sotto le ipotesi sopra formulate, il valore atteso dello stimatore formula_124 è uguale al vettore di parametri formula_85; tale proprietà è detta correttezza; al fine di verificare la correttezza di formula_124, è sufficiente osservare che:
La varianza (in effetti, matrice varianza-covarianza) di formula_200 si ottiene come:
Il teorema di Gauss-Markov stabilisce che tale varianza è minima tra quelle degli stimatori di formula_85 ottenibili come combinazione lineare delle osservazioni formula_27; in questo senso formula_124 è uno stimatore efficiente (in effetti si tratta di uno stimatore "BLUE", dall'inglese "Best Linear Unbiased Estimator", il migliore stimatore corretto lineare).
Poiché formula_205 e le combinazioni lineari di variabili casuali normali indipendenti sono ancora normali, se ne conclude che:
Volendo stimare il parametro formula_207, un naturale candidato sarebbe la varianza campionaria:
In effetti lo stimatore sopra sarebbe anche lo stimatore di massima verosimiglianza per formula_209. Semplici manipolazioni mostrano tuttavia che tale stimatore non gode della proprietà di correttezza; infatti:
dove formula_212. Il valore atteso dell'espressione sopra è:
dove formula_214 denota l'operatore traccia di una matrice. Lo stimatore corretto per il parametro formula_207 è dunque:
Infatti:
Si osservi inoltre che, poiché formula_112, formula_219 ha una distribuzione chi quadro con formula_220 gradi di libertà.
Le tecniche del modello lineare sopra esposte possono trovare diverse applicazioni; con una qualche semplificazione, due sono i principali usi della regressione lineare:
Confinando la nostra attenzione al secondo punto, nell'ambito della statistica classica (cioè non bayesiana) condurre un test statistico non può portare ad "accettare" un'ipotesi nulla, ma al più a "non rifiutarla", un po' come dire che lo statistico assolve per mancanza di prove.
Un primo ordine di test concerne i singoli coefficienti del modello; volere stabilire se la j-esima variabile delle formula_65 abbia o meno potere esplicativo nei confronti della formula_27 equivale a sottoporre a verifica l'ipotesi nulla che il corrispondente coefficiente formula_223 sia nullo. A tal fine si ricorre alla statistica test:
dove formula_225, che sotto l'ipotesi nulla formula_226 ha distribuzione t di Student.
Un caso più complesso, e di maggiore interesse, riguarda il test di un insieme di restrizioni lineari sui coefficienti del modello; si consideri al riguardo ad un'ipotesi nulla nella forma:
dove formula_228 è una matrice di rango formula_229. Ad esempio, volendo testare l'ipotesi che il primo e il terzo coefficiente siano uguali, sarà sufficiente ricorrere la matrice (in questo particolare caso, vettore) formula_230, con formula_231, così che l'ipotesi nulla risulti: formula_232.
Al fine di sottoporre a verifica ipotesi di questo tipo, è sufficiente considerare che, essendo la combinazione lineare di variabili casuali normali ancora normale:
sotto l'ipotesi nulla formula_227. Ne consegue che:
per la nota proprietà per cui la combinazione lineare dei quadrati variabili casuali normali standardizzate ha distribuzione chi quadro, con gradi di libertà pari al rango della matrice formula_236, formula_229 (si osservi che in generale formula_238, e che formula_229 sarà solitamente pari al numero di restrizioni imposte sui parametri del modello). Naturalmente in generale il parametro formula_207 è incognito, per cui l'espressione sopra non può essere usata direttamente per fare inferenza statistica. Si ricorda tuttavia che:
Essendo noto che il rapporto tra due variabili casuali aventi distribuzione chi quadro, divise per i rispettivi gradi di libertà, è distribuito come una F di Fisher, è possibile utilizzare la statistica test:
avente sotto l'ipotesi nulla distribuzione F di Fisher con formula_229 e formula_220 gradi di libertà.
Se due o più colonne della matrice dei regressori formula_65 sono linearmente dipendenti, non esiste l'inversa formula_246 per cui il vettore di stime OLS non può essere determinato. Se da un lato è assai improbabile che questa eventualità si verifichi nelle applicazioni pratiche, è comunque possibile che alcune colonne della matrice formula_68 siano prossime alla dipendenza lineare; in tal caso sarà ancora possibile ottenere un vettore di stime OLS, ma sorgerà il problema della multicollinearità.
Si parla di multicollinearità allorché una o più colonne della matrice dei regressori formula_65 sono prossime a essere linearmente dipendenti. L'effetto della multicollinearità è che la matrice formula_249 è prossima all'essere singolare. Questo ha due conseguenze di particolare rilievo nelle applicazioni:
Il primo punto implica che gli intervalli di confidenza per i valori dei coefficienti saranno relativamente ampi; se tali intervalli includono lo zero, non si può rifiutare l'ipotesi nulla che la variabile corrispondente non abbia alcun effetto sulla variabile dipendente.
Un indicatore di multicollinearità spesso utilizzato nella pratica è il "variance inflation factor" (fattore di inflazione della varianza), o VIF. Il VIF è calcolato per ciascuna variabile del modello (spesso automaticamente da diversi software statistici), in base all'espressione:
dove formula_251 è il coefficiente R² di una regressione della colonna formula_5-esima di formula_65 su tutti gli altri regressori (incluso il termine costante, se è presente). È possibile dimostrare che la varianza dell'elemento formula_5-esimo del vettore delle stime OLS formula_124 è proporzionale al VIF; dunque un VIF elevato comporterà una minore significatività del coefficiente formula_256, andando a ridurre il valore della statistica formula_192 di Student associata. Un formula_251 elevato è indice di dipendenza lineare tra la colonna formula_5-esima e le restanti colonne della matrice formula_65, ossia è un indice di multicollinearità. Non esiste, tuttavia, un particolare valore soglia del VIF che determina inequivocabilmente la multicollinearità; sta alla sensibilità del ricercatore valutare, con l'ausilio dell'indicazione del VIF, se sussista o meno multicollinearità, nel qual caso è opportuno rimuovere il regressore formula_5-esimo (colonna formula_5-esima della matrice formula_65 sulla quale si è riscontrata multicollinearità).
Le stime e le statistiche test presentate sopra costituiscono l'obiettivo del ricercatore che effettua un'analisi di regressione lineare. Sebbene le convenzioni nella presentazione dei risultati varino significativamente a seconda dell'ambito scientifico o del tipo di pubblicazione, alcuni standard sono in generale rispettati. I risultati della stima di un modello di regressione lineare potrebbero e dovrebbero riportare:
Particolare attenzione si deve porre nel ritenere che un modello: 
implichi che le variabili ricomprese nella matrice formula_65 "causino" la formula_27. È importante osservare che l'esistenza di regressione (formalmente definita nei paragrafi precedenti) "non implica altro che l'esistenza di un valore atteso condizionato":
In particolare, non si può in generale affermare che l'espressione sopra significhi che le variabili in formula_65 "causino" il comportamento della formula_27. Come espresso con efficacia da Cochrane (2003), "le regressioni non hanno cause al secondo membro e conseguenze al primo membro." Tuttavia resta vero che uno dei principali task dell'analisi di regressione verte proprio sulle indagini di tipo causale; peraltro in contesti sperimentali "controllati" questa possibilità è tipicamente accettata. Inoltre anche in contesti osservazionali l'interpretazione causale, anche se molto più delicata, non si esclude assolutamente, anzi in certi contesti resta il task più importante. Particolare rilievo in questo contesto è giocato dal problema delle "variabili omesse", se siamo portati a ritenere che tale problema non sia rilevante, allora l'interpretazione causale è lecita.
I concetti di validità esterna ed interna forniscono uno schema di riferimento per valutare se uno studio statistico o econometrico sia utile per rispondere ad una domanda specifica di interesse.
L'analisi è esternamente valida se le sue inferenze e conclusioni possono essere generalizzate dalla popolazione e dal contesto studiati ad altre popolazioni e contesti. Deve essere giudicata usando la conoscenza specifica della popolazione e del contesto usato e di quelli oggetto d'interesse.
Un'ipotesi cruciale del modello classico di regressione lineare è che i regressori siano ortogonali al disturbo stocastico, ossia, formalmente:
Il motivo per cui tale ipotesi — anche nota come "ipotesi di esogeneità" — è fondamentale è presto illustrato; basta osservare che:
così che:
In altri termini: l'ipotesi di esogeneità dei regressori è condizione necessaria per la correttezza dello stimatore formula_118 del metodo dei minimi quadrati (un'analoga argomentazione può essere data in termini asintotici, passando dalla correttezza alla consistenza dello stimatore).
In tutti i casi in cui si ha motivo di credere che l'ipotesi di esogeneità sia violata — tutti i casi in cui si sospetta "endogeneità" dei regressori — non si può fare affidamento sui risultati di una regressione condotta col metodo dei minimi quadrati ordinari (la soluzione è di ricorrere a una regressione con variabili strumentali).
È la differenza tra la popolazione studiata e la popolazione d'interesse. Un esempio è quello di effettuare lo stesso test sui topi e sugli uomini senza chiedersi se vi siano delle differenze che inficino l'analisi.
Anche se la popolazione studiata e quella d'interesse fossero uguali, sarebbe opportuno valutarne il contesto. Un esempio è uno studio su una campagna di alcolici su degli studenti universitari e su degli studenti delle classi primarie.
Un'analisi statistica è internamente valida se le inferenze statistiche sugli effetti causali sono validi per la popolazione oggetto di studio.
La distorsione da variabile omessa nasce quando viene omessa una variabile dalla regressione, che è una determinante di formula_1 ed è correlata con uno o più dei regressori.
L'omissione di variabili rilevanti (nel senso precisato in quanto segue) può rendere le stime OLS inconsistenti. Si supponga che il modello "vero" sia:
ma si stimi un modello:
che omette la variabile "rilevante" formula_282 che contribuisce a spiegare la variabile dipendente formula_27. Si ha allora:
Poiché formula_285, nel secondo modello il regressore formula_286 è correlato col disturbo formula_287. Per la precisione: 
Risulta così violata una delle ipotesi del modello classico di regressione lineare, e le stime del parametro formula_289 col metodo dei minimi quadrati ordinari sono inconsistenti.
Si osservi che, qualora la variabile rilevante formula_282 sia ortogonale a formula_286 (e, di conseguenza, formula_292), il problema scompare (il teorema di Frisch-Waugh-Lovell precisa ed estende quest'ultima considerazione).
Questo errore sorge quando la funzione di regressione che descrive i dati non è corretta. Ad esempio una funzione di regressione di una popolazione non lineare è descritta come lineare.
Tipicamente è un errore di misura o confusione, che va a distorcere l'intero data set.
La distorsione di causalità simultanea si verifica in una regressione di Y su X quando, in aggiunta al legame causale d'interesse da formula_65 a formula_1, c'è un legame causale da formula_1 a formula_65. Questa causalità inversa rende formula_65 correlato con l'errore statistico nella regressione d'interesse.
Si verifica quando il processo di selezione è legato al valore della variabile dipendente; ciò può introdurre la correlazione tra l'errore statistico ed il regressore, portando così ad una distorsione dello stimatore OLS.
Si supponga di non poter osservare direttamente un regressore, che deve essere stimato (o "generato", secondo una diversa terminologia); per concretezza, si consideri un "vero" modello:
e si ipotizzi di disporre soltanto di una stima di formula_300:
Se si procede nella stima di:
Si ottiene:
con:
Supponendo che formula_307, la stima del parametro formula_85 risulta più vicina a zero di quanto non sia il "vero" valore del parametro (questo effetto è noto con termine inglese come "attenuation bias"). È immediato osservare che il problema è meno pronunciato laddove la varianza dell'errore nell'osservazione di formula_300, formula_310 risulta minore della varianza di formula_300 stesso — ossia, non sorprendentemente, quando formula_300 può essere stimato con relativa precisione.
Si osservi infine che nessun problema si pone nel caso in cui la variabile dipendente — formula_27 — sia stimata o generata. In tal caso, il termine di errore in essa contenuto sarà semplicemente incorporato nel disturbo della regressione — formula_314, senza ledere la consistenza delle stime OLS.
Le proprietà sopra esposte possono essere generalizzate al caso in cui le ipotesi sulla distribuzione dei termini di errore non siano necessariamente valide per campioni di dimensione finita. In questo caso, si ricorre alle proprietà "asintotiche" delle stime, supponendo implicitamente che, per campioni di dimensione sufficientemente grande, la distribuzione asintotica delle stime coincida, o approssimi ragionevolmente, quella effettiva. I risultati si fondano sul teorema del limite centrale, o su sue generalizzazioni.
Al fine di illustrare le proprietà asintotiche degli stimatori dei minimi quadrati ordinari, si ipotizzi:
dove formula_318 denota la convergenza in probabilità e formula_319 la matrice identità.
L'espressione per lo stimatore dei minimi quadrati ordinari può essere riscritta come:
Passando al limite per formula_321, si ha allora:
(si osservi che il limite in probabilità dell'inversa di formula_323 è l'inversa di formula_324). Dunque, lo stimatore formula_118 converge in probabilità al vero valore del vettore di parametri formula_85 – si dice dunque che formula_118 gode della proprietà di consistenza.
Applicando una banale estensione del teorema del limite centrale al caso multivariato, si ha inoltre:
dove formula_329 denota la convergenza in distribuzione. Da quest'ultimo risultato discende allora che:
In altre parole, lo stimatore dei minimi quadrati ordinari è non solo consistente, ma anche asintoticamente normalmente distribuito; l'insieme di queste proprietà si indica con la sigla inglese CAN ("Consistent and Asymptotically Normal").
I metodi sopra esposti costituiscono il nucleo del modello classico di regressione lineare; quantunque validi strumenti di analisi per un ampio spettro di discipline e casi di studio, essi prestano il fianco a una serie di critiche, incentrate sulla semplicità delle ipotesi alla base del modello.
Tali critiche hanno portato alla formulazione di modelli più generali, caratterizzati da ipotesi meno restrittive rispetto a quelle poste sopra. L'analisi ha battuto alcune vie principali:
Ciò ha consentito lo sviluppo di modelli alternativi, o quantomeno complementari, al modello classico; tra i più noti, il metodo dei minimi quadrati generalizzati, metodi di stima tramite variabili strumentali, i vari modelli di regressione robusta, nonché numerosi modelli sviluppati nell'ambito dell'analisi delle serie storiche e dei dati panel.
Nell'ambito dell'econometria (manuali, si veda anche l'articolo econometria):
Nell'ambito della finanza:
Nell'ambito della fisica:
Nell'ambito della Ricerca sociale:
Nell'ambito della linguistica, in particolare della psicolinguistica:
</text>
</doc>
<doc id="605" url="https://it.wikipedia.org/wiki?curid=605">
<title>Apprendimento automatico</title>
<text>
L’apprendimento automatico (noto anche come machine learning) è una branca dell'intelligenza artificiale che raccoglie un insieme di metodi, sviluppati a partire dagli ultimi decenni del XX secolo in varie comunità scientifiche, sotto diversi nomi quali: statistica computazionale, riconoscimento di pattern, reti neurali artificiali, filtraggio adattivo, teoria dei sistemi dinamici, elaborazione delle immagini, data mining, algoritmi adattivi, ecc; che utilizza metodi statistici per migliorare progressivamente la performance di un algoritmo nell'identificare pattern nei dati. Nell'ambito dell'informatica, l'apprendimento automatico è una variante alla programmazione tradizionale nella quale si predispone in una macchina l'abilità di apprendere qualcosa dai dati in maniera autonoma, senza ricevere istruzioni esplicite a riguardo.
Lo stesso Arthur Samuel che coniò il termine nel 1959 in linea di principio identifica due approcci distinti. Il primo metodo, indicato come rete neurale, porta allo sviluppo di macchine ad apprendimento automatico per impiego generale in cui il comportamento è appreso da una rete di commutazione connessa casualmente, a seguito di una routine di apprendimento basata su ricompensa e punizione (apprendimento per rinforzo). Il secondo metodo, più specifico, consiste nel riprodurre l'equivalente di una rete altamente organizzata progettata per imparare solo alcune attività specifiche. La seconda procedura, che necessita di supervisione, richiede la riprogrammazione per ogni nuova applicazione, ma risulta essere molto più efficiente dal punto di vista computazionale.
L'apprendimento automatico è strettamente legato al riconoscimento di pattern e alla teoria computazionale dell'apprendimento ed esplora lo studio e la costruzione di algoritmi che possano apprendere da un insieme di dati e fare delle predizioni su questi, costruendo in modo induttivo un modello basato su dei campioni. L'apprendimento automatico viene impiegato in quei campi dell'informatica nei quali progettare e programmare algoritmi espliciti è impraticabile; tra le possibili applicazioni citiamo il filtraggio delle email per evitare spam, l'individuazione di intrusioni in una rete o di intrusi che cercano di violare dati, il riconoscimento ottico dei caratteri, i motori di ricerca e la visione artificiale.
L'apprendimento automatico è strettamente collegato, e spesso si sovrappone con la statistica computazionale, che si occupa dell'elaborazione di predizioni tramite l'uso di computer. L'apprendimento automatico è anche fortemente legato all'ottimizzazione matematica, che fornisce metodi, teorie e domini di applicazione a questo campo. Per usi commerciali, l'apprendimento automatico è conosciuto come analisi predittiva.
L'apprendimento automatico si sviluppa con lo studio dell'intelligenza artificiale, e vi è strettamente collegato: infatti già dai primi tentativi di definire l'intelligenza artificiale come disciplina accademica, alcuni ricercatori si erano mostrati interessati alla possibilità che le macchine imparassero dai dati. Questi ricercatori, in particolare Marvin Minsky, Arthur Samuel e Frank Rosenblatt, provarono ad avvicinarsi al problema sia attraverso vari metodi formali, sia con quelle che vengono definite reti neurali nei tardi anni '50. Le reti neurali erano allora costituite da singoli percettroni e da modelli matematici derivati dal modello lineare generalizzato della statistica, come l'ADALINE di Widrow. Si provò a sfruttare anche ragionamenti probabilistici, in particolare nelle diagnosi mediche automatiche.
Sempre negli anni '50, Alan Turing propose l'idea di una "macchina che apprende", ovvero in grado di imparare e dunque diventare intelligente. La proposta specifica di Turing anticipa gli algoritmi genetici.
Tuttavia già dalla metà degli anni '50 lo studio dell'intelligenza artificiale si stava concentrando su approcci logici di tipo "knowledge-based", nota oggi sotto il nome di GOFAI, causando un distacco tra lo studio dell'IA e quello dell'apprendimento automatico. Sistemi di tipo probabilistico erano invasi di problemi sia teoretici sia pratici in termini di acquisizione e rappresentazione dei dati. Negli anni Ottanta, i sistemi esperti dominavano il campo dell'IA, e i sistemi basati sulla statistica non venivano più studiati.
Lo studio dell'apprendimento simbolico e "knowledge-based" continuò nell'ambito dell'IA, portando a sviluppare la programmazione logica induttiva, ma ora la ricerca più prettamente statistica si svolgeva al di fuori del campo vero e proprio dell'intelligenza artificiale, nel riconoscimento di pattern e nell'information retrieval.
Un altro motivo per cui lo studio dell'apprendimento automatico fu abbandonato fu la pubblicazione del libro "Perceptrons: an introduction to computational geometry" di Marvin Minsky e Seymour Papert, che vi descrivevano alcune delle limitazioni dei percettroni e delle reti neurali. La ricerca sulle reti neurali subì un significativo rallentamento a causa dell'interpretazione del libro, che le descriveva come intrinsecamente limitate. Anche la linea di ricerca sulle reti neurali continuò al di fuori del campo dell'IA, portata avanti da ricercatori provenienti da altre discipline quali Hopfield, Rumelhart, Hinton e Fukushima. Il loro successo principale fu a metà degli anni '80 con la riscoperta della "backpropagation" e della self-organization.
L'apprendimento automatico, sviluppatosi come campo di studi separato dall'IA classica, cominciò a rifiorire negli anni '90. Il suo obiettivo cambiò dall'ottenere l'intelligenza artificiale ad affrontare problemi risolvibili di natura pratica. Distolse inoltre la propria attenzione dagli approcci simbolici che aveva ereditato dall'IA, e si diresse verso metodi e modelli presi in prestito dalla statistica e dalla teoria della probabilità. L'apprendimento automatico ha inoltre beneficiato dalla nascita di Internet, che ha reso l'informazione digitale più facilmente reperibile e distribuibile.
Tom M. Mitchell ha fornito la definizione più citata di apprendimento automatico nel suo libro ""Machine Learning"": ""Si dice che un programma apprende dall'esperienza E con riferimento a alcune classi di compiti T e con misurazione della performance P, se le sue performance nel compito T, come misurato da P, migliorano con l'esperienza E."" In poche parole, si potrebbe semplificare dicendo che un programma apprende se c'è un miglioramento delle prestazioni dopo un compito svolto. Questa definizione di Mitchell è rilevante poiché fornisce una definizione operativa dell'apprendimento automatico, invece che in termini cognitivi. Fornendo questa definizione, Mitchell di fatto segue la proposta che Alan Turing fece nel suo articolo ""Computing Machinery and Intelligence"", sostituendo la domanda ""Le macchine possono pensare?"" con la domanda ""Le macchine possono fare quello che noi (in quanto entità pensanti) possiamo fare?"".
L'obiettivo principe dell'apprendimento automatico è che una macchina sia in grado di generalizzare dalla propria esperienza, ossia che sia in grado di svolgere ragionamenti induttivi. In questo contesto, per generalizzazione si intende l'abilità di una macchina di portare a termine in maniera accurata esempi o compiti nuovi, che non ha mai affrontato, dopo aver fatto esperienza su un insieme di dati di apprendimento. Gli esempi di addestramento (in inglese chiamati "training examples") si assume provengano da una qualche distribuzione di probabilità, generalmente sconosciuta e considerata rappresentativa dello spazio delle occorrenze del fenomeno da apprendere; la macchina ha il compito di costruire un modello probabilistico generale dello spazio delle occorrenze, in maniera tale da essere in grado di produrre previsioni sufficientemente accurate quando sottoposta a nuovi casi.
L'analisi computazionale degli algoritmi di apprendimento automatico e delle loro prestazioni è una branca dell'Informatica teorica chiamata teoria dell'apprendimento. Dato che gli esempi di addestramento sono insiemi finiti di dati e non c'è modo di sapere l'evoluzione futura di un modello, la teoria dell'apprendimento non offre alcuna garanzia sulle prestazioni degli algoritmi. D'altro canto, è piuttosto comune che tali prestazioni siano vincolate da limiti probabilistici. Il bias-variance tradeoff è uno dei modi di quantificare l'errore di generalizzazione.
Affinché la generalizzazione offra le migliori prestazioni possibili, la complessità dell'ipotesi induttiva deve essere pari alla complessità della funzione sottostante i dati. Se l'ipotesi è meno complessa della funzione, allora il modello manifesta "underfitting". Quando la complessità del modello viene aumentata in risposta, allora l'errore di apprendimento diminuisce. Al contrario invece se l'ipotesi è troppo complessa, allora il modello manifesta overfitting e la generalizzazione sarà più scarsa.
Oltre ai limiti di prestazioni, i teorici dell'apprendimento studiano la complessità temporale e la fattibilità dell'apprendimento stesso. Una computazione è considerata fattibile se può essere svolta in tempo polinomiale.
I compiti dell'apprendimento automatico vengono tipicamente classificati in tre ampie categorie, a seconda della natura del "segnale" utilizzato per l'apprendimento o del "feedback" disponibile al sistema di apprendimento. Queste categorie, anche dette paradigmi, sono:
A metà strada tra l'apprendimento supervisionato e quello non supervisionato c'è l'apprendimento semi-supervisionato, nel quale l'insegnante fornisce un dataset incompleto per l'allenamento, cioè un insieme di dati per l'allenamento tra i quali ci sono dati senza il rispettivo output desiderato. La trasduzione è un caso speciale di questo principio, nel quale l'intero insieme delle istanze del problema è noto durante l'apprendimento, eccetto la parte degli output desiderati che è mancante.
Un'altra categorizzazione dei compiti dell'apprendimento automatico si rileva quando si considera l'output desiderato del sistema di apprendimento automatico.
L'apprendimento automatico e la statistica sono discipline strettamente collegate. Secondo Michael I. Jordan, le idee dell'apprendimento automatico, dai principi metodologici agli strumenti teorici, sono stati sviluppati prima in statistica. Jordan ha anche suggerito il termine data science come nome con cui chiamare l'intero campo di studi.
Leo Breiman ha distinto due paradigmi statistici di modellazione: modello basato sui dati e modello basato sugli algoritmi, dove "modello basato sugli algoritmi" indica approssimativamente algoritmi di apprendimento automatico come la foresta casuale.
Alcuni statistici hanno adottato metodi provenienti dall'apprendimento automatico, il che ha portato alla creazione di una disciplina combinata chiamata "apprendimento statistico".
L'apprendimento automatico viene a volte unito al data mining, che si focalizza maggiormente sull'analisi esplorativa dei dati ed utilizza principalmente il paradigma di apprendimento chiamato "apprendimento non supervisionato". Invece, l'apprendimento automatico può essere anche supervisionato.
L'apprendimento automatico e il "data mining" infatti si sovrappongono in modo significativo, ma mentre l'apprendimento automatico si concentra sulla previsione basata su proprietà note apprese dai dati, il data mining si concentra sulla scoperta di proprietà prima "sconosciute" nei dati. Il data mining sfrutta i metodi dell'apprendimento automatico, ma con obiettivi differenti; d'altro canto, l'apprendimento automatico utilizza i metodi di data mining come metodi di apprendimento non supervisionato o come passi di preprocessing per aumentare l'accuratezza dell'apprendimento. Gran parte della confusione tra le due comunità di ricerca scaturisce dall'assunzione di base del loro operato: nell'apprendimento automatico, le prestazioni sono generalmente valutate in base all'abilità di riprodurre conoscenza già acquisita, mentre in data mining il compito chiave è la scoperta di conoscenza che prima non si aveva.
L'apprendimento automatico ha legami molto stretti con l'ottimizzazione: molti problemi di apprendimento sono formulati come la minimizzazione di una qualche funzione di costo su un insieme di esempi di apprendimento. La funzione di costo (o funzione di perdita) rappresenta la discrepanza tra le previsioni del modello che si sta addestrando e le istanze del problema reale. Le differenze tra i due campi (l'apprendimento automatico e l'ottimizzazione) sorgono dall'obiettivo della generalizzazione: mentre gli algoritmi di ottimizzazione possono minimizzare la perdita su un insieme di apprendimento, l'apprendimento automatico si preoccupa di minimizzare la perdita su campioni mai visti dalla macchina.
La risoluzione automatica di problemi avviene, nel campo dell'informatica, in due modi differenti: tramite paradigmi di "hard computing" o tramite paradigmi di "soft computing". Per "hard computing" si intende la risoluzione di un problema tramite l'esecuzione di un algoritmo ben definito e decidibile. La maggior parte dei paradigmi di "hard computing" sono metodi ormai consolidati, ma presentano alcuni lati negativi: infatti richiedono sempre un modello analitico preciso e definibile, e spesso un alto tempo di computazione. 
Le tecniche di "soft computing" d'altro canto antepongono il guadagno nella comprensione del comportamento di un sistema a scapito della precisione, spesso non necessaria. I paradigmi di "soft computing" si basano su due principi: 
L'apprendimento automatico si avvale delle tecniche di "soft computing".
La programmazione logica induttiva (anche ILP, dall'inglese "inductive logic programming") è un approccio all'apprendimento di regole che usa la programmazione logica come rappresentazione uniforme per gli esempi di input, per la conoscenza di base della macchina, e per le ipotesi. Data una codifica della (nota) conoscenza di base e un insieme di esempi rappresentati come fatti in una base di dati logica, un sistema ILP deriva un programma logico ipotetico da cui conseguono tutti gli esempi positivi, e nessuno di quelli negativi. La programmazione induttiva è un campo simile che considera ogni tipo di linguaggio di programmazione per rappresentare le ipotesi invece che soltanto la programmazione logica, come ad esempio programmi funzionali.
L'albero di decisione è un metodo di apprendimento per approssimazione di una funzione obiettivo discreta in cui l'elemento che apprende è rappresentato da un albero di decisione. Gli alberi di decisione possono essere rappresentati da un insieme di regole if-else per migliorare la leggibilità umana.
L'apprendimento automatico basato su regole di associazione è un metodo di apprendimento che identifica, apprende ed evolve delle "regole" con l'intento di immagazzinare, manipolare e applicare conoscenza. La caratteristica principale di questo tipo di apprendimento è l'identificazione ed utilizzo di un insieme di regole relazionali che rappresenta nel suo insieme la conoscenza catturata dal sistema. Ciò si pone in controtendenza con altri tipi di apprendimento automatico che normalmente identificano un singolo modello che può essere applicato universalmente ad ogni istanza per riuscire a fare su di essa una previsione. Gli approcci dell'apprendimento basato su regole di associazione includono il sistema immunitario artificiale.
Una rete neurale artificiale è un sistema adattivo che cambia la sua struttura basata su informazioni esterne o interne che scorrono attraverso la rete durante la fase di apprendimento.
In termini pratici le reti neurali sono strutture non-lineari di dati statistici organizzate come strumenti di modellazione. Esse possono essere utilizzate per simulare relazioni complesse tra ingressi e uscite che altre funzioni analitiche non riescono a rappresentare. Inoltre esse sono robuste agli errori presenti nel training data.
Gli algoritmi genetici forniscono un approccio all'apprendimento che è liberamente ispirato all'evoluzione simulata. La ricerca di una soluzione del problema inizia con una popolazione di soluzioni iniziale. I membri della popolazione attuale danno luogo a una popolazione di nuova generazione per mezzo di operazioni quali la mutazione casuale e crossover, che sono modellati sui processi di evoluzione biologica. Ad ogni passo, le soluzioni della popolazione attuale sono valutate rispetto a una determinata misura di fitness, con le ipotesi più adatte selezionate probabilisticamente come semi per la produzione della prossima generazione. Gli algoritmi genetici sono stati applicati con successo a una varietà di compiti di apprendimento e di altri problemi di ottimizzazione. Ad esempio, essi sono stati usati per imparare raccolte di norme per il controllo del robot e per ottimizzare la topologia dei parametri di apprendimento per reti neurali artificiali.
Il ragionamento bayesiano fornisce un approccio probabilistico di inferenza. Esso si basa sul presupposto che le quantità di interesse sono disciplinate da distribuzioni di probabilità e che le decisioni ottimali possono essere prese a seguito dell'analisi di queste probabilità insieme ai dati osservati. Nell'ambito dell'apprendimento automatico, la teoria Bayesiana è importante perché fornisce un approccio quantitativo per valutare le prove a sostegno dell'ipotesi alternativa. Il Ragionamento bayesiano fornisce la base per l'apprendimento negli algoritmi che manipolano direttamente le probabilità.
Macchine a vettori di supporto ("Support Vector Machine", SVM) sono un insieme di metodi di apprendimento supervisionato usati per la classificazione e la regressione di pattern. Dato un insieme di esempi di addestramento, ciascuno contrassegnato come appartenente a due possibili categorie, un algoritmo di addestramento SVM costruisce un modello in grado di prevedere a quale categoria deve appartenere un nuovo esempio di input.
La discesa dei prezzi per l'hardware e lo sviluppo di GPU per uso personale negli ultimi anni hanno contribuito allo sviluppo del concetto di apprendimento profondo, che consiste nello sviluppare livelli nascosti multipli nelle reti neurali artificiali. Questo approccio tenta di modellizzare il modo in cui il cervello umano processa luce e suoni e li interpreta in vista e udito. Alcune delle applicazioni più affermate dell'apprendimento profondo sono la visione artificiale e il riconoscimento vocale.
La cluster analisi, o clustering, è in grado di rilevare similarità strutturali tra le osservazioni di un dataset attraverso l'assegnazione di un insieme di osservazioni in sottogruppi ("cluster") di elementi tra loro omogenei. Il clustering è un metodo di apprendimento non supervisionato, e una tecnica comune per l'analisi statistica dei dati.
Tutti i sistemi di riconoscimento vocale di maggior successo utilizzano metodi di apprendimento automatico. Ad esempio, il SPHINXsystem impara le strategie di altoparlanti specifici per riconoscere i suoni primitivi (fonemi) e le parole del segnale vocale osservato. Metodi di apprendimento basati su reti neurali e su modelli di Markov nascosti sono efficaci per la personalizzazione automatica di vocabolari, caratteristiche del microfono, rumore di fondo, ecc.
Metodi di apprendimento automatico sono stati usati per addestrare i veicoli controllati da computer. Ad esempio, il sistema ALVINN ha usato le sue strategie per imparare a guidare senza assistenza a 70 miglia all'ora per 90 miglia su strade pubbliche, tra le altre auto. Con tecniche simili sono possibili applicazioni in molti problemi di controllo basato su sensori.
Metodi di apprendimento automatico sono stati applicati ad una varietà di database di grandi dimensioni per imparare regolarità generali implicito nei dati. Ad esempio, algoritmi di apprendimento basati su alberi di decisione sono stati usati dalla NASA per classificare oggetti celesti a partire dal secondo Palomar Observatory Sky Survey. Questo sistema è oggi utilizzato per classificare automaticamente tutti gli oggetti nel Sky Survey, che si compone di tre terabyte di dati immagine.
I programmi per computer di maggior successo per il gioco del backgammon sono basati su algoritmi di apprendimento. Ad esempio, il miglior programma di computer al mondo per backgammon, TD-Gammon, ha sviluppato la sua strategia giocando oltre un milione di partite di prova contro se stesso. Tecniche simili hanno applicazioni in molti problemi pratici in cui gli spazi di ricerca molto rilevanti devono essere esaminati in modo efficiente.
L'apprendimento automatico solleva un numero di problematiche etiche. I sistemi addestrati con insiemi di dati faziosi o pregiudizievoli possono esibire questi pregiudizi quando vengono interpellati: in questo modo possono essere digitalizzati pregiudizi culturali quali il razzismo istituzionale e il classismo. Di conseguenza la raccolta responsabile dei dati può diventare un aspetto critico dell'apprendimento automatico.
In ragione dell'innata ambiguità dei linguaggi naturali, le macchine addestrate su corpi linguistici necessariamente apprenderanno questa ambiguità.
</text>
</doc>
<doc id="3550" url="https://it.wikipedia.org/wiki?curid=3550">
<title>Probabilità condizionata</title>
<text>
In teoria della probabilità la probabilità condizionata di un evento "A" rispetto a un evento "B" è la probabilità che si verifichi "A", sapendo che "B" è verificato. Questa probabilità, indicata formula_1 o formula_2, esprime una "correzione" delle aspettative per "A", dettata dall'osservazione di "B".
Poiché, come si vedrà nella successiva definizione, formula_3 compare al denominatore, formula_1 ha senso solo se "B" ha una probabilità non nulla di verificarsi. 
È utile osservare che la notazione con il simbolo "Barra verticale" è comune con la definizione del connettivo logico NAND.
Per esempio, la probabilità di ottenere "4" con il lancio di un dado a sei facce (evento "A") ha probabilità "P(A)=1/6" di verificarsi. "Sapendo" però che il risultato del lancio è un numero tra "4", "5" e "6" (evento "B"), la probabilità di "A" diventa 
Si consideri questo secondo esempio, la probabilità di ottenere "1" con il lancio di un comune dado (evento "A") ha probabilità "P(A)=1/6" di verificarsi. "Sapendo" però che il risultato del lancio è un numero tra "4", "5" e "6" (evento "B"), la probabilità di "A" diventa 
La probabilità di "A" condizionata da "B" è
dove formula_8 è la probabilità congiunta dei due eventi, ovvero la probabilità che si verifichino entrambi.
In termini più rigorosi, dato uno spazio misurabile formula_9 di misura "P", ogni evento "B" eredita una struttura di spazio misurato formula_10, restringendo gli insiemi misurabili a quelli contenuti in "B", ed induce una nuova misura formula_11 su formula_9, con formula_13.
Se formula_14 è uno spazio probabilizzato (formula_15) e "B" non è trascurabile (formula_16), allora riscalando formula_17 a formula_18 si ottiene lo spazio probabilizzato formula_19 delle probabilità condizionate da "B".
La formula della probabilità condizionata permette di descrivere la probabilità congiunta come
Ovvero, la probabilità che si verifichino sia "A" sia "B" è pari alla probabilità che si verifichi "B" moltiplicata per la probabilità che si verifichi "A" supponendo che "B" sia verificato.
Due eventi "A" e "B" sono indipendenti quando vale una delle tre equazioni equivalenti
Per trovare la probabilità dell'evento a destra negato si può usare la seguente formula:
formula_24.
Se "A" e "B" sono eventi disgiunti, cioè se formula_25, le loro probabilità condizionate sono nulle: sapendo che uno dei due eventi si è verificato, è impossibile che si sia verificato "anche" l'altro.
Se l'evento "A" implica l'evento "B", cioè se formula_26, allora la loro intersezione è "A", per cui formula_27 e:
Nel caso di una misura di probabilità uniforme su uno spazio Ω finito, questa formula per "P(A|B)" esprime la definizione classica di probabilità come "casi favorevoli ("A") su casi possibili ("B")". 
Invece, per "P(B|A)" otteniamo il valore 1 che, per un numero finito di valori lo stesso Bayes interpretò in senso lato come la certezza che il tutto sia condizionato dalla parte.
La speranza condizionata formula_30 di una variabile aleatoria "X" ad un evento "B" è la speranza di "X" calcolata sulle probabilità formula_31 (condizionate da "B").
La probabilità di un evento "A" può essere condizionata da una variabile aleatoria discreta "X", originando una nuova variabile aleatoria, formula_32, che per "X=x" assume il valore formula_33.
Il teorema di Bayes esprime l'uguaglianza simmetrica formula_34 del teorema della probabilità composta come
Questo teorema è alla base dell'inferenza bayesiana in statistica, dove "P" è detta "probabilità "a priori" di "B"" e "P" "probabilità "a posteriori" di "B"".
Molti paradossi sono legati alla probabilità condizionata e derivano sia da un'errata formulazione del problema sia dalla confusione di "P(A|B)" con "P(A)" o con "P(B|A)".
Esempi particolari sono il paradosso delle due buste, il paradosso dei due bambini, il problema di Monty Hall e il paradosso di Simpson.
</text>
</doc>
<doc id="2735" url="https://it.wikipedia.org/wiki?curid=2735">
<title>Mediana (statistica)</title>
<text>
In statistica, in particolare in statistica descrittiva, data una distribuzione di un carattere quantitativo oppure qualitativo ordinabile (ovvero le cui modalità possano essere ordinate in base a qualche criterio), si definisce la mediana (o valore mediano) come il valore/modalità (o l'insieme di valori/modalità) assunto dalle unità statistiche che si trovano nel mezzo della distribuzione.
La mediana è un indice di posizione e rientra nell'insieme delle statistiche d'ordine.
Il termine "mediano" venne introdotto da Antoine Augustin Cournot e adottato da Francis Galton.
Gustav Theodor Fechner sviluppò l'uso della mediana come sostituto della media in quanto riteneva che il calcolo della media fosse troppo laborioso rispetto al vantaggio in termini di precisioni che offriva.
Se si procede al riordinamento delle unità in base ai valori crescenti del carattere da esse detenuto, in sostanza la mediana bipartisce la distribuzione in due sotto-distribuzioni: la prima a sinistra della mediana (costituita dalla metà delle unità la cui modalità è minore o uguale alla mediana) e la seconda a destra della mediana (costituita dalla metà delle unità la cui modalità è maggiore o uguale alla mediana). Tecnicamente si afferma che la mediana è il valore/modalità per il quale la frequenza relativa cumulata vale (o supera) 0,5, cioè il secondo quartile, ossia il 50° percentile. Usualmente si indica la mediana con Me.
Per calcolare la mediana di formula_1 dati:
Se le modalità sono raggruppate in classi non si definisce un valore univoco, ma una classe mediana formula_7.
La determinazione di tale classe avviene considerando le frequenze cumulate; indicando con formula_8 la generica frequenza cumulata relativa dell'osservazione i-esima sarà:formula_9 e formula_10. Pur essendo corretto considerare un qualsiasi elemento dell'intervallo formula_7 un valore mediano si è soliti procedere, al fine di avere una misura unica del valore, a un'approssimazione della mediana con la seguente formula:
se si assume che la distribuzione dei dati all'interno della classe sia uniforme, che corrisponde ad un processo di interpolazione.
Una proprietà della mediana è di rendere minima la somma dei valori assoluti degli scarti delle formula_13 da un generico valore
Infatti, sia formula_15 la variabile aleatoria alla quale si riferiscono le osservazioni formula_13. Per la linearità del valore atteso e dell'operatore di derivazione si ha
dove formula_18 è la funzione segno di formula_19. Per la definizione di valore atteso
dove formula_21 indica la probabilità che formula_15 sia minore di formula_23 e formula_24 quella che formula_15 sia maggiore di formula_23. Per le proprietà di normalizzazione della probabilità, cioè formula_27, l'equazione diventa
Quindi
cioè formula_23 è la mediana.
In un sondaggio fatto all'interno di una facoltà composta da 250 studenti (la popolazione statistica), si intende rilevare il carattere "Gradimento dei professori", secondo le cinque modalità "molto deluso", "insoddisfatto", "parzialmente soddisfatto", "soddisfatto", "entusiasta". Risulta che 10 studenti si dicono entusiasti dell'operato dei professori, 51 si dicono soddisfatti, 63 parzialmente soddisfatti, 90 insoddisfatti, 36 molto delusi.
La distribuzione di frequenza viene rappresentata con una tabella come la seguente:
Nel caso ipotizzato, la mediana è rappresentata dalla modalità "insoddisfatto". Questo significa che "almeno" la metà degli studenti non è soddisfatto dei professori.
</text>
</doc>
<doc id="2767" url="https://it.wikipedia.org/wiki?curid=2767">
<title>Moda (statistica)</title>
<text>
In statistica, la moda (o norma) di una distribuzione di frequenza X è la modalità (o la classe di modalità) caratterizzata dalla massima frequenza e viene spesso rappresentata con la simbologia ν. In altre parole, è il valore che compare più frequentemente.
Una distribuzione è "unimodale" se ammette un solo valore modale, è "bimodale" se ne ammette due (ossia: se esistono due valori che compaiono entrambi con la frequenza massima nella data distribuzione), "trimodale" se ne ha tre, ecc. 
La presenza di due (o più) mode all'interno di un collettivo potrebbe essere sintomo della non omogeneità del collettivo stesso: potrebbero cioè esistere al suo interno due (o più) sottogruppi omogenei al loro interno, ma distinti l'uno dall'altro per un'ulteriore caratteristica rispetto a quella osservata. 
Per la determinazione della classe modale è opportuno ricorrere all'istogramma, individuando "l'intervallo di altezza massima", ovvero il "punto di massimo della curva".
La classe con la maggiore densità media (che corrisponde all'altezza dell'istogramma) è quella modale.
Nel caso particolare della distribuzione normale, detta anche "gaussiana", la moda coincide con la media e la mediana.
Indicando con formula_1 il numero di elementi che cadono nella classe formula_2, l'altezza formula_3 sarà data da:
L'utilità della moda risiede nell'essere l'unico degli indici di tendenza centrale in grado di sintetizzare caratteri qualitativi su scala nominale.
</text>
</doc>
<doc id="26299" url="https://it.wikipedia.org/wiki?curid=26299">
<title>Elaborazione dati</title>
<text>
L'elaborazione dati (in inglese "data processing") è un qualsiasi procedimento informatico che comporta la conversione dei dati in informazioni tramite algoritmi.
Fino agli anni 1980, i reparti informatici di aziende ed enti pubblici erano chiamati "Centro elaborazione dati" (CED), perché si poneva l'accento sul fatto che in tali uffici venivano immagazzinati moli di dati dalle quali, previa elaborazione, si potevano estrarre informazioni utili alle attività dell'organizzazione. L'espressione inglese "data processing" rimane ampiamente usata nei sistemi SCADA, per indicare le elaborazioni effettuate su dati acquisiti dai sensori.
Attualmente prevale l'espressione "sistema informativo", perché ci si è resi conto che gli algoritmi utilizzati per l'elaborazione dei dati sono ormai standardizzati, mentre il vero lavoro consiste nell'analisi e gestione dei flussi di informazioni tra le persone dell'organizzazione e nell'automazione informatica di tali comunicazioni.
Più in generale, si usa l'espressione "elaborazione dati" per indicare qualsiasi conversione di dati da un formato a un altro, anche se in questo caso il termine più adatto sarebbe "conversione dei dati". Nell'ambito dell'elaborazione dati da parte degli elaboratori elettronici assume particolare importanza il concetto di velocità prestazionale di elaborazione dati, che in un normale computer è funzione della frequenza di clock del processore ed è a volte calcolata in FLOPS o MIPS. 
Nel processo:
In altre parole, usando il linguaggio dell'automatica si può definire "elaborazione" quel processo che, dato un input specificato, produce un determinato output dopo una certa manipolazione/trasformazione. Più in particolare, alla base dell'elaborazione da parte del sistema di elaborazione vi è il ciclo di fetch-execute da parte del processore che agisce sui dati provenienti dalla memoria sulla base dell'insieme d'istruzioni scandito dal clock di sistema.
</text>
</doc>
<doc id="4314" url="https://it.wikipedia.org/wiki?curid=4314">
<title>Teorema di Bayes</title>
<text>
Il teorema di Bayes (conosciuto anche come formula di Bayes o teorema della probabilità delle cause), proposto da Thomas Bayes, deriva da due teoremi fondamentali delle probabilità:
il teorema della probabilità composta e il teorema della probabilità assoluta. Viene impiegato per calcolare la probabilità di una causa che ha scatenato l'evento verificato. Per esempio si può calcolare la probabilità che una certa persona soffra della malattia per cui ha eseguito il test diagnostico (nel caso in cui questo sia risultato negativo) o viceversa non sia affetta da tale malattia (nel caso in cui il test sia risultato positivo), conoscendo la frequenza con cui si presenta la malattia e la percentuale di efficacia del test diagnostico.
Formalmente il teorema di Bayes è valido in tutte le interpretazioni della probabilità. In ogni caso, l'importanza di questo teorema per la statistica è tale che la divisione tra le due scuole (statistica bayesiana e statistica frequentista) nasce dall'interpretazione che si dà al teorema stesso.
Considerando un insieme di alternative formula_1 che partizionano lo spazio degli eventi formula_2 (ossia formula_3 e formula_4) si trova la seguente espressione per la probabilità condizionata:
Dove:
Intuitivamente, il teorema descrive il modo in cui le opinioni nell'osservare A siano arricchite dall'aver osservato l'evento E.
Si consideri una scuola che ha il 60% di studenti maschi e il 40% di studentesse femmine.
Le studentesse indossano in egual numero gonne o pantaloni; gli studenti indossano tutti quanti i pantaloni. Un osservatore, da lontano, nota un generico studente coi pantaloni. Qual è la probabilità che quello studente sia una femmina?
Il problema può essere risolto con il teorema di Bayes, ponendo l'evento A che lo studente osservato sia femmina, e l'evento B che lo studente osservato indossi i pantaloni. Per calcolare P(A|B), dovremo sapere:
Ciò detto, possiamo applicare il teorema:
C'è pertanto 1/4 di probabilità che lo studente sia femmina cioè 25%.
Il teorema deriva dalla definizione di probabilità condizionata. La probabilità di un evento "A", noto un evento "B", risulta:
In modo analogo, la probabilità di un evento "B" noto un evento "A":
Pertanto:
Sostituendo nella prima uguaglianza, si trova il teorema di Bayes:
Si supponga di partecipare a un gioco a premi, in cui si può scegliere fra tre porte: dietro una di esse c'è un'automobile, dietro le altre, due capre. Si sceglie una porta, diciamo la numero 1, e il conduttore del gioco a premi, che sa cosa si nasconde dietro ciascuna porta, ne apre un'altra, diciamo la 3, rivelando una capra. Quindi domanda: «Vorresti scegliere la numero 2?» Ti conviene cambiare la tua scelta originale?
Si potrebbe pensare che, con due porte chiuse, si abbia una probabilità 50:50 per ognuna, e che quindi non ci sia motivo di cambiare porta. Non è questo il caso. Chiamiamo l'evento che la macchina si trovi dietro una certa porta rispettivamente A, A e A.
All'inizio, è ovvio che:
formula_11
Come detto prima, la porta scelta è la numero 1. Chiamiamo B l'evento "il presentatore apre la porta 3". Ora:
La probabilità a priori per l'evento B è del 50%, infatti:
Da cui:
Da ciò è evidente che si deve sempre cambiare con la porta 2.
I filtri bayesiani sono uno strumento utilizzato per combattere lo spam che deve il suo funzionamento proprio al teorema di Bayes. Un filtro bayesiano fa uso di un classificatore bayesiano per riconoscere se una certa sequenza di simboli (come una parola) si presenta spesso nei messaggi di spam, quindi applica l'inferenza bayesiana per calcolare la probabilità che un determinato messaggio sia spam.
Il teorema si chiama così in onore del reverendo Thomas Bayes (1702–1761), il quale studiò come calcolare una distribuzione per il parametro di una distribuzione binomiale. Un suo amico, Richard Price, pubblicò il lavoro nel 1763, dopo la morte di Bayes, nell'articolo "Essay Towards Solving a Problem in the Doctrine of Chances". 
Alcuni anni dopo (nel 1774) viene formulato da Pierre Simon Laplace che probabilmente non era a conoscenza del lavoro di Bayes.
Una ricerca da parte di un professore di statistica (Stigler, 1982) sembrerebbe suggerire che il teorema di Bayes sia stato scoperto da Nicholas Saunderson anni prima di Bayes.
</text>
</doc>
<doc id="28852" url="https://it.wikipedia.org/wiki?curid=28852">
<title>Iperpiano</title>
<text>
La nozione di iperpiano è nata in geometria come generalizzazione della nozione di piano e successivamente ha avuto una riformulazione nella combinatoria, più precisamente nella teoria delle matroidi.
Si tratta essenzialmente di un sottospazio lineare di dimensione inferiore di uno ("n" − 1) rispetto allo spazio in cui è contenuto ("n"). Se lo spazio ha dimensione 3, i suoi iperpiani sono i piani.
Nello spazio tridimensionale euclideo un piano è un insieme di punti che soddisfa un'equazione lineare e separa i punti rimanenti dell'intero spazio in due semispazi. Una nozione corrispondente in uno spazio bidimensionale, come nel piano cartesiano riferito ad un sistema di assi cartesiani ortogonali, è data da quella di linea retta, insieme che soddisfa un'equazione lineare e separa lo spazio in due semipiani. In uno spazio unidimensionale, cioè in una linea retta, si ha un punto che separa lo spazio in due semirette. Questa situazione si può riscontrare in uno spazio di 4 o più dimensioni e l'insieme lineare che separa i punti rimanenti in due insiemi viene detto "iperpiano".
Più formalmente, dato uno spazio (proiettivo, vettoriale, affine) di dimensione finita formula_1 è detto iperpiano (proiettivo, vettoriale, affine) un sottospazio (proiettivo, vettoriale, affine) di dimensione formula_2 vale a dire di codimensione formula_3
Un iperpiano affine in uno spazio vettoriale formula_1-dimensionale, può essere descritto da un'equazione lineare non degenere della seguente forma:
Qui non degenere significa che i coefficienti formula_6 delle variabili, sono non tutti nulli. Se formula_7, si ottiene un iperpiano che è un sottospazio vettoriale e passa per l'origine.
I due semispazi chiusi definiti da un iperpiano in uno spazio vettoriale formula_1-dimensionale sono:
e
Le matroidi sono entità che si possono definire a partire da numerose nozioni diverse che si rivelano criptomorfe. In una definizione delle matroidi gli iperpiani sono sottoinsiemi di un insieme ambiente caratterizzati assiomaticamente. Nelle definizioni rimanenti gli iperpiani si definiscono costruttivamente a partire dalle entità introdotte mediante assiomi caratteristici: insiemi indipendenti, basi, insiemi dipendenti, circuiti, funzione rango ecc.
Un piano invariante emerge, ad esempio, ogniqualvolta il momento angolare di un corpuscolo sia conservato, infatti formula_11è l'equazione di un iperpiano.
</text>
</doc>
<doc id="847" url="https://it.wikipedia.org/wiki?curid=847">
<title>Campionamento statistico</title>
<text>
In statistica il campionamento statistico (che si appoggia alla teoria dei campioni o teoria del campionamento), sta alla base dell'inferenza statistica, la quale si divide in due grandi capitoli: la teoria della stima e la verifica d'ipotesi.
In particolare, una rilevazione si dice "campionaria" quando è utile per fare inferenza ossia per desumere dal campione stesso un'informazione relativa all'intera popolazione.
Le indagini censuarie riguardano l'intera popolazione e pur essendo più affidabili riguardo al parametro oggetto d'indagine soffrono di:
Quindi mentre l'indagine censuaria fornisce il valore vero dei parametri di interesse (proporzioni, percentuali, medie, totali...) quella campionaria restituisce una sua stima al quale è associato un certo grado di fiducia (ovvero un'incertezza) quantificabile quando la formazione del campione risponde a determinati criteri di tipo probabilistico.
Il campionamento si usa quando si vuole conoscere uno o più parametri di una popolazione, senza doverne analizzare ogni elemento: questo per motivi di costi intesi in termini monetari, di tempo, di qualità o di disagio o perché analizzare un elemento lo distrugge rendendo inutilizzabile l'informazione ottenuta.
Modalità di selezione del campione sono:
Nella pratica quotidiana dei sondaggi di opinione e delle ricerche di mercato vengono usati tutti e quattro gli approcci.
La scelta di un tipo di campionamento avviene in base alle proprietà degli stimatori di alcuni parametri oppure per tener conto di problemi di costo, mobilità o altro.
Concetti chiave sono:
Benché già nel Settecento si sia notato il vantaggio nell'esaminare un sottinsieme della popolazione per generalizzare i risultati alla popolazione complessiva, è solo dalla fine dell'Ottocento che la discussione sulla "scientificità" del campionamento viene posta in modo esplicito alla comunità statistica.
Già agli inizi del Novecento si vanno delineando le caratteristiche che un campione deve avere, ovvero che deve essere scelto in maniera casuale, e nell'arco di pochi anni compaiono i primi studi che mettono in evidenza che il campione non deve essere necessariamente un campione semplice ma può essere più complesso, per esempio stratificando.
Importanti autori che hanno fatto la storia della teoria dei campioni sono stati tra gli altri: 
Nel 1925, durante il congresso di Roma, l'Istituto Internazionale di Statistica accetta definitivamente come scientifico il metodo campionario, distinguendo il campionamento casuale dal campionamento ragionato.
Altri autori importanti nella ricerca teorica ed applicata sul campionamento furono George Gallup e William G. Cochran.
</text>
</doc>
<doc id="896" url="https://it.wikipedia.org/wiki?curid=896">
<title>Correlazione (statistica)</title>
<text>
In statistica, una correlazione è una relazione tra due variabili tale che a ciascun valore della prima corrisponda un valore della seconda, seguendo una certa regolarità .
Il termine apparve per la prima volta in un'opera di Francis Galton, "Hereditary Genius" (1869). Non fu definita in modo più approfondito (la moralità di un individuo e la sua instabilità morale sono non correlate).
Otto anni dopo, nel 1877, lo stesso Galton scoprì che i coefficienti di regressione lineare tra X e Y sono gli stessi se - ad entrambe le variabili - viene applicata la deviazione standard σ e σ: Galton utilizzò in realtà lo scarto interquartile, definendo il parametro "coefficiente di co-relazione" e abbreviando "regressione" in "r".
In base alle caratteristiche presentate, la correlazione può definirsi:
Inoltre, le correlazioni possono essere:
Il grado di correlazione tra due variabili viene espresso tramite l'indice di correlazione. Il valore che esso assume è compreso tra −1 (correlazione inversa) e 1 (correlazione diretta e assoluta), con un indice pari a 0 che comporta l'assenza di correlazione; il valore nullo dell'indice non implica, tuttavia, che le variabili siano indipendenti.
I coefficienti di correlazione sono derivati dagli indici, tenendo presenti le grandezze degli scostamenti dalla media. In particolare, l'indice di correlazione di Pearson è calcolato come rapporto tra la covarianza delle due variabili e il prodotto delle loro deviazioni standard.:
Va comunque notato che gli indici e i coefficienti di correlazione siano da ritenersi sempre approssimativi, a causa dell'arbitrarietà con cui sono scelti gli elementi: ciò è vero, in particolare, nei casi di correlazioni multiple.
Contrariamente a quanto si potrebbe intuire, la correlazione non dipende da un rapporto di causa-effetto quanto dalla tendenza di una variabile a cambiare in funzione di un'altra. Le variabili possono essere tra loro dipendenti (per esempio la relazione tra stature dei padri e dei figli) oppure comuni (relazione tra altezza e peso di una persona). 
Nel cercare una correlazione statistica tra due grandezze, per determinare un possibile rapporto di causa-effetto, essa non deve risultare una correlazione spuria.
</text>
</doc>
<doc id="1555" url="https://it.wikipedia.org/wiki?curid=1555">
<title>Scarto quadratico medio</title>
<text>
Lo scarto quadratico medio (o deviazione standard o scarto tipo) è un indice di dispersione statistico, vale a dire una stima della variabilità di una popolazione di dati o di una variabile casuale.
È uno dei modi per esprimere la dispersione dei dati intorno ad un indice di posizione, quale può essere, ad esempio, la media aritmetica o una sua stima. Ha pertanto la stessa unità di misura dei valori osservati (al contrario della varianza che ha come unità di misura il quadrato dell'unità di misura dei valori di riferimento). In statistica la precisione si può esprimere come lo scarto quadratico medio.
Il termine ""standard deviation"" è stato introdotto in statistica da Pearson nel 1894 assieme alla lettera greca formula_1 (sigma) che lo rappresenta. Il termine italiano "deviazione standard" ne è la traduzione più utilizzata nel linguaggio comune; il termine dell'Ente Nazionale Italiano di Unificazione è tuttavia "scarto tipo", definito come la radice quadrata positiva della varianza per lo meno fin dal 1984.
Se non indicato diversamente, lo scarto quadratico medio è la radice quadrata della varianza, la quale viene coerentemente rappresentata con il quadrato di sigma (formula_2).
In statistica lo scarto quadratico medio di un carattere rilevato su una popolazione di formula_3 unità statistiche si definisce esplicitamente come:
dove formula_5 è la media aritmetica di formula_6.
Formalmente lo scarto quadratico medio di una variabile può essere calcolata a partire dalla funzione generatrice dei momenti, in particolare è la radice quadrata della differenza tra il momento secondo ed il momento primo elevato al quadrato.
A partire dallo scarto quadratico medio si definisce anche il coefficiente di variazione o la "deviazione standard relativa" come il rapporto tra lo scarto tipo formula_7 e il valore assoluto della media aritmetica della variabile in esame:
Questo indice relativo (che viene spesso espresso in termini percentuali) consente di effettuare confronti tra dispersioni di dati di tipo diverso, indipendentemente dalle loro quantità assolute.
Nell'ambito della statistica inferenziale (dove è noto solo un campione della popolazione), soprattutto nell'ambito della teoria della stima, a volte si rimpiazza il denominatore formula_3 con formula_10 ottenendo:
Sostanzialmente, poiché non è nota la media dell'intera popolazione, ma solo una sua stima (la media del campione), bisogna utilizzare formula_10 per ottenere uno stimatore corretto formula_13 della varianza incognita formula_7 di formula_6 sull'intera popolazione a partire dai dati del campione. La sua radice quadrata diviene lo scarto quadratico medio "corretto".
Questa correzione al denominatore fa sì che la nuova definizione sia un po' più grande della precedente, correggendo così la tendenza della precedente a sottostimare le incertezze soprattutto nel caso in cui si lavori con pochi dati (formula_3 piccolo).
Osserviamo il caso limite di formula_17, cioè quando si ha un campione di un solo elemento: la prima definizione dà il risultato formula_18, che ovviamente non è molto ragionevole nell'ambito della statistica inferenziale, mentre quella "corretta" dà un risultato non definito del tipo formula_19, rispecchiando così la totale ignoranza inerente all'incertezza su una singola misura. In questo senso, si dice che la statistica non dice nulla sul singolo caso.
Osserviamo che la differenza tra le due definizioni per campioni molto estesi è spesso numericamente insignificante.
Il calcolo può essere semplificato come segue:
cioè, applicando il tutto alla formula originale:
Sia formula_6 una variabile aleatoria, lo scarto quadratico medio è definito come la radice quadrata della varianza di formula_6
Formalmente lo scarto quadratico medio di una variabile aleatoria può essere calcolato a partire dalla funzione generatrice dei momenti, in particolare è la radice quadrata della differenza tra il momento secondo ed il momento primo elevato al quadrato, cioè
dove formula_26 è il valore atteso di formula_6.
In ambito finanziario, lo scarto quadratico medio viene usato per indicare la variabilità di un'attività finanziaria e dei suoi payoff (rendimenti). Esso fornisce quindi, implicitamente, una misura della volatilità dell'attività, quindi del suo rischio.
In fisica, è un ottimo indice dell'errore casuale della misurazione di una grandezza fisica.
In ambito sportivo è utilizzato per valutare la prestazione di un giocatore di bowling in riferimento ad un certo numero di partite. Il valore trovato non incide sul punteggio ma sintetizza le capacità e i miglioramenti del giocatore.
In ingegneria, è uno dei parametri da considerare per valutare la capacità di un processo produttivo.
Nelle applicazioni informatiche, è a volte conveniente utilizzare la formula
che consente, con sole tre variabili formula_29, di calcolare lo scarto quadratico medio, oltre che la media, di un flusso di numeri di lunghezza formula_3 senza dover ricorrere ad una memorizzazione degli stessi.
</text>
</doc>
<doc id="26742" url="https://it.wikipedia.org/wiki?curid=26742">
<title>Tipo di dato</title>
<text>
Un tipo di dato, in informatica, indica l'insieme di valori che una variabile, o il risultato di un'espressione, possono assumere e le operazioni che su tali valori si possono effettuare. Dire per esempio che la variabile X è di tipo "numero intero" significa affermare che X può assumere come valori solo numeri interi (appartenenti ad un certo intervallo) e che su tali valori sono ammesse solo certe operazioni (ad esempio le operazioni aritmetiche elementari).
Ogni linguaggio di programmazione consente di usare, in modo più o meno esplicito, un certo numero di tipi di dati predefiniti di uso generale, e di solito fornisce strumenti per definire nuovi tipi sulla base delle necessità specifiche di un programma.
Può anche accadere, durante la scrittura di un programma, che sia utile o necessario "tradurre" una variabile di un certo tipo in una variabile di un altro tipo (l'operazione è detta "type casting"): alcuni linguaggi mettono a disposizione costrutti sintattici per questo scopo, ma in altri casi è necessario scrivere una funzione che associ i valori di un tipo a quelli dell'altro.
Si parla di "tipizzazione statica" quando a una variabile viene associato rigidamente un tipo che rimane lo stesso per tutto il programma e di "tipizzazione dinamica" quando una variabile può cambiare tipo durante l'esecuzione del programma.
Ad esempio, il C è un linguaggio con tipizzazione statica; C++ e Java permettono sia tipizzazione statica che dinamica; Lisp, Visual Basic e Python sono linguaggi con tipizzazione dinamica.
Per vedere come funziona il controllo sui tipi, si può considerare il seguente esempio in pseudocodice:
In questo esempio: (1) dichiara la variabile x, (2) associa a x il valore di tipo intero 5, (3) associa a x il valore di tipo stringa "ciao" (qui si suppone che "intero" e "stringa" siano due tipi). Nella maggior parte dei linguaggi con tipizzazione statica un codice di questo tipo sarebbe illegale, poiché (2) e (3) associano alla variabile x valori appartenenti a tipi diversi; al contrario un linguaggio con tipizzazione totalmente dinamica troverebbe questo codice perfettamente legale. In quest'ultimo caso, ovviamente, la dichiarazione iniziale in (1) avrebbe dovuto specificare, con una qualche sintassi, il tipo da associare a x. Un esempio in Java potrebbe essere come segue:
Un linguaggio con tipizzazione dinamica consente di catturare "errori di tipo" (cioè errori dovuti a un uso scorretto dei valori che una variabile può assumere) solo durante l'esecuzione del programma.
Si consideri ad esempio il seguente pseudocodice:
In questo esempio: (1) associa a x il valore 5, (2) associa a y il valore "ciao" e (3) cerca di sommare x e y. In un linguaggio con tipizzazione dinamica, durante l'esecuzione del frammento di pseudocodice indicato, la variabile x risulterebbe (in quel momento) di tipo intero con valore 5, mentre la variabile y risulterebbe di tipo stringa con valore "ciao" (qui si suppone che "intero" e "stringa" siano due tipi). Se la definizione del linguaggio non ammette l'operazione di addizione fra un intero e una stringa, durante l'esecuzione del programma verrà segnalato un errore.
I tipi di dati possono essere classificati secondo la struttura in tipi "atomici" o "primitivi" e tipi "derivati". I tipi primitivi sono i tipi semplici che non possono essere decomposti, come ad esempio numeri interi o booleani; ogni linguaggio tipizzato ne possiede. I tipi derivati si ottengono dai tipi atomici mediante opportuni operatori forniti dal linguaggio: essi includono i tipi strutturati ("record") o gli "array", ma anche i puntatori di un tipo fissato (in linguaggi come il C), i tipi funzione (specialmente nei linguaggi funzionali), le classi dei linguaggi object-oriented e così via.
Un'altra classificazione suddivide i tipi di dati in "predefiniti" e "definiti dall'utente". Si potrebbe pensare che i tipi predefiniti coincidano con i tipi atomici, mentre i tipi definiti dall'utente siano essenzialmente quelli derivati: in realtà le due classificazioni non sono perfettamente sovrapponibili – ad esempio le "enumerazioni" in linguaggi come il C sono tipi atomici definiti dall'utente, mentre sempre in C le stringhe sono tipi derivati (dal tipo carattere) ma predefinite dal linguaggio. D'altra parte è quasi sempre vero che i tipi definiti dal programmatore sono necessariamente tipi derivati.
Alcuni dei tipi di dati più comuni nei linguaggi di programmazione sono i seguenti.
Il tipo booleano ha due soli valori: "true" ("vero") e "false" ("falso"). Essi vengono utilizzati in modo speciale nelle espressioni condizionali per controllare il flusso di esecuzione e possono essere manipolati con gli operatori booleani AND, OR, NOT e così via.
Anche se in teoria basterebbe un solo bit per memorizzare un valore booleano, per motivi di efficienza si usa in genere un'intera parola di memoria, come per i numeri interi "piccoli" (una parola di memoria a 8 bit, per esempio, può memorizzare numeri da 0 a 255, ma il tipo booleano utilizza solo i valori 0 e 1).
I tipi di dati numerici includono i numeri interi e i numeri razionali in virgola mobile, che sono astrazioni dei corrispondenti insiemi di numeri della matematica. Quasi tutti i linguaggi includono tipi di dati numerici come tipi predefiniti e forniscono un certo numero di operatori aritmetici e di confronto su di essi.
A differenza degli insiemi numerici della matematica, i tipi di dati numerici sono spesso limitati (includono cioè un "massimo" e un "minimo" numero rappresentabile), dovendo essere contenuti in una singola parola ("word") di memoria.
Il tipo "carattere" contiene un carattere, generalmente ASCII, memorizzato in un byte. Tuttavia in questi anni si sta affermando il nuovo standard Unicode per i caratteri, che prevede 16 bit (che generalmente corrisponde a una parola di memoria) per la rappresentazione di un singolo carattere. Molti linguaggi tradizionali si sono adattati a questo standard emergente introducendo, in aggiunta al tipo "carattere a 8 bit", un nuovo tipo "carattere a 16 bit", talvolta detto "wide char" (il linguaggio Java è invece un esempio di linguaggio moderno che gestisce direttamente tutti i caratteri nel formato Unicode).
Le "stringhe" sono sequenze di caratteri di lunghezza finita. I linguaggi possono fornire operazioni per la concatenazione di stringhe, la selezione di sottostringhe di una stringa data, ecc.
Le enumerazioni sono insiemi finiti di identificatori, generalmente specificati dal programmatore. In linguaggi come C e C++ è possibile definire dei tipi enumerazione con una sintassi simile alla seguente:
Una variabile di tipo "Color" potrà in tal caso assumere solo i valori "RED", "GREEN" e "BLUE". Rispetto alle tecniche tradizionali per gestire analoghi generi di dati, che prevedevano semplicemente di adottare una convenzione numerica implicita (per esempio scrivo "1" per intendere "rosso", "2" per intendere "verde" e così via), i tipi enumerati forniscono una maggiore leggibilità e una migliore astrazione sui dati.
I valori di tipo puntatore sono indirizzi di memoria di variabili, oggetti (o altri elementi di programma). L'operatore con cui, dato un puntatore, si accede all'oggetto puntato viene detto "operatore di dereferenziazione" ("dereferencing"). Molti linguaggi offrono anche un operatore "inverso", spesso detto "operatore indirizzo-di", che data una variabile consente di ricavarne l'indirizzo. Un insieme esteso di operazioni sui puntatori viene fornito dai linguaggi dotati di aritmetica dei puntatori.
L'uso di puntatori è spesso necessario per costruire strutture dati complesse e dalla forma non prevedibile a priori e/o variabile nel tempo come grafi, alberi, liste e così via; inoltre, i puntatori possono essere usati per realizzare il passaggio di parametri per riferimento nei linguaggi che non lo offrono come meccanismo nativo. Se usati in modo indiscriminato, tuttavia, i puntatori possono portare allo sviluppo di software molto complesso e, soprattutto, condurre a errori di programmazione difficili da individuare. Per questo motivo alcuni linguaggi, tra cui Java, tentano di limitarne l'uso.
Alcuni linguaggi forniscono un meccanismo simile ai puntatori, ma caratterizzato da dereferenziazione implicita; le variabili di questo tipo sono dette "riferimenti".
Le caratteristiche dei riferimenti sono diverse nei diversi linguaggi, ma in generale hanno una caratteristica comune: sintatticamente, i tipi riferimento non richiedono l'uso di un operatore di dereferenziazione, e non prevedono un operatore "indirizzo di"; di conseguenza, sui riferimenti non è possibile l'aritmetica dei puntatori.
Esempi di linguaggi che ne fanno uso, con caratteristiche diverse, sono il C++ (vedi anche riferimento (C++)) e Java.
Un array è una sequenza finita di elementi appartenenti a un determinato tipo, indicizzata mediante un numero intero.
I "record", detti anche "tuple" o "strutture", sono aggregati di tipi di dati più semplici, la cui composizione può essere definita dall'utente. Un record è necessario per mantenere informazioni eterogenee correlate: potrebbero ad esempio essere usati per modellare le schede dell'archivio di una biblioteca, che devono contenere stringhe per il titolo di un libro e il nome del suo autore, ma anche un valore numerico indicante la collocazione; ciascuna di queste informazioni ("campi" del record) può essere acceduta in modo indipendente specificandone il nome: in molti linguaggi, specialmente quelli derivati dal C, questa operazione è specificata mediante l'operatore . (punto):
Poiché in molti casi è necessario imporre una coerenza ai dati contenuti in un record, certi linguaggi consentono di definire tipi di dati astratti, che sono essenzialmente record la cui struttura fisica non è visibile e che possono essere manipolati soltanto mediante certe operazioni fidate(non necessariamente in ogni caso) specificate in un'interfaccia. Sui tipi di dati astratti si basano anche i tipi classe tipici della programmazione orientata agli oggetti.
In molti linguaggi, una variabile può contenere un puntatore a funzione. Dereferenziando la variabile sarà quindi possibile invocare una funzione definita a runtime.
Per garantire che la funzione sia invocata con argomenti di tipo corretto, il tipo di una variabile funzione è definito dalla "signature" della funzione, ovvero dal tipo del valore ritornato e dal tipo e ordine degli argomenti della funzione.
Di conseguenza, le funzioni possono restituire funzioni così come gli altri tipi di dati, e possono ricevere funzioni come argomenti, al pari degli altri tipi di dati. Un esempio in Perl:
I costruttori di tipo visti finora sono costrutti definiti dal linguaggio di programmazione, e possono essere utilizzati dal programmatore per costruire nuovi tipi di dato sulla base di tipi esistenti.
In alcuni linguaggi è anche possibile specificare anche "tipi generici" (detti anche "parametrici" o "template"), che sono in effetti costruttori di tipo definiti dal programmatore. Essi non possono essere direttamente usati nei programmi, ma devono essere applicati ad uno o più tipi esistenti (che sono quindi i parametri del costruttore) per generare nuovi tipi di dato. La relazione tra costruttore di tipo e tipo concreto può quindi essere vista in analogia a quella tra classe ed oggetto nella programmazione orientata agli oggetti.
Sarà quindi possibile definire il template "lista", che può quindi essere istanziato sugli interi per ottenere il tipo "lista di interi", o su booleani per ottenere "lista di booleani".
Il principale beneficio è quello di non essere costretti a dare una definizione diversa delle liste per ogni possibile tipo contenuto, in modo tale da ridurre il codice replicato.
Nei linguaggi orientati agli oggetti, i template sono costruttori di classi parametrici rispetto ad uno o più tipi, e quindi includono anche il codice per operare sulle classi stesse. In quanto tali, sono particolarmente adatti alla definizione di tipi "contenitore" (come lista, insieme, albero), che comprendono sia la struttura dati per rappresentare in memoria il contenitore che le operazioni per accedere ai suoi membri.
In questo modo, il codice per la definizione e l'accesso alle strutture dati viene reso ortogonale sia rispetto ai tipi di dato memorizzati che rispetto agli algoritmi che operano sulle strutture dati. Questo approccio è stato seguito in modo organico dalla libreria STL, inclusa nel linguaggio C++.
Il "controllo sui tipi" ("type checking") è il procedimento che permette di verificare se i vincoli imposti dai tipi sono soddisfatti. Tale verifica può avvenire sia durante la compilazione, e si parla in tal caso di "controllo statico" (in inglese "static check"), sia durante l'esecuzione del programma, e si parla in tal caso di "controllo dinamico" (in inglese "dynamic check"). Alcuni linguaggi operano alcuni controlli di tipo in fase di compilazione e altri durante l'esecuzione.
Se un linguaggio impone regole rigide sui tipi, impedendo qualsiasi uso dei dati incoerente col tipo specificato in fase di dichiarazione, si dice che esso è "fortemente tipizzato"; in caso contrario, che è "debolmente tipizzato".
</text>
</doc>
<doc id="2292" url="https://it.wikipedia.org/wiki?curid=2292">
<title>Ipotesi nulla</title>
<text>
Un'ipotesi nulla (in inglese "null hypothesis," che significa letteralmente ipotesi zero) è un'affermazione sulla distribuzione di probabilità di una o più variabili casuali. Si intende per ipotesi nulla l'affermazione secondo la quale non ci sia differenza oppure non vi sia relazione tra due fenomeni misurati, o associazione tra due gruppi. Solitamente viene assunta vera finché non si trova evidenza che la confuti.
Nel test statistico viene verificata in termini probabilistici la validità di un'ipotesi statistica, detta appunto ipotesi nulla, di solito indicata con "H".
Attraverso una funzione dei dati campionari si decide se accettare l'ipotesi nulla o meno. Nel caso l'ipotesi nulla venga rifiutata si accetterà l'ipotesi alternativa, indicata con "H".
Se si rifiuta un'ipotesi nulla che nella realtà è vera allora si dice che si è commesso un errore di prima specie (o falso positivo). Accettando invece un'ipotesi nulla falsa si commette un errore di seconda specie (o falso negativo).
L'ipotesi può essere di tipo funzionale se riferita alla forma della f (x;θ) con f funzione di densità o di probabilità, o parametrica se riferita al vettore incognito θ.
L'ipotesi è semplice quando specifica completamente la f (x;θ). Nel caso un'ipotesi non sia semplice si dirà composta.
Quando si considera un solo parametro l'ipotesi semplice è del tipo θ=θ, dove θ è un valore particolare. Un'ipotesi è unilaterale se è del tipo θ &gt; θ oppure del tipo θ &lt; θ.
Un'ipotesi è bilaterale se è del tipo θ ≠ θ oppure del tipo θ &lt; θ e θ &gt; θ.
</text>
</doc>
<doc id="2380" url="https://it.wikipedia.org/wiki?curid=2380">
<title>Indice di posizione</title>
<text>
tatistica, gli indici di posizione (anche detti indicatori di posizione, o indici di tendenza centrale o misure di tendenza centrale, in quanto in generale tendono a essere posizionati centralmente rispetto agli altri dati della distribuzione) sono indici che danno un'idea approssimata dell'ordine di grandezza (la posizione sulla scala dei numeri, appunto) dei valori esistenti.
Sono indici di posizione:
Un modo per rappresentare graficamente 
alcuni indici di posizione è il box-plot.
</text>
</doc>
<doc id="19386" url="https://it.wikipedia.org/wiki?curid=19386">
<title>Algoritmo genetico</title>
<text>
Un algoritmo genetico è un algoritmo euristico utilizzato per tentare di risolvere problemi di ottimizzazione per i quali non si conoscono altri algoritmi efficienti di complessità lineare o polinomiale. L'aggettivo "genetico", ispirato al principio della selezione naturale ed evoluzione biologica teorizzato nel 1859 da Charles Darwin, deriva dal fatto che, al pari del modello evolutivo darwiniano che trova spiegazioni nella branca della biologia detta genetica, gli algoritmi genetici attuano dei meccanismi concettualmente simili a quelli dei processi biochimici scoperti da questa scienza.
In sintesi gli algoritmi genetici consistono in algoritmi che permettono di valutare diverse soluzioni di partenza (come se fossero diversi individui biologici) e che ricombinandole (analogamente alla riproduzione biologica sessuata) ed introducendo elementi di disordine (analogamente alle mutazioni genetiche casuali) producono nuove soluzioni (nuovi individui) che vengono valutate scegliendo le migliori (selezione ambientale) nel tentativo di convergere verso soluzioni "di ottimo". Ognuna di queste fasi di ricombinazione e selezione si può chiamare generazione come quelle degli esseri viventi. Nonostante questo utilizzo nell'ambito dell'ottimizzazione, data la natura intrinsecamente casuale dell'algoritmo genetico, non vi è modo di sapere a priori se sarà effettivamente in grado di trovare una soluzione accettabile al problema considerato. Se si otterrà un soddisfacente risultato, non è detto che si capisca perché abbia funzionato, in quanto non è stato progettato da nessuno ma da una procedura casuale.
Gli algoritmi genetici rientrano nello studio dell'intelligenza artificiale e più in particolare nella branca della "computazione evolutiva", vengono studiati e sviluppati all'interno del campo dell'intelligenza artificiale e delle tecniche di soft computing, ma trovano applicazione in un'ampia varietà di problemi afferenti a diversi contesti quali l'elettronica, la biologia e l'economia.
La nascita degli algoritmi genetici trova origine dalle prime teorizzazioni di Ingo Rechenberg che, per la prima volta, nel 1960, cominciò a parlare di "strategie evoluzionistiche" all'interno dell'informatica.
La vera prima creazione di un algoritmo genetico è tuttavia storicamente attribuita a John Henry Holland che, nel 1975, nel libro "Adaptation in Natural and Artificial Systems" pubblicò una serie di teorie e di tecniche tuttora di fondamentale importanza per lo studio e lo sviluppo della materia. Agli studi di Holland si deve infatti sia il teorema che assicura la convergenza degli algoritmi genetici verso soluzioni ottimali sia il cosiddetto teorema degli schemi, conosciuto anche come "teorema fondamentale degli algoritmi genetici". Quest'ultimo teorema fu originariamente pensato e dimostrato su ipotesi di codifica binaria ma nel 1991, Wright, l'ha estesa a casi di codifica con numeri reali dimostrando anche che una tale codifica è preferibile nel caso di problemi continui d'ottimizzazione.
Enormi contributi si devono anche a John Koza che nel 1992 inventò la programmazione genetica ossia l'applicazione degli algoritmi genetici alla produzione di software in grado di evolvere diventando capace di svolgere compiti che in origine non era in grado di svolgere.
Nel 1995 Stewart Wilson re-inventò i sistemi a classificatori dell'intelligenza artificiale ri-denominandoli come XCS e rendendoli capaci di apprendere attraverso le tecniche degli algoritmi genetici mentre nel 1998 Herrera e Lozano presentarono un'ampia rassegna di operatori genetici. Gli operatori di Herrera e Lozano sono applicabili a soluzioni codificate mediante numeri reali ed hanno reso il campo dei numeri reali un'appropriata e consolidata forma di rappresentazione per gli algoritmi genetici in domini continui.
Prima dell'effettiva spiegazione del funzionamento degli algoritmi genetici, è necessario premettere che questi ereditano e riadattano dalla biologia alcune terminologie che vengono qui preventivamente presentate per una successiva maggiore chiarezza espositiva:
Un tipico algoritmo genetico, nel corso della sua esecuzione, provvede a fare evolvere delle soluzioni secondo il seguente schema di base:
L'iterazione dei passi presentati permette l'evoluzione verso una soluzione ottimizzata del problema considerato.
Poiché questo algoritmo di base soffre del fatto che alcune soluzioni ottime potrebbero essere perse durante il corso dell'evoluzione e del fatto che l'evoluzione potrebbe ricadere e stagnare in "ottimi locali" spesso viene integrato con la tecnica dell'"elitarismo" e con quella delle mutazioni casuali. La prima consiste in un ulteriore passo precedente al punto "3" che copia nelle nuove popolazioni anche gli individui migliori della popolazione precedente, la seconda invece successiva al punto "4" introduce nelle soluzioni individuate delle occasionali mutazioni casuali in modo da permettere l'uscita da eventuali ricadute in ottimi locali.
Come accennato le soluzioni al problema considerato, siano queste quelle casuali di partenza o quelle derivate da evoluzione, devono essere codificate con qualche tecnica.
Le codifiche più diffuse sono:
All'interno della codifica vettoriale è giusto introdurre anche i concetti di "schema" e di "blocchi costruttori" strettamente legati poi al teorema degli schemi di Holland.
La funzione di fitness è quella che permette di associare ad ogni soluzione uno o più parametri legati al modo in cui quest'ultima risolve il problema considerato. Generalmente è associata alle prestazioni computazionali e quindi alle prestazioni temporali della soluzione.
A causa di complessi fenomeni di interazione non lineare (epistaticità), non è dato per scontato né che da due soluzioni promettenti ne nasca una terza più promettente né che da due soluzioni con valori di fitness basso ne venga generata una terza con valore di fitness più basso. Per ovviare a questi problemi, durante la scelta delle soluzioni candidate all'evoluzione, oltre che sul parametro ottenuto dalla funzione di fitness ci si basa anche su particolari tecniche di "selezione". Le più comuni sono:
Per semplicità, durante la spiegazione del crossover, si farà riferimento alle codifiche vettoriali ma il procedimento per le codifiche ad albero è simile ed invece che essere applicato ai campi dei vettori viene applicato ai nodi dell'albero.
In base ad un operatore stabilito inizialmente, alcune parti dei geni delle soluzioni candidate all'evoluzione vengono mescolate per ricavare nuove soluzioni.
Gli operatori più comunemente utilizzati sono:
Non è detto che il crossover debba avvenire ad ogni iterazione dell'algoritmo genetico. Generalmente la frequenza di crossover è regolata da un apposito parametro comunemente denominato formula_9.
La mutazione consiste nella modifica pseudocasuale di alcune parti dei geni in base a coefficienti definiti inizialmente.
Queste modifiche alle volte sono utilizzate per migliorare il valore della funzione di fitness per la soluzione in questione e altre volte sono utilizzate per ampliare lo spazio di ricerca ed attuare la tecnica dell'elitarismo per non far ricadere l'evoluzione in ottimi locali.
La frequenza con cui deve avvenire una mutazione è generalmente regolata da un apposito parametro comunemente denominato formula_10.
Nel caso in cui si abbia più di un obiettivo da ottimizzare, è possibile utilizzare un algoritmo genetico multiobiettivo.
Sostanzialmente l'algoritmo funziona come quando va a perseguire un singolo obiettivo, quindi parte sempre da un certo numero di possibili soluzioni (la popolazione) e cerca di individuare, mediante diverse iterazioni, un certo numero di soluzioni ottimali, che si andranno a trovare su un fronte di Pareto. La diversità sta nel fatto che ora esistono due o più funzioni fitness da valutare.
In questa sezione verranno analizzati ed affrontati dei problemi didattici per mostrare come si applica un algoritmo genetico.
Il problema dello zaino consiste nel riuscire ad inserire in uno zaino con una certa capienza più oggetti possibili prelevati da un elenco dato rispettando anche particolari vincoli di peso.
La soluzione ottima consiste nel riuscire ad inserire nello zaino quanti più oggetti possibili senza superare i limiti di peso imposti.
Il problema del commesso viaggiatore consiste nel riuscire a visitare almeno una volta tutte le città presenti in un elenco, sfruttando al meglio i collegamenti tra queste e percorrendo meno strada possibile.
</text>
</doc>
<doc id="19505" url="https://it.wikipedia.org/wiki?curid=19505">
<title>Sistema esperto</title>
<text>
Un sistema esperto è un programma che cerca di riprodurre le prestazioni di una o più persone esperte in un determinato campo di attività, ed è un'applicazione o una branca dell'intelligenza artificiale.
I programmi utilizzati dai sistemi esperti sono in grado di porre in atto procedure di inferenza adeguate alla risoluzione di problemi particolarmente complessi, a cui potrebbe, se posto in una dimensione umana, porre rimedio solo un esperto del settore disciplinare in cui rientra la questione da risolvere. Ciò implica che tale sistema possa avvalersi in modo risoluto e autorevole delle istanze inferenziali che soggiacciono al corretto funzionamento del programma, cosicché sia capace di superare le incertezze e le difficoltà su cui volge la propria attività.
I sistemi esperti si differenziano dunque da altri programmi simili, in quanto, facendo riferimento a tecnologie elaborate in funzione dell'intelligenza artificiale, sono sempre in grado di esibire i passaggi logici che soggiacciono alle loro decisioni: proposito che, ad esempio, non è attuabile da parte della mente umana.
Il sistema esperto si compone principalmente di tre sezioni: 
Queste informazioni sono piuttosto generiche, ed estremamente flessibili per ciò che concerne la designazione di un programma con una tale definizione. Non esistono infatti sistemi capaci per davvero di soddisfare nella sua interezza il tipo di conoscenza che dovrebbe caratterizzare un sistema di tale fatta. Difatti, nella maggior parte dei programmi, le componenti che presiedono alle procedure di inferenza, non riescono ad attenere il rigore connaturato ad un algoritmo, in quanto nelle situazioni altamente complicate sarebbe troppo dispendioso analizzare ogni possibilità; si ricorre così allo stratagemma dell'euristica, che, tramite ragionamenti approssimativi ("fuzzy logic"), sacrifica la sicurezza dell'algoritmo per giungere a risultati altamente probabili, ma comunque fallibili.
I sistemi esperti si dividono in due categorie principali.
I sistemi esperti basati su regole sono dei programmi composti da regole nella forma codice_1 (se condizione, allora azione). Dati una serie di fatti, i sistemi esperti, grazie alle regole di cui sono composti, riescono a dedurre nuovi fatti.
Per esempio, supponiamo di avere un problema di salute, forniamo al sistema esperto i seguenti fatti:
il sistema esperto assume i fatti e sceglie una regola così formata:
Esempi di sistemi a regole sono Jess e CLIPS.
Un sistema esperto basato su alberi, dato un insieme di dati ed alcune deduzioni, creerebbe un albero che classificherebbe i vari dati. Nuovi dati verrebbero analizzati dall'albero e il nodo di arrivo rappresenterebbe la deduzione.
È da notare che un sistema esperto non è "intelligente" nel senso comune della parola, ossia in modo creativo. Le deduzioni di un sistema esperto non possono uscire dall'insieme di nozioni immesse inizialmente e dalle loro conseguenze. Ciò che li rende utili è che, come i calcolatori elettronici, possono maneggiare una gran quantità di dati molto velocemente e tenere quindi conto di una miriade di regole e dettagli che un esperto umano può ignorare, tralasciare o dimenticare.
</text>
</doc>
<doc id="19781" url="https://it.wikipedia.org/wiki?curid=19781">
<title>Clustering</title>
<text>
In statistica, il clustering o analisi dei gruppi (dal termine inglese "cluster analysis" introdotto da Robert Tryon nel 1939) è un insieme di tecniche di analisi multivariata dei dati volte alla selezione e raggruppamento di elementi omogenei in un insieme di dati. Le tecniche di "clustering" si basano su misure relative alla somiglianza tra gli elementi. In molti approcci questa similarità, o meglio, dissimilarità, è concepita in termini di distanza in uno spazio multidimensionale. La bontà delle analisi ottenute dagli algoritmi di "clustering" dipende molto dalla scelta della metrica, e quindi da come è calcolata la distanza. Gli algoritmi di "clustering" raggruppano gli elementi sulla base della loro distanza reciproca, e quindi l'appartenenza o meno ad un insieme dipende da quanto l'elemento preso in esame è distante dall'insieme stesso.
Le tecniche di "clustering" si possono basare principalmente su due "filosofie":
Esistono varie classificazioni delle tecniche di clustering comunemente utilizzate. Una prima categorizzazione dipende dalla possibilità che un elemento possa o meno essere assegnato a più cluster:
Un'altra suddivisione delle tecniche di clustering tiene conto del tipo di algoritmo utilizzato per dividere lo spazio:
Queste due suddivisioni sono del tutto trasversali, e molti algoritmi nati come "esclusivi" sono stati in seguito adattati nel caso "non-esclusivo" e viceversa.
Gli algoritmi di clustering di questa famiglia creano una partizione delle osservazioni minimizzando una certa funzione di costo:
dove formula_2 è il numero dei cluster, formula_3 è il formula_4-esimo cluster e formula_5 è la funzione di costo associata al singolo cluster. L'algoritmo più famoso appartenente a questa famiglia è il k-means, proposto da MacQueen nel 1967. Un altro algoritmo abbastanza conosciuto appartenente a questa classe è il Partitioning Around Medioid (PAM).
Le tecniche di clustering gerarchico non producono un partizionamento "flat" dei punti, ma una rappresentazione gerarchica ad albero. 
Questi algoritmi sono a loro volta suddivisi in due classi:
Una rappresentazione grafica del processo di clustering è fornita dal dendrogramma.
In entrambi i tipi di clustering gerarchico sono necessarie funzioni per selezionare la coppia di cluster da fondere ("agglomerativo"), oppure il cluster da dividere ("divisivo").
Nel primo caso, sono necessarie funzioni che misurino la "similarità" (o, indistintamente, la "distanza") tra due cluster, in modo da fondere quelli più simili. Le funzioni utilizzate nel caso agglomerativo sono:
Nei 4 casi precedenti, formula_10 indica una qualsiasi funzione distanza su uno spazio metrico.
Invece nel clustering divisivo è necessario individuare il cluster da suddividere in due sottogruppi. Per questa ragione sono necessarie funzioni che misurino la compattezza del cluster, la densità o la sparsità dei punti assegnati ad un cluster. Le funzioni normalmente utilizzate nel caso divisivo sono:
Nel "Clustering density-based", il raggruppamento avviene analizzando l'intorno di ogni punto dello spazio. In particolare, viene considerata la densità di punti in un intorno di raggio fissato.
Un esempio è il metodo di clustering Dbscan.
Algoritmi di clustering molto usati sono:
Il QT ("Quality Threshold") Clustering (Heyer et al., 1999) è un metodo alternativo di partizionare i dati, inventato per il clustering dei geni. Richiede più potenza di calcolo rispetto al "K"-Means, ma non richiede di specificare il numero di cluster "a priori", e restituisce sempre lo stesso risultato quando si ripete diverse volte.
L'algoritmo è:
La distanza tra un punto ed un gruppo di punti è calcolata usando il concatenamento completo, cioè come la massima distanza dal punto di ciascun membro del gruppo (vedi il "Clustering gerarchico agglomerativo" sulla distanza tra i cluster nella sezione clustering gerarchico).
</text>
</doc>
<doc id="3854" url="https://it.wikipedia.org/wiki?curid=3854">
<title>Scarto interquartile</title>
<text>
In statistica lo scarto interquartile (o differenza interquartile o ampiezza interquartile, in inglese "interquartile range" o "IQR") è la differenza tra il terzo e il primo quartile, ovvero l'ampiezza della fascia di valori che contiene la metà "centrale" dei valori osservati.
Lo scarto interquartile è un indice di dispersione, cioè una misura di quanto i valori si allontanino da un valore centrale. Viene utilizzato nel disegno del diagramma box-plot.
Lo scarto interquartile di una variabile aleatoria si ottiene tramite la funzione di ripartizione, come differenza formula_1
Per una variabile casuale normale formula_2 lo scarto interquartile è circa formula_3.
Per una variabile casuale di Cauchy formula_4 lo scarto interquartile è formula_5.
</text>
</doc>
<doc id="3872" url="https://it.wikipedia.org/wiki?curid=3872">
<title>Statistica descrittiva</title>
<text>
La statistica descrittiva è la branca della statistica che studia i criteri di rilevazione, classificazione, sintesi e rappresentazione dei dati appresi dallo studio di una popolazione o di una parte di essa (detta campione).
I risultati ottenuti nell'ambito della statistica descrittiva si possono definire certi, a meno di errori di misurazione dovuti al caso, che sono in media pari a zero. Da questo punto di vista si differenzia dalla statistica inferenziale, alla quale sono associati inoltre errori di valutazione.
La rilevazione dei dati di un'intera popolazione è detta "censimento". Quando invece l'indagine si concentra su un determinato campione rappresentativo, si parla di "sondaggio".
I dati raccolti possono essere classificati attraverso distribuzioni semplici o complesse:
I dati raccolti possono essere sintetizzati attraverso famiglie di indici, quali:
I dati di un'indagine possono essere rappresentati attraverso molteplici rappresentazioni grafiche, tra cui:
</text>
</doc>
<doc id="8208004" url="https://it.wikipedia.org/wiki?curid=8208004">
<title>Bagging</title>
<text>
Il bagging è una tecnica di machine learning che rientra nella categoria dell'Apprendimento ensemble. Nel bagging più modelli dello stesso tipo vengono addestrati su dataset diversi, ciascuno ottenuto dal dataset iniziale tramite campionamento casuale con rimpiazzo (bootstrap). Il nome "bagging" deriva dalla combinazione delle parole inglesi "bootstrap" (ovvero il campionamento casuale con rimpiazzo) e "aggregation" (in riferimento all'aggregazione di più modelli, tipico dell'Apprendimento ensemble).
</text>
</doc>
<doc id="4145289" url="https://it.wikipedia.org/wiki?curid=4145289">
<title>Distribuzione di probabilità a priori</title>
<text>
Nell'ambito dell'inferenza statistica bayesiana, una distribuzione di probabilità a priori, detta spesso anche distribuzione a priori, di una quantità incognita "p" (per esempio, supponiamo "p" essere la proporzione di votanti che voteranno per il politico Rossi in un'elezione futura) è la distribuzione di probabilità che esprimerebbe l'incertezza di "p" prima che i "dati" (per esempio, un sondaggio di opinione) siano presi in considerazione. Il proposito è di attribuire incertezza piuttosto che casualità a una quantità incerta. La quantità incognita può essere un parametro o una variabile latente.
Si applica il teorema di Bayes, moltiplicando la distribuzione a priori per la funzione di verosimiglianza e quindi normalizzando, per ottenere la distribuzione di probabilità a posteriori, la quale è la distribuzione condizionata della quantità incerta una volta ottenuti i dati.
Spesso una distribuzione a priori è l'accertamento soggettivo (elicitazione) di una persona esperta. Quando possibile, alcuni sceglieranno una "distribuzione a priori coniugata" per rendere più semplice il calcolo della distribuzione a posteriori.
I parametri di una distribuzione a priori sono chiamati "iperparametri", per distinguerli dai parametri del modello dei dati sottostanti. Per esempio, se si sta usando una distribuzione beta per modellare la distribuzione di un parametro "p" di una distribuzione di Bernoulli, allora:
Una "distribuzione a priori informativa" esprime una specifica, definita informazione circa una variabile.
Un esempio è la distribuzione a priori per la temperatura di domattina.
Un approccio ragionevole è costruire la distribuzione a priori come una distribuzione normale con valore atteso uguale alla temperatura mattutina di oggi, con una varianza uguale alla varianza giorno per giorno della temperatura atmosferica, oppure come una distribuzione della temperatura per quel tal giorno dell'anno.
Questo esempio ha una proprietà in comune con molte distribuzioni a priori, ovvero che la distribuzione a posteriori di un problema (temperatura odierna) diventa la distribuzione a priori per un altro problema (temperatura di domani); l'evidenza preesistente, che è già stata tenuta in conto, è parte della distribuzione a priori e come ulteriore evidenza viene accumulata. 
La distribuzione a priori è largamente determinata dall'evidenza piuttosto che da qualche assunzione originale, sempre che l'assunzione originale ammetta la possibilità (ossia sia compatibile) con quello che l'evidenza suggerisce. I termini "a priori" e "a posteriori" sono generalmente relativi a un dato o un'osservazione specifica.
Una "distribuzione a priori non informativa" esprime vaghezza o informazione a carattere generale circa una variabile.
Il termine "non informativa" può essere un po' fuorviante; spesso, tale tipo di distribuzione è chiamata "a priori non molto informativa", oppure "a priori oggettiva", cioè una distribuzione che non è soggettivamente esplicitata.
Le distribuzioni a priori non informative possono esprimere informazione "oggettiva" come ad esempio "la variabile è positiva" oppure "la variabile è minore di tal limite".
La più semplice e vecchia regola per determinare una distribuzione a priori non informativa è il principio d'indifferenza, il quale assegna a tutti gli eventi uguale probabilità.
In problemi di stima parametrica, l'uso di una distribuzione a priori non informativa dà risultati che sono non troppo differenti dall'analisi statistica convenzionale. Questo accade in quanto la funzione di verosimiglianza fornisce la parte maggiore dell'informazione rispetto a quella fornita dalla distribuzione a priori non informativa nel determinare una distribuzione a posteriori.
Vari tentativi sono stati fatti per trovare probabilità a priori, cioè distribuzioni di probabilità in un certo senso logicamente richieste dalla natura di uno stato di incertezza; queste sono soggette a controversia filosofica, con i sostenitori del metodo bayesiano approssimativamente divisi in due scuole: i "bayesiani oggettivistici", che credono che tali distribuzioni a priori esistano in molte situazioni, e i "bayesiani soggettivisti" che credono che in pratica le distribuzioni a priori rappresentino giudizi di opinione che non possono essere rigorosamente giustificati. Per la maggiore le più forti argomentazioni a favore della scuola oggettivistica furono date da Edwin T. Jaynes.
Come esempio di una distribuzione a priori, dovuta a, consideriamo una situazione in cui sappiamo che una pallina è nascosta sotto una di tre tazze rovesciate, A, B o C, ma nessun'altra informazione è disponibile circa la sua posizione. In questo caso una distribuzione a priori uniforme di formula_1 sembra intuitivamente verosimile la sola scelta ragionevole. Più formalmente, noi possiamo vedere che il problema rimane lo stesso se scambiamo le lettere identificative "A", "B" e "C" delle tazze. Sarebbe perciò strano scegliere una distribuzione a priori per la quale una permutazione delle lettere causerebbe un cambio nella nostra predizione circa la posizione dove la pallina sarà trovata; la distribuzione a priori uniforme è la sola che preserva questa invarianza. Se si accetta questo principio di invarianza allora si può vedere che la distribuzione a priori uniforme è la distribuzione logicamente corretta che rappresenta questo stato di conoscenza a priori. Si avrà notato che questa distribuzione a priori è "oggettiva" nel senso di essere la scelta corretta per rappresentare un particolare stato di conoscenza, ma non è oggettiva nel senso di essere una caratteristica del sistema osservato indipendente dall'osservatore: in realtà la pallina esiste sotto una specifica tazza e in questa situazione ha solo senso parlare di probabilità se c'è un osservatore con una conoscenza limitata del sistema ossia della posizione della pallina sotto le tazze.
Come esempio più controverso, Jaynes pubblicò un argomento basato sui gruppi di Lie suggerente che la distribuzione a priori rappresentante in maniera completa l'incertezza sarebbe la distribuzione a priori di Haldane "p"(1 − "p"). L'esempio fornito da Jaynes è quello di trovare un chimico in un laboratorio e di chiedergli di eseguire ripetutamente degli esperimenti di dissoluzione in acqua. La distribuzione a priori di Haldane da prevalentemente la maggiore probabilità agli eventi formula_2 and formula_3, indicando che il campione ogni volta si scioglierà oppure no, con uguale probabilità. Tuttavia se sono stati osservati campioni non disciogliersi in un esperimento e disciogliersi in un altro, allora questa distribuzione a priori è aggiornata alla distribuzione uniforme sull'intervallo [0, 1]. Questo risultato si ottiene applicando il teorema di Bayes all'insieme di dati consistente in un'osservazione di dissoluzione e una di non dissoluzione, usando la distribuzione a priori precedente. sulla base che essa fornisce una distribuzione a posteriori impropria che pone il 100% del contenuto di probabilità sia a "p" = 0 o a "p" = 1 se un numero finito di esperimenti ha dato lo stesso risultato (ad esempio il discioglimento). La distribuzione a priori di Jeffreys "p"(1 − "p") è perciò preferita ("cfr." sotto).
Se lo spazio parametrico X è dotato di una struttura di gruppo naturale che lascia invariato il nostro stato di conoscenza bayesiano, allora la distribuzione a priori può essere costruita proporzionale alla Misura di Haar. Questo può essere visto come una generalizzazione del principio di invarianza che giustificava la distribuzione a priori uniforme dell'esempio delle tre tazze visto sopra. Per esempio, in fisica ci si aspetta che un esperimento dia i medesimi risultati indipendentemente dalla scelta dell'origine del sistema di coordinate. Questo induce la struttura gruppale del gruppo delle traslazioni su "X", il quale determina la distribuzione di probabilità a priori come una distribuzione a priori impropria costante. Analogamente alcuni sistemi fisici presentano un'invarianza di scala (ossia i risultati sperimentali sono indipendenti dal fatto che, ad esempio, usiamo centimetri o pollici). In tal caso il gruppo di scala è la struttura di gruppo naturale, e la corrispondente distribuzione a priori su "X" è proporzionale a 1/"x". Qualche volta risulta importante se viene usata la misura di Haar invariante a sinistra piuttosto che quella invariante a destra. Per esempio, le misure di Haar invarianti a destra e a sinistra sul gruppo affine non sono uguali. Berger (1985, p. 413) arguisce che la scelta corretta è la misura di Haar invariante a destra.
Un'altra idea, supportata da Edwin T. Jaynes, è di usare il principio di massima entropia (MAXENT). La motivazione è che l'entropia di Shannon di una distribuzione di probabilità misura l'ammontare di informazione contenuta nella distribuzione. Maggiore è l'entropia, minore è l'informazione fornita dalla distribuzione. Perciò, mediante la massimizzazione dell'entropia sopra un adeguato insieme di distribuzioni di probabilità su "X", si trova la distribuzione che è meno informativa nel senso che essa contiene il minore ammontare di informazione consistente con le costrizioni definite dall'insieme scelto. Per esempio, la distribuzione a priori di massima entropia su uno spazio discreto, dato solo il fatto che la probabilità è normalizzata a 1, è la distribuzione a priori che assegna uguale probabilità ad ogni stato. Mentre nel caso continuo, la distribuzione a priori di massima entropia con densità normalizzata, media nulla e varianza unitaria, è la ben nota distribuzione normale. Il principio di minima entropia incrociata generalizza il principio di massima entropia al caso di "aggiornamento" di una distribuzione a priori arbitraria con adeguate costrizioni nel senso di massima entropia.
Un'idea collegata, la distribuzione a priori di riferimento, fu introdotta da José-Miguel Bernardo. Qui l'idea è di massimizzare il valore atteso della divergenza di Kullback–Leibler della distribuzione a posteriori rispetto alla distribuzione a priori. Questo massimizza l'informazione attesa riguardante "X" quando la densità a priori è "p"("x"); perciò, in un certo senso, "p"("x") è la distribuzione a priori "meno informativa" riguardo X. La distribuzione a priori di riferimento è definita nel limite asintotico, cioè si considera il limite delle distribuzioni a priori così ottenute come il numero di dati va all'infinito. Nei problemi multivariati spesso vengono scelte come distribuzioni a priori oggettive le distribuzioni a priori di riferimento, dato che altre scelte (ad esempio la regola di Jeffreys possono portare a distribuzioni a priori dal comportamento problematico.
Distribuzioni a priori oggettive possono anche essere derivate da altri principi, come le teorie dell'informazione o le teorie della codifica (vedi ad esempio lunghezza di descrizione minima) oppure della statistica frequentista.
Problemi filosofici legati alle distribuzioni a priori non informative sono associati alla scelta di una metrica appropriata o scala di misurazione. Supponiamo di volere una distribuzione a priori per la valocità di un corridore a noi sconosciuto. Potremmo specificare, diciamo, per la sua velocità una distribuzione a priori di tipo normale, ma in alternativa potremmo specificare una distribuzione a priori normale per il tempo impiegato a percorrere 100 metri, il quale è proporzionale al reciproco della prima distribuzione a priori. Queste due distribuzioni a priori sono effettivamente differenti, ma non è chiaro quale delle due preferire. Il metodo, spesso sopravvalutato, di trasformazione dei gruppi di Jaynes può rispondere a tale questione in varie situazioni.
In maniera simile, se ci è chiesto di stimare una proporzione incognita tra 0 e 1, noi possiamo affermare che tutte le proporzioni sono ugualmente probabili ed usare una distribuzione a priori uniforme. Alternativamente, potremmo dire che tutti gli ordini di grandezza per la proporzione sono ugualmente probabili, e scegliere la distribuzione a priori logaritmica, la quale è la distribuzione a priori uniforme sul logaritmo della proporzione. La distribuzione a priori di Jeffreys tenta di risolvere questo problema calcolando una distribuzione a priori che esprime la medesima credenza indipendentemente dalla metrica utilizzata. La distribuzione a priori di Jeffreys per una proporzione incognita "p" è "p"(1 − "p"), che differisce da quella raccomandata da Jaynes.
Distribuzioni a priori basate sulla nozione di probabilità algoritmica vengono impiegate nel campo dell'inferenza induttiva come base induttiva in configurazioni del tutto generali.
Problemi pratici associati con le distribuzioni a priori non informative includono il requisito che la distribuzione a posteriori sia propria. Le distribuzioni a priori non informative su variabili continue, non limitate sono improprie. Questo non è necessariamente un problema se la distribuzione a posteriori è propria. Un altro argomento importante è quello in cui se una distribuzione a priori non informativa viene usata in maniera regolare, cioè con svariati insiemi di dati, allora essa avrebbe buone proprietà frequentiste. Normalmente un bayesiano non dovrebbe porsi questo problema, ma potrebbe essere importante farlo in questa situazione. Per esempio, uno potrebbe volere che qualsiasi regola di decisione basata sulla distribuzione a posteriori sia ammissibile sotto la funzionedi perdita adottata. Sfortunatamente, l'ammissibilità è difficile da verificare, nonostante vari risultati siano noti ("cfr." ad esempio, Berger and Strawderman, 1996). Il problema è particolarmente acuto con i modelli di Bayes gerarchici; le distribuzioni a priori usuali (ad esempio la distribuzione a priori di Jeffreys) possono dare regole di decisione praticamente inammissibili se impiegate ai livelli gerarchici più elevati.
Se il teorema di Bayes viene scritto come
allora è chiaro che si otterrebbe il medesimo risultato se tutte le probabilità a priori "P"("A") e "P"("A") fossero moltiplicate per una data costante; lo stesso sarebbe vero per una variabile casuale continua. Se la sommatoria al denominatore converge, le probabilità a posteriori sommeranno (o integreranno) ancora a 1 anche se i valori della distribuzione a priori non lo fanno, e in tal modo può solo essere necessario richiedere alle distribuzioni a priori di essere specificate nella proporzione corretta. Spingendo oltre questa idea, in molti casi non è neanche richiesto che la somma o l'integrale dei valori della distribuzione a priori sia finita per ottenere risposte significative circa le probabilità a posteriori. Quando questo è il caso, la distribuzione a priori è chiamata distribuzione a priori impropria. Tuttavia, se la distribuzione a priori è impropria, allora non è necessario che la distribuzione a posteriori sia propria. Questo è chiaro nella situazione in cui l'evento "B" è indipendente da tutti gli altri eventi "A".
Vari statistici usano le distribuzioni a priori improprie come distribuzioni a priori non informative. Per esempio, se hanno bisogno di una distribuzione a priori per la media e la varianza di una variabile casuale, allora essi assumono "p"("m", "v") ~ 1/"v" (per "v" &gt; 0) il che suggerirebbe che qualsiasi valore per la media è "ugualmente probabile" e che un valore per la varianza positiva diventa "meno probabile" in proporzione inversa al suo valore. Molti autori (Lindley, 1973; De Groot, 1937; Kass and Wasserman, 1996) mettono in guardia contro il pericolo di sovra-interpretare tali distribuzioni a priori poiché non sono densità di probabilità. La loro sola rilevanza che esse hanno si trova nella distribuzione a posteriori corrispondente, fintanto che questa è ben definita per tutte le osservazioni. (La distribuzione a priori di Haldane è un tipico controesempio.)
Esempi di distribuzioni a priori includono:
Il concetto di probabilità algoritmica fornisce una via per specificare la probabilità delle distribuzioni a priori basata sulla complessità relativa di modelli presi in considerazione e tra loro alternativi.
</text>
</doc>
<doc id="4302983" url="https://it.wikipedia.org/wiki?curid=4302983">
<title>Foresta casuale</title>
<text>
Una foresta casuale (in inglese: "random forest") è un classificatore d'insieme ottenuto dall'aggregazione tramite bagging di alberi di decisione
L'algoritmo per la creazione di una una foresta casuale fu sviluppato orignariamente da Leo Breiman e Adele Cutler.
Il nome viene dalle foreste di decisione casuali che furono proposte per primo da Tin Kam Ho dei Bell Labs nel 1995.
Il metodo combina l'idea dell'insaccamento di Breiman della selezione casuale delle caratteristiche, introdotta indipendentemente da Ho e Amit Geman per costruire una collezione di alberi di decisione con la variazione controllata.
La selezione di un sottoinsieme di caratteristiche è un esempio del metodo del sottoinsieme casuale che, nella formulazione di Ho, è un modo di implementare la discriminazione stocastica proposta da Eugene Kleinberg.
</text>
</doc>
<doc id="4303005" url="https://it.wikipedia.org/wiki?curid=4303005">
<title>Apprendimento ensemble</title>
<text>
L'Apprendimento ensemble (apprendimento d'insieme) in statistica e apprendimento automatico sono una serie di metodi d'insieme che usano modelli multipli per ottenere una migliore prestazione predittiva rispetto ai modelli da cui è costituito.
A differenza dell'Insieme statistico che si ritiene infinito, in meccanica statistica, in un insieme di apprendimento automatico ci si riferisce solo ad un insieme concreto e finito di modelli alternativi.
Nel machine learning l'apprendimento ensemble si divide in tre tecniche fondamentali:
</text>
</doc>
<doc id="4100372" url="https://it.wikipedia.org/wiki?curid=4100372">
<title>Rete neurale artificiale</title>
<text>
Nel campo dell'apprendimento automatico, una rete neurale artificiale (in inglese "artificial neural network", abbreviato in ANN o anche come NN) è un modello computazionale composto di "neuroni" artificiali, ispirato vagamente dalla semplificazione di una rete neurale biologica.
Questi modelli matematici sono troppo semplici per ottenere una comprensione delle reti neurali biologiche, ma sono utilizzati per tentare di risolvere problemi ingegneristici di intelligenza artificiale come quelli che si pongono in diversi ambiti tecnologici (in elettronica, informatica, simulazione, e altre discipline).
Una rete neurale artificiale può essere realizzata sia da programmi software che da hardware dedicato (DSP, "Digital Signal Processing"). Questa branca può essere utilizzata in congiunzione alla logica fuzzy.
L'ampia varietà di modelli non può prescindere dal costituente di base, il neurone artificiale proposto da W.S. McCulloch e Walter Pitts in un famoso lavoro del 1943: ""A logical calculus of the ideas immanent in nervous activity"", il quale schematizza un combinatore lineare a soglia, con dati binari multipli in entrata e un singolo dato binario in uscita: un numero opportuno di tali elementi, connessi in modo da formare una rete, è in grado di calcolare semplici funzioni booleane.
Le prime ipotesi di apprendimento furono introdotte da D. O. Hebb nel libro del 1949: ""The organization of behavior"", nel quale vengono proposti collegamenti con i modelli complessi del cervello.
Nel 1958, J. Von Neumann nella sua opera ""The computer and the brain"" esamina le soluzioni proposte dai precedenti autori sottolineando la scarsa precisione che queste strutture possedevano per potere svolgere operazioni complesse.
Nello stesso anno, Frank Rosenblatt nel libro "Psychological review" introduce il primo schema di rete neurale, detto "Perceptron" (percettrone), antesignano delle attuali reti neurali, per il riconoscimento e la classificazione di forme, allo scopo di fornire un'interpretazione dell'organizzazione generale dei sistemi biologici. Il modello probabilistico di Rosenblatt è quindi mirato all'analisi, in forma matematica, di funzioni quali l'immagazzinamento delle informazioni, e della loro influenza sul riconoscimento dei pattern; esso costituisce un progresso decisivo rispetto al modello binario di McCulloch e Pitts, perché i suoi pesi sinaptici sono variabili e quindi il percettrone è in grado di apprendere.
L'opera di Rosenblatt stimola una quantità di studi e ricerche che dura per un decennio, e suscita un vivo interesse e notevoli aspettative nella comunità scientifica, destinate tuttavia ad essere notevolmente ridimensionate allorché nel 1969 Marvin Minsky e Seymour A. Papert, nell'opera ""An introduction to computational geometry"", mostrano i limiti operativi delle semplici reti a due strati basate sul percettrone, e dimostrano l'impossibilità di risolvere per questa via molte classi di problemi, ossia tutti quelli non caratterizzati da separabilità lineare delle soluzioni: questo tipo di rete neurale non è abbastanza potente: non è infatti neanche in grado di calcolare la funzione "or esclusivo" (XOR). A causa di queste limitazioni, al periodo di euforia dovuto ai primi risultati della cibernetica (come veniva chiamata negli anni sessanta) segue un periodo di diffidenza durante il quale tutte le ricerche in questo campo non ricevono più alcun finanziamento dal governo degli Stati Uniti d'America; le ricerche sulle reti tendono, di fatto, a ristagnare per oltre un decennio, e l'entusiasmo iniziale risulta fortemente ridimensionato.
Il contesto matematico per addestrare le reti MLP ("Multi-Layers Perceptron", ossia percettrone multistrato) fu stabilito dal matematico americano Paul Werbos nella sua tesi di dottorato (Ph.D.) del 1974. Non fu dato molto peso al suo lavoro tanto fu forte la confutazione dimostrata da Minsky e Papert anni prima, e solo l'intervento di J. J. Hopfield, nel 1982, che in un suo lavoro studia dei modelli di riconoscimento di pattern molto generali, si oppose in modo diretto alla confutazione di Minsky riaprendo così degli spiragli per la ricerca in questo campo.
Uno dei metodi più noti ed efficaci per l'addestramento di tale classe di reti neurali è il cosiddetto algoritmo di retropropagazione dell'errore (error backpropagation), proposto nel 1986 da David E. Rumelhart, G. Hinton e R. J. Williams, il quale modifica sistematicamente i pesi delle connessioni tra i nodi, così che la risposta della rete si avvicini sempre di più a quella desiderata. Tale lavoro fu prodotto riprendendo il modello creato da Werbos. L'algoritmo di retropropagazione ("backpropagation" o BP) è una tecnica d'apprendimento tramite esempi, costituente una generalizzazione dell'algoritmo d'apprendimento per il percettrone sviluppato da Rosenblatt nei primi anni '60. Mediante questa tecnica era possibile, come detto, trattare unicamente applicazioni caratterizzabili come funzioni booleane linearmente separabili.
L'algoritmo di apprendimento si basa sul metodo della discesa del gradiente che permette di trovare un minimo locale di una funzione in uno spazio a N dimensioni. I pesi associati ai collegamenti tra gli strati di neuroni si inizializzano a valori piccoli (ovvero molto inferiori ai valori reali che poi assumeranno) e casuali e poi si applica la regola di apprendimento presentando alla rete dei pattern di esempio. Queste reti neurali sono poi capaci di generalizzare in modo appropriato, cioè di dare risposte plausibili per input che non hanno mai visto.
L'addestramento di une rete neurale di tipo BP avviene in due diversi stadi: "forward-pass" e "backward-pass". Nella prima fase i vettori in input sono applicati ai nodi in ingresso con una propagazione in avanti dei segnali attraverso ciascun livello della rete ("forward-pass"). Durante questa fase i valori dei pesi sinaptici sono tutti fissati. Nella seconda fase la risposta della rete viene confrontata con l'uscita desiderata ottenendo il segnale d'errore. L'errore calcolato è propagato nella direzione inversa rispetto a quella delle connessioni sinaptiche. I pesi sinaptici infine sono modificati in modo da minimizzare la differenza tra l'uscita attuale e l'uscita desiderata ("backward-pass").
Tale algoritmo consente di superare le limitazioni del percettrone e di risolvere il problema della separabilità non lineare (e quindi di calcolare la funzione XOR), segnando il definitivo rilancio delle reti neurali, come testimoniato anche dall'ampia varietà d'applicazioni commerciali: attualmente la BP rappresenta un algoritmo di largo uso in molti campi applicativi.
Una rete neurale artificiale (ANN ""Artificial Neural Network"" in inglese), normalmente chiamata solo "rete neurale" (NN ""Neural Network"" in inglese), è un modello matematico/informatico di calcolo basato sulle reti neurali biologiche. Tale modello è costituito da un gruppo di interconnessioni di informazioni costituite da neuroni artificiali e processi che utilizzano un approccio di connessionismo di calcolo. Nella maggior parte dei casi una rete neurale artificiale è un sistema adattivo che cambia la propria struttura in base a informazioni esterne o interne che scorrono attraverso la rete stessa durante la fase di apprendimento.
In termini pratici le reti neurali sono strutture non-lineari di dati statistici organizzate come strumenti di modellazione. Esse possono essere utilizzate per simulare relazioni complesse tra ingressi e uscite che altre funzioni analitiche non riescono a rappresentare.
Una rete neurale artificiale riceve segnali esterni su uno strato di nodi (unità di elaborazione) d'ingresso, ciascuno dei quali è collegato con numerosi nodi interni, organizzati in più livelli. Ogni nodo elabora i segnali ricevuti e trasmette il risultato a nodi successivi.
Il concetto di rete neurale si pone perché una funzione formula_1 è definita come una composizione di altre funzioni formula_2, che possono a loro volta essere ulteriormente definite come composizione di altre funzioni. Questo può essere comodamente rappresentato come una struttura di reti, con le frecce raffiguranti le dipendenze tra variabili. Una rappresentazione ampiamente utilizzata è la somma ponderata non lineare, dove formula_3, dove formula_4 è una funzione predefinita, come ad esempio la tangente iperbolica. Sarà conveniente per le seguenti far riferimento ad un insieme di funzioni come un vettore formula_5.
La Figura 1 esemplifica una decomposizione della funzione formula_6, con dipendenze tra le variabili indicate dalle frecce. Queste possono essere interpretate in due modi:
I due punti di vista sono in gran parte equivalenti. In entrambi i casi, per questa particolare architettura di rete, i componenti dei singoli strati sono indipendenti l'uno dall'altro (ad esempio, le componenti di formula_8 sono indipendenti l'una dall'altra, dato il loro ingresso formula_15). Questo, naturalmente, permette un certo grado di parallelismo nella costruzione del sistema.
Reti, come ad esempio quelle precedenti vengono comunemente chiamate ""feedforward"", perché il loro è un grafico aciclico diretto. Reti con cicli al loro interno sono comunemente chiamati reti ricorrenti. Tali reti sono comunemente raffigurate nel modo indicato nella parte superiore della Figura 2, dove la funzione formula_6 è mostrata come dipendente su se stessa. Tuttavia, vi è una dipendenza temporale implicita che non è possibile dimostrare. Questo significa in pratica che il valore di formula_6 ad un certo punto nel tempo formula_18 dipende dai valori di formula_6 al tempo zero o su uno o più altri punti temporali. Il modello del grafico nella parte inferiore della Figura 2 illustra il caso in cui il valore di formula_6 al tempo formula_18 dipende solo dal suo valore finale.
Tuttavia la funzionalità più interessante di queste funzioni, ciò che ha attirato l'interesse e lo studio per la maggior parte delle reti neurali, è la possibilità di apprendimento, che in pratica significa la seguente:
Ciò comporta la definizione di una funzione di costo formula_24 tale che, per la soluzione ottimale formula_25 formula_26 nessuna soluzione ha un costo inferiore al costo della soluzione ottimale.
La funzione di costo formula_27 è un concetto importante nell'apprendimento, poiché si tratta di una misura di quanto è lontana da noi la soluzione ottimale del problema che vogliamo risolvere. Quindi vi sono una serie di algoritmi di apprendimento che cercano nello spazio delle soluzioni al fine di trovare una funzione che abbia il minor costo possibile.
Per applicazioni in cui la soluzione dipende da alcuni dati, il costo deve essere necessariamente funzione delle osservazioni.
Mentre è possibile definire per alcune reti una funzione di costo ad hoc, spesso si può utilizzare una particolare funzione di costo poiché gode delle proprietà desiderate (ad esempio, la convessità), o perché proviene da una particolare formulazione del problema (vale a dire, in una formulazione probabilistica, la probabilità a posteriori del modello può essere utilizzata come l'inverso del costo). In ultima analisi, la funzione di costo dipenderà dal compito.
Vi sono tre grandi paradigmi di apprendimento, ciascuno corrispondente ad un particolare compito astratto di apprendimento. Si tratta dell'apprendimento supervisionato, apprendimento non supervisionato e l'apprendimento per rinforzo. Di solito un tipo di architettura di rete può essere impiegato in qualsiasi di tali compiti.
L'algoritmo di apprendimento hebbiano (1984) si basa sul semplice principio che se due neuroni si attivano contemporaneamente, la loro interconnessione deve essere rafforzata.
formula_28 dove formula_29,
dove formula_30 è l'formula_31 ingresso e formula_32 è il tasso di apprendimento formula_33.
La regola di Hebb è la seguente: l'efficacia di una particolare sinapsi cambia se e solo se c'è un'intensa attività simultanea dei due neuroni, con un'alta trasmissione di input nella sinapsi in questione.
Esempio di procedura:
In questo modo le connessioni possono solo irrobustirsi.
Le connessioni si considerano irrobustite quando le unità presinaptica e postsinaptica sono d'accordo, altrimenti si indeboliscono.
Si considerano funzioni bipolari (-1,1) invece che booleane (0,1).
Le reti neurali si basano principalmente sulla simulazione di neuroni artificiali opportunamente collegati. Il modello rappresentato in figura è quello proposto da McCulloch e Pitts.
I suddetti neuroni ricevono in ingresso degli stimoli e li elaborano. L'elaborazione può essere anche molto sofisticata ma in un caso semplice si può pensare che i singoli ingressi vengano moltiplicati per un opportuno valore detto peso, il risultato delle moltiplicazioni viene sommato e se la somma supera una certa soglia il neurone si attiva attivando la sua uscita. Il peso indica l'efficacia sinaptica della linea di ingresso e serve a quantificarne l'importanza, un ingresso molto importante avrà un peso elevato, mentre un ingresso poco utile all'elaborazione avrà un peso inferiore. Si può pensare che se due neuroni comunicano fra loro utilizzando maggiormente alcune connessioni allora tali connessioni avranno un peso maggiore, fino a che non si creeranno delle connessioni tra l'ingresso e l'uscita della rete che sfruttano "percorsi preferenziali". Tuttavia è sbagliato pensare che la rete finisca col produrre un unico percorso di connessione: tutte le combinazioni infatti avranno un certo peso, e quindi contribuiscono al collegamento ingresso/uscita.
Il modello in figura rappresenta una classica rete neurale pienamente connessa.
I singoli neuroni vengono collegati alla schiera di neuroni successivi, in modo da formare una rete di neuroni. Normalmente una rete è formata da tre strati. Nel primo abbiamo gli ingressi (I), questo strato si preoccupa di trattare gli ingressi in modo da adeguarli alle richieste dei neuroni. Se i segnali in ingresso sono già trattati può anche non esserci. Il secondo strato è quello nascosto (H, "hidden"), si preoccupa dell'elaborazione vera e propria e può essere composto anche da più colonne di neuroni. Il terzo strato è quello di uscita (O) e si preoccupa di raccogliere i risultati ed adattarli alle richieste del blocco successivo della rete neurale. Queste reti possono essere anche molto complesse e coinvolgere migliaia di neuroni e decine di migliaia di connessioni.
Per costruire la struttura di una rete neurale multistrato si possono inserire formula_38 strati "hidden." L'efficacia di generalizzare di una rete neurale multistrato dipende ovviamente dall'addestramento che ha ricevuto e dal fatto di essere riuscita o meno ad entrare in un minimo locale buono.
L'algoritmo di retropropagazione dell'errore ("backpropagation") è utilizzato nell'apprendimento con supervisione. Esso permette di modificare i pesi delle connessioni in modo tale che si minimizzi una certa funzione errore E. Tale funzione dipende dal vettore h-esimo di output formula_39 restituito dalla rete, dato il vettore h-esimo di ingresso formula_40 e dal vettore h-esimo di output formula_41che noi desideriamo (che fa parte del training set). Il training set è dunque un insieme di N coppie di vettori formula_42, con formula_43. La funzione errore che si deve minimizzare si può scrivere come:
formula_44
dove l'indice k rappresenta il valore corrispondente al k-esimo neurone di output. E(w) è una funzione dipendente dai pesi (che in generale variano nel tempo), per minimizzarla si può usare l'algoritmo della discesa del gradiente ("gradient descent"). L'algoritmo parte da un punto generico formula_45 e calcola il gradiente formula_46. Il gradiente dà la direzione verso cui muoversi lungo la quale si ha il massimo incremento (o decremento se considero formula_47). Definita la direzione ci si muove di una distanza formula_32 predefinita a priori e si trova un nuovo punto formula_49 sul quale è calcolato nuovamente il gradiente. Si continua iterativamente finché il gradiente non è nullo.
L'algoritmo di backpropagation può essere diviso in due passi:
I passi logici per addestrare una rete neurale con apprendimento supervisionato sono i seguenti:
Per l'addestramento di reti neurali profonde, impiegando dataset molto vasti, la discesa del gradiente classica risulta computazionalmente proibitiva, per cui nell'ottimizzare i parametri del modello si fa tipicamente uso dell'algoritmo di discesa stocastica del gradiente.
Nel 1982, il fisico John J. Hopfield pubblica un articolo fondamentale in cui presenta un modello matematico comunemente noto appunto come rete di Hopfield: tale rete si distingue per "l'emergere spontaneo di nuove capacità computazionali dal comportamento collettivo di un gran numero di semplici elementi d'elaborazione". Le proprietà collettive del modello producono una memoria associativa per il riconoscimento di configurazioni corrotte e il recupero di informazioni mancanti.
Inoltre, Hopfield ritiene che ogni sistema fisico possa essere considerato come un potenziale dispositivo di memoria, qualora esso disponga di un certo numero di stati stabili, i quali fungano da attrattore per il sistema stesso. Sulla base di tale considerazione, egli si spinge a formulare la tesi secondo cui la stabilità e la collocazione di tali attrattori sono proprietà spontanee di sistemi costituiti, come accennato, da considerevoli quantità di neuroni reciprocamente interagenti.
Le applicazioni delle reti di Hopfield riguardano principalmente la realizzazione di memorie associative, resistenti all'alterazione delle condizioni operative, e la soluzione di problemi d'ottimizzazione combinatoriale.
Da un punto di vista strutturale, la rete di Hopfield costituisce una rete neurale ricorrente simmetrica, di cui è garantita la convergenza.
Una rete ricorrente è un modello neurale in cui è presente un flusso bidirezionale d'informazioni; in altri termini, mentre nelle reti di tipo feedforward la propagazione dei segnali avviene unicamente, in maniera continua, nella direzione che conduce dagli ingressi alle uscite, nelle reti ricorrenti tale propagazione può anche manifestarsi da uno strato neurale successivo ad uno precedente, oppure tra neuroni appartenenti ad uno stesso strato, e persino tra un neurone e sé stesso.
Un significativo e noto esempio di semplice rete ricorrente è dovuto a Jeffrey L. Elman (1990). Essa costituisce una variazione sul tema del percettrone multistrato, con esattamente tre strati e l'aggiunta di un insieme di neuroni "contestuali" nello strato d'ingresso. Le connessioni retroattive si propagano dallo strato intermedio (e nascosto) a tali unità contestuali, alle quali si assegna peso costante e pari all'unità.
In ciascun istante, gli ingressi si propagano nel modo tradizionale e tipico delle reti feedforward, compresa l'applicazione dell'algoritmo d'apprendimento (solitamente la "backpropagation"). Le connessioni retroattive fisse hanno come effetto quello di mantenere una copia dei precedenti valori dei neuroni intermedi, dal momento che tale flusso avviene sempre prima della fase d'apprendimento.
In questo modo la rete di Elman tiene conto del suo stato precedente, cosa che le consente di svolgere compiti di previsione di sequenze temporali che sono difficilmente alla portata dei percettroni multistrato convenzionali.
Infine, un ultimo interessante tipo di rete è costituita dalla cosiddetta mappa auto-organizzante o rete SOM ("Self-Organizing Map"). Tale innovativo tipo di rete neurale è stata elaborata da Teuvo Kohonen dell'Università Tecnologica di Helsinki; il suo algoritmo d'apprendimento è senza dubbio una brillante formulazione di apprendimento non supervisionato, e ha dato luogo a un gran numero di applicazioni nell'ambito dei problemi di classificazione. Una mappa o rete SOM è basata essenzialmente su un reticolo o griglia di neuroni artificiali i cui pesi sono continuamente adattati ai vettori presentati in ingresso nel relativo insieme di addestramento. Tali vettori possono essere di dimensione generica, anche se nella maggior parte delle applicazioni essa è piuttosto alta. Per ciò che riguarda le uscite della rete, al contrario, ci si limita di solito ad una dimensione massima pari a tre, il che consente di dare luogo a mappe 2D o 3D.
In termini più analitici, l'algoritmo può essere agevolmente descritto, come accennato, nei termini di un insieme di neuroni artificiali, ciascuno con una precisa collocazione sulla mappa rappresentativa degli "output", che prendono parte ad un processo noto come "winner takes all" ("Il vincitore piglia tutto"), al termine del quale il nodo avente un vettore di pesi più vicino ad un certo "input" è dichiarato vincitore, mentre i pesi stessi sono aggiornati in modo da avvicinarli al vettore in ingresso. Ciascun nodo ha un certo numero di nodi adiacenti. Quando un nodo vince una competizione, anche i pesi dei nodi adiacenti sono modificati, secondo la regola generale che più un nodo è lontano dal nodo vincitore, meno marcata deve essere la variazione dei suoi pesi. Il processo è quindi ripetuto per ogni vettore dell'insieme di "training", per un certo numero, solitamente grande, di cicli. Va da sé che ingressi diversi producono vincitori diversi.
Operando in tal modo, la mappa riesce alfine ad associare i nodi d'uscita con i gruppi o schemi ricorrenti nell'insieme dei dati in ingresso. Se questi schemi sono riconoscibili, essi possono essere associati ai corrispondenti nodi della rete addestrata. In maniera analoga a quella della maggioranza delle reti neurali artificiali, anche la mappa o rete SOM può operare in due distinte modalità:
In generale una ANN ("Attractor Neural Network") è una rete di nodi (es: biologicamente ispirati), spesso interconnessi in modo ricorsivo, la cui dinamica nel tempo stabilisce un assestamento in un particolare modo di oscillazione. Questo modo di oscillazione può essere stazionario, variante nel tempo o di tipo stocastico ed è chiamato il suo 'attrattore'. In neuroscienza teorica diversi tipi di reti ad attrattori sono state associate a differenti funzioni, come: memoria, attenzione, condotta del moto e classificazione.
Più precisamente, una rete ad attrattori è una rete di N nodi connessi in modo che la loro intera dinamica diventi stabile in uno spazio D dimensionale, dove solitamente N»D. Ciò assume che non vi sia più input dall'esterno del sistema. La stabilità nello stato ad attrattore indica l'esistenza di uno stato stabile in una qualche varietà algebrica (es: linea, cerchio, piano, toroide).
L'utilità dei modelli di rete neurale sta nel fatto che queste possono essere usate per comprendere una funzione utilizzando solo le osservazioni sui dati. Ciò è particolarmente utile nelle applicazioni in cui la complessità dei dati o la difficoltà di elaborazione rende la progettazione di una tale funzione impraticabile con i normali procedimenti di analisi manuale.
I compiti a cui le reti neurali sono applicate possono essere classificate nelle seguenti grandi categorie di applicazioni:
Le aree di applicazione includono i sistemi di controllo (controllo di veicoli, controllo di processi), simulatori di giochi e processi decisionali (backgammon, scacchi), riconoscimento di pattern (sistemi radar, identificazione di volti, riconoscimento di oggetti, ecc), riconoscimenti di sequenze (riconoscimento di gesti, riconoscimento vocale, OCR), diagnosi medica, applicazioni finanziarie, data mining, filtri spam per e-mail.
Le reti neurali per come sono costruite lavorano in parallelo e sono quindi in grado di trattare molti dati. Si tratta in sostanza di un sofisticato sistema di tipo statistico dotato di una buona immunità al rumore; se alcune unità del sistema dovessero funzionare male, la rete nel suo complesso avrebbe delle riduzioni di prestazioni ma difficilmente andrebbe incontro ad un blocco del sistema. I software di ultima generazione dedicati alle reti neurali richiedono comunque buone conoscenze statistiche; il grado di apparente utilizzabilità immediata non deve trarre in inganno, pur permettendo all'utente di effettuare subito previsioni o classificazioni, seppure con i limiti del caso.
Da un punto di vista industriale, risultano efficaci quando si dispone di dati storici che possono essere trattati con gli algoritmi neurali. Ciò è di interesse per la produzione perché permette di estrarre dati e modelli senza effettuare ulteriori prove e sperimentazioni.
I modelli prodotti dalle reti neurali, anche se molto efficienti, non sono spiegabili in linguaggio simbolico umano: i risultati vanno accettati "così come sono", da cui anche la definizione inglese delle reti neurali come "black box": in altre parole, a differenza di un sistema algoritmico, dove si può esaminare passo-passo il percorso che dall'input genera l'output, una rete neurale è in grado di generare un risultato valido, o comunque con una alta probabilità di essere accettabile, ma non è possibile spiegare come e perché tale risultato sia stato generato.
Come per qualsiasi algoritmo di modellazione, anche le reti neurali sono efficienti solo se le variabili predittive sono scelte con cura.
Non sono in grado di trattare in modo efficiente variabili di tipo categorico (per esempio, il nome della città) con molti valori diversi. Necessitano di una fase di addestramento del sistema che fissi i pesi dei singoli neuroni e questa fase può richiedere molto tempo, se il numero dei record e delle variabili analizzate è molto grande. Non esistono teoremi o modelli che permettano di definire la rete ottima, quindi la riuscita di una rete dipende molto dall'esperienza del creatore.
Le reti neurali vengono solitamente usate in contesti dove i dati possono essere parzialmente errati oppure dove non esistano modelli analitici in grado di affrontare il problema. Un loro tipico utilizzo è nei software di OCR, nei sistemi di riconoscimento facciale e più in generale nei sistemi che si occupano di trattare dati soggetti a errori o rumore. Esse sono anche uno degli strumenti maggiormente utilizzati nelle analisi di Data mining.
Le reti neurali vengono anche utilizzate come mezzo per previsioni nell'analisi finanziaria o meteorologica. Negli ultimi anni è aumentata notevolmente la loro importanza anche nel campo della bioinformatica nel quale vengono utilizzate per la ricerca di pattern funzionali e/o strutturali in proteine e acidi nucleici. Mostrando opportunamente una lunga serie di input (fase di training o apprendimento), la rete è in grado di fornire l'output più probabile. Negli ultimi anni inoltre sono in corso studi per il loro utilizzo nella previsione degli attacchi Epilettici (Analisi dei Dati provenienti dall' EEG).
Recenti studi hanno dimostrato buone potenzialità delle reti neurali in sismologia per la localizzazione di epicentri di terremoti e predizione della loro intensità.
</text>
</doc>
<doc id="5960762" url="https://it.wikipedia.org/wiki?curid=5960762">
<title>Retropropagazione dell'errore</title>
<text>
La retropropagazione dell'errore (in lingua inglese "backward propagation of errors", solitamente abbreviato in backpropagation), è un algoritmo per l'allenamento delle reti neurali artificiali, usato in combinazione con un metodo di ottimizzazione come per esempio la discesa stocastica del gradiente.
La retropropagazione richiede un'uscita desiderata per ogni valore in ingresso per poter calcolare il gradiente della funzione di perdita (funzione di costo). Viene considerato quindi un metodo di apprendimento supervisionato, sebbene venga usato anche in reti non supervisionate come gli autocodificatori o Reti Diabolo.
È una generalizzazione della regola delta di reti feed-forward multistrato, resa possibile usando la regola di catena che iterativamente calcola i gradienti per ogni strato.
La retropropagazione richiede che la funzione d'attivazione usata dai neuroni artificiali (o "nodi") sia differenziabile.
Una delle principali difficoltà nell'uso della retropropagazione dell'errore è il problema noto come scomparsa del gradiente, dovuto all'uso di funzioni di attivazione non lineari che causano una diminuzione esponenziale del valore del gradiente all'aumentare della profondità della rete neurale.
</text>
</doc>
<doc id="5960768" url="https://it.wikipedia.org/wiki?curid=5960768">
<title>Rete neurale feed-forward</title>
<text>
Una rete neurale feed-forward ("rete neurale con flusso in avanti") o rete feed-forward è una rete neurale artificiale dove le connessioni tra le unità non formano cicli, differenziandosi dalle reti neurali ricorrenti. Questo tipo di rete neurale fu la prima e più semplice tra quelle messe a punto. In questa rete neurale le informazioni si muovono solo in una direzione, avanti, rispetto a nodi d'ingresso, attraverso nodi nascosti (se esistenti) fino ai nodi d'uscita. Nella rete non ci sono cicli. Le reti feed-forward non hanno memoria di input avvenuti a tempi precedenti, per cui l'output è determinato solamente dall'attuale input.
La più semplice rete feed-forward è il "percettrone a singolo strato" (SLP dall'inglese single layer perceptron), utilizzato verso la fine degli anni '60. Un SLP è costituito da un strato in ingresso, seguito direttamente dall'uscita. Ogni unità di ingresso è collegata ad ogni unità di uscita. In pratica questo tipo di rete neurale ha un solo strato che effettua l'elaborazione dei dati, e non presenta nodi nascosti, da cui il nome.
Gli SLP sono molto limitati a causa del piccolo numero di connessioni e dell'assenza di gerarchia nelle caratteristiche che la rete può estrarre dai dati (questo significa che è capace di combinare i dati in ingresso una sola volta). Famosa fu la dimostrazione che un SLP non riesce neanche a rappresentare la funzione XOR. Questo risultato, apparso nel 1969, scoraggiò i ricercatori e bloccò la ricerca sulle reti neurali per diversi anni.
Questa classe di reti feedforward si distingue dalla precedente dal fatto che tra lo strato di input e quello di output abbiamo uno o più strati di neuroni nascosti (hidden layers). Ogni strato ha connessioni entranti dal precedente strato e uscenti in quello successivo, quindi la propagazione del segnale avviene in avanti senza cicli e senza connessioni trasversali.
Questo tipo di architettura fornisce alla rete una prospettiva globale in quanto aumentano le interazioni tra neuroni.
</text>
</doc>
<doc id="6121838" url="https://it.wikipedia.org/wiki?curid=6121838">
<title>Regressione di Poisson</title>
<text>
In statistica, la regressione di Poisson è una forma di modello lineare generalizzato di analisi di regressione usato per modellare il conteggio dei dati in tabelle contingenti.
La regressione di Poisson assume che la variabile di risposta Y ha una distribuzione di Poisson, e assume che il logaritmo del suo valore aspettato possa essere modellato da una combinazione lineare di parametri sconosciuti. La regressione di Poisson è talvolta conosciuta anche come modello log-lineare, specialmente quando viene usato per modellare tabelle contingenti.
La regressione binomiale negativa (NB2) è una famosa generalizzazione della regressione di Poisson perché allenta il presupposto altamente restrittivo che la varianza è uguale alla media come nel modello di Poisson
La NB2 si basa sulla distribuzione mista Poisson-Gamma. Questo modello è molto usato perché modella l'eterogeneità della Poisson con la distribuzione Gamma.
</text>
</doc>
<doc id="6394873" url="https://it.wikipedia.org/wiki?curid=6394873">
<title>F1 score</title>
<text>
Nell'analisi statistica della classificazione binaria, lF score (nota anche come F-score o F-measure, letteralmente "misura F") è una misura dell'accuratezza di un test. La misura tiene in considerazione precisione e recupero del test, dove la precisione è il numero di veri positivi diviso il numero di tutti i risultati positivi, mentre il recupero è il numero di veri positivi diviso il numero di tutti i test che sarebbero dovuti risultare positivi (ovvero veri positivi più falsi negativi). L'F viene calcolato tramite la media armonica di precisione e recupero:
Può assumere valori compresi fra 0 e 1. Assume valore 0 solo se almeno uno dei due vale 0, mentre assume valore 1 sia precisione che recupero valgono 1. L'F score è anche noto come coefficiente di Sørensen-Dice (DSC), o semplicemente coefficiente di Dice.
La formula generale è:
per valori di β reali positivi.
La formula in termini di errori di primo e secondo tipo:
Due particolari istanze della formula solitamente utilizzate sono la misura formula_4 (che pone maggiore enfasi sui falsi negativi) ed formula_5 (la quale attenua l'influenza dei falsi negativi).
In generale, formula_6 "misura l'efficacia del recupero rispetto ad un utente attribuisce al recupero un'importanza di β volte quella della precisione".
L'F-score è solitamente usata nel campo del recupero dell'informazione per misurare l'accuratezza delle ricerche o della classificazione dei documenti. Inizialmente l'F score era l'unica misura ad essere considerata, ma con la proliferazione in larga scala di motori di ricerca gli obiettivi di prestazione iniziarono a variare, divenendo necessario porre maggiore enfasi su precisione o recupero.
L'F-score è usata anche nel campo dell'apprendimento automatico ed è vastamente impiegata nella letteratura sull'elaborazione del linguaggio naturale.
Da notare, comunque, che non viene mai preso in considerazione il numero di veri negativi. In tal senso, misure come il coefficiente di correlazione di Matthews o il Kappa di Cohen possono generare risultati più adeguati alle proprie esigenze.
Mentre l'F-measure è una media armonica di recupero e precisione, la cosiddetta G-measure è una media geometrica:
Dove "PPV" sta per "Positive Predictive Value" ("valore predittivo positivo") e "TPR" per "True Positive Rate" (o indice di sensibilità).
È nota anche come indice di Fowlkes-Mallows.
</text>
</doc>
<doc id="1216500" url="https://it.wikipedia.org/wiki?curid=1216500">
<title>Bootstrap (statistica)</title>
<text>
Il bootstrap è una tecnica statistica di ricampionamento con reimmissione per approssimare la distribuzione campionaria di una statistica. 
Permette perciò di approssimare media e varianza di uno stimatore, costruire intervalli di confidenza e calcolare p-values di test quando, in particolare, non si conosce la distribuzione della statistica di interesse.
Nel caso semplice di campionamento casuale semplice, il funzionamento è il seguente: consideriamo un campione effettivamente osservato di numerosità pari ad "n", diciamo formula_1. Da formula_2 si ricampionano "m" altri campioni di numerosità costante pari ad "n", diciamo formula_3; in ciascuna estrazione bootstrap, i dati provenienti dal primo elemento del campione, cioè formula_4, possono essere estratti più di una volta e ciascun dato ha probabilità pari a "1/n" di essere estratto.
Sia formula_5 lo stimatore di formula_6 che ci interessa studiare, diciamo formula_7. Si calcola tale quantità per ogni campione bootstrap, formula_8. In questo modo si hanno a disposizione "m" stime di formula_6, dalle quali è possibile calcolare la media bootstrap, la varianza bootstrap, i percentili bootstrap etc. che sono approssimazioni dei corrispondenti valori ignoti e portano informazioni sulla distribuzione di formula_10. 
Partendo quindi da queste quantità stimate è possibile calcolare intervalli di confidenza, saggiare ipotesi, etc.
</text>
</doc>
<doc id="1238309" url="https://it.wikipedia.org/wiki?curid=1238309">
<title>Algoritmo apriori</title>
<text>
In informatica e in data mining, l'algoritmo Apriori è un classico algoritmo di ricerca delle associazioni. È utilizzato per la generazione degli itemset frequenti, per approssimazioni successive, a partire dagli itemset con un solo elemento. In sintesi, il presupposto teorico su cui si basa l'algoritmo parte dalla considerazione che se un insieme di oggetti (itemset) è frequente, allora anche tutti i suoi sottoinsiemi sono frequenti, ma se un itemset non è frequente, allora neanche gli insiemi che lo contengono sono frequenti (principio di anti-monotonicità).
Un ambito dove questo algoritmo trova grande applicabilità è il "market/basket problem". Per ricavare le associazioni viene impiegato un approccio "bottom up", dove i sottoinsiemi frequenti sono costruiti aggiungendo un item per volta (generazione dei candidati); i gruppi di candidati sono successivamente verificati sui dati e l'algoritmo termina quando non ci sono ulteriori estensioni possibili. In questo processo, il numero delle iterazioni è formula_1, dove formula_2 indica la cardinalità massima di un itemset frequente.
Vi sono altri algoritmi con finalità analoghe (Winepi e Minepi), e che tuttavia sono più diffusi in ambiti dove i dati sono privi di timestamp (ad esempio le sequenze di DNA).
Apriori, anche se storicamente significativo, soffre di alcune inefficienze. In particolare, la generazione dei candidati crea molti sottoinsiemi. Nel processo vengono individuati i sottoinsiemi significativi solo dopo aver trovato tutti i formula_3 sottoinsiemi propri, dove S è il gruppo di elementi specifico (Supporto) in cui un particolare sottoinsieme di oggetti compare.
I passi dell'algoritmo per trovare gli insiemi frequenti formula_4 nel Database formula_5:
dove formula_7 è il candidato itemset di grandezza formula_12 e dove inoltre formula_13 è l'itemset frequente di grandezza formula_12
Questo esempio mostra il processo di selezione ovvero generazione di una lista ordinata di itemset candidati.
Il compito consiste nella costruzione di un insieme ordinato di formula_12 nodi, in modo seriale, a partire da itemset di grandezza formula_16.
Ad esempio, con formula_17, supponiamo che ci siano due di tali insiemi di grandezza formula_16
e 
ebbene i due itemset candidati generati saranno
e 
"Apriori" formula_23
</text>
</doc>
<doc id="1103542" url="https://it.wikipedia.org/wiki?curid=1103542">
<title>K-medoids</title>
<text>
 è un algoritmo di clustering partizionale correlato all'algoritmo K-means. Prevede in input un insieme di n oggetti e un numero k che determina quanti cluster si vogliono in output.
Entrambi gli algoritmi sono partizionali (suddividendo il dataset in gruppi) ed entrambi cercano di minimizzare l'errore quadratico medio, la distanza tra punti di un cluster e il punto designato per esserne il centro. In K-means il punto è "artificiale" — è la pura media di tutti i punti nel cluster. Nel K-medoids è usato il punto collocato più centralmente, in questo modo il centro è uno dei datapoint attuali. K-medoids è più robusto al rumore e agli outlier rispetto al k-means.
Un medoid può essere definito come un oggetto di un cluster la cui dissimilarità media rispetto a tutti gli oggetti nel cluster è minima, in questo modo esso sarà il punto più centrale di un dato dataset.
L'algoritmo di clustering è il seguente:
Si deve clusterizzare il seguente data set di 10 oggetti in 2 cluster, quindi n è 10 e k è 2:
Si inizializzano i k centri.
Assumiamo che C1=(3,4) e C2=(7,4) siano i nostri medoid iniziali.
Calcoliamo la distanza così da associare ogni data object al suo medoid più vicino.
Iniziamo quindi il clustering:
Essendo (3,4) (2,6) (3,8) e (4,7) punti vicini a c1 essi formeranno un cluster mentre i punti rimanenti ne formeranno un altro.
Il costo totale sarà 20.
Il costo tra 2 punti qualsiasi è trovato usando la formula
formula_1
Il costo totale è la somma dei costi per gli oggetti dal proprio medoid.
Costo totale= {cost((3,4),(2,6)) + cost((3,4),(3,8)) + cost((3,4),(4,7))} + {cost((7,4),(6,2)) + cost((7,4),(6,4)) + cost((7,4),(7,3)) + cost((7,4),(8,5)) + cost((7,4),(7,6))} = 3 + 4 + 4 + 3 + 1 + 1 + 2 + 2 = 20
Selezione di un nonmedoid O' in modo casuale.
Assumiamo O'=(7,3)
I medoid sono quindi c1(3,4) e O'(7,3).
Se c1 e O' sono nuovi medoid, si calcola il costo totale usando la formula al passo 1.
Costo totale = 3 + 4 + 4 + 2 + 2 + 1 + 3 + 3 = 22
Così il costo per cambiare il medoid da c2 a O' sarà:
S = Costo totale attuale – Costo totale precedente = 22 - 20 = 2 &gt; 0
Quindi cambiare medoid in O' non è una buona idea, la scelta precedente è stata buona e l'algoritmo termina in questo punto (in quanto non ci sono cambiamenti per i medoid).
Può accadere che qualche data point possa migrare da un cluster ad un altro, ciò dipende dalla vicinanza rispetto al nuovo medoid scelto.
</text>
</doc>
<doc id="1199503" url="https://it.wikipedia.org/wiki?curid=1199503">
<title>Popolazione statistica</title>
<text>
In statistica per popolazione (o collettivo statistico o aggregato) si intende l'insieme degli elementi che sono oggetto di studio, ovvero l'insieme delle unità (dette "unità statistiche") sulle quali viene effettuata la rilevazione delle modalità con le quali il fenomeno studiato si presenta. Tali unità presentano tutte almeno una caratteristica comune, che viene accuratamente definita al fine di delimitare il loro insieme; ad esempio con "italiani" si può intendere sia le persone di nazionalità italiana, anche se residenti all'estero, sia le persone residenti in Italia, indipendentemente da quale sia la loro nazionalità.
Una popolazione statistica va definita anche rispetto al tempo; ad esempio si possono considerare gli italiani che risultano residenti in Italia alle ore 12 di un dato giorno (popolazione definita secondo una caratteristica riferita ad un "istante" di tempo), oppure quelli nati dal 1º gennaio al 31 dicembre di un dato anno (popolazione definita secondo una caratteristica riferita ad un "periodo" di tempo).
Una popolazione statistica, peraltro, non è sempre un insieme biologico; costituisce una popolazione anche l'insieme delle lampadine prodotte da un'azienda in un dato periodo di tempo, l'insieme delle nazioni del continente europeo in un dato anno o l'insieme degli anni di un dato secolo.
I collettivi statistici, o popolazioni, possono essere distinti in:
oppure in:
o ancora tra:
</text>
</doc>
<doc id="1105351" url="https://it.wikipedia.org/wiki?curid=1105351">
<title>Dbscan</title>
<text>
Il DBSCAN ("Density-Based Spatial Clustering of Applications with Noise") è un metodo di clustering proposto nel 1996 da Martin Ester, Hans-Peter Kriegel, Jörg Sander and Xiaowei Xu. È basato sulla densità perché connette regioni di punti con densità sufficientemente alta. DBSCAN è l'algoritmo più comunemente usato ed è anche il più citato nella letteratura scientifica.
DBSCAN usa una definizione di cluster basata sulla nozione di "density-reachability". Un punto formula_1 è direttamente raggiungibile da un punto formula_2 se la loro distanza è minore di un assegnato formula_3 (cioè, è parte del suo formula_3-vicinato) e se formula_2 è circondato da un sufficiente numero di punti, allora formula_2 e formula_1 possono essere considerati parti di un cluster. Il punto formula_1 è "density-reachable" da formula_2 se c'è una sequenza formula_10 di punti con formula_11 e formula_12 dove ogni formula_13 è density-reachable direttamente da formula_14. Si osservi che la relazione density-reachable non è simmetrica dato che formula_1 potrebbe situarsi su una periferia del cluster, avendo un numero insufficiente di vicini per considerarlo un elemento genuino del cluster. Di conseguenza la nozione "density-connected" diventa: due punti formula_2 e formula_1 sono density-connected se c'è un punto formula_18 tale che sia formula_18 e formula_2 sia formula_18 e formula_1 sono density-reachable.
Un cluster, che è un sotto-insieme dei punti del database, soddisfa due proprietà:
DBSCAN necessita di due parametri: formula_3 (eps) e del numero minimo di punti richiesti per formare un cluster (minPts). Si comincia con un punto casuale che non è stato ancora visitato. Viene calcolato il suo formula_3-vicinato e se contiene un numero sufficiente di punti viene creato un nuovo cluster. Se ciò non avviene il punto viene etichettato come rumore e successivamente potrebbe essere ritrovato in un formula_3-vicinato sufficientemente grande riconducibile ad un punto differente entrando a far parte di un cluster.
Se un punto è associato ad un cluster anche i punti del suo formula_3-vicinato sono parte del cluster. Conseguentemente tutti i punti trovati all'interno del suo formula_3-vicinato sono aggiunti al cluster, così come i loro formula_3-vicinati. Questo processo continua fino a quando il cluster viene completato. Il processo continua fino a quando non sono stati visitati tutti i punti.
 DBSCAN(D, eps, MinPts)
DBSCAN visita ogni punto del database, anche più volte nel caso di punti candidati a cluster differenti. Tuttavia per considerazioni pratiche la complessità temporale è per lo più governata dal numero di invocazioni a getVicini, in riferimento allo pseudo codice di cui sopra. DBSCAN esegue esattamente una invocazione per ogni punto e se viene utilizzata una struttura indicizzata che esegue un'interrogazione del vicinato in formula_29, si ottiene un tempo globale di esecuzione pari a formula_30. Senza l'uso di strutture indicizzate, il tempo di esecuzione è pari a formula_31. Spesso la matrice delle distanze di dimensione formula_32 viene creata per evitare appunto il ricalcolo delle distanze riducendo il tempo di elaborazione a spese della memoria utilizzata pari a formula_31.
DBSCAN presenta i seguenti vantaggi:
Il rilevamento del vicinato più vicino avviene nella funzione getVicini(P,epsilon). Per ogni punto P vengono determinati tutti gli altri punti che sono all'interno dell'intervallo epsilon, basandosi sulla funzione della distanza usata nell'algoritmo. L'analisi richiede che sia calcolata una matrice delle distanze per l'intero data set. La generazione della matrice delle distanze ha una complessità di formula_34dato che è necessaria solo una matrice triangolare superiore. All'interno della matrice delle distanze il vicinato più vicino può essere calcolato selezionando la tupla che ha come valori il minimo delle funzioni su riga e colonna. La ricerca ha spinto il rilevamento del vicinato, nei database tradizionali, per migliorare la velocità. Questi ultimi risolvono il problema utilizzando indici specificamente progettati per questo tipo di applicazioni.
Ogni processo di data mining ha il problema dei parametri. Ogni parametro influenza l'algoritmo in modo specifico. Per il DBSCAN i parametri epsilon e MinPnts sono necessari. I parametri devono essere specificati dall'utente dato che ogni data set richiede parametri differenti. Un valore iniziale per formula_3 può essere determinato come un k-distance graph. Come per le regole del pollice, formula_36 può essere derivato dal numero di dimensioni nel data set formula_37 come formula_38. Tuttavia valori maggiori sono usualmente migliori per data set con rumore.
Anche se questa stima dei parametri restituisce un insieme sufficiente di parametri, la classificazione risultante può rivelarsi diversa da ciò che si aspetta, pertanto la ricerca ha realizzato un'incrementale ottimizzazione dei parametri su particolari valori.
Per ogni oggetto vengono trovati i vicini che ricadono in un raggio dato come parametro in ingresso; se il numero di questi vicini è superiore ad un fattore di soglia, anch'esso fornito in input all'algoritmo, allora questi punti fanno parte del medesimo cluster di quello dell'oggetto che si sta osservando e in questo caso il punto è denominato core point.
Al termine dell'algoritmo ci potrebbero essere alcuni punti non appartenenti a cluster catalogati come "rumore".
Se c'è una catena di oggetti da attraversare (con i consueti vincoli) per raggiungere un punto "q" da uno "p", allora "q" sarà detto semplicemente rintracciabile.
Ultimo caso è quello in cui due oggetti "p" e "q" sono detti connessi: per essere definiti in tal modo, deve esistere un terzo punto "o", per cui "p" e "q" sono entrambi rintracciabili.
</text>
</doc>
<doc id="91024" url="https://it.wikipedia.org/wiki?curid=91024">
<title>Data mining</title>
<text>
Il data mining (letteralmente dall'inglese "estrazione di dati") è l'insieme di tecniche e metodologie che hanno per oggetto l'estrazione di informazioni utili da grandi quantità di dati (es. database, datawarehouse ecc...), attraverso metodi automatici o semi-automatici (es. machine learning) e l'utilizzo scientifico, aziendale/industriale o operativo delle stesse.
La statistica può essere definita altrimenti come ""estrazione di informazione utile da insiemi di dati"".
Il concetto di "data mining" è simile, ma con una sostanziale differenza: la statistica permette di elaborare informazioni generali riguardo ad una popolazione (es. percentuali di disoccupazione, nascite), mentre il "data mining "viene utilizzato per cercare correlazioni tra più variabili relativamente ai singoli individui; ad esempio conoscendo il comportamento medio dei clienti di una compagnia telefonica cerco di prevedere quanto spenderà il cliente medio nell'immediato futuro.
In sostanza il data mining è ""l'analisi, da un punto di vista matematico, eseguita su database di grandi dimensioni"", preceduta tipicamente da altre fasi di preparazione/trasformazione/filtraggio dei dati come il data cleaning. Il termine "data mining" è diventato popolare nei tardi anni novanta come versione abbreviata della definizione appena esposta; oggi il "data mining" ha una duplice valenza:
In entrambi i casi i concetti di informazione e di significato sono legati strettamente al dominio applicativo in cui si esegue data mining, in altre parole un dato può essere interessante o trascurabile a seconda del tipo di applicazione in cui si vuole operare.
Questo tipo di attività è cruciale in molti ambiti della ricerca scientifica, ma anche in altri settori (per esempio in quello delle ricerche di mercato). Nel mondo professionale è utilizzata per risolvere problematiche diverse tra loro, che vanno dalla gestione delle relazioni con i clienti (CRM), all'individuazione di comportamenti fraudolenti, fino all'ottimizzazione di siti web.
I fattori principali che hanno contribuito allo sviluppo del data mining sono:
Le tecniche di data mining sono fondate su specifici algoritmi. I pattern identificati possono essere, a loro volta, il punto di partenza per ipotizzare e quindi verificare nuove relazioni di tipo causale fra fenomeni; in generale, possono servire in senso statistico per formulare previsioni su nuovi insiemi di dati.
Un concetto correlato al data mining è quello di apprendimento automatico ("Machine learning"); infatti, l'identificazione di pattern può paragonarsi all'apprendimento, da parte del sistema di data mining, di una relazione causale precedentemente ignota, cosa che trova applicazione in ambiti come quello degli algoritmi euristici e dell'intelligenza artificiale. Tuttavia, occorre notare che il processo di data mining è sempre sottoposto al rischio di rivelare relazioni causali che poi si rivelano inesistenti.
Tra le tecniche maggiormente utilizzate in questo ambito vi sono:
Un'altra tecnica molto diffusa per il data mining è l'apprendimento mediante classificazione. Questo schema di apprendimento parte da un insieme ben definito di esempi di classificazione per casi noti, dai quali ci si aspetta di dedurre un modo per classificare esempi non noti. Tale approccio viene anche detto "con supervisione" ("supervised"), nel senso che lo schema di apprendimento opera sotto la supervisione fornita implicitamente dagli esempi di classificazione per i casi noti; tali esempi, per questo motivo, vengono anche detti "training examples", ovvero "esempi per l'addestramento". La conoscenza acquisita per apprendimento mediante classificazione può essere rappresentata con un albero di decisione.
L'estrazione dei dati vera e propria giunge quindi al termine di un processo che comporta numerose fasi: si individuano le fonti di dati; si crea un unico set di dati aggregati; si effettua una pre-elaborazione (data cleaning, analisi esplorative, selezione, ecc.); si estraggono i dati con l'algoritmo scelto; si interpretano e valutano i pattern; l'ultimo passaggio va dai pattern alla nuova conoscenza così acquisita.
Vi sono diverse proposte e tecniche aventi ognuna specifiche caratteristiche e vantaggi.
Che cosa "è" "data mining"?
Che cosa "non è" "data mining"?
È una forma particolare di data mining nella quale i dati consistono in testi in lingua naturale, in altre parole, documenti "destrutturati". Il text mining unisce la tecnologia della lingua con gli algoritmi del data mining. L'obiettivo è sempre lo stesso: l'estrazione di informazione implicita contenuta in un insieme di documenti.
Ha avuto un notevole sviluppo, grazie ai progressi delle tecniche di elaborazione del linguaggio naturale (NLP in inglese), della disponibilità di applicazioni complesse attraverso gli "Application service provider" (ASP) e dell'interesse verso le tecniche automatiche di gestione della lingua mostrato sia dagli accademici, sia dai produttori di software, sia dai gestori dei motori di ricerca.
Una delle evoluzioni più recenti del data mining è la "data visualisation". Settore specialistico dell'infografica, la data visualisation si occupa non solamente di rendere graficamente intelligibile un testo, ma entra in relazione più diretta con la strutturazione dei database e l'esportazione di grafici dai dati.
Un'altra nuova frontiera è il «social data mining»: l'analisi di informazioni generate dalle reti sociali online, come ad esempio l'analisi del sentiment.
L'utilizzo del data mining nella ricerca di mercato è volto ad ampliare la conoscenza su cui basare i processi decisionali. Nel contesto aziendale il data mining è considerato parte del processo che porta alla creazione di un data warehouse. È efficace soprattutto per la valorizzazione delle informazioni aziendali residenti in questi grandi depositi di dati. Affinché l'informazione estratta dai dati esistenti sia significativa, e quindi potenzialmente utile, deve essere:
In questo contesto, un pattern (schema) non è altro che la rappresentazione delle relazioni chiave che vengono scoperte durante il processo di estrazione dati: sequenze ripetute, omogeneità, emergenza di regole, ecc. Per esempio, se un pattern mostra che i clienti di una certa area demografica sono molto propensi ad acquistare uno specifico prodotto, allora un'interrogazione ("query") selettiva ad un data warehouse di probabili compratori può essere usata per generare un elenco di indirizzi promozionali.
L'esempio classico spesso usato nei corsi universitari è quello di una catena non meglio specificata di supermercati (probabilmente statunitense) che avrebbe scoperto, analizzando gli scontrini, qualcosa altrimenti difficilmente immaginabile: le persone che acquistavano pannolini spesso compravano più birra degli altri, per cui mettendo la birra più costosa non lontano dai pannolini, poteva incrementarne le vendite. Infatti quelle persone che avevano figli piccoli passavano più serate in casa a guardare la TV bevendo birra in casa non potendo uscire con gli amici. È doveroso tuttavia precisare che non è chiaro quale sia la catena di supermercati in questione, e l'esempio, seppur ottimo per scopi didattici e largamente utilizzato anche in ambito giornalistico, potrebbe essere stato inventato così come potrebbe essere vero.
</text>
</doc>
<doc id="49731" url="https://it.wikipedia.org/wiki?curid=49731">
<title>Analisi delle componenti principali</title>
<text>
L'analisi delle componenti principali (in inglese "principal component analysis" o abbreviata "PCA"), anche nota come trasformata di Karhunen-Loève, trasformata di Hotelling o decomposizione ortogonale propria, è una tecnica per la semplificazione dei dati utilizzata nell'ambito della statistica multivariata. Questo metodo fu proposto per la prima volta nel 1901 da Karl Pearson e sviluppato poi da Harold Hotelling nel 1933, e fa parte dell'analisi fattoriale. Lo scopo della tecnica è quello di ridurre il numero più o meno elevato di variabili che descrivono un insieme di dati a un numero minore di variabili latenti, limitando il più possibile la perdita di informazioni.
Ciò avviene tramite una trasformazione lineare delle variabili che proietta quelle originarie in un nuovo sistema cartesiano in cui la nuova variabile con la maggiore varianza viene proiettata sul primo asse, la variabile nuova, seconda per dimensione della varianza, sul secondo asse e così via.
La riduzione della complessità avviene limitandosi ad analizzare le principali, per varianza, tra le nuove variabili.
Diversamente da altre trasformazioni lineari di variabili praticate nell'ambito della statistica, in questa tecnica sono gli stessi dati che determinano i vettori di trasformazione.
Assumendo che a ciascuna delle variabili originarie venga sottratta la loro media e pertanto la nuova variabile (X) abbia media nulla,
Dove arg max indica l'insieme degli argomenti "w" in cui è raggiunto il massimo. Con i primi (k-1) componenti, il k-esimo componente può essere trovato sottraendo i primi (k-1) componenti principali a "X"
e sostituendo questo
Un metodo più semplice per calcolare la componente w utilizza la matrice delle covarianze di x. La stessa operazione può essere eseguita partendo dalla matrice dei coefficienti di correlazione anziché dalla matrice di varianza-covarianza delle variabili "x".
Innanzitutto si devono trovare gli autovalori della matrice di covarianza o della matrice dei coefficienti di correlazione. Si ottengono tanti autovalori quante sono le variabili x. Se viene utilizzata la matrice di correlazione, l'autovalore relativo alla prima componente principale, ossia quella con varianza massima, sarà pari ad 1. In ogni caso l'autovalore con il maggiore valore corrisponde alla dimensione w che ha la maggiore varianza: esso sarà dunque la varianza della componente principale 1. In ordine decrescente, il secondo autovalore sarà la varianza della componente principale 2, e così via per gli n autovalori. Per ciascun autovalore viene calcolato il corrispondente autovettore, ossia la matrice (riga vettore) dei coefficienti che moltiplicano le vecchie variabili x nella combinazione lineare per l'ottenimento delle nuove variabili w. Questi coefficienti sono anche definiti loading. La matrice degli autovettori, ossia la matrice che ha per riga ciascun autovettore prima calcolato, è la cosiddetta matrice di rotazione V. Eseguendo l'operazione matriciale formula_4, dove W è il vettore colonna avente come elementi le nuove variabili w1, w2, ..., wn e X è il vettore colonna avente come elementi le "vecchie variabili" x1, x2, ..., xn, si possono trovare le coordinate di ciascun punto nel nuovo spazio vettoriale. Utilizzando le coordinate per ciascun punto relative alle componenti principali si costruisce il grafico denominato score plot. Se le componenti principali sono 3 si avrà un grafico tridimensionale, se sono 2 sarà bidimensionale, se invece si è scelta una sola componente principale lo score plot sarà allora monodimensionale. Mediante lo score plot è possibile verificare quali dati sono simili tra di loro e quindi si può ad esempio dedurre quali campioni presentano la medesima composizione.
In PCA esiste anche un altro tipo di grafico, definito loading plot, in cui sono le variabili x ad essere riportate nel nuovo sistema avente per assi le componenti principali. Con questo tipo di grafico è possibile osservare se due variabili sono simili, e pertanto forniscono lo stesso tipo di informazione, oppure se sono distanti (e quindi non sono simili).
Quindi gli elementi dell'autovettore colonna corrispondente a un autovalore esprimono il legame tra le variabili di partenza e la componente considerata attraverso dei pesi. Il numero di variabili latenti da considerare come componenti principali si fonda sulla grandezza relativa di un autovalore rispetto agli altri. Invece nel caso in cui sia l'operatore a scegliere le componenti principali senza considerare la relativa varianza espressa dai rispettivi autovalori, si ha un supervised pattern recognition.
Si può costruire la matrice dei fattori, in pratica una matrice modale, che elenca per riga le variabili originarie e per colonna le variabili latenti: ogni valore, compreso tra 0 e 1, dice quanto le seconde incidano sulle prime.
Invece la matrice del punteggio fattoriale ha la stessa struttura della precedente, ma dice quanto le singole variabili originarie abbiano pesato sulla determinazione della grandezza di quelle latenti.
Si supponga di disporre di un'indagine che riporta per 10 soggetti: voto medio (da 0 a 33), intelligenza (da 0 a 10), media ore studiate in un giorno e zona d'origine, che varia da 1 a 3. Si standardizzino i valori con la formula:
formula_5
E(x) è il valore atteso di X, ovvero il valor medio, SD è la deviazione standard.
La matrice dei coefficienti di correlazione è:
La diagonale principale è composta da valori uguali ad 1 perché è il coefficiente di correlazione di una variabile con se stessa. È pure una matrice simmetrica perché il coefficiente di correlazione tra la variabile "x" e la variabile "y" è uguale a quello tra "y" e "x". Si vede come ci sia un forte legame tra voto, media ore studio e intelligenza.
Dall'analisi degli autovalori si possono trarre conclusioni:
Gli autovalori sono in ordine decrescente e il loro rapporto con la somma degli autovalori dà la percentuale di varianza che spiegano. Sono stati selezionati arbitrariamente solo quelli che hanno valore maggiore di 1 in quanto più significativi, che spiegano il 70,708% e il 26,755% rispettivamente.
Si osservi alla matrice delle componenti principali:
Il fattore 1 pesa fortemente sul voto medio. Sembrerebbe pure che pesi in maniera negativa sulla variabile della zona di origine; chiaramente questa affermazione non ha senso perché inverte il nesso di causalità: spetta allo statistico dare una spiegazione e una lettura sensate.
Si calcoli quindi la matrice di punteggio fattoriale:
Come si vede la variabile provenienza continua ad avere un influsso di segno negativo sull'autovalore principale. Le altre variabili invece hanno peso positivo.
</text>
</doc>
<doc id="71806" url="https://it.wikipedia.org/wiki?curid=71806">
<title>Metodo della massima verosimiglianza</title>
<text>
Il metodo della massima verosimiglianza, in statistica, è un procedimento matematico per determinare uno stimatore. Caso particolare della più ampia classe di metodi di stima basata sugli stimatori d'estremo, il metodo consiste nel massimizzare la funzione di verosimiglianza, definita in base alla probabilità di osservare una data realizzazione campionaria, condizionatamente ai valori assunti dai parametri statistici oggetto di stima. Il metodo è stato sviluppato, originariamente, dal genetista e statistico sir Ronald Fisher, tra il 1912 e il 1922.
Data una distribuzione di probabilità formula_1, con funzione di massa (o densità, se continua) di probabilità formula_2, caratterizzata da un parametro formula_3, dato un campione di dati osservati formula_4 di dimensione formula_5 si può calcolare la probabilità associata ai dati osservati:
D'altra parte, può darsi che il parametro formula_3 sia ignoto, sebbene sia noto che il campione è estratto dalla distribuzione formula_1. Un'idea per stimare formula_3 è allora utilizzare i dati a nostra disposizione: formula_4 per ottenere informazioni su formula_3. 
Il metodo della massima verosimiglianza ricerca il valore più "verosimile" di formula_3, ossia ricerca, all'interno dello spazio formula_13 di tutti i possibili valori di formula_3, il valore del parametro che "massimizza" la probabilità di aver ottenuto il campione dato. Da un punto di vista matematico, formula_15 o equivalentemente formula_16 è detta funzione di verosimiglianza, e lo stimatore di massima verosimiglianza è ottenuto come:
Al fine di illustrare il metodo della massima verosimiglianza, si consideri un campione formula_18 di variabili casuali identicamente e indipendentemente distribuite, con distribuzione normale: formula_19. La funzione di verosimiglianza associata è:
La massimizzazione della funzione di verosimiglianza è equivalente a massimizzarne il logaritmo:
I parametri formula_22 e formula_23 sono determinati risolvendo il problema di massimo:
Le condizioni del primo ordine per un massimo definiscono il seguente sistema di equazioni in formula_22 e formula_23:
dove i segni di apice sopra i parametri denotano i loro stimatori. Dalla prima equazione discende immediatamente lo stimatore di massima verosimiglianza per la media:
cioè la media campionaria. La varianza dello stimatore formula_30 è data dalla seguente espressione:
Sostituendo formula_30 nella seconda equazione, si ha lo stimatore di massima verosimiglianza per la varianza:
cioè la varianza campionaria. 
L'esempio è particolarmente calzante, perché consente di illustrare alcune proprietà degli stimatori di massima verosimiglianza. È immediato verificare la correttezza (o "unbiasedness") di formula_30:
D'altra parte, formula_36 non gode di tale proprietà. Ricordando che:
segue che:
Dunque formula_36 non è uno stimatore corretto; un tale stimatore sarebbe dato dalla statistica:
Val la pena d'altra parte di osservare che lo stimatore di massima verosimiglianza è comunque uno stimatore asintoticamente corretto; infatti:
In particolare, qualunque stimatore di massima verosimiglianza è asintoticamente corretto e asintoticamente normalmente distribuito. 
L'espressione per la varianza dello stimatore formula_36 è al di là degli scopi di questo esempio.
È interessante osservare che gli stimatori derivati in questa sezione sono identici a quelli ottenibili, nelle stesse condizioni, impiegando il metodo dei momenti; a scanso di equivoci, si precisa che i due metodi di ricerca degli stimatori non conducono necessariamente a individuare gli stessi stimatori in condizioni più generali.
Al di là dei problemi evidenziati negli esempi sopra, altre difficoltà, di portata più generale, possono essere associate agli stimatori di massima verosimiglianza. 
Il valore dello stimatore di massima verosimiglianza può non appartenere allo spazio dei parametri formula_13. Si consideri il caso di un campione formula_44 di v.c. identicamente e indipendentemente distribuite, con distribuzione di Poisson di parametro formula_45. La funzione di verosimiglianza associata è:
Così che la funzione di log-verosimiglianza risulta:
Lo stimatore di massima verosimiglianza sarebbe dunque formula_48. Si supponga tuttavia che formula_49; poiché formula_50, la stima ottenuta con il metodo della massima verosimiglianza non è ammissibile. 
A prima vista il problema potrebbe apparire un dettaglio matematico di scarso rilievo nella pratica; la sua portata nelle applicazioni è tuttavia più rilevante di quanto sembri. Restando nell'ambito dell'esempio testé esposto, si osservi che la variabile casuale poissoniana è spesso utilizzata come modello per il numero di arrivi a uno sportello, un ufficio, la fermata di un autobus, etc. (si tratta di un'applicazione della teoria delle code, che fa per la precisione riferimento al processo di Poisson); in tale contesto, formula_51 rappresenta il tasso atteso di arrivi per unità di tempo. È chiaro che ipotizzare formula_52 in qualche misura snatura il processo sotto esame: può darsi che, nell'intervallo di tempo corrispondente al campione utilizzato per la stima, nessun cliente sia arrivato allo sportello (nessun passeggero alla fermata dell'autobus, etc.); ciò non significa che ci si debba aspettare che nessun cliente (o passeggero, etc.) arrivi mai!
Lo stimatore di massima verosimiglianza, inoltre, "non è necessariamente unico". Si consideri, ad esempio, il caso di un campione formula_53 di variabili casuali identicamente e indipendentemente distribuite, aventi distribuzione uniforme sull'intervallo formula_54, con formula_55. La funzione di verosimiglianza associata è:
dove formula_57 denota la funzione indicatrice. Si supponga che il campione sia ordinato in modo tale che:
(tale ipotesi è lecita in quanto le formula_59 sono indipendentemente distribuite). È facile mostrare che:
Ne consegue che lo stimatore di massima verosimiglianza per formula_3 è unico se e solo se formula_62; diversamente, un numero infinito di valori dello stimatore formula_63 massimizza la funzione di verosimiglianza.
Se formula_63 è lo stimatore di massima verosimiglianza per il parametro formula_3, allora lo stimatore di massima verosimiglianza per formula_66 è formula_67, purché formula_68 sia una funzione biiettiva.
Gli stimatori di massima verosimiglianza, come illustrato negli esempi, possono essere distorti (cioè non corretti o all'inglese "biased"), anche in maniera consistente. D'altra parte essi sono asintoticamente corretti.
Gli stimatori di massima verosimiglianza non conseguono in generale il limite inferiore per la varianza stabilito dal risultato di Cramér-Rao, lo conseguono però asintoticamente, cioè la varianza si discosta dal limite inferiore di Cramér-Rao per una quantità infinitesima al crescere di "n". Gli stimatori di massima verosimiglianza sono inoltre asintoticamente normalmente distribuiti.
</text>
</doc>
<doc id="71808" url="https://it.wikipedia.org/wiki?curid=71808">
<title>Funzione di verosimiglianza</title>
<text>
In statistica, la funzione di verosimiglianza (o funzione di likelihood) è una funzione di probabilità condizionata, considerata come funzione del suo "secondo" argomento, mantenendo fissato il primo argomento.
In gergo colloquiale spesso "verosimiglianza" è usato come sinonimo di "probabilità", ma in campo statistico vi è una distinzione tecnica precisa. Questo esempio chiarisce la differenza tra i due concetti: una persona potrebbe chiedere "Se lanciassi una moneta non truccata 100 volte, qual è la probabilità che esca testa tutte le volte?" oppure "Dato che ho lanciato una moneta 100 volte ed è uscita testa 100 volte, qual è la verosimiglianza che la moneta sia truccata?". Scambiare tra loro, nelle due frasi, i termini "verosimiglianza" e "probabilità" sarebbe errato.
Una distribuzione di probabilità che dipende da un parametro può essere considerata in due modi differenti:
Formalmente la funzione di verosimiglianza è una funzione:
Si definisce ancora funzione di verosimiglianza ogni funzione proporzionale a tale probabilità. Dunque, la funzione di verosimiglianza per formula_2 è la classe delle funzioni:
per ogni costante formula_4. A causa di ciò, l'esatto valore di formula_5 non è in generale rilevante; ciò che è importante sono rapporti nella forma: formula_6, invarianti rispetto alla costante di proporzionalità.
A livello interpretativo, l'uso di una funzione di verosimiglianza trae giustificazione dal teorema di Bayes, in base al quale, per due qualsiasi eventi formula_7 e formula_2:
dove sia formula_10 che formula_11 sono funzioni di verosimiglianza. L'uso di funzioni di verosimiglianza ai fini dell'inferenza statistica costituisce un tratto distintivo dell'inferenza classica, o "frequentista"; esso rappresenta inoltre una fondamentale differenza rispetto alla scuola dell'inferenza bayesiana, in quanto lo statistico bayesiano conduce inferenza tramite la probabilità formula_12 nell'espressione sopra.
Alcune idee relative alla funzione di verosimiglianza sembrano essere state introdotte da T. N. Thiele in un lavoro del 1889. Il primo contributo in cui il concetto di funzione di verosimiglianza è esplicitamente formulato è tuttavia dovuto a Ronald Fisher in un suo lavoro del 1922. In tale lavoro, Fisher usa inoltre l'espressione metodo della massima verosimiglianza; argomenta inoltre contro il ricorso alla condizionata nella forma formula_13 nell'espressione sopra, da lui ritenuta ingiustificabile a causa dell'elemento di soggettività introdotto tramite la probabilità "a priori" (nel linguaggio che ora è proprio della statistica bayesiana) formula_14. 
Il metodo della massima verosimiglianza ha le sue applicazioni più rilevanti nella prassi come metodo di stima di modelli parametrici. Considerando un insieme di osservazioni formula_15, e una famiglia di funzioni di densità (o di massa, nel caso di distribuzioni discrete), parametrizzate tramite il vettore formula_16:
la funzione di verosimiglianza associata è:
Nel caso in cui, come normalmente si ipotizza, gli formula_19 siano indipendenti e identicamente distribuiti, inoltre:
Poiché l'espressione sopra può risultare scarsamente trattabile, specie nei problemi di massimizzazione collegati al metodo della massima verosimiglianza, spesso risulta preferibile lavorare sul logaritmo della funzione di verosimiglianza, in gergo chiamata "log-verosimiglianza":
</text>
</doc>
<doc id="100003" url="https://it.wikipedia.org/wiki?curid=100003">
<title>Albero di decisione</title>
<text>
Nella teoria delle decisioni (per esempio nella gestione dei rischi), un albero di decisione è un grafo di decisioni e delle loro possibili conseguenze, (incluso i relativi costi, risorse e rischi) utilizzato per creare un 'piano di azioni' ("plan") mirato ad uno scopo ("goal"). Un albero di decisione è costruito al fine di supportare l'azione decisionale ("decision making").
Nel machine learning un albero di decisione è un modello predittivo, dove ogni nodo interno rappresenta una variabile, un arco verso un nodo figlio rappresenta un possibile valore per quella proprietà e una foglia il valore predetto per la variabile obiettivo a partire dai valori delle altre proprietà, che nell'albero è rappresentato dal cammino ("path") dal nodo radice ("root") al nodo foglia.
Normalmente un albero di decisione viene costruito utilizzando tecniche di apprendimento a partire dall'insieme dei dati iniziali ("data set"), il quale può essere diviso in due sottoinsiemi: il "training set" sulla base del quale si crea la struttura dell'albero e il "test set" che viene utilizzato per testare l'accuratezza del modello predittivo così creato.
Nel data mining un albero di decisione viene utilizzato per classificare le istanze di grandi quantità di dati (per questo viene anche chiamato albero di classificazione). In questo ambito un albero di decisione descrive una struttura ad albero dove i nodi foglia rappresentano le classificazioni e le ramificazioni l'insieme delle proprietà che portano a quelle classificazioni. Di conseguenza ogni nodo interno risulta essere una macro-classe costituita dall'unione delle classi associate ai suoi nodi figli.
Il predicato che si associa ad ogni nodo interno (sulla base del quale avviene la ripartizione dei dati) è chiamato "condizione di split".
In molte situazioni è utile definire un criterio di arresto ("halting"), o anche "criterio di potatura" ("pruning") al fine di determinarne la profondità massima. Questo perché il crescere della profondità di un albero (ovvero della sua dimensione) non influisce direttamente sulla bontà del modello. Infatti, una crescita eccessiva della dimensione dell'albero potrebbe portare solo ad aumento sproporzionato della complessità computazionale rispetto ai benefici riguardanti l'accuratezza delle previsioni/classificazioni.
Una sua evoluzione è la tecnica foresta casuale ("random forest").
I parametri più largamente usati per le condizioni di split sono:
formula_1
L'indice di Gini raggiunge il suo minimo (zero) quando il nodo appartiene ad una singola categoria. 
formula_2
In entrambe le formule "f" rappresenta la frequenza del valore "j" nel nodo "i".
L'indice di Gini e la variazione di entropia sono i parametri che vengono usualmente utilizzati per guidare la costruzione dell'albero, mentre la valutazione del tasso di errore nella classificazione viene utilizzato per effettuare una ottimizzazione dell'albero nota come processo di "pruning" ("potatura" dei nodi superflui). Poiché, in generale, in un buon albero di decisione i nodi foglia dovrebbero essere il più possibile "puri" (ovvero contenere solo istanze di dati che appartengono ad una sola classe), un'ottimizzazione dell'albero consiste nel cercare di minimizzare il livello di entropia man mano che si scende dalla radice verso le foglie. In tal senso, la valutazione dell'entropia determina quali sono, fra le varie scelte a disposizione, le condizioni di split ottimali per l'albero di classificazione.
</text>
</doc>
<doc id="40388" url="https://it.wikipedia.org/wiki?curid=40388">
<title>Funzione di densità di probabilità</title>
<text>
In matematica, una funzione di densità di probabilità (o PDF dall'inglese "probability density function") è l'analogo della funzione di probabilità di una variabile casuale nel caso in cui la variabile casuale formula_1 sia continua, cioè l'insieme dei possibili valori che ha la potenza del continuo.
Essa descrive la "densità" di probabilità in ogni punto nello spazio campionario.
La funzione di densità di probabilità di una variabile casuale formula_1 è un'applicazione formula_3 non negativa integrabile secondo Lebesgue e reale di variabile reale tale che la probabilità dell'insieme "A" sia data da
per tutti i sottinsiemi "A" dello spazio campionario.
Intuitivamente, se una distribuzione di probabilità ha densità formula_3, allora l'intervallo formula_6 ha probabilità formula_7. Da ciò deriva che la funzione formula_3 è un'applicazione definita come
Assumendo formula_10, ciò corrisponde al limite della probabilità che formula_11 si trovi nell'intervallo formula_6 per formula_13 che tende a zero. Di qui il nome di funzione di 'densità', in quanto essa rappresenta il rapporto tra una probabilità e un'ampiezza.
Per la condizione di normalizzazione l'integrale su tutto lo spazio di formula_3 deve essere 1. Di conseguenza ogni funzione non negativa, integrabile secondo Lebesgue, con integrale su tutto lo spazio uguale a 1, è la funzione densità di probabilità di una ben definita distribuzione di probabilità. Una variabile casuale che possiede densità si dice "variabile casuale continua".
Per le variabili casuali multivariate (o vettoriali) la trattazione formale è assolutamente identica: formula_15 si dice assolutamente continua se esiste una funzione a valori reali definita in formula_16, detta densità congiunta, tale che per ogni sottoinsieme "A" dello spazio campionario
Essa conserva tutte le proprietà di una densità scalare: è una funzione non negativa a integrale unitario su tutto lo spazio. Una proprietà importante è che se formula_15 è assolutamente continua allora lo è ogni sua componente; il viceversa invece non vale. La densità di una componente, detta densità marginale, si ottiene con un ragionamento analogo al teorema della probabilità assoluta, cioè fissando l'insieme di suoi valori di cui si vuole determinare la probabilità e lasciando libere di variare tutte le altre componenti. Infatti (nel caso bivariato per semplicità) l'evento formula_19 è l'evento formula_20, dunque
utilizzando il teorema di Fubini. La densità marginale di formula_1 è data dunque da
La funzione di densità della variabile casuale normale di media 0
e varianza 1 (detta "normale standard"), di cui a destra è riportato il grafico e l'espressione analitica della corrispondente densità nel caso generico (media formula_24 e varianza formula_25).
Un altro esempio può essere dato dalla densità di probabilità uniforme su un segmento (0,1). Si può verificare immediatamente che è densità di probabilità facendo l'integrale tra (0,1).
</text>
</doc>
<doc id="43370" url="https://it.wikipedia.org/wiki?curid=43370">
<title>Test di verifica d'ipotesi</title>
<text>
Il test di verifica d'ipotesi si utilizza per verificare la bontà di un'ipotesi.
Per ipotesi è da intendersi un'affermazione che ha come oggetto accadimenti nel mondo reale, che si presta ad essere confermata o smentita dai dati osservati sperimentalmente.
Il metodo con cui si valuta l'attendibilità di un'ipotesi è il metodo sperimentale. Quest'ultimo consiste nel determinare le conseguenze di un'ipotesi in termini di eventi osservabili, e di valutare se la realtà effettivamente osservata si accorda o meno con l'ipotesi su di essa fatta. 
A tal riguardo si distinguono due ambiti in cui tale attività si esplica:
Nell'ambito statistico, a seconda delle ipotesi si distingue tra:
Nel primo caso, si tende a pervenire a delle conclusioni più sicure possibili. Ad esempio volendo provare se in un circuito elettrico passa corrente si inserirà una lampadina o un amperometro e si constaterà l'accensione o l'attivazione dello strumento. In tal caso si perviene con maggior sicurezza alla conclusione. Se la lampadina si accende allora passa corrente; in caso contrario il circuito non è predisposto correttamente.
In questo ambito, se nel circuito passa corrente la maggior parte delle volte che si inserisce una lampadina questa si accende. In caso contrario il ripetuto inserimento della lampadina darà sempre esito negativo.
Nel secondo caso la situazione è modificata in quanto interviene un elemento nuovo, ovvero il caso e/o l'errore di misura. Si supponga di avere una moneta recante due facce contrassegnate con testa e croce. Volendo verificare l'ipotesi di bilanciamento della moneta si eseguono 20 lanci e si contano quelli che danno esito testa. La conseguenza del bilanciamento consiste nell'osservare un valore di teste attorno a 10. Tuttavia anche in ipotesi di bilanciamento non si può escludere di osservare 20 teste. D'altronde, l'ipotesi di bilanciamento è logicamente compatibile con un numero di teste variante da 0 a 20. In tale contesto una qualsiasi decisione in merito all'ipotesi da verificare comporta un rischio di errore. Ad esempio rigettare l'ipotesi di bilanciamento della moneta avendo osservato 20 teste su 20 lanci comporta il rischio di prendere una decisione errata. 
Nel procedere alla verifica dell'ipotesi di bilanciamento della moneta, si ricorre a una variabile casuale X. Tale variabile casuale X è una variabile aleatoria discreta con distribuzione binomiale B(20; 0,5), dove 20 indica il numero di lanci e 0,5 la probabilità che si verifichi l'evento "testa".
Il risultato sperimentale si deve quindi confrontare con tale distribuzione: quanto è distante tale risultato dal valore medio della distribuzione B(20; 0,5)? Per rispondere alla domanda si deve individuare un valore caratteristico della distribuzione B(20; 0,5). Nel nostro caso tale valore caratteristico è il valore medio 20/2 = 10. Per valutare la distanza tra il valore sperimentale e quello atteso si valuta la probabilità di ottenere un valore sperimentale lontano dal valore medio di B(20; 0,5), ossìa nel caso che dal nostro esperimento risulti X=15 (15 teste dopo 20 lanci), si calcola P{|X-10|&gt;=15-10} quindi P{X&lt;=5 oppure X&gt;=15}=0,041.
Quindi, usando una moneta ben bilanciata, la probabilità di ottenere un numero di teste X &gt;= 15 (oppure X &lt;= 5) dopo 20 lanci è pari a 0,041 ossia al 4,1%. Giudicando bassa tale probabilità si rifiuterà l'ipotesi di bilanciamento della moneta in esame, accettando quindi il rischio del 4,1% di compiere un errore nel rifiutarla. Di solito, il valore della probabilità adottato per rifiutare l'ipotesi nulla è &lt; 0,05. Tale valore è detto livello di significatività ed è definibile come segue: il livello di significatività sotto l'ipotesi nulla è la probabilità di cadere nella zona di rifiuto quando l'ipotesi nulla è vera. Tale livello di significatività si indica convenzionalmente con α. Il livello di significatività osservato α del test per il quale si rifiuterebbe l'ipotesi nulla è detto valore-p ("p-value"). Riprendendo l'esempio sopra riportato il valore-p è pari a 0,041.
Adottando nell'esempio α = 0,05, si rifiuterà l'ipotesi se P{|X-10|&gt;=x}&lt;0,05. Tale condizione si raggiunge appunto se X&lt;6 oppure X&gt;14. Tale insieme di valori si definisce convenzionalmente come regione di rifiuto. Viceversa l'insieme { 6,7…14} si definisce regione di accettazione. In questo modo si è costruita una regola di comportamento per verificare l'ipotesi di bilanciamento della moneta. Tale regola definisce il test statistico.
In termini tecnici l'ipotesi da verificare si chiama ipotesi nulla e si indica con "H", mentre l'ipotesi alternativa con "H". Nel caso della moneta, se "p" è la probabilità di ottenere testa in un lancio la verifica di ipotesi si traduce nel seguente sistema:
Come già osservato, il modo di condurre un test statistico comporta un rischio di errore. Nella pratica statistica si individuano due tipi di errori:
Tornando all'esempio della moneta in cui la regione di accettazione è data dall'insieme di valori {6..14}, la probabilità di rifiutare H quando è vera è stato calcolato pari a 0,041.Tale probabilità rappresenta il rischio di incorrere in un errore di primo tipo e si indica con α. Per valutare la probabilità di un errore di secondo tipo è necessario specificare un valore di "p" in caso di verità di H. Si supponga che p=0,80, in tal caso la distribuzione di X è una B(20;0,80)
Con tale distribuzione di probabilità, l'errore di tipo "2" si calcola sommando le probabilità relative ai valori di X della zona di accettazione, ciò supponendo H vera. Si trova quindi che la probabilità cercata è pari a circa 0,20. Tale probabilità quantifica il rischio di incorrere nell'errore di tipo "2." e si indica convenzionalmente con β. La quantità 1-β si chiama "potenza del test" ed esprime quindi la capacità di un test statistico di riconoscere la falsità di H quando questa è effettivamente falsa. La potenza del test trova applicazione nella pratica statistica in fase di pianificazione di un esperimento.
</text>
</doc>
<doc id="64313" url="https://it.wikipedia.org/wiki?curid=64313">
<title>Istogramma</title>
<text>
L'istogramma è la rappresentazione grafica (diagramma) di una distribuzione in classi di un carattere continuo.
È costituito da rettangoli adiacenti le cui basi sono allineate su un asse orientato e dotato di unità di misura (l'asse ha l'unità di misura del carattere e può tranquillamente essere inteso come l'asse delle ascisse). L'adiacenza dei rettangoli dà conto della continuità del carattere. Ogni rettangolo ha base di lunghezza pari all'ampiezza della corrispondente classe; l'altezza invece è calcolata come densità di frequenza, ovvero essa è pari al rapporto fra la frequenza (assoluta) associata alla classe e l'ampiezza della classe.
L'area della superficie di ogni rettangolo coincide con la frequenza associata alla classe cui il rettangolo si riferisce e per tale caratteristica gli istogrammi rappresentano un tipo di areogramma. La somma delle aree dei rettangoli è uguale alla somma delle frequenze dei valori appartenenti alle varie classi.
Volendo si può scegliere di rappresentare nell'istogramma le frequenze relative (anziché le semplici frequenze assolute) delle varie classi.
Dividendo le frequenze relative di un istogramma per l'ampiezza di ciascuna classe si attuerà un processo di normalizzazione dell'istogramma ottenendo così un istogramma di densità la cui somma delle aree delle ampiezze di ciascuna classe rappresentata sarà uguale ad 1.
Nell'ipotesi che la numerosità dei valori osservati tenda a infinito, e contemporaneamente l'ampiezza delle classi tenda a zero, l'istogramma converge, a sua volta, a una stima (seppur distorta) della legge di probabilità che regola l'esperimento casuale da cui si osserva il carattere.
Gli istogrammi non devono essere confusi con i grafici a colonne: questi ultimi infatti, a differenza dei primi, hanno altezza proporzionale alla frequenza e sono costituiti da rettangoli separati tra loro.
Gli istogrammi vengono spesso utilizzati nella fotografia digitale e nel fotoritocco per analizzare la luminosità di un'immagine.
L'istogramma è uno dei sette strumenti della qualità, si costruisce partendo dalla massima escursione tra i dati dividendola per gli intervalli desiderati.
</text>
</doc>
<doc id="4414189" url="https://it.wikipedia.org/wiki?curid=4414189">
<title>Probabilità a posteriori</title>
<text>
In statistica bayesiana, la probabilità a posteriori di un evento aleatorio o di una proposizione incerta, è la probabilità condizionata che è assegnata dopo che si è tenuto conto dell'informazione rilevante o degli antefatti relativi a tale evento aleatorio o a tale proposizione incerta. Similmente, la distribuzione di probabilità a posteriori è la distribuzione di una quantità incognita, trattata come una variabile casuale, condizionata sull'informazione posta in evidenza da un esperimento o da un processo di raccolta di informazione rilevanti (es. un'ispezione, un'indagine conoscitiva, ecc.).
La probabilità a posteriori è la probabilità dei parametri formula_1 data la conoscenza di formula_2: formula_3.
Essa differisce dalla funzione di verosimiglianza, che è la probabilità di possedere una data conoscenza una volta dati i parametri: formula_4.
I due concetti sono però tra loro collegati:
Supponiamo di avere una credenza a priori che la funzione di distribuzione di probabilità sia formula_5 e i dati osservati formula_2 con una verosimiglianza formula_4, allora la probabilità a posteriori è definita come
La probabilità a posteriori può essere scritta in una forma mnemonica come
Consideriamo una scuola mista composta dal 60% di ragazzi e dal 40% di ragazze. Le ragazze indossano pantaloni o gonne in numeri eguali, i ragazzi indossano tutti pantaloni. Un osservatore vede da distante uno studente (a caso); tutto quello che può dire è che indossa pantaloni. Qual è la probabilità che lo studente sia una ragazza? La risposta corretta può essere dedotta applicando il teorema di Bayes.
L'evento "G" è quello in cui lo studente visto è una ragazza, e l'evento "T" è quello in cui lo studente visto indossa pantaloni. Per calcolare P("G"|"T") abbiamo prima bisogno di sapere:
Una volta ottenute tutte queste informazioni, la probabilità che l'osservatore abbia individuato una ragazza una volta visto uno studente che indossa pantaloni può essere calcolata sostituendo i valori nella formula:
La distribuzione di probabilità a posteriori di una variabile casuale dato il valore di un'altra, può essere calcolata con il teorema di Bayes moltiplicando la distribuzione di probabilità a priori per la funzione di verosimiglianza, e quindi dividendo per una costante di normalizzazione come segue:
la quale fornisce la funzione di densità di probabilità per una variabile casuale "X" una volta dato "Y" = "y", dove
Nell'ambito della classificazione statistica le probabilità a posteriori riflettono l'incertezza nell'assegnare un'osservazione ad una classe particolare.
Mentre i metodi di classificazione statistica per definizione generano probabilità a posteriori, gli apprenditori automatici solitamente forniscono valori di appartenenza che non inducono alcuna confidenza di tipo probabilistico. È desiderabile trasformare o convertire i valori di appartenenza a valori di probabilità di appartenenza ad una certa classe in quanto tali classi sono, in confronto ai primi, di più facile trattamento in susseguenti elaborazioni.
</text>
</doc>
<doc id="4562805" url="https://it.wikipedia.org/wiki?curid=4562805">
<title>Minima lunghezza di descrizione</title>
<text>
Il principio della minima lunghezza di descrizione (MLD) (in inglese "minimum description length [MDL] principle") è una formalizzazione del Rasoio di Occam nella quale la migliore ipotesi per un determinato insieme di dati è quella che conduce alla migliore compressione dei dati. La MLD fu introdotta da Jorma Rissanen nel 1978. È un importante concetto nella teoria dell'informazione e nella teoria dell'apprendimento.
Qualunque insieme di dati può essere rappresentato da una stringa di simboli da un alfabeto finito (diciamo, binario).
Per selezionare l'ipotesi che cattura la maggior parte della regolarità nei dati, gli scienziati cercano l'ipotesi con la quale si può raggiungere la migliore compressione. Per fare questo, si fissa un codice per comprimere i dati, più generalmente con un linguaggio informatico (Turing equivalente). Un programma per produrre i dati è scritto in quel linguaggio; pertanto il programma rappresenta efficacemente i dati. La lunghezza del programma più breve che produce i dati si chiama complessità di Kolmogorov dei dati. Questa è l'idea centrale della teoria idealizzata dell'inferenza induttiva di Ray Solomonoff.
Tuttavia, questa teoria matematica non fornisce un modo pratico di raggiungere un'inferenza. Le ragioni più importanti di questo sono:
L'MLD tenta di rimediare a questi inconvenienti:
Piuttosto che di "programmi", nella teoria della MLD di solito si parla di ipotesi, modelli o codici candidati. L'insieme di dati consentiti è poi chiamato classe di modelli. (Alcuni autori definiscono la classe di modelli come il modello.) Poi si seleziona il codice per il quale la somma della descrizione del codice e la descrizione dei dati che usano il codice è minima.
Una delle importanti proprietà dei metodi della MLD è che forniscono una salvaguardia naturale contro l'adattamento eccessivo ("overfitting"), perché attuano un compromesso tra la complessità dell'ipotesi (classe di modelli) e la complessità dei dati considerata l'ipotesi .
Si lancia una moneta 1.000 volte e si registra il numero di teste e di croci. Si considerino due classi di modelli: 
Per questa ragione un metodo statistico ingenuo potrebbe scegliere il secondo modello come migliore spiegazione per i dati. Tuttavia, un approccio MLD costruirebbe un unico codice basato sull'ipotesi, invece di usare semplicemente il migliore. Per fare questo, è più semplice usare un codice in due parti nel quale è specificato l'elemento della classe di modelli con la migliore prestazione. Poi si specificano i dati usando quel codice. Occorrono molti bit per specificare quale codice usare; pertanto la lunghezza totale del codice basato sulla seconda classe di modelli potrebbe essere più grande di 1.000 bit. Perciò la conclusione quando si segue un approccio di MLD è la seguente : è inevitabile che non vi siano abbastanza prove per sostenere l'ipotesi della moneta distorta, anche se il miglior elemento della seconda classe di modelli fornisce un miglior adattamento ai dati.
Centrale per la teoria della MLD è la corrispondenza biunivoca tra le funzioni di lunghezza del codice e le distribuzioni di probabilità. (Questo segue dalla disuguaglianza di Kraft-McMillan.) Per una qualsiasi distribuzione di probabilità formula_1, è possibile costruire un codice formula_2 tale che la lunghezza (in bit) di formula_3 sia uguale a formula_4; questo codice minimizza la lunghezza attesa del codice. Viceversa, dato un codice formula_2, si può costruire una distribuzione di probabilità formula_1 tale che valga lo stesso risultato. (Le questioni di arrotondamento qui sono ignorate.) In altre parole, cercare un codice efficiente si riduce a cercare una buona distribuzione di probabilità, e viceversa.
L'MLD è collegata molto strettamente alla teoria delle probabilità e alla statistica attraverso la corrispondenza tra i codici e le distribuzioni di probabilità menzionata sopra. Ciò ha condotto ricercatori quali David MacKay a considerare l'MLD come equivalente all'inferenza bayesiana: la lunghezza del codice nel modello e la lunghezza del codice del modello e dei dati insieme nella MLD corrispondono rispettivamente alla probabilità "a priori" e alla verosimiglianza marginale nella cornice bayesiana.
Sebbene il meccanismo bayesiano sia spesso utile nel costruire codici MLD efficienti, la cornice MLD soddisfa anche altri codici che non sono bayesiani. Un esempio è il "codice normalizzato della massima verosimiglianza" di Shtarkov, che gioca un ruolo centrale nell'attuale teoria della MDL, ma non ha alcun equivalente nell'inferenza bayesiana. Inoltre, Rissanen sottolinea che non dovremmo fare nessuna assunzione circa il processo di generazione di dati "veri": in pratica, una classe di modelli è tipicamente una semplificazione della realtà e pertanto non contiene nessun codice o distribuzione di probabilità che siano veri in un qualunque senso oggettivo. Nell'ultimo riferimento menzionato Rissanen basa il fondamento matematico della MLD sulla funzione di struttura di Kolmogorov.
Secondo la filosofia della MLD, i metodi bayesiani dovrebbero essere scartati se sono basati su precedenti che condurrebbero a risultati scadenti. I precedenti che sono accettabili da un punto di vista della MLD tendono ad essere favoriti anche nell'analisi "bayesiana oggettiva"; là, tuttavia, la motivazione di solito è diversa.
La MLD non fu il primo approccio informativo-teorico all'apprendimento; già nel 1968 Wallace e Boulton sperimentarono per primi un concetto correlato chiamato minima lunghezza del messaggio (MLM) ("minimum message length" [MML]). La differenza tra MDL ed MML è una fonte di perdurante confusione. Superficialmente, i metodi appaiono per la maggior parte equivalenti, ma ci sono alcune differenza significative, specialmente nell'interpretazione:
</text>
</doc>
<doc id="2246484" url="https://it.wikipedia.org/wiki?curid=2246484">
<title>Precisione e recupero</title>
<text>
, o richiamo (in inglese "precision" e "recall") sono due comuni classificazioni statistiche, utilizzate in diversi ambiti del sapere, come per es. l'information retrieval. La precisione può essere vista come una misura di "esattezza" o fedeltà, mentre il recupero è una misura di "completezza".
Nell'Information Retrieval, la precisione è definita come il numero di documenti attinenti recuperati da una ricerca diviso il numero totale di documenti recuperati dalla stessa ricerca, e il recupero è definito come il numero di documenti attinenti recuperati da una ricerca diviso il numero totale di documenti attinenti esistenti (che dovrebbe essere stato recuperato).
In un processo di classificazione statistica, la precisione per una classe è il numero di veri positivi (il numero di oggetti etichettati correttamente come appartenenti alla classe) diviso il numero totale di elementi etichettati come appartenenti alla classe (la somma di veri positivi e falsi positivi, che sono oggetti etichettati erroneamente come appartenenti alla classe).
Recupero in questo contesto è definito come il numero di veri positivi diviso il numero totale di elementi che attualmente appartengono alla classe (per esempio la somma di veri positivi e falsi negativi, che sono oggetti che non sono stati etichettati come appartenenti alla classe ma dovrebbero esserlo).
Nell'Information Retrieval, un valore di precisione di 1.0 significa che ogni risultato recuperato da una ricerca è attinente mentre un valore di recupero pari a 1.0 significa che tutti i documenti attinenti sono stati recuperati dalla ricerca.
In un processo di classificazione, un valore di precisione di 1.0 per la classe C significa che ogni oggetto che è stato etichettato come appartenente alla classe C vi appartiene davvero (ma non dice niente sul numero di elementi della classe C che non sono stati etichettati correttamente) mentre un valore di recupero pari ad 1.0 significa che ogni oggetto della classe C è stato etichettato come appartenente ad essa (ma non dice niente sul numero di elementi etichettati non correttamente con C).
Nell'information retrieval, precisione e recupero sono definite in termini di insieme di documenti recuperati (lista di documenti restituiti da un motore di ricerca rispetto ad una query) e un insieme di documenti attinenti (lista di tutti i documenti che sono attinenti per l'argomento cercato).
formula_1
formula_2
In un processo di classificazione, i termini vero positivo, vero negativo, falso positivo e falso negativo sono usati per confrontare la classificazione di un oggetto (l'etichetta di classe assegnata all'oggetto da un classificatore) con la corretta classificazione desiderata (la classe a cui in realtà appartiene l'oggetto).
Precisione e recupero sono definite come:<br/>
formula_3<br/>
formula_4
La precisione è la probabilità che un documento recuperato (selezionato casualmente) sia attinente.
Il recupero è la probabilità che un documento attinente (selezionato casualmente) sia recuperato in una ricerca.
</text>
</doc>
<doc id="2244534" url="https://it.wikipedia.org/wiki?curid=2244534">
<title>Receiver operating characteristic</title>
<text>
Nella teoria delle decisioni, le curve ROC (Receiver Operating Characteristic, anche note come Relative Operating Characteristic) sono degli schemi grafici per un classificatore binario. Lungo i due assi si possono rappresentare la sensibilità e (1-specificità), rispettivamente rappresentati da "True Positive Rate" (TPR, frazione di veri positivi) e "False Positive Rate" (FPR, frazione di falsi positivi). In altre parole, si studiano i rapporti fra allarmi veri ("hit rate") e falsi allarmi.
La curva ROC viene creata tracciando il valore del "True Positive Rate" (TPR, frazione di veri positivi) rispetto al "False Positive Rate" (FPR, frazione di falsi positivi) a varie impostazioni di soglia. Il tasso di veri positivi è anche noto come sensibilità, richiamo o probabilità di rilevazione. Il tasso di falsi positivi è anche noto come fall-out o probabilità di falsi allarmi e può essere calcolato come (1 - specificità). Può anche essere pensato come un diagramma della potenza in funzione dell'errore di tipo I :quando la prestazione viene calcolata da un solo campione della popolazione, può essere considerata come una stima di queste quantità. La curva ROC è quindi il tasso dei veri positivi in funzione del tasso dei falsi positivi. In generale, se sono note le distribuzioni di sensibilità e 1-specificità, la curva ROC può essere generata tracciando la funzione di distribuzione cumulativa (area sotto la distribuzione di probabilità da formula_1 alla soglia di discriminazione) della probabilità di rilevamento nell'asse y rispetto alla funzione di distribuzione cumulativa della probabilità di falso allarme sull'asse x.
Il ROC è anche noto come curva Receiver Operating Characteristic, poiché è un confronto tra due caratteristiche operative (TPR e FPR) al cambiare del criterio.
Le curve ROC furono utilizzate per la prima volta durante la seconda guerra mondiale, da alcuni ingegneri elettrotecnici che volevano individuare i nemici utilizzando il radar durante le battaglie aeree. Recentemente le curve ROC sono utilizzate in medicina, radiologia, psicologia, meteorologia, veterinaria, fisica e altri ambiti, come il machine learning ed il data mining.
Se si considera un problema di predizione a 2 classi (classificatore binario come da figura: distribuzione rossa e azzurra), scelto un valore di soglia ("threshold" o "cut-off"), rispetto a cui decidere il risultato, ovvero se appartenente alla classe positiva ("p") o negativa ("n"), dato che le due curve di distribuzione di probabilità risultano in parte sovrapposte, sono possibili quattro risultati a seconda della posizione del valore di cut-off:
È inoltre possibile rappresentare questo tipo di situazione utilizzando una tabella di contingenza di tipo 2×2, dove le colonne rappresentano la distinzione tra soggetti sani e malati; le righe invece rappresentano il risultato del test sui pazienti. Un risultato qualitativo del test potrebbe essere quello di andare a valutare il numero di falsi positivi e negativi; meno ve ne saranno e maggiormente il test sarà valido.
Una curva ROC è il grafico dell'insieme delle coppie (FP, TP) al variare di un parametro del classificatore. Per esempio, in un classificatore a soglia, si calcola la frazione di veri positivi e quella di falsi positivi per ogni possibile valore della soglia; tutti i punti così ottenuti nello spazio FP-TP descrivono la curva ROC.
Attraverso l'analisi delle curve ROC si valuta la capacità del classificatore di discernere, ad esempio, tra un insieme di popolazione "sana" e "malata", calcolando l'area sottesa alla curva ROC ("Area Under Curve", AUC). Il valore di AUC, compreso tra 0 e 1, equivale infatti alla probabilità che il risultato del classificatore applicato ad un individuo estratto a caso dal gruppo dei malati sia superiore a quello ottenuto applicandolo ad un individuo estratto a caso dal gruppo dei sani.
Le curve ROC passano per i punti (0,0) e (1,1), avendo inoltre due condizioni che rappresentano due curve limite:
</text>
</doc>
<doc id="840579" url="https://it.wikipedia.org/wiki?curid=840579">
<title>Variabilità</title>
<text>
Nella terminologia statistica, la variabilità di un carattere X, rilevato su n unità statistiche, è l'attitudine di questo a manifestarsi in diversi modi, ossia con diverse modalità.
Quando il carattere è "quantitativo", la variabilità può essere misurata usando indici basati sulla distanza delle modalità rispetto ad un indice di posizione (generalmente rispetto alla media aritmetica o alla mediana); gli indici di variabilità più utilizzati sono la varianza, lo scarto quadratico medio o deviazione standard, il coefficiente di variazione.
Se invece il carattere è "qualitativo", la variabilità può essere misurata con indici di eterogeneità.
Le proprietà della variabilità sono:
Esempio 1: rileviamo il carattere reddito su 5 unità statistiche; supponiamo che il risultato della rilevazione sia 1.000 euro su ognuna delle 5 unità: in tal caso la variabilità del carattere sarà nulla perché il carattere reddito si è manifestato sempre nello stesso modo (ossia con un'unica modalità: 1.000).
Esempio 2: supponiamo che, nel contesto dell'esempio precedente, il risultato della rilevazione sia 1.000 sulla prima unità, 1.100 sulla seconda, 1.500 sulla terza, 5.000 sulla quarta e 8.500 sulla quinta; in tal caso, la variabilità del carattere risulta maggiore di zero, perché il carattere si è manifestato sulle cinque unità statistiche con diverse modalità.
Esempio 3: supponiamo ora che il risultato della rilevazione sia 800 sulla prima unità, 12.000 sulla seconda, 6.500 sulla terza, 9.000 sulla quarta e 2.500 sulla quinta; in tal caso la variabilità aumenta perché le modalità rilevate, oltre ad essere tutte diverse, risultano essere più "distanti" tra loro.
In generale, la variabilità di un carattere quantitativo è tanto maggiore quanto più numerose sono le modalità con cui esso si manifesta sulle unità statistiche e quanto più le modalità rilevate sono "distanti" tra loro.
</text>
</doc>
<doc id="558366" url="https://it.wikipedia.org/wiki?curid=558366">
<title>Rete bayesiana</title>
<text>
Una rete bayesiana (BN, "Bayesian network") è un modello grafico probabilistico che rappresenta un insieme di variabili stocastiche con le loro dipendenze condizionali attraverso l'uso di un grafo aciclico diretto (DAG). Per esempio una rete Bayesiana potrebbe rappresentare la relazione probabilistica esistente tra i sintomi e le malattie. Dati i sintomi, la rete può essere usata per calcolare la probabilità della presenza di diverse malattie.
Il termine "modello gerarchico" è talvolta considerato un particolare tipo di rete Bayesiana, ma non ha nessuna definizione formale. Qualche volta viene usato per modelli con tre o più livelli di variabili stocastiche; in altri casi viene usato per modelli con variabili latenti. Comunque in generale qualsiasi rete Bayesiana moderatamente complessa viene usualmente detta "gerarchica".
Formalmente le reti Bayesiane sono grafi diretti aciclici i cui nodi rappresentano variabili casuali in senso Bayesiano: possono essere quantità osservabili, variabili latenti, parametri sconosciuti o ipotesi. Gli archi rappresentano condizioni di dipendenza; i nodi che non sono connessi rappresentano variabili che sono condizionalmente indipendenti tra di loro. Ad ogni nodo è associata una funzione di probabilità che prende in input un particolare insieme di valori per le variabili del nodo genitore e restituisce la probabilità della variabile rappresentata dal nodo. Per esempio, se i genitori del nodo sono variabili booleane allora la funzione di probabilità può essere rappresentata da una tabella in cui ogni entry rappresenta una possibile combinazione di valori vero o falso che i suoi genitori possono assumere.
Esistono algoritmi efficienti che effettuano inferenza e apprendimento a partire dalle reti Bayesiane. Le reti Bayesiane che modellano sequenze di variabili che variano nel tempo sono chiamate reti Bayesiane dinamiche.
Matematicamente, una rete bayesiana è un grafo aciclico orientato in cui:
Una rete bayesiana rappresenta la distribuzione della probabilità congiunta di un insieme di variabili.
</text>
</doc>
<doc id="599528" url="https://it.wikipedia.org/wiki?curid=599528">
<title>K-means</title>
<text>
L'algoritmo K-means è un algoritmo di clustering partizionale che permette di suddividere un insieme di oggetti in K gruppi sulla base dei loro attributi. È una variante dell'algoritmo di aspettativa-massimizzazione (EM) il cui obiettivo è determinare i K gruppi di dati generati da distribuzioni gaussiane. Si assume che gli attributi degli oggetti possano essere rappresentati come vettori, e che quindi formino uno spazio vettoriale.
L'obiettivo che l'algoritmo si prepone è di minimizzare la varianza totale intra-cluster. Ogni cluster viene identificato mediante un centroide o punto medio. L'algoritmo segue una procedura iterativa. Inizialmente crea K partizioni e assegna ad ogni partizione i punti d'ingresso o casualmente o usando alcune informazioni euristiche. Quindi calcola il centroide di ogni gruppo. Costruisce quindi una nuova partizione associando ogni punto d'ingresso al cluster il cui centroide è più vicino ad esso. Quindi vengono ricalcolati i centroidi per i nuovi cluster e così via, finché l'algoritmo non converge.
Dati N oggetti con formula_1 attributi, modellizzati come vettori in uno spazio vettoriale formula_1-dimensionale, definiamo formula_3 come insieme degli oggetti. Ricordiamo che si definisce partizione degli oggetti il gruppo di insiemi formula_4 che soddisfano le seguenti proprietà:
Ovviamente deve valere anche che formula_8; non avrebbe infatti senso né cercare un solo cluster né avere un numero di cluster pari al numero di oggetti.
Una partizione viene rappresentata mediante una matrice formula_9, il cui generico elemento formula_10 indica l'appartenenza dell'oggetto formula_11 al cluster formula_1.
Indichiamo quindi con formula_13 l'insieme dei formula_14 centroidi.
A questo punto definiamo la funzione obiettivo come:
e di questa calcoliamo il minimo seguendo la procedura iterativa vista sopra:
Tipici criteri di convergenza sono i seguenti:
L'algoritmo ha acquistato notorietà dato che converge molto velocemente. Infatti, si è osservato che generalmente il numero di iterazioni è minore del numero di punti. Comunque, l'algoritmo può essere molto lento nel caso peggiore: D. Arthur e S. Vassilvitskii hanno mostrato che esistono certi insiemi di punti per i quali l'algoritmo impiega un tempo superpolinomiale, formula_24, a convergere. Più recentemente, A. Vattani ha migliorato questo risultato mostrando che l'algoritmo può impiegare tempo esponenziale, formula_25, a convergere anche per certi insiemi di punti sul piano. D'altra parte, D. Arthur, B. Manthey e H. Roeglin hanno mostrato che la smoothed complexity dell'algoritmo è polinomiale, la qual cosa è a supporto del fatto che l'algoritmo è veloce in pratica.
In termini di qualità delle soluzioni, l'algoritmo non garantisce il raggiungimento dell'ottimo globale. La qualità della soluzione finale dipende largamente dal set di cluster iniziale e può, in pratica, ottenere una soluzione ben peggiore dell'ottimo globale. Dato che l'algoritmo è di solito estremamente veloce, è possibile applicarlo più volte e fra le soluzioni prodotte scegliere quella più soddisfacente.
Un altro svantaggio dell'algoritmo è che esso richiede di scegliere il numero di cluster(k) da trovare. Se i dati non sono naturalmente partizionati si ottengono risultati strani. Inoltre l'algoritmo funziona bene solo quando sono individuabili cluster sferici nei dati.
È possibile applicare l'algoritmo K-means in Matlab utilizzando la funzione kmeans(DATA, N_CLUSTER), che individua N_CLUSTER numeri di cluster nel data set DATA. Il seguente m-file mostra una possibile applicazione dell'algoritmo per la clusterizzazione di immagini basata sui colori.
"img_segm.m"
La funzione legge l'immagine utilizzando la funzione Matlab imread, che riceve in ingresso il nome del file contenente l'immagine e restituisce una matrice il cui elemento formula_26 contiene il codice di colore del pixel i,j. Successivamente costruisce la matrice delle osservazioni con due semplici cicli for. Viene infine passata in ingresso all'algoritmo di clustering la matrice delle osservazioni e, dopo aver generato le matrici utili per visualizzare i cluster prodotti in un'immagine, queste vengono mostrate a video con la funzione image.
Ad esempio, eseguendo il comando:
img_segm('kmeans0.jpg',2);
si ottiene il seguente risultato:
</text>
</doc>
<doc id="605046" url="https://it.wikipedia.org/wiki?curid=605046">
<title>Campionamento casuale</title>
<text>
In statistica il campionamento casuale corrisponde ad un'estrazione da una popolazione distribuita secondo la sua legge (funzione di densità) di un determinato numero di individui/oggetti.
La scelta del campione nel campionamento casuale è affidata al caso e non deve essere influenzata, più o meno consciamente, da chi compie l'indagine.
Le caratteristiche essenziali di un campionamento casuale semplice sono:
a) tutte le unità della popolazione hanno eguale probabilità di fare parte del campione;
b) ogni campione di ampiezza n ha la stessa probabilità di essere formato.
Un modo semplice per operare tale campionamento consiste nel numerare tutte le unità della popolazione, mettere in un'urna tante palline numerate, tutte uguali fra loro, quante sono le unità della popolazione e quindi sorteggiare da tale urna le palline per formare il campione.
Invece dell'urna si preferisce oggi ricorrere a una tavola di numeri casuali. Le tavole dei numeri casuali si costruivano, un tempo, con metodi empirici; attualmente si utilizzano gli elaboratori elettronici; per utilizzare le tavole dei numeri casuali, si parte da un punto qualunque, solitamente, estratto a sorte, e si procede in orizzontale, o in verticale, o in diagonale.
Il campionamento casuale può essere:
Nell'estrazione in blocco le n unità statistiche che compongono il campione vengono estratte contemporaneamente, e di conseguenza non si può distinguere l'ordine con cui gli n elementi si presentano. Quindi ad esempio in questo caso il campione "A B C D" è considerato uguale al campione "A C B D".
mentre il campionamento senza riposizione consiste nell'estrazione di un elemento alla volta senza il reinserimento nella popolazione dello stesso. In questo caso l'ordine con cui vengono scelti gli elementi conta. il numero dei possibili campioni è:
Ogni unità statistica estratta viene rimessa nella popolazione e quindi la stessa unità può essere
nuovamente estratta
Si potrebbe formare il campione anche estraendo successivamente le "n" unità senza reimmissione, ma tenendo conto dell'ordine in cui le singole unità sono estratte. In questo caso il numero dei campioni di "n" elementi che si possono estrarre dalla popolazione di N elementi è dato dalle disposizioni semplici D(N,n) degli N elementi di classe "n", ma questo procedimento è molto difficile da trovare.
Conoscendo la distribuzione della popolazione è possibile:
</text>
</doc>
<doc id="2901939" url="https://it.wikipedia.org/wiki?curid=2901939">
<title>Distanza di Minkowski</title>
<text>
In matematica, la distanza di Minkowski è una distanza in uno spazio euclideo che può essere considerata una generalizzazione sia della distanza euclidea sia della distanza di Manhattan.
La distanza di Minkowski di ordine formula_1 tra due punti formula_2 e formula_3 in formula_4 è definita come:
Questa distanza si usa tipicamente con formula_6 o formula_7: il primo la distanza di Manhattan e il secondo rappresenta la distanza euclidea.
Per formula_8 la distanza di Minkowski è una "metrica", nel senso che soddisfa la disuguaglianza triangolare come conseguenza della disuguaglianza di Minkowski. Quando formula_9, la distanza tra formula_10 e formula_11 è formula_12, ma il punto formula_13 è a distanza 1 da entrambi.
Nel caso limite in cui formula_1 tende a infinito si ha la distanza di Čebyšëv:
Per formula_1 che tende a formula_17, in modo simile si ha:
</text>
</doc>
<doc id="3645149" url="https://it.wikipedia.org/wiki?curid=3645149">
<title>Regole di associazione</title>
<text>
Nel data mining, le regole di associazione sono uno dei metodi per estrarre relazioni nascoste tra i dati.
Agrawal et al. introdussero le regole di associazione per la scoperta di regolarità all'interno delle transazioni registrate nelle vendite dei supermercati. Per esempio, la regola formula_1 individuata nell'analisi degli scontrini di un supermercato indica che il se il cliente compra insieme cipolle e patate è probabile che acquisti anche della carne per hamburger. Tale informazione può essere utilizzata come base per le decisioni riguardanti le attività di marketing, come ad esempio le offerte promozionali o il posizionamento dei prodotti negli scaffali.
Le regole di associazione sono anche usate in molte altre aree, quali il Web mining, la scoperta di anomalie e la bioinformatica.
Il concetto di regola di associazione divenne popolare a causa di un articolo del 1993 di Agrawal et al.. Secondo Google Scholar esso possiede più di 9500 citazioni (Settembre 2010) ed è uno degli articoli più citati nel campo del data mining. Tuttavia è possibile che quella che viene chiamata come "regola di associazione" sia simile a un approccio di data mining presentato nel 1966 e sviluppato da Hájek et al..
Seguendo la definizione originale di Agrawal et al. il problema della scoperta di regole di associazione è rappresentato come segue.
Consideriamo l'insieme di formula_2 attributi binari ("oggetti" o "item") formula_3 e l'insieme di transazioni ("database")formula_4. Ciascuna transazione appartenente a formula_5 possiede un codice identificativo (ID) e contiene un sottoinsieme degli oggetti contenuti in formula_6. Una "regola" è definita come un'implicazione nella forma formula_7 dove formula_8
e formula_9. L'insieme di oggetti (o "itemsets") formula_10 e formula_11 vengono chiamati rispettivamente "antecendente" e "conseguente" della regola.
Per illustrare questo concetto, è possibile usare un esempio giocattolo riguardante un supermercato.
L'insieme di oggetti è formula_12 e il database contenente gli oggetti è rappresentato nella tabella a destra, dove 1 indica la presenza di un oggetto in una transazione e 0 l'assenza. Un esempio di regola di associazione potrebbe essere: formula_13. Essa indica che se il cliente acquista pane e burro, comprerà anche il latte.
Attenzione: questo esempio è estremamente piccolo. In un'applicazione reale una regola necessita di un supporto di diverse centinaia di transazioni perché sia considerata statisticamente significativa e il database deve contenere migliaia (o milioni) di transazioni.
</text>
</doc>
<doc id="3612028" url="https://it.wikipedia.org/wiki?curid=3612028">
<title>Classificazione statistica</title>
<text>
La classificazione statistica è quell'attività che si serve di un algoritmo statistico al fine di individuare una rappresentazione di alcune caratteristiche di un'entità da classificare (oggetto o nozione), associandole una etichetta classificatoria. Tale attività può essere svolta mediante algoritmi di apprendimento automatico supervisionato o non supervisionato. Esempi di questi algoritmi sono:
I programmi che effettuano l'attività di classificazione sono detti classificatori. Talora si usa l'aggettivo "statistica" anche per classificazioni utilizzate per costruire indicazioni statistiche sulle entità assegnate ai diversi contenitori di una classificazione, soprattutto nel caso delle tassonomie, mentre nella definizione della classificazione non si sono utilizzati precisi metodi statistici.
</text>
</doc>
<doc id="3678589" url="https://it.wikipedia.org/wiki?curid=3678589">
<title>Distribuzione condizionata</title>
<text>
variabili aleatorie "X" e "Y", la distribuzione condizionata di Y dato X è la probabilità di Y quando è conosciuto il valore assunto da X. A ogni distribuzione condizionata è associato un valore atteso condizionato e una varianza condizionata.
Nel caso di variabili aletorie discrete, la distribuzione condizionata di "Y" dato "X=x", è data da:
È necessario quindi che "P(X=x)&gt;0".
Nel caso di variabili aleatorie continue, la densità condizionata di "Y" dato "X=x" è data da
Anche in questo caso, si deve avere che formula_3.
Se per due variabili aleatorie "X" e "Y" si ha che "P"("Y" = "y" | "X" = "x") = "P"("Y" = "y") per ogni "x" e "y" o, nel caso continuo, "f"("y" | "X=x") = "f"("y") per ogni "x" e "y", allora le due variabili sono dette indipendenti
</text>
</doc>
<doc id="3587354" url="https://it.wikipedia.org/wiki?curid=3587354">
<title>Distribuzione congiunta</title>
<text>
In probabilità, date due variabili aleatorie "X" e "Y", definite sullo stesso spazio di probabilità, si definisce la loro distribuzione congiunta come la distribuzione di probabilità associata al vettore formula_1. Nel caso di due sole variabili, si parla di distribuzione bivariata, mentre nel caso di più variabili si parla di distribuzione multivariata.
La funzione di ripartizione di una distribuzione congiunta è definita come
o più generalmente
Nel caso di variabili aleatorie discrete, la densità discreta congiunta (o funzione di massa di probabilità congiunta) è data da
Siccome la densità congiunta è anch'essa una densità, è soddisfatta la seguente equazione:
Nel caso di variabili aleatorie continue, la densità congiunta è data da
dove "f"("y"|"x") e "f"("x"|"y") sono le distribuzioni condizionate di Y dato X=x e di X dato Y=y, mentre "f"("x") e "f"("y") sono le distribuzioni marginali della densità congiunta, rispettivamente per X e Y.
Anche in questo caso, è soddisfatto
</text>
</doc>
<doc id="110194" url="https://it.wikipedia.org/wiki?curid=110194">
<title>Indice di rifrazione</title>
<text>
sica, l'indice di rifrazione di un materiale è una grandezza adimensionale che quantifica la diminuzione della velocità di propagazione della radiazione elettromagnetica quando attraversa un materiale. La diminuzione della velocità di propagazione viene accompagnata dalla variazione della sua direzione, secondo il fenomeno della rifrazione.
Si tratta di una grandezza utilizzata in svariati ambiti della scienza, e la sua misura può essere usata per identificare la natura del materiale in cui si propaga la radiazione. Ad esempio, in chimica vengono comunemente effettuate misure dell'indice di rifrazione con lo scopo di trarne indicazioni analitiche. In funzione dei parametri solvente, lunghezza d'onda incidente e temperatura, si effettua la misura del parametro utilizzando un rifrattometro. Questa metodica analitica viene utilizzata in vari campi: in campo medico per analisi del sangue e delle urine, in ambito industriale nell'analisi dei materiali, per determinare la concentrazione zuccherina in succhi di frutta o il grado alcolico di bevande, per certificare il livello qualitativo o evidenziare sofisticazioni di alimenti quali l'olio, il latte e il burro.
La radiazione viaggia alla massima velocità formula_1, detta velocità della luce, quando si trova nel vuoto. L'indice di rifrazione è il rapporto tra formula_1 e la velocità formula_3 della radiazione nel mezzo:
Dipende in generale dalla frequenza della radiazione, e pertanto si tratta di un numero complesso strettamente legato alla permittività elettrica.
Si consideri un'onda elettromagnetica monocromatica, che scritta in funzione del campo elettrico formula_5 ha la forma:
dove formula_7 è l'ampiezza e formula_8 è la frequenza angolare dell'onda. Il vettore d'onda è dato da formula_9, con formula_10 la direzione di propagazione e formula_11 il numero d'onda:
in cui il numero:
è la lunghezza d'onda della radiazione quando si propaga nel vuoto. La lunghezza d'onda nel materiale è data da:
e l'indice di rifrazione (in assenza di assorbimento) è:
dove formula_16 è la velocità di fase, cioè la velocità alla quale si propagano le creste dell'onda.
Le equazioni di Maxwell in un materiale possono essere scritte come:
assieme alle equazioni costitutive:
che descrivono la reazione nel mezzo alla presenza di un campo elettromagnetico.
Per risolvere queste equazioni è necessario formulare delle ipotesi (che rappresentano inevitabilmente un'approssimazione) sulla dipendenza di formula_20 ed formula_21 da formula_5 e da formula_23. Assumendo formula_24, un'approssimazione al primo ordine è che la polarizzazione del mezzo sia lineare con il campo elettrico:
dove formula_26 è la suscettività elettrica. Questa approssimazione è valida a meno che non si considerino campi estremamente intensi, come quelli che si possono ottenere con un laser: quando non è più valida si entra nel regime dell'ottica non lineare. Si assume inoltre che non ci siano cariche libere, ovvero che formula_27 e formula_28:
derivando rispetto al tempo la quarta equazione e facendo il rotore della seconda si ottiene:
Uguagliando allora il rotore della derivata nel tempo di formula_23 (dalla prima relazione) con la derivata nel tempo del rotore di formula_23 (primo termine della seconda relazione) si ha:
La prima equazione implica che la divergenza del campo elettrico è nulla. Dall'analisi differenziale è noto che per un generico vettore formula_36 si ha:
Da queste segue che il rotore del rotore del campo elettrico è pari all'opposto del laplaciano del campo stesso:
Ricordandosi che la velocità della luce può essere scritta come:
questa diventa:
ossia l'equazione di un'onda che si propaga, non a velocità formula_1 ma ad una velocità di fase inferiore pari a:
Il fattore formula_43 è l'indice di rifrazione, e può essere riscritto in funzione della costante dielettrica e della permeabilità magnetica del mezzo come:
Nel caso in cui sia formula_45 che formula_46 siano negativi la soluzione corretta delle equazioni di Maxwell impone che si debba scegliere come indice di rifrazione la radice negativa, e quindi formula_47. Questa condizione non viene mai raggiunta nei materiali reali ma è stata dimostrata la possibilità di usare dei metamateriali per ottenerla.
La legge di Snell descrive quanto la direzione di propagazione della luce è deviata nel passare da un mezzo ad un altro. Essa afferma che se il raggio proviene da una regione con indice di rifrazione formula_48 ed entra in un mezzo ad indice formula_49, gli angoli di incidenza formula_50 e di rifrazione formula_51 sono legati dall'espressione:
dove formula_53 e formula_54 sono le velocità nei mezzi, e la velocità della radiazione deve cambiare da formula_55 a formula_56. Se non vi è nessun angolo formula_51 che soddisfa la relazione, ovvero:
la luce non viene trasmessa nel secondo mezzo e si verifica il fenomeno di riflessione interna totale.
A partire dalle equazioni di Maxwell è possibile dimostrare, sfruttando il fatto che il campo elettrostatico è conservativo, che passando da un mezzo ad un altro la componente del campo elettrico tangente all'interfaccia è continua. Questo si relaziona al fatto che, dal momento che l'intensità del vettore d'onda formula_59 è proporzionale all'energia del fotone incidente, la sua componente trasversale si deve conservare. Dato che la componente trasversale del vettore d'onda resta uguale, si ha che formula_60 e quindi:
da cui formula_62.
Quando un'onda elettromagnetica incide sul materiale, parte di essa viene riflessa. La quantità di luce che viene riflessa dipende dalla riflettanza della superficie. Tale grandezza può essere calcolata a partire dall'indice di rifrazione e dall'angolo di incidenza per mezzo dell'equazione di Fresnel, secondo la quale la componente normale della riflessione viene ridotta di:
Per il vetro immerso in aria formula_64 e formula_65, che significa che il 4% della potenza viene riflessa.
Vi è un angolo formula_66, detto angolo di Brewster, per cui la radiazione polarizzata lungo il piano di incidenza viene totalmente trasmessa:
e non vi è dunque riflessione.
In tutti i sistemi reali l'indice di rifrazione varia con la frequenza dell'onda incidente, e per la legge di Snell a frequenze diverse corrispondono angoli di rifrazione diversi. Un esempio ben noto di questo fenomeno è il fatto che la luce bianca (contenente tutte le componenti spettrali) viene scomposta da un prisma.
Quando un materiale presenta assorbimento non è più possibile descrivere l'indice di rifrazione tramite un numero reale ma bisogna definire un indice di rifrazione complesso:
dove formula_69 definisce la velocità di fase con cui si propaga l'onda e formula_70 è proporzionale al coefficiente di assorbimento del sistema. L'assorbimento dell'energia della radiazione da parte del materiale è strettamente legato al fenomeno della dispersione, e le quantità formula_69 e formula_70 sono legate dalla relazione di Kramers-Kronig.
Per mostrare che formula_70 quantifica l'assorbimento dell'energia del campo è sufficiente inserire formula_69 nell'espressione del campo elettrico in un'onda piana che si propaga in direzione "z":
Considerando il vettore d'onda come numero complesso formula_76, la cui parte reale è formula_77, si ha:
Si nota che formula_70 fornisce un decadimento esponenziale, come previsto dalla Legge di Lambert-Beer. Dato che l'intensità dell'onda è proporzionale al quadrato dell'intensità del campo elettrico, il coefficiente di assorbimento diventa formula_80.
In alcune condizioni particolari (ad esempio vicino a delle risonanze dell'assorbimento) è possibile che formula_81 sia minore di 1. In questi casi la velocità di fase può essere superiore alla velocità della luce. Questo però non viola la relatività ristretta perché la velocità del segnale è la velocità di gruppo la quale rimane sempre inferiore a "c".
L'assorbimento di un materiale è la sua capacità di assorbire l'energia associata alla radiazione elettromagnetica che si propaga all'interno di esso. Si tratta dell'energia dei fotoni che viene ceduta agli elettroni, atomi e molecole del materiale: l'energia del campo elettromagnetico si trasforma in questo modo in energia interna del materiale, come ad esempio la sua energia termica. Solitamente l'intensità dell'onda elettromagnetica non influisce sull'assorbimento (in caso contrario si parla di "assorbimento non lineare"), e la sua riduzione è anche detta attenuazione.
L'assorbimento dipende sia dalla natura del materiale, sia dalla frequenza della radiazione, e può essere quantificato attraverso la permittività elettrica: si tratta di una funzione complessa della frequenza dell'onda, attraverso la quale è possibile trattare la propagazione del campo elettromagnetico in mezzi dissipativi. Normalmente il valore della permittività elettrica viene scritto come il prodotto formula_82 della permittività relativa formula_83 e della permettività del vuoto formula_84, detta anche costante dielettrica del vuoto. Poiché varia a seconda della direzione del campo elettrico rispetto al mezzo, essa è rappresentata attraverso un tensore, e solo nel caso di un dielettrico perfetto tutte le componenti del tensore hanno lo stesso valore, chiamato impropriamente "costante dielettrica".
La permittività e l'indice di rifrazione sono legati dalla relazione:
dove formula_86 è la permeabilità magnetica relativa e formula_83 la permittività elettrica relativa, un numero complesso:
In un mezzo con formula_89 (approssimazione valida per la maggior parte dei materiali), la permittività elettrica è dunque il quadrato dell'indice di rifrazione complesso. Valgono le seguenti relazioni:
Quando si analizza la permittività dal punto di vista della frequenza del campo si nota che essa può presentare un comportamento anomalo in corrispondenza di certe lunghezze d'onda. Infatti, la parte immaginaria della permittività elettrica segue un andamento risonante in corrispondenza dei suoi poli, dove presenta uno o più picchi. In corrispondenza di questi picchi l'assorbimento da parte del materiale dell'energia posseduta dal campo è massimo.
Nei materiali anisotropi la polarizzazione formula_92 non dipende solo dall'intensità del campo elettrico ma anche dalla sua polarizzazione, ovvero la costante dielettrica non è uguale sui tre assi del sistema di riferimento. Di conseguenza la costante dielettrica non può più essere descritta tramite uno scalare ma deve essere rappresentato tramite una matrice (o, più formalmente, tramite un tensore). In questo caso si ha il fenomeno detto di birifrangenza dove fasci di luce con polarizzazione diversa ed incidenti ad angoli diversi "vedono" un indice di rifrazione diverso e quindi vengono rifratti in direzioni diverse. Storicamente questo fenomeno è stato osservato per la prima volta nella calcite.
La birifrangenza è molto sfruttata sia nell'ottica non lineare che per la realizzazione di dispositivi elettro-ottici, per esempio lamine che funzionano da ritardatore di fase (lamine a mezz'onda o quarto d'onda), o dispositivi per la generazione di seconda armonica in un laser.
</text>
</doc>
<doc id="210081" url="https://it.wikipedia.org/wiki?curid=210081">
<title>Forza elettromotrice</title>
<text>
La forza elettromotrice, comunemente abbreviata in f.e.m., è il rapporto tra il lavoro compiuto da un generatore elettrico per muovere le cariche (convenzionalmente positive) dal polo a basso potenziale al polo a potenziale più alto e l’unità di carica spostata. Sebbene la forza elettromotrice non sia conservativa, essa è numericamente uguale alla differenza di potenziale massima ai capi di un generatore elettrico sconnesso dal circuito elettrico.
La f.e.m. si differenzia da una differenza di potenziale, in quanto è sempre maggiore della differenza di potenziale utile presente quando il generatore viene connesso al circuito elettrico, dal momento che la resistenza interna del generatore riduce questa tensione.
L'utilizzo della parola "forza" ha un significato differente da quello oggi generalmente accettato, cioè di forza in senso meccanico; essa, tuttavia, trova ancora applicazione, ad esempio, per esprimere la massima differenza di potenziale che un generatore di tensione produce fra i suoi poli o la differenza di potenziale fra gli elettrodi di una cella elettrochimica. In particolare, nel caso di una cella galvanica, la forza elettromotrice corrisponde alla differenza di potenziale che si instaura in corrispondenza dei morsetti della cella a circuito aperto (cioè in assenza di circolazione di corrente, ovvero all'equilibrio).
Nel 1800, dopo un disaccordo professionale sulla risposta galvanica sostenuta da Luigi Galvani, Alessandro Volta sviluppò la nota pila voltaica, precursore della batteria, che produce una tensione costante ai suoi capi. Nei suoi studi Volta accanto ai concetti di "capacità" e di "quantità" usò per la prima volta il concetto di "tensione elettrica" per rendere conto delle proprietà intensive dell'elettricità.
All'interno di un generatore elettrico si verificano processi che trasportano le cariche positive verso il polo positivo e le cariche negative verso quello negativo. Questi processi si oppongono alla repulsione fra cariche elettriche dello stesso segno.
Essi possono essere di natura elettrochimica, elettromagnetica, termoelettrica, fotoelettrica, piezoelettrica e così via.
Il lavoro "L" necessario al trasporto delle cariche verso i rispettivi poli è direttamente proporzionale alla quantità di carica "q"; la forza elettromotrice "E" è definita come quantità di lavoro compiuto per unità di carica, secondo la formula:
L'unità di misura SI della forza elettromotrice è il volt, la stessa che si impiega per misurare il potenziale e la tensione; l'unità di misura CGS è lo statvolt. Nelle formule, la forza elettromotrice viene indicata talora con le lettere "f", "e", "E" o "V".
In un circuito chiuso, la differenza di potenziale Δ"V" misurata ai poli di un generatore reale risulta sempre leggermente inferiore alla forza elettromotrice del generatore per effetto della resistenza interna "r" dello stesso:
La forza elettromotrice indotta in un circuito chiuso è uguale all'opposto della variazione del flusso magnetico Φ che lo attraversa in una unità di tempo, come stabilisce la Legge di Faraday-Neumann-Lenz nella notazione di Newton:
E per la definizione di induttanza elettrica formula_4,
Il segno - è dovuto al fatto che la forza elettromotrice indotta si oppone alla variazione del flusso magnetico che l’ha generata in virtù della legge di Lenz.
Nel caso in cui la variazione del flusso sia dovuta a una modifica meccanica del sistema, come ad esempio la riduzione dell'area di una spira, si parla di forza elettromotrice cinetica.
</text>
</doc>
<doc id="200509" url="https://it.wikipedia.org/wiki?curid=200509">
<title>Rifrazione</title>
<text>
La rifrazione è la deviazione subita da un'onda che ha luogo quando questa passa da un mezzo a un altro otticamente differenti nel quale la sua velocità di propagazione cambia. La rifrazione della luce è l'esempio più comunemente osservato, ma ogni tipo di onda può essere rifratta, per esempio quando le onde sonore passano da un mezzo a un altro o quando le onde dell'acqua si spostano a zone con diversa profondità.
In ottica, la rifrazione avviene quando una onda luminosa passa da un mezzo a un altro avente un indice di rifrazione diverso. Sul bordo dei due mezzi, la velocità di fase dell'onda è modificata, cambia direzione e la sua lunghezza d'onda è aumentata o diminuita. Per esempio, i raggi di luce si rifrangono quando entrano o escono dal vetro; la comprensione di questo concetto ha consentito l'invenzione delle lenti e del telescopio a rifrazione.
La rifrazione può essere osservata guardando all'interno di un bicchiere pieno d'acqua. L'aria ha un indice di rifrazione di circa 1,0003, mentre l'acqua ha un indice di circa 1,33. Se si guarda un oggetto dritto, come una penna parzialmente immersa e inclinata, l'oggetto appare piegato dalla superficie dell'acqua. 
L'estremità x della penna, interessata dall'energia radiante, si comporta come sorgente secondaria di radiazione ed emette raggi di luce in tutte le possibili direzioni dello spazio. Consideriamo adesso il percorso di due di questi raggi, quelli evidenziati in rosso in figura. Tali raggi, in corrispondenza alla superficie di discontinuità fra l'acqua e l'aria si piegano, allontanandosi dalla normale alla superficie condotta nel punto in cui i raggi stessi incidono. I raggi rifratti vengono catturati dall'occhio umano e l'intersezione dei loro prolungamenti determina un punto virtuale, indicato con y in figura, dal quale l'occhio umano ha la sensazione che provenga l'immagine. In altre parole, l'occhio dell'osservatore non vede l'estremità x della penna, ma una sua immagine virtuale, ottenuta dal prolungamento dei due raggi rifratti.
La rifrazione è responsabile degli arcobaleni e della scomposizione della luce bianca nei colori dell'arcobaleno che avviene quando la luce passa attraverso un prisma. Il vetro ha un alto indice di rifrazione rispetto all'aria e le diverse frequenze della luce viaggiano a velocità diverse (dispersione), causando la rifrazione dei colori a diversi angoli, e quindi la scomposizione. La differenza nella frequenza corrisponde nella diversità della tonalità.
Altri fenomeni ottici sono il miraggio e la fata Morgana. Questi sono causati dal cambiamento dell'indice di rifrazione in funzione della temperatura dell'aria.
Recentemente sono stati sviluppati alcuni metamateriali che hanno un "indice di rifrazione negativo".
La legge di Snell descrive quanto i raggi sono deviati quando passano da un mezzo a un altro. Se il raggio proviene da una regione con indice di rifrazione formula_1 ed entra in un mezzo con indice formula_2 gli angoli di incidenza formula_3 e di rifrazione formula_4 sono legati dall'espressione:
dove formula_6 e formula_7 sono le velocità nei mezzi.
Nel 1880 H.A. Lorentz e L.V. Lorenz introdussero il concetto di rifrazione molare relativamente a una mole di sostanza definendolo in base all'equazione
dove "R" è la rifrazione molare, "n" l'indice di rifrazione della sostanza e "V" il volume molare dato dal rapporto tra il peso molecolare e la densità del materiale in esame.
La rifrazione molare è una grandezza additiva ed è quindi ricavabile anche sommando i valori tabulati in letteratura relativi a singoli atomi e legami chimici che caratterizzano il composto in esame, il tutto in buon accordo coi dati sperimentali.
Lorentz e Lorenz correlarono anche il volume di polarizzabilità con la rifrazione molare a lunghezza d'onda infinita (frequenza della radiazione utilizzata eguale a zero) tramite l'equazione
dove "R" è la rifrazione molare a lunghezza d'onda infinita, "N" la costante di Avogadro e α' il volume di polarizzabilità.
</text>
</doc>
<doc id="109621" url="https://it.wikipedia.org/wiki?curid=109621">
<title>Livello energetico</title>
<text>
Mentre nella meccanica classica l'energia è un continuo, la meccanica quantistica prevede la possibilità che ci siano solo certi valori (livelli) dell'energia accessibili al sistema.
Sin dalle origini della chimica come scienza era stato osservato che l'assorbimento della luce da parte dei gas non varia in maniera morbida con la lunghezza d'onda ma è caratterizzato dalla presenza di un gran numero di picchi estremamente sottili e disposti in maniera complicata.
Una prima trattazione fenomenologica del problema venne fatta nel 1885 da Johann Balmer che notò come la disposizione di un gruppo di righe di assorbimento dell'atomo di idrogeno venisse ben descritta dalla formula:
Questa formula venne successivamente generalizzata da Johannes Rydberg e Walther Ritz in modo da poter descrivere la posizione di tutte le righe di assorbimento ma questa rimaneva una descrizione fenomenologica, per quanto accurata, senza un vero fondamento scientifico.
Il tentativo di spiegare la natura dei picchi di assorbimento fu alla base del Modello atomico di Bohr: già Rutherford aveva proposto l'idea che l'atomo fosse composto da un piccolo nucleo carico positivamente attorno al quale ruotavano in orbite circolari gli elettroni. Tuttavia un sistema del genere non è stabile perché un elettrone accelerato emette radiazione e quindi perde energia. Per risolvere il problema Bohr, seguendo le idee proposte da Planck e Einstein, ipotizzò che gli elettroni potessero trovarsi solo su determinati "livelli energetici" e potessero muoversi dall'uno all'altro solo per passi discreti. Questo semplice (ma rivoluzionario) modello permette di descrivere molto bene le righe di assorbimento di molti gas e fu una delle basi per lo sviluppo della meccanica quantistica.
In meccanica quantistica il sistema più semplice nel quale sia possibile ottenere una "quantizzazione dell'energia" (ovvero il fatto che solo certi livelli discreti siano accessibili) è la cosiddetta buca di potenziale infinita. Se prendiamo una particella di massa formula_2 in una dimensione e consideriamo un potenziale della forma:
e quindi l'equazione di Schrödinger stazionaria sarà data dall'equazione differenziale
formula_4
Per formula_5 il potenziale ha un valore infinito e quindi l'unica soluzione del problema è formula_6. All'interno della buca invece il potenziale è nullo e quindi la particella è libera e sarà descritta dalla funzione d'onda
Per individuare i valori delle costanti di scala A e B dobbiamo imporre la continuità della funzione d'onda. Facendolo si osserva che questa condizione può essere soddisfatta solamente per un insieme discreto di valori dell'energia "E" dati da
dove "n" è un numero intero.
Un esempio molto chiaro è quello dell'oscillatore armonico in una dimensione. Nel caso della meccanica classica un oscillatore armonico di costante elastica "k" ha un'energia
dove L è l'ampiezza dell'oscillazione. Come si vede sono accettabili tutte le energie da zero (dove L=0 ovvero non c'è alcuna oscillazione) a quella associata alla massima elongazione che il sistema può sostenere senza danneggiarsi e queste energie formano un continuo.
Nel caso quantistico invece si ottiene che è possibile imporre le condizioni al contorno solo quando l'energia assume i valori
dove formula_11 è la frequenza dell'oscillazione, formula_12 è pari alla costante di Planck divisa per formula_13 e formula_14 è un numero intero.
In questo caso sono accettabili solo alcuni valori dell'energia (infiniti ma numerabili con l'indice "n"), equispaziati tra loro (motivo per cui l'oscillatore, in analogia al caso delle onde acustiche, è chiamato "armonico"). La ragione per la quale nella vita di tutti i giorni non ci accorgiamo di questa "quantizzazione" dei livelli dell'energia è che formula_12 è piccolissima (circa formula_16 J formula_17 s) e quindi la separazione fra i vari stati non è apprezzabile.
Il modello della particella su una sfera è utilizzato per descrivere il moto rotazionale di una particella nello spazio tridimensionale. Come noto dalla meccanica classica, l'energia di una particella che ruota su un piano xy lungo un asse z è data dalla relazione:
dove "J" è la componente del momento angolare lungo l'asse z e "I" è il momento di inerzia. In base alla relazione di de Broglie si ricava
Le condizioni al contorno ciclico impongono che la funzione d'onda assuma lo stesso valore lungo un percorso passante per i poli così come lungo l'equatore. Ciò equivale alla quantizzazione dell'energia. Risolvendo l'equazione di Schrödinger, dopo avere separato le variabili ed espresso la funzione d'onda come prodotto "Θ(θ)Φ(φ)", si ottiene l'energia che in termini quantomeccanici assume i valori
dove "l" è il numero quantico azimutale. Esistendo per ogni valore del numero quantico magnetico "2l+1" differenti funzioni d'onda caratterizzate dalla medesima energia, segue che un livello con numero quantico "l" è "2l+1" volte degenere. Da notare come invece nel caso dell'espressione dell'energia per la rotazione in due dimensioni (particella su un anello) nella formulazione quantomeccanica compaia il numero quantico magnetico:
In questo caso la condizione al contorno ciclico è solo una e impone che la funzione d'onda torni ad assumere il medesimo valore iniziando un nuovo "giro".
</text>
</doc>
<doc id="117978" url="https://it.wikipedia.org/wiki?curid=117978">
<title>Magnete</title>
<text>
Un magnete (o calamita) è un corpo che genera un campo magnetico. Il nome deriva dal 
"μαγνήτης λίθος" ("magnétes líthos"), cioè "pietra di Magnesia", dal nome di una località dell'Asia Minore, nota sin dall'antichità per gli ingenti depositi di magnetite. Un campo magnetico è invisibile all'occhio umano, ma i suoi effetti sono ben noti: sposta materiali ferromagnetici come il ferro e fa attrarre o respingere due magneti.
Un magnete permanente è formato da un materiale ferromagnetico (soltanto alcuni) che è stato magnetizzato e crea un proprio campo magnetico. I materiali che possono essere magnetizzati sono anche quelli fortemente attratti da una calamita, e sono chiamati ferromagnetici (o ferrimagnetici); questi includono ferro, nichel, cobalto, alcune leghe di terre rare e alcuni minerali naturali come la magnetite. Anche se i materiali ferromagnetici (e ferrimagnetici) sono gli unici attratti da una calamita così intensamente da essere comunemente considerati "magnetici", tutte le sostanze rispondono debolmente ad un campo magnetico, attraverso uno dei 
numerosi tipi di magnetismo.
I materiali ferromagnetici possono essere suddivisi in materiali magneticamente ""morbidi"" (come ad esempio il ferro ricotto), che possono essere magnetizzati ma che tendono a non rimanere in tale stato, e materiali magneticamente ""duri"", che invece rimangono magnetici. I magneti permanenti sono costituiti da materiali ferromagnetici "duri" sottoposti durante la loro produzione ad un trattamento speciale in un potente campo magnetico, che allinea la loro struttura microcristallina interna e li rende molto difficili da smagnetizzare. Per smagnetizzare un magnete di questo tipo, infatti, deve essere applicato un certo campo magnetico la cui intensità dipende dalla coercitività del materiale corrispondente; i materiali "duri" hanno alta coercitività, mentre quelli "morbidi" hanno bassa coercitività.
Un elettromagnete è costituito da una bobina di filo conduttore che agisce come un magnete quando una corrente elettrica passa attraverso di essa, ma che smette di essere una calamita quando la corrente si ferma. Spesso un elettromagnete è avvolto attorno ad un nucleo di materiale ferromagnetico (per esempio l'acciaio) per aumentare il campo magnetico prodotto dalla bobina.
La forza complessiva di un magnete è misurata dal suo momento magnetico, o in alternativa dal flusso magnetico totale che produce. La forza locale del magnetismo in un materiale viene misurata dalla sua magnetizzazione.
Il campo magnetico (solitamente indicato con la lettera B) è un campo vettoriale caratterizzato da una "direzione", ricavabile tramite l'utilizzo di una semplice bussola, da un verso e da un"'intensità".
L'unità di misura SI del campo magnetico è il Tesla, mentre l'unità di misura del flusso magnetico totale è il weber; 1 Tesla è pari a 1 Weber per metro quadro (un valore molto elevato del flusso magnetico).
Il momento magnetico (chiamato anche momento di dipolo magnetico e indicato dalla lettera greca μ) è un vettore che caratterizza le proprietà magnetiche di un corpo: in una barra magnetica, per esempio, il verso del momento magnetico è diretto dal polo sud al polo nord della barra e la sua intensità dipende dalla forza dei poli e dalla loro distanza.
Un magnete produce un campo magnetico ed è a sua volta influenzato dai campi magnetici. L'intensità del campo magnetico prodotto è proporzionale al momento magnetico, e anche il momento meccanico di cui il magnete risente, una volta posto in un campo magnetico esterno, è proporzionale ad esso (oltre che all'intensità e alla direzione del campo esterno).
In unità del Sistema Internazionale, il momento magnetico è misurato in A·m (Ampere per metro quadrato): ad esempio, una spira con sezione circolare pari ad "S" percorsa da una corrente elettrica di intensità "I" è un magnete con un momento di dipolo magnetico di intensità "I S"
La magnetizzazione di un corpo è il valore del suo momento magnetico per unità di volume, solitamente indicato con M e misurato in A/m. È un campo vettoriale (come il campo magnetico e a differenza del momento magnetico), poiché il suo valore varia al variare delle diverse sezioni del corpo. Una buona barra magnetica solitamente possiede un momento magnetico di circa 0.1 A·m² e quindi, supponendo un volume di 1 cm³ (ovvero 0,000001 m³), una magnetizzazione di A/m. Il ferro può raggiungere anche il milione di A/m di magnetizzazione.
Tutti i magneti hanno almeno due poli: possiedono cioè almeno un polo "nord" e un polo "sud"; il "polo" non è un'entità materiale, bensì un concetto utilizzato nella descrizione dei magneti.
Per comprenderne il significato, si può fare un esempio immaginando una fila di persone allineate e rivolte verso la medesima direzione: benché abbia un lato "frontale" e uno "posteriore", non c'è un luogo particolare della fila in cui si trovano solo i "lati frontali" delle persone o i loro "lati posteriori"; una persona ha di fronte a sé la schiena della persona davanti e dietro di sé un'altra persona rivolta in avanti. Se si divide la fila in due file più piccole, esse continueranno ad avere comunque un orientamento. Continuando a dividere le file, anche arrivando al singolo individuo si manifesta ancora lo stesso orientamento fronte/retro.
Lo stesso accade con i magneti: non c'è un'area all'interno del magnete in cui si trovano solo i poli nord o solo i poli sud, anche dividendo in due parti il magnete, entrambi i magneti risultanti avranno un polo nord e un polo sud. Anche questi magneti più piccoli possono essere suddivisi ulteriormente, ottenendo ancora dei magneti con un polo nord e un polo sud. Se si continua a dividere il magnete in parti sempre più piccole, ad un certo punto queste parti saranno troppo piccole anche per mantenere un campo magnetico (ciò non significa che sono diventati singoli poli, ma semplicemente che hanno perso la capacità di generare del magnetismo). Per alcuni materiali, si può arrivare al livello molecolare e osservare ancora un campo magnetico, con poli nord e sud (sono i "magneti molecolari"). Alcune teorie fisiche tuttavia prevedono l'esistenza di un monopolo magnetico nord e sud.
In termini del campo di induzione magnetica B, in un magnete permanente si ha che le linee di forza entrano dal polo sud ed escono dal polo nord. Allo stesso modo, in un solenoide percorso da corrente continua si possono identificare un polo nord e un polo sud.
Storicamente, i termini "polo nord" e "polo sud" di un magnete rispecchiano la consapevolezza delle interazioni tra esso e il campo geomagnetico: un magnete liberamente sospeso in aria si orienterà lungo la direzione nord-sud a causa dell'attrazione dei poli magnetici nord e sud della Terra; l'estremità del magnete che punta verso il "polo nord geografico" della Terra viene chiamato polo nord del magnete, mentre ovviamente l'altra estremità sarà il polo sud del magnete.
L'odierno "polo nord geografico" della Terra non corrisponde però al suo "polo sud magnetico"; complicando ulteriormente lo scenario, si è scoperto che le rocce magnetizzate presenti nei fondali oceanici mostrano come il campo geomagnetico abbia invertito la propria polarità più volte nel passato. Fortunatamente, utilizzando un elettromagnete e la regola della mano destra, l'orientamento di un qualsiasi campo magnetico può essere definito senza doversi riferire al campo geomagnetico.
Per evitare ulteriori confusioni tra poli geografici e magnetici, questi ultimi vengono spesso indicati come "positivo" e "negativo" (dove il polo positivo è quello corrispondente al polo nord geografico).
Il termine "magnete" è in genere riservato a quegli oggetti che producono un proprio campo magnetico persistente anche in assenza di un campo magnetico esterno applicato. Solo alcune classi di materiali possono fare ciò, mentre la maggior parte produce un campo magnetico solo in risposta ad un campo magnetico esterno; ci sono dunque diversi tipi di magnetismo, e tutti i materiali ne presentano una qualche forma. Il comportamento magnetico complessivo di un materiale può variare notevolmente a seconda della sua struttura, in particolare della sua configurazione elettronica. Sono state osservate diverse forme di comportamento magnetico nei diversi materiali:
Qualsiasi oggetto comune è composto da particelle come i protoni, i neutroni e gli elettroni; ciascuna di esse ha tra le sue proprietà quanto-meccaniche lo spin, che associa a queste particelle un campo magnetico. Da questo punto di vista, ci si aspetta che qualsiasi corpo materiale, essendo composto da innumerevoli particelle, possieda caratteri magnetici (persino le particelle di antimateria hanno proprietà magnetiche); l'esperienza quotidiana, tuttavia, smentisce questa affermazione.
All'interno di ogni atomo o molecola, le disposizioni di ogni spin seguono rigidamente il Principio di esclusione di Pauli; comunque sia, nelle sostanze diamagnetiche non esiste un ordinamento "a lungo raggio" di questi spin, per cui non esiste un campo magnetico, dato che ogni momento magnetico di una particella è annullato da quello di un'altra.
Nei magneti permanenti, invece, questo ordinamento a lungo raggio esiste; il grado più elevato di ordinamento è quello presente nei cosiddetti domini magnetici: essi possono essere considerati come microscopiche regioni dove una forte interazione tra particelle, detta interazione di scambio, genera una situazione estremamente ordinata; più elevato è il grado di ordine del dominio, più forte risulterà il campo magnetico generato.
Un ordinamento a scale elevate (e quindi un forte campo magnetico) è una delle caratteristiche principali dei materiali ferromagnetici.
Uno stratagemma che si sfrutta per generare campi magnetici molto intensi è quello di orientare tutti i domini magnetici di un ferromagnete con un campo meno intenso, generato da un avvolgimento di materiale conduttore all'interno del quale è fatta passare una corrente elettrica: è l'elettromagnete.
Gli elettroni giocano un ruolo primario nella formazione del campo magnetico; in un atomo, gli elettroni si possono trovare sia singolarmente sia a coppie, all'interno di ciascun orbitale. Se sono in coppia, ciascun elettrone ha spin opposto rispetto all'altro (spin su e spin giù); dal momento che gli spin hanno direzione opposta, essi si annullano a vicenda: una coppia di elettroni non può dunque generare un campo magnetico.
In molti atomi, però, si trovano elettroni spaiati: tutti i materiali magnetici possiedono elettroni di questo tipo, ma non è detto che al contrario un atomo con elettroni spaiati sia ferromagnetico. Per poter essere ferromagnetico, gli elettroni spaiati del materiale devono anche interagire fra di loro a larghe scale, in modo da essere tutti orientati nella medesima direzione. La specifica configurazione elettronica degli atomi, così come la distanza tra ciascun atomo, è il principale fattore che guida questo ordinamento a lungo raggio. Se gli elettroni mostrano lo stesso orientamento, essi si trovano nello stato a minore energia.
L'esempio più semplice di elettromagnete è quello di un filo arrotolato a mo' di bobina una o più volte: questa configurazione prende il nome, rispettivamente, di spira o solenoide. Quando la corrente elettrica attraversa la bobina, quest'ultima genera un campo magnetico attorno a sé. L'orientamento del campo magnetico può essere determinato attraverso la regola della mano destra, mentre la sua intensità dipende da vari fattori: dal numero di spire si ricava la superficie dell'interazione, dalla densità di corrente elettrica l'attività; più spire sono presenti (o più grande è la densità di corrente), più risulterà elevato il campo magnetico.
Se la bobina è vuota al suo interno, il campo generato sarà estremamente debole; vari materiali ferromagnetici o paramagnetici possono essere utilizzati per costituire il nucleo di un elettromagnete: l'aggiunta di queste componenti può far aumentare l'intensità del campo magnetico di 100 o addirittura 1000 volte.
A distanze considerevoli rispetto alle dimensioni del magnete, il campo magnetico osservato segue la legge dell'inverso del cubo: l'intensità del campo è inversamente proporzionale al cubo della distanza.
Se l'elettromagnete poggia su una lastra metallica, la forza necessaria a separare i due oggetti sarà tanto più grande quanto più le due superfici saranno piatte e lisce: in questo caso infatti avranno un maggior numero di punti di contatto e più piccola sarà la riluttanza del circuito magnetico.
Gli elettromagneti trovano applicazioni in diverse situazioni, dagli acceleratori di particelle, ai motori elettrici, alle macchine per l'imaging a risonanza magnetica. Vi sono anche macchinari più complessi dove non si utilizzano semplici dipoli magnetici, bensì quadrupoli magnetici, con lo scopo, per esempio, di concentrare i fasci di particelle. Un esempio è costituito dallo spettrometro di massa.
Recentemente campi di svariati milioni di tesla sono stati prodotti in solenoidi micrometrici nei quali veniva fatta passare una corrente di milioni di ampere, mediante scarica impulsiva di una batteria di condensatori. Le intense forze generate dalla scarica portavano il sistema ad implodere, distruggendo l'esperimento in pochi millisecondi.
I magneti trovano applicazione in una vasta gamma di strumenti, tra i quali:
I materiali ferromagnetici possono essere "magnetizzati" in diversi modi:
Possono essere invece "smagnetizzati" con i seguenti procedimenti:
In un elettromagnete contenente un nucleo di ferro, interrompere il flusso di corrente significa eliminare la maggior parte del campo magnetico (permangono dei deboli effetti magnetici dovuti al fenomeno dell'isteresi).
Molti materiali hanno coppie di elettroni con spin spaiati, e la maggior parte di essi è paramagnetica. Se i due elettroni interagiscono fra loro in modo tale che i loro spin si allineano spontaneamente, tali materiali divengono ferromagnetici (o semplicemente ""magnetici""). A seconda della struttura atomica dei cristalli da cui sono formati, molti metalli sono già ferromagnetici quando sono ancora minerali, per esempio minerali del ferro (la magnetite), del cobalto, del nickel o anche di terre rare come il gadolinio o il disprosio. Questi magneti "naturali" sono stati ovviamente i primi ad essere utilizzati per le loro proprietà magnetiche, seguiti da altri di fabbricazione artificiale, come ad esempio il boro, un materiale molto magnetico utilizzato per i flap degli aerei, permettendo un volo comodo e agevole.
Gli elementi chimici chiamati "terre rare" (ovvero i lantanidi) hanno il livello elettronico "f" (che può ospitare fino a 14 elettroni) riempito solo in parte. Lo spin degli elettroni di questo livello si può facilmente allineare in presenza di forti campi magnetici, e perciò è proprio in queste situazioni che vengono utilizzati i magneti costituiti da terre rare. Le varietà più comuni di questi magneti sono i magneti samario-cobalto e i magneti neodimio-ferro-boro.
Negli anni novanta si scoprì come certi tipi di molecole contenenti ioni metallici paramagnetici fossero capaci di conservare il proprio momento magnetico anche a temperature estremamente basse. Tale meccanismo è differente da quello utilizzato dai magneti convenzionali e teoricamente risulta anche più efficiente. Le ricerche che interessano questi magneti molecolari, o SMM (""single-molecule magnet"") sono tuttora in corso.
Molti SMM contengono manganese, mentre in altri si trovano anche il vanadio, il ferro, il nickel e il cobalto.
Il primo materiale organico, magnetico a temperatura ambiente, è stato ottenuto in diclorometano, dalla reazione del dibenzene-vanadio con tetracianoetilene (TCNE) e si presenta come un materiale nero, amorfo, di composizione V(TCNE) ½CHCl, magnetico sino alla temperatura di decomposizione a 77 °C. Così ottenuto risulta poco stabile, mentre ottenuto dalla fase gassosa di TCNE e V(CO) (Vanadio esacarbonile) e condensato direttamente su supporti, rigidi o flessibili, forma film magnetici abbastanza stabili all'aria.
Calcolare la forza di attrazione o repulsione tra due magneti è, in generale, un'operazione estremamente complessa, che dipende dalla forma, dal grado di magnetizzazione, dall'orientamento e dalla distanza dei due magneti.
La forza esistente tra due monopoli magnetici è espressa dalla seguente formula:
dove
Questa equazione non descrive una situazione finora osservabile; è tuttavia l'esempio più semplice di calcolo della forza magnetica.
dove
La forza che si instaura tra due barre magnetiche cilindriche e identiche è pari a:
dove
L'equazione seguente lega invece la densità del flusso magnetico in un polo alla magnetizzazione:
Nel caso di magneti cilindrici con raggio formula_7 ed altezza formula_8, con i poli allineati, la forza che si instaura tra di loro può essere ben approssimata (solo per distanze paragonabili a formula_8) dalla seguente equazione:
dove formula_11 è la magnetizzazione dei magneti e formula_12 la distanza tra essi. In questo caso, la legge che lega il flusso formula_13 alla magnetizzazione formula_11 è:
L'effettivo dipolo magnetico può essere scritto come:
dove formula_17 è il volume del magnete; per un cilindro, esso è pari a formula_18.
Se formula_19, si ottiente una formula approssimata:
che ricorda quella già incontrata in precedenza del caso dei due monopoli.
</text>
</doc>
<doc id="107562" url="https://it.wikipedia.org/wiki?curid=107562">
<title>Capacità elettrica</title>
<text>
In elettrotecnica, la capacità elettrica è una grandezza fisica scalare che quantifica l'attitudine di un corpo conduttore ad accumulare carica elettrica qualora sia dotato di un potenziale elettrico rispetto all'ambiente o sia soggetto di una differenza di potenziale elettrico rispetto ad altri corpi conduttori. Il corpo conduttore deve essere elettricamente isolato rispetto all'ambiente od agli altri corpi conduttori, affinché sia possibile mantenere costante il potenziale.
La capacità di un corpo conduttore non dipende dalla sostanza da cui è costituito ma solo dalle sue caratteristiche geometriche, come forma e dimensioni, e dalla sua posizione rispetto ad altri conduttori, soprattutto rispetto a conduttori vicini messi a terra. Ad esempio, la capacità di un conduttore diviene grandissima quanto più se ne avvicini la superficie a quella parallela di un altro corpo conduttore collegato a terra. Questa configurazione definisce il condensatore elettrico. Se invece il corpo conduttore è isolato e, ad esempio, ha forma sferica, la capacità è proporzionale al raggio.
Un dispositivo elettrico dotato di capacità è detto "capacitivo". Un dispositivo puramente capacitivo è il condensatore, che riveste grande importanza in elettronica ed elettrotecnica, e rappresenta un elemento circuitale di base.
In elettrostatica, un corpo conduttore carico immerso in un ambiente privo di altre cariche e altri corpi assume un potenziale elettrico determinato dalla sua distribuzione di carica, secondo l'equazione di Poisson. Non essendovi altri corpi, il riferimento del potenziale è posto all'infinito, che si assume abbia potenziale nullo.
Il potenziale elettrico del corpo è uniforme al suo interno e sulla superficie; in tal modo il campo elettrico nel corpo è nullo e non vi è movimento di cariche dentro e sul bordo del conduttore, che sarebbe in contrasto con l'ipotesi elettrostatica. Pertanto, il potenziale del corpo è pari ad un unico valore "V". Inoltre, la carica elettrica "Q" nel corpo conduttore si distribuisce solo sulla superficie, infatti una carica all'interno del conduttore determinerebbe un campo elettrico nello stesso, ed il suo valore è proporzionale al potenziale del corpo e ciò è dovuto alla linearità dell'equazione di Poisson rispetto alla distribuzione di carica.
La capacità elettrica "C" di un conduttore isolato, cioè posto a sufficiente distanza da altri conduttori, è definita come il rapporto tra la carica elettrica "q" e il suo potenziale elettrico "V":
Tale rapporto, sempre positivo, dipende dalla forma e dalle dimensioni geometriche del corpo considerato, oltre che dalla permittività elettrica dell'ambiente nel quale è immerso. La grandezza inversa è detta elastanza elettrica ed è definita come:
formula_2
In presenza di due corpi conduttori isolati tra loro che abbiano carica elettrica uguale in modulo ma di segno opposto (formula_3), le linee del campo elettrico collegano le superfici dei due corpi, determinando una differenza di potenziale tra di essi formula_4 (in questo caso il potenziale è calcolato rispetto ad un riferimento di massa), proporzionale alla carica. Pertanto la capacità si definisce come:
Anche in questo caso la capacità è sempre positiva.
In presenza di più corpi conduttori isolati tra loro, la carica che ognuno di essi assume è linearmente dipendente dai potenziali di tutti i conduttori rispetto ad un riferimento di massa. Si può scrivere pertanto:
dove formula_7 è detta "mutua capacità" tra il conduttore "i" e il conduttore "j", mentre formula_8 è detta "autocapacità". Si dimostra che le mutue capacità sono simmetriche formula_9 e che sono negative, mentre le autocapacità sono positive: formula_10.
L'unità di misura della capacità elettrica nel Sistema internazionale di unità di misura è il farad, corrispondente alla capacità assunta da un conduttore di forma tale da assumere, con la carica di un Coulomb, il potenziale di un Volt, mentre l'elastanza elettrica si misura in F.
In realtà il farad è un'unità di misura di enorme grandezza: la capacità di 1 F è quella di una sfera conduttrice dal raggio pari a 9x10 m. Per questo motivo, le unità usate nella pratica sono i suoi sottomultipli, come il microfarad ("μF"), corrispondente a un milionesimo di Farad.
Nella pratica, per ottenere una carica netta uguale ed opposta su una coppia di conduttori isolati inizialmente neutri, si collegano galvanicamente tra loro e si avvicinano ad un corpo elettricamente carico; questo crea una separazione di cariche sulla superficie dei conduttori per induzione elettrostatica. Eliminando poi la connessione galvanica, si ottengono due conduttori con cariche uguali ed opposte (dato che il sistema dei due conduttori è rimasto complessivamente neutro). In alternativa, si possono collegare i due conduttori tramite una batteria, che impone una differenza di potenziale fissa tra di essi; questo provoca un movimento di cariche tra i corpi finché non si raggiunge una separazione di carica proporzionale alla tensione della batteria.
Sebbene la capacità sia definita in elettrostatica, anche in presenza di campi variabili lentamente (come la tensione alternata ad una frequenza di 50 Hz) il concetto di capacità rimane valido (dato che il tempo di riconfigurazione delle cariche è molto più rapido rispetto alla variazione del campo). Un condensatore ideale collegato in un circuito a tensione alternata permette il passaggio di corrente ai suoi capi, perché sebbene i due conduttori (solitamente piastre metalliche) siano isolati tra loro, vi è un continuo processo di carica e scarica degli stessi che crea una corrente alternata. Dato che tutti gli oggetti metallici sono dotati di capacità (rispetto ad altri conduttori ed alla terra), questo processo avviene in tutti componenti di una rete elettrica in c.a..
La capacità di un corpo che si comporta da condensatore dipende dalla forma e dalle dimensioni dei suoi elementi, e dalla permittività del dielettrico che li separa.
Per alcuni tipi di condensatore è possibile determinare la capacità in modo esatto.
La tabella seguente illustra alcuni esempi.
! Tipo di condensatore
! Capacità
! width="200" | Schema
dove formula_16 è l'induzione elettrica ed formula_17 è quello del campo elettrico.
In un mezzo lineare come il vuoto si semplifica e si ottiene:
Il significato fisico di capacità è stato utilizzato, in matematica, per dar vita a un concetto analogo nella teoria del potenziale, quello di capacità di un insieme, introdotto da Gustave Choquet nel 1950.
</text>
</doc>
<doc id="111557" url="https://it.wikipedia.org/wiki?curid=111557">
<title>Onda d'urto (fluidodinamica)</title>
<text>
In fluidodinamica ed aerodinamica con il termine onda d'urto si indica un sottile strato di forte variazione dei campi di pressione, temperatura, densità e velocità del fluido. Tale sottile spessore, dell'ordine di 10 nm, viene modellato matematicamente come una discontinuità.
Un'onda d'urto può essere normale oppure obliqua alla direzione della velocità relativa tra onda e corrente, e può altresì essere stazionaria oppure spostarsi rispetto ad un corpo che la genera. Le onde sonore, essendo identificabili come "piccoli disturbi" di pressione e di velocità, in quanto queste ultime grandezze sono legate nelle equazioni che governano il fenomeno, rappresentano delle onde d'urto che, per la loro bassa intensità, possono essere considerate isoentropiche, cioè onde che non modificano sensibilmente l'entropia del flusso che le attraversa o che attraversano (sono anche dette onde di Mach). Il meccanismo delle onde d'urto oblique è in grado di deviare un flusso supersonico.
Di particolare interesse sono anche le onde d'urto adiabatiche, cioè quelle che si possono verificare in una corrente di fluido animata da moto omoenergetico.
 Si consideri la figura a destra. Si immagini un serbatoio a monte del condotto di figura che per qualche motivo si svuoti generando un flusso di fluido (che considereremo gas perfetto) all'interno del condotto. Dette 1 e 2 le due sezioni di controllo, detta "T" la temperatura totale nel serbatoio, e "p" la pressione totale, detto "τ" il volume di controllo, e siano le variazioni di sezione fra 1 e 2 trascurabili, individuando con formula_1 la normale alla sezione 1 e con formula_2 alla sezione 2, si immagini che, a causa delle condizioni di pressione a valle del condotto, o delle condizioni di raccordo del condotto stesso, il fluido sia costretto a cambiare repentinamente le sue proprietà di pressione, velocità e temperatura all'interno di un piccolo volume (indicato appunto con "τ").
Chiameremo questa zona di discontinuità onda d'urto normale.
Supponendo il flusso stazionario, e cioè nulle le derivate delle quantità rispetto al tempo, facciamo il bilancio della massa e della quantità di moto. Ipotizzando un flusso all'ingresso del volume di controllo supersonico unidimensionale, indicheremo con "ρ" la densità del fluido, con u la velocità e con A la sezione.
Bilancio di massa:
Coincidendo formula_4 con formula_5 il bilancio diviene
dove G è una costante invariante a monte e a valle del volume di controllo.
Bilancio della quantità di moto:
Abbiamo indicato con formula_8 la risultante delle azioni del condotto sul fluido, con M la massa di fluido, e con formula_9 l'accelerazione di gravità.
Trascuriamo ora il peso del fluido e l'azione del condotto sul fluido stesso, agendo essa sull'area laterale del volume, di ordine inferiore rispetto alle aree frontali. Dunque poiché formula_10 e formula_11 allora il bilancio della quantità di moto diviene semplicemente formula_12 invariante a monte e a valle del volume di controllo.
Facciamo ora il bilancio dell'energia:
Abbiamo dunque tre invarianti: G, I, e formula_14. Ricordiamo la definizione di velocità del suono critica formula_19:
Si è indicata con formula_21 la velocità del suono ad entalpia totale e formula_22.
Inoltre formula_23 e dunque giungiamo all'equazione che regola le onde d'urto normali:
Chiamiamo formula_25 e formula_26 le due soluzioni dell'equazione (reali e distinte oppure reali e coincidenti), poiché per la nota proprietà delle equazioni di secondo grado formula_27, allora in un urto normale è formula_28, dove con formula_29 abbiamo indicato il numero di Mach critico, definito come formula_30. Da questa relazione notiamo subito che un flusso attraverso un'onda d'urto normale passa da supersonico a subsonico o viceversa (ma quest'ultima alternativa è impossibile perché viola il 2º principio della termodinamica). 
La relazione che lega i numeri di Mach "veri" è la seguente:
Osservando tale relazione si nota che per formula_32 allora anche formula_33 (in questo caso avremo una zona di debole discontinuità, fenomeno quasi isoentropico chiamato "onda di Mach"). Se invece formula_34 allora formula_35.
Per quanto riguarda le velocità:
La velocità dunque attraverso un urto normale diminuisce.
Per le pressioni:
La pressione aumenta, dunque, attraverso l'onda.
Dalle leggi di Poisson si ricava poi:
Se formula_39 allora anche formula_40 e viceversa se formula_41. Indicando con formula_42 l'entropia, poiché formula_43 e che formula_44 per il secondo principio della termodinamica, allora è che formula_45 e dunque formula_39. Sono dunque possibili onde d'urto normali solo con flusso in ingresso supersonico. 
Per quanto riguarda la temperatura:
Da cui formula_48 perché il primo membro dell'equazione detta è negativo. Dunque la temperatura aumenta attraverso l'onda.
 Le onde d'urto oblique sono zone di discontinuità del campo fluidodinamico poste con un angolo diverso da 90° rispetto al flusso. Considerando la figura a destra, si chiami v la velocità di un sistema di riferimento che trasli senza accelerare rispetto ad un'onda d'urto normale. Chiamo formula_25 la velocità del fluido in ingresso rispetto ad un riferimento fermo, mentre formula_50 la velocità vista secondo il sistema di riferimento traslante. L'osservatore solidale con il sistema di riferimento traslante vede entrare un flusso con angolo formula_51 rispetto all'onda, e lo vede uscire deviato di un angolo formula_52. Rispetto alla trattazione fatta nel paragrafo precedente, cambieranno le quantità relative alle velocità, ma non quelle relative all'entalpia o all'entropia. Chiamo formula_53 la nuova entalpia totale, sempre invariante, mentre individuo in formula_54 l'entalpia totale relativa alla parte normale della velocità del fluido. Poiché energeticamente non è cambiato nulla rispetto alla situazione precedente, il salto di entropia sarà lo stesso.
Dunque la relazione che lega il numero di Mach d'entrata e uscita nel sistema di riferimento mobile sarà:
Il salto di densità è dato da:
La pressione varia secondo la relazione:
 La relazione tra formula_61 e formula_62, il cui grafico troviamo a sinistra, è la seguente:
Fissato un certo Mach in ingresso, come si vede dal grafico data la svolta formula_61 esistono due possibili soluzioni: una con il flusso in uscita supersonico ed una con flusso in uscita subsonico (una con formula_62 maggiore, ed una con formula_62 minore). Inoltre si individua un angolo di svolta massimo, indicato nel grafico come formula_67. Il significato fisico di questo angolo massimo è molto importante e si intuisce immediatamente che un flusso supersonico deviato da un'onda obliqua non potrà effettuare svolte superiori al formula_67 indicato in figura.
</text>
</doc>
<doc id="121180" url="https://it.wikipedia.org/wiki?curid=121180">
<title>Frattura (meccanica)</title>
<text>
La frattura (o rottura) in meccanica è un fenomeno che si presenta a causa delle sollecitazioni agenti sul materiale e consiste, a livello macroscopico, nella disgregazione del materiale stesso (o oggetto) in frammenti minori.
Tale fenomeno è caratterizzato dalla disgregazione dei legami chimici che tengono uniti e coesi gli atomi costituenti il materiale; tali legami vengono rotti per effetto di un'energia esterna fornita per esempio da una sollecitazione esterna troppo intensa.
In ciascun materiale, si distinguono diversi tipi di frattura: fragile, duttile, di fatica, per creep e così via, tutti fenomeni derivanti appunto dalla rottura più o meno immediata dei legami interatomici del materiale.
La frattura può avvenire anche con basse sollecitazioni agenti sia come frattura fragile sia come collasso per fatica. La frattura può propagarsi lungo il bordo dei grani (cristalli di metallo) che compongono il materiale fratturato ("frattura intergranulare") o attraverso una frattura entro i grani stessi ("frattura transgranulare").
Generalmente si distinguono due tipi di frattura:
Nei metalli e nelle leghe, a livello microscopico, i più comuni meccanismi di frattura sono tre: frattura duttile, frattura fragile per clivaggio e frattura fragile intergranulare.
Un materiale metallico soggetto a frattura duttile presenta, macroscopicamente, sulle superfici di frattura un aspetto fibroso. Questo aspetto è dovuto alla presenza sulle superfici di micro-cavità chiamate "dimples".
Usualmente, gli stadi che si osservano nella frattura duttile sono:
Nel caso di materiali metallici per uso tecnico, a causa della loro elevata resistenza, la fase di coalescenza dei microvuoti può non avvenire. Il congiungimento delle cavità ha luogo per il cedimento lungo bande di taglio, indebolite dalla presenza delle microcavità.
La frattura per clivaggio (o "frattura di schianto") è caratterizzata da una brusca rottura del materiale, causata da una impossibilità di quest'ultimo di deformarsi plasticamente.
I materiali metallici, alle temperature di impiego comuni, difficilmente vanno incontro a frattura per clivaggio. Le condizioni che possono causare questo tipo di frattura sono le basse temperature, un alto grado di triassialità delle tensioni ed elevate velocità di applicazione della sollecitazione.
In generale la frattura per clivaggio è transgranulare e si propaga lungo i piani atomici a più bassa densità atomica, caratteristici del tipo di reticolo cristallino. Per i reticoli cubici a corpo centrato il suddetto piano è l'100 mentre per gli esagonali compatti è lo 0001.
All'inizio della frattura all'interno dello stesso grano possono crearsi più piani di frattura paralleli che tendono successivamente a confluire in un unico piano. Ciò crea i cosiddetti "river patterns" segni sulla superficie di frattura la cui direzione indica l'origine della frattura; la superficie di frattura risulta quindi composta da "faccette" lisce separate appunto dai "river patterns".
Nelle leghe polifasiche, la complessità della microstruttura fa sì che la frattura pur essendo transgranulare non avvenga su di un unico tipo di piano cristallino, creando quindi faccette meno lisce e definite.
La frattura intergranulare avviene per decoesione dei grani cristallini a livello del bordo grano. Essa è associata ad una bassa tenacità a frattura del materiale e quindi una elevata fragilità. Le superfici di frattura si presentano sfaccettate e brillanti.
La frattura intergranulare avviene quando i bordi grano, solitamente più resistenti di grani stessi, sono indeboliti diventando inevitabilmente sede del cammino di frattura.
Le principali cause della debolezza del bordo grano sono le seguenti:
Negli acciai la penetrazione nella matrice ferrosa degli atomi di idrogeno è causa dell'infragilimento da idrogeno o EAC ("Envirometal Assisted Cracking") che determina la frattura fragile dell'acciaio.
La sensibilità a questo fenomeno è tanto maggiore quanto maggiori sono le proprietà meccaniche del materiale.
Tale fenomeno si presenta ad esempio nelle condotte in acciaio interrate e condotte sottomarine dotate di protezione catodica.
Infatti in questo caso nella zona catodica dove normalmente vi è una maggiore concentrazione di idrogeno a causa di una sovraprotezione vi può essere la penetrazione di atomi idrogeno nell'acciaio.
Questa diffusione dipende anche dalla struttura microcristallina dell'acciaio.
Le cause sono varie, comunque i metalli con reticolo cristallino cubico a facce centrate (cfc) (es. Al e acciai inossidabili austenitici) non presentano frattura fragile anche a basse temperature (70 K), invece i materiali con reticolo cubico a corpo centrato (ccc) (es. acciai ferritici) presentano una temperatura di transizione, al di sotto della quale il comportamento passa da duttile a fragile. Questa temperatura, oltre che dalla composizione di lega, è fortemente influenzata dai trattamenti termici subiti dall'acciaio nel corso della costruzione e può essere influenzata anche da fenomeni esterni come la corrosione intergranulare o l'irraggiamento neutronico che subiscono gli acciai costituenti il recipiente reattore in impianti nucleari.
Se si analizza a livello atomico la frattura, si può ragionevolmente pensare che essa avvenga in corrispondenza della rottura dei legami interatomici. Affinché i legami atomici vengano rotti deve essere applicato una sforzo di trazione, almeno pari alla sforzo coesivo dei suddetti legami.
Si consideri un solo legame atomico con distanza di equilibrio formula_1 al quale viene applicata una forza formula_2 di trazione che tende ad aumentare la distanza interatomica formula_3.
L'andamento della forza applicata in funzione dello spostamento interatomico può essere in prima approssimazione rappresentata da metà periodo di sinusoide (per semplicità consideriamo l'origine in formula_1)
dove formula_6 è la forza di coesione (che corrisponde alla forza massima), e formula_7 è una opportuna costante.
La rigidezza del legame, calcolata come la derivata in formula_8 della forza applicata è
Moltiplicando entrambi i termini della precedente equazione per il numero di legami per unità di area e per la distanza interatomica formula_1 si ricava:
che risolta per formula_12 risulta:
dove formula_12 è lo sforzo di coesione e formula_15 è il modulo elastico a trazione del materiale.
Si può calcolare inoltre l'energia di superficie come metà dell'energia di frattura (poiché due sono le superfici che si formano),
Combinando quest'ultima con l'espressione precedente di formula_12 risulta
Perché avvenga la frattura lo sforzo applicato deve essere maggiore o uguale allo sforzo coesivo e, se si approssima ragionevolmente la costante formula_7 a formula_1, si nota che formula_21. Ciò però non concorda con quanto trovato sperimentalmente, in quanto gli sforzi di frattura misurati sperimentalmente sono in valore due o tre ordini di grandezza minori del modulo elastico.
Ciò è dovuto all'effetto dell'intrinseca presenza di difetti nei materiali, che viene studiato dalla meccanica della frattura.
</text>
</doc>
<doc id="104443" url="https://it.wikipedia.org/wiki?curid=104443">
<title>Ampiezza</title>
<text>
, in fisica, è la massima variazione di una grandezza in un'oscillazione periodica.
Nell'equazione semplice dell'onda
"f" rappresenta l'ampiezza dell'onda.
</text>
</doc>
<doc id="115062" url="https://it.wikipedia.org/wiki?curid=115062">
<title>Conduttore elettrico</title>
<text>
Il conduttore elettrico è un materiale in grado di far scorrere corrente elettrica al suo interno. I materiali conduttori sono caratterizzati dalla presenza di elettroni liberi nella banda di valenza degli atomi del reticolo cristallino (conduttori di prima specie) o contengono specie ioniche che si fanno carico di trasportare la corrente elettrica (conduttori di seconda specie).
La conducibilità elettrica di un conduttore di prima specie può essere interpretata mediante il modello delle bande. La carica netta su un conduttore si distribuisce sulla sua superficie, poiché in questo modo le singole cariche (che si respingono) massimizzano la loro distanza reciproca raggiungendo una configurazione che minimizza l'energia.
I materiali metallici (metalli e loro leghe) sono in genere buoni conduttori; i migliori in ordine decrescente sono:
D'altra parte possono condurre facilmente l'elettricità anche:
All'interno dei conduttori sono presenti cariche elettriche libere di muoversi, pertanto una volta raggiunto l'equilibrio elettrostatico, necessariamente il campo elettrico all'interno del conduttore è pari a zero (se così non fosse le cariche sarebbero accelerate e non vi sarebbe equilibrio). Tenendo conto di questo e grazie al teorema del flusso si ha che le cariche elettriche (o meglio gli eccessi di carica) si dispongono sulle superfici esterne dei conduttori.
Essendo
il campo nullo significa che lo spazio entro il conduttore è equipotenziale. È possibile dimostrare che, fissate le condizioni esterne, la distribuzione di carica superficiale del conduttore è unica (a meno di un coefficiente costante che dipende dal potenziale) e dipende dalla geometria del conduttore. 
Si deve notare che questa è una "definizione media macroscopica". Nelle immediate vicinanze dei nuclei atomici ci sono campi elettrici molto intensi, che tengono legati gli elettroni non liberi. 
All'esterno della superficie e in prossimità di essa per il teorema di Coulomb:
dove formula_3 è la densità di carica superficiale. Inoltre, poiché il campo è nullo entro il conduttore, il vettore campo elettrico ha direzione normale in ogni punto alla superficie:
Si nota che il valore del campo elettrostatico è maggiore dove formula_3 è maggiore, ed è possibile dimostrare che la densità superficiale di carica è maggiore dove il raggio di curvatura della superficie è minore. In altre parole il campo elettrostatico è più intenso nelle zone di una superficie a forma di punta, per via del cosiddetto fenomeno del potere disperdente delle punte. Da questo effetto hanno origine molti fenomeni come la formazione di scintille tra elettrodi di forma appuntita. Il discorso fatto finora non vale soltanto per singoli conduttori, ma anche per sistemi di più corpi conduttori posti a contatto, ad esempio, tramite un filo conduttore.
Nel caso il conduttore presenti una o più cavità al suo interno i risultati non cambiano. In effetti se si vuole calcolare il flusso di Gauss entro la cavità esso è nullo poiché non contiene cariche. Anche dentro una cavità il campo elettrostatico è nullo. Il potenziale elettrico rimane costante.
Un altro fenomeno di particolare rilevanza è l'induzione elettrostatica; tale fenomeno porta un conduttore a dividere le sue cariche se messo in prossimità di un altro corpo carico. Un esempio di induzione elettrostatica si ha nel caso di due conduttori di cui uno cavo che contiene un altro conduttore (per esempio positivo). Se i due conduttori non sono posti a contatto, la parete interna del conduttore cavo si carica negativamente poiché le cariche negative vengono attratte dal conduttore interno e quelle positive vengono respinte dallo stesso. Così sulla superficie esterna del conduttore cavo si ha una carica positiva uguale a quella del conduttore interno, in modo da mantenere equipotenziale lo spazio occupato dai due conduttori.
Si parla di induzione completa quando due conduttori sono disposti in maniera tale che tutte le linee di flusso partono da un conduttore e arrivano sull'altro. Due conduttori tra cui ci sia induzione completa formano un condensatore. Caratteristica quantitativa dei conduttori e dei condensatori è la capacità elettrica, che rappresenta appunto la capacità di un conduttore o condensatore di immagazzinare energia.
Quando un conduttore carico viene collegato "a massa" (per esempio la Terra), dopo un breve istante la differenza di potenziale tra i due conduttori si annulla, poiché la carica presente sul conduttore si trasferisce tutta alla massa, lasciando il conduttore neutro.
</text>
</doc>
<doc id="115508" url="https://it.wikipedia.org/wiki?curid=115508">
<title>Voltmetro</title>
<text>
Il voltmetro (anche voltometro o voltimetro) è uno strumento per la misura della differenza di potenziale elettrico tra due punti di un circuito, la cui unità di misura è il volt con simbolo V. L'unità di misura possiede questo nome in onore del fisico italiano Alessandro Volta. Insieme all'amperometro, wattmetro, varmetro, frequenzimetro, cosfimetro (o fasometro) e altri, è uno strumento per misurare le grandezze elettriche.
Come per altri strumenti, i parametri fondamentali di un voltmetro sono tre (vedi strumenti di misura per grandezze elettriche):
Un altro parametro non meno importante è la tensione di isolamento. Di un voltmetro occorre conoscere anche il tipo di tensione misurata: tensione continua, tensione alternata e in quest'ultimo caso se è solamente sinusoidale o anche a forme d'onda diversa.
Da non confondere con il voltametro, che è invece uno strumento per la misura della carica elettrica.
Esistono vari strumenti per misurare la tensione tra due punti di un circuito. Esistono strumenti ad assorbimento di corrente (che sono i più comuni) detti amperometrici ed esistono anche strumenti senza assorbimento di corrente (es. Voltmetri elettrostatici). Tutti questi strumenti permettono di misurare una tensione direttamente per cui si chiamano metodi diretti ma esistono anche i metodi indiretti per misurare una tensione (es. vedi metodo di opposizione).
Il voltmetro ideale è un bipolo la cui resistenza è infinita, equivale quindi ad un circuito aperto. Essendo a resistenza infinita la sua inserzione tra due punti di un circuito non altera in alcun modo il funzionamento del circuito medesimo. Malgrado non esistano nella realtà voltmetri ideali, ha una notevole importanza teorica e nella simulazione dei circuiti.
Per il principio di funzionamento vedi Strumenti di misura per grandezze elettriche
In pratica non esiste alcuna differenza tra un voltmetro magnetoelettrico e un amperometro magnetoelettrico. L'unica vera differenza consiste nel fatto che il voltmetro ha un equipaggio mobile che è un milliamperometro e in serie ha una resistenza di grande valore e esattamente tarata. In questo modo se applichiamo una tensione ai capi della serie (milliamperometro - resistenza di grande valore), nello strumento circolerà una corrente che è proporzionale alla tensione applicata. La scala viene tarata direttamente in volt o suoi multipli/sottomultipli. Per poter misurare tensioni più elevate, basta aumentare la resistenza in serie all'equipaggio mobile.
Una caratteristica presente nei voltmetri è la resistenza per unità di tensione. Più questo valore è elevato più lo strumento è buono e va a perturbare poco il nostro circuito. In genere un voltmetro magnetoelettrico ha 20.000 Ω/V e con questi valori possiamo dire che il voltmetro è di buona qualità.
Per il principio di funzionamento vedi Strumenti di misura per grandezze elettriche.
Anche per questo strumento deriva dal corrispondente amperometro. Viene usato un amperometro di piccola portata (si utilizza un milliamperometro elettromagnetico) ed in serie si inseriscono delle resistenze di grande precisione e valore. Con questo metodo è possibile creare dei voltmetri per portate relativamente piccole (circa 50 V.), se abbiamo la necessità di portate voltmetriche più piccole dobbiamo inserire in parallelo delle opportune resistenze.
Questi tipi di voltmetri vengono, in genere, costruiti per portate dirette sino a 100 V. Se abbiamo la necessità di misurare una tensione superiore devono essere inseriti con dei trasformatori di tensione.
Questi strumenti hanno una resistenza per unità di tensione che si aggira sui 50 Ω/V.
Per il funzionamento vedi Strumenti di misura per grandezze elettriche.
Le due bobine presenti in questo strumento vengono collegate in serie, e come per i voltmetri visti precedentemente per poter misurare una tensione si inserisce in serie un'apposita resistenza.
Questi tipi di voltmetri vengono, in genere, costruiti per portate dirette sino a 100 V. Se abbiamo la necessità di misurare una tensione superiore devono essere inseriti con dei trasformatori di tensione.
Questi strumenti hanno una resistenza per unità di tensione che si aggira sui 50 Ω/V.
Per il principio di funzionamento vedi Strumenti di misura per grandezze elettriche.
Anche questo strumento deriva dal corrispondente amperometro. Funzionando con temperature molto elevate è da un lato un vantaggio, ma anche uno svantaggio. Vediamo i motivi di queste due affermazioni. Il vantaggio consiste che non c'è la necessità di inserire una resistenza compensativa delle variazioni termiche (cosa che è sempre necessaria per gli altri voltmetri con bobine). Lo svantaggio è che hanno una resistenza per unità di tensione che si aggira sui 5 Ω/V.
Anche questo strumento deriva dal corrispondente amperometro. Rispetto ai voltmetri a dilatazione termica possiedono alcuni vantaggi tra cui la resistenza per unità di tensione che si aggira dai 5 Ω/V sino ai 1.000 Ω/V. La classe di precisione si aggira intorno allo 0,5% nei migliori strumenti. Per questo motivo nel caso si debba misurare tensioni piccolissime (dai millivolt sino a qualche volt) questo strumento è preferibile al corrispondente voltmetro elettrodinamico.
Per il principio di funzionamento vedi Strumenti di misura per grandezze elettriche.
Il Voltmetro elettrostatico derivano direttamente dall'elettrometro con tutti gli accorgimenti per renderlo più robusto e direttamente utilizzabile. Visto che la coppia in gioco è molto piccola. Per cui, o si installano sospensioni molto delicate (e qui si torna all'elettrometro) oppure si deve aumentare la tensione di utilizzo. Per questo motivo il voltmetro elettrostatico è uno strumento per medie ed alte tensioni (oltre i 10.000 V).
Se si costruisce un voltmetro elettrostatico contrapponendo una serie alternata di armature (molte armature fisse contrapposte ad altrettante armature mobili, in modo da moltiplicarne l'effetto finale) si possono avere degli strumenti per tensioni anche più modeste (dell'ordine dei 100 V). In questi strumenti la coppia di smorzamento è data da un magnete permanente che agisce sulle armature mobili del voltmetro. Per evitare che lo strumento si possa danneggiare quando viene alimentato, esiste una resistenza di grande valore, tra un morsetto esterno e l'armatura fissa con il preciso scopo di rendere abbastanza lento il caricamento delle armature dello strumento. Come per tutti gli altri strumenti la tensione all'armatura mobile è data attraverso la molla antagonista.
Se abbiamo la necessità di misurare tensioni elevate viene utilizzato un voltmetro elettrostatico con minor armarure contrapposte oppure anche un voltmetro dove il principio di funzionamento si basa sulla variazione della distanza tra le armature. Questo tipo di voltmetro elettrostatico può misurare tensioni ben oltre i 500.000 V Il grande pregio di questa classa di strumenti di misura è il loro bassissimo autoconsumo che è dell'ordine di poche decine di milionesimi di Ampere (circa 10 microampere) anche con tensioni a frequenza industriale (42 - 60 Hz).
Uno strumento che spesso e volentieri è confuso con il voltmetro elettrostatico è lo spinterometro. La differenza è enorme sotto il profilo pratico, visto che il voltmetro elettrostatico dà un valore di tensione esistente nel mio circuito senza alterarne il regime, cosa che invece lo spinterometro fa, visto che ad una certa tensione avviene la scarica, la quale va ad alterarne il regime. In genere lo spinterometro si utilizza per effettuare le prove di tensione di isolamento sulle varie macchine elettriche. In questo ben preciso utilizzo si posiziona lo spinterometro in parallelo alla macchina in prova, in modo che la tensione di prova alimenti anche lo spinterometro. Lo spinterometro è generalmente costituito da due sfere metalliche, contrapposte tra di loro, alimentate dalla tensione di prova. Esistono spinterometri ad asse orizzontale e verticale, anche se sostanzialmente non cambia il loro modo di funzionare.Visto che il valore di scarica è influenzato dalla distanza tra le sfere, dal loro diametro, da altri oggetti conduttori circostanti, dalla temperatura, dalla pressione e dall'umidità ambientale, esistono delle norme dove vengono prescritte, in dettaglio (che qui ovviamente non è il caso di elencare), su come devono essere utilizzati e costruiti tali apparecchi. Una volta che lo spinterometro è corrottamente installato si aumenta la tensione di prova in modo graduale; quando la rigidità dielettrica dell'aria viene superata, avviene la scarica. In questo modo si ottengono due vantaggi: il primo è quello di aver sollecitato la macchina in prova alla tensione che volevamo (ed un po' oltre), il secondo è a favore della sicurezza della macchina, visto che la scarica viene prodotta nello spinterometro, senza intaccare l'isolamento della macchina, determinando ciò un chiaro vantaggio economico. Ovviamente se utilizzato con tensioni alternate la scarica avviene al valore di cresta.
Per il principio di funzionamento vedi Strumenti di misura per grandezze elettriche.
Anche questo strumento deriva dal corrispondente amperometro. L'unica vera differenza consiste nel fatto che il voltmetro ha un equipaggio che è un milliamperometro e in serie ha una resistenza di grande valore e esattamente tarata. In questo modo se applichiamo una tensione ai capi della serie (milliamperometro - resistenza di grande valore), nello strumento circolerà una corrente che è proporzionale alla tensione applicata. La scala, ovviamente verrà tarata direttamente in volt o suoi multipli/sottomultipli. Per poter misurare tensioni più elevate, basta aumentare la resistenza in serie. Oltre un certo valore vengono utilizzati dei trasformatori di tensione. Come per i corrispondenti amperometri, anche i voltmetri ad induzione il valore letto è influenzabile dalla frequenza della tensione incognita. Per questo motivo questi strumenti possiedono una scala che è tracciata empiricamente e non possono venir utilizzati con tensioni alternate che hanno una frequenza diversa da quella di taratura.
Quando invece si voglia misurare tensioni su circuiti ad alta frequenza operanti con deboli correnti, il solo contatto dei puntali del tester può costituire un tale carico, da portare provvisoriamente il circuito a cessare di funzionare, in questi casi la misura si deve effettuare con voltmetri di altro tipo.
Nel caso si voglia conoscere il valore di tensione accettando l'errore di lettura tipico degli strumenti a indice, si impiega un voltmetro elettronico, formato da un milliamperometro collegato ad un circuito amplificatore costituito da valvole o FET, questi componenti permettono valori d'impedenza d'ingresso dell'ordine delle decine di megaohm. Nel caso il valore lo si voglia misurare con la massima precisione, si impiega il multimetro o il voltmetro digitale, i cui ingressi presentano un'impedenza standard di 10 megaohm, impedenze di ingresso così elevate sono presenti anche nelle sonde attive per oscilloscopi.
I voltmetri digitali solitamente impiegano un circuito convertitore analogico-digitale, in genere costituito da un integratore a doppia rampa. Una tensione di riferimento nota viene applicata all'integratore per un tempo fisso, facendo salire linearmente la rampa di tensione, dopodiché viene applicata la tensione da misurare e misurato il tempo impiegato dalla rampa per passare al nuovo valore.
La tensione ignota è data dal prodotto tra la tensione di riferimento per il tempo di salita diviso per il tempo di discesa. È importante che il valore della tensione di riferimento rimanga il più costante possibile durante la misurazione, mantenendosi stabile nel tempo, anche rispetto a variazioni della temperatura. Per questa ragione nei voltmetri ad alte prestazioni il circuito convertitore, solitamente un chip ibrido, viene progettato dal costruttore stesso dello strumento, risultando pertanto un custom non disponibile in commercio. Perché le misure siano affidabili, i voltmetri digitali ad alte prestazioni, al pari di altri strumenti, devono essere periodicamente sottoposti a calibrazione a intervalli di tempo indicati dal costruttore nel manuale di servizio abbinato allo strumento; non sono rari i casi in cui il costruttore garantisce le specifiche dichiarate solo entro un lasso di tempo di 90 giorni, a partire dalla data di ricalibrazione dello strumento. In pratica, per avere la certezza che i valori letti dallo strumento corrispondano alle specifiche dichiarate, lo stesso deve essere sottoposto a calibrazione a intervalli di tre mesi.
Le tensioni di riferimento campione sono ottenute con la pila Weston o con sistemi elettronici basati per esempio sulla banda di conduzione di un semiconduttore; questi sono utilizzati per la taratura dei calibratori primari. I voltmetri digitali di classe elevata, tramite bus IEEE-488 possono essere collegati in rete con altri strumenti ed essere gestiti da computer.
Il primo voltmetro digitale fu inventato e prodotto da Andrew Kay della "Non-Linear Systems "nel 1954.
Nel sistema a potenziometro o potenziometrico, metodo in disuso da tempo, viene utilizzata una resistenza variabile costituita da un filo resistivo ai cui capi è applicata una tensione. Un contatto mobile può scorrere sul filo ed è connesso al circuito di misura attraverso un rivelatore di corrente, per esempio un galvanometro. Il cursore che agisce sul contatto mobile viene regolato fino a quando la tensione sul cursore equivale alla tensione da misurare e la corrente è quindi nulla, ed è misurata la lunghezza del filo verso la massa. L'operazione viene poi ripetuta con una sorgente di tensione di riferimento.
A questo punto, la tensione ignota può essere calcolata come il prodotto della tensione di riferimento per la lunghezza del filo corrispondente alla tensione ignota, diviso per la lunghezza del filo corrispondente alla tensione di riferimento.
Il metodo all'oscilloscopio si basa sulla deflessione di un fascio di elettroni all'interno di un tubo catodico. La deflessione può essere ottenuta con un campo magnetico oppure con un campo elettrico, in entrambi i casi la tensione può essere calcolata per confronto con la deflessione prodotta da una tensione di riferimento nota, o letta direttamente su una scala graduata costituita dal reticolo posizionato sulla superficie del tubo, nei moderni oscilloscopi il tubo è stato sostituito con un pannello LCD monocromatico o a colori, e il voltmetro è integrato nel circuito dello strumento, fornendo in forma digitale sullo schermo, il valore in tensione del segnale elettrico visualizzato, la precisione di questo tipo di lettura è comunque inferiore a quella effettuata con un multimetro o voltmetro digitale da banco.
</text>
</doc>
<doc id="115537" url="https://it.wikipedia.org/wiki?curid=115537">
<title>Compressione (fisica)</title>
<text>
In fisica, con il termine compressione si indica il processo fisico mediante il quale si ottiene un aumento della pressione di un corpo (aeriformi, liquidi o solidi).
Essendo una trasformazione che implica una modifica del livello energetico del corpo cui la compressione è applicata, essa richiede la fornitura di energia. Dal punto di vista teorico, tale energia non è necessaria invece nel caso della cosiddetta "compressione spontanea" (che sarebbe il processo inverso dell'espansione libera), che però nella pratica non avviene.
Gli aeriformi (gas, vapore, vapore surriscaldato, ecc.) possono essere compressi facilmente diminuendone il volume a temperatura costante o diminuendone la temperatura a volume costante o diminuendo contemporaneamente il volume e la temperatura.
Comprimendo gli aeriformi oltre un certo limite, si ha il passaggio di stato dallo stato aeriforme allo stato liquido.
Sebbene i liquidi da un punto di vista teorico siano spesso approssimati a fluidi incomprimibili, quindi che non possono subire compressione, in realtà essi possono essere compressi, sebbene l'energia richiesta per la compressione di un liquido è generalmente molto maggiore dell'energia richiesta per la compressione di un aeriforme a parità di variazione di pressione e la variazione di volume che ne consegue è molto minore.
Un'evidenza della comprimibilità dei liquidi è ad esempio il fenomeno del colpo d'ariete, che consiste in fluttuazioni di pressione in una condotta associate appunto alle variazioni di volume del liquido.
Comprimendo i liquidi oltre un certo limite, si ha il passaggio di stato dallo stato liquido allo stato solido.
In meccanica la compressione è una delle sollecitazioni meccaniche elementari insieme alla trazione e la flessione. Per semplificazione si può dire che la compressione è la sollecitazione a cui è sottoposto un corpo che è soggetto a un sistema di forze convergenti.
Rispetto agli aeriformi e ai liquidi, i solidi risentono in misura molto minore degli effetti della variazione di volume associata alla compressione. Spesso tale variazione di volume è pressoché assente. Si ha invece un aumento degli sforzi interni e una deformazione del corpo. Nel caso di compressione monoassiale, si assiste in particolare ad un accorciamento del corpo nella direzione lungo la quale è impressa la sollecitazione, mentre nelle direzioni a questa trasversali si assiste ad un rigonfiamento spesso non uniforme (più accentuato verso il centro e meno accentuato vicino ai punti di applicazione della sollecitazione), determinando spesso al cosiddetto fenomeno del "barrelling" (dall'inglese ""barrell"", barile), così chiamato per via della forma che assume il corpo.
La compressione è un processo usato sin dalla più remota antichità. Il primo compressore usato è probabilmente il mantice, che nella prima configurazione era costituito da un otre di pelle gonfiato e poi schiacciato per ottenere un getto d'aria relativamente potente. Il mantice a moto alternativo nasce probabilmente con la metallurgia del rame, quando diventa essenziale garantire un corretto flusso di ossigeno al combustibile usato per la fusione del rame nativo.
In epoche più recenti, la scoperta del pistone e del manovellismo di spinta rotativa hanno permesso di costruire compressori di efficienza assai più elevata e nel XVII secolo l'applicazione del vapore come fluido motore ha consentito di applicare potenze più grandi alla compressione, ottenendo pressioni e portate più elevate; lo stimolo a ciò venne dalla necessità di ventilare in profondità le miniere Inglesi di carbone, sempre più profonde.
Nell'ambito dell'ingegneria di processo, la compressione è un'operazione unitaria di largo impiego per ottenere l'innalzamento della pressione di un fluido, realizzata attraverso macchine dette "compressori". In tale ambito, si intende per "compressione" il processo applicato ad un aeriforme, che comporta una riduzione del volume specifico del gas e nella maggior parte dei casi un aumento di temperatura; se la compressione è applicata ad un liquido, si utilizza invece il termine "pompaggio".
I compressori sono in genere di tipo meccanico, mossi da motori elettrici o turbine a vapore, anche se a volte vengono usate altre sorgenti di energia per compiere il lavoro di compressione.
I compressori possono essere di vari tipi:
La compressione viene realizzata anche all'interno dei motori dei veicoli. Essa costituisce ad esempio una fase dei motori a quattro tempi, detta appunto "compressione".
La fase di compressione è presente anche nei motori jet: in questo caso la compressione è realizzata da un compressore accoppiato ad una turbina (turbocompressore).
La compressione costituisce una delle quattro fasi del ciclo frigorifero, utilizzato per produrre basse temperature. Tale tecnica è sfruttata ad esempio in apparecchiature dell'ambito domestico per la refrigerazione (come frigoriferi e congelatori) oppure nei climatizzatori.
La compressione si rende necessaria per il riempimento di camere d'aria, come ad esempio pneumatici, palloni, gommoni o altre strutture gonfiabili. In questo caso, l'aria viene compressa e spinta all'interno della camera d'aria che, essendo flessibile, risponde alla pressione interna deformandosi e "gonfiandosi". Una volta chiusa la camera d'aria attraverso una valvola, la pressione all'interno della risulta superiore alla pressione atmosferica. Ciò è possibile proprio grazie al fatto che l'aria viene compressa prima di essere spinta all'interno della camera d'aria attraverso un compressore. Tali compressori spesso sono chiamati "pompe", usando quindi una terminologia differente rispetto a quella dell'ambito dell'ingegneria di processo, dove con il termine "pompa" si indica invece un dispositivo per innalzare la pressione di un liquido.
Anche la compressione dei solidi è applicata per realizzare soluzioni tecnologiche. Ad esempio la forma delle molle è appositamente studiata in un modo da esaltare la deformazione associata al fenomeno della compressione.
Un altro esempio di applicazione sono le guarnizioni in gomma o altro materiale più o meno elastico, che una volta posizionate sono schiacciate e quindi compresse. Tale compressione è necessaria a garantire ad esempio la tenuta stagna di contenitori.
</text>
</doc>
<doc id="115603" url="https://it.wikipedia.org/wiki?curid=115603">
<title>Amperometro</title>
<text>
L'amperometro è uno strumento per la misura dell'intensità della corrente elettrica che percorre una sezione di un conduttore. Il suo nome deriva dall'unità di misura della corrente, l'ampere, (leggi "ampèr"), il cui simbolo è A, che a sua volta ha questo nome in onore del fisico e matematico francese André-Marie Ampère. 
L'amperometro è, insieme al voltmetro, wattmetro, varmetro, frequenzimetro, cosfimetro (o fasometro) ecc. uno strumento per misurare le grandezze elettriche.
Come per altri strumenti, i parametri fondamentali di un amperometro sono tre (vedi Strumenti di misura per grandezze elettriche):
Un altro parametro non meno importante è la tensione di isolamento. Di un amperometro occorre conoscere anche il tipo di corrente misurata: Corrente continua o Corrente alternata, e in quest'ultimo caso se è solamente sinusoidale o anche a forme d'onda diversa.
Esistono vari strumenti per misurare la corrente che passa in una sezione di conduttore, basati sulla misurazione del campo magnetico generato dalla corrente (misuratori magnetoelettrici), del riscaldamento indotto in una resistenza campione (misuratori termici), della tensione indotta su una resistenza campione (misuratori elettrostatici o voltmetrici).
L'amperometro ideale è un bipolo la cui resistenza è nulla e che misura la corrente che passa in un ramo di un circuito. Essendo a resistenza nulla la sua inserzione in serie a qualsiasi ramo del circuito non altera in alcun modo il funzionamento del circuito medesimo; l'amperometro ideale si comporta infatti come un cortocircuito. Malgrado (ovviamente) non esistano amperometri ideali nella realtà, ha una notevole importanza teorica e nella simulazione dei circuiti.
Gli amperometri magnetoelettrici misurano la corrente attraverso una misurazione indiretta del campo magnetico generato da essa. Il primo tipo di amperometro è stato proprio magnetoelettrico: il galvanometro di Deprez-d'Arsonval.
Per misurare correnti tra le decine di milliampere (millesimi di ampere) e circa 1.000 ampere, si colloca in parallelo allo strumento una resistenza, chiamata "shunt" (derivatore di corrente), che viene attraversato dalla stragrande maggioranza della corrente che lo strumento misura, scavalcando così lo strumento di misura vero e proprio. Questo accorgimento consente di riportare la corrente che passa per lo strumento di misura ad un valore all'interno del suo range operativo. 
Nel caso si debba misurare un corrente ben oltre i 1.000 ampere fino ad oltre 100.000 ampere, lo shunt (deviatore) è molto complesso nella sua costruzione. Questo è necessario nel caso si debba misurare la corrente di alcune macchine operatrici molto grandi (laminatoi, ecc.) o ad esempio di processi elettrochimici o anche per la trazione elettrica in corrente continua o anche il trasporto di energia (facciamo alcuni esempi: collegamento Francia-Inghilterra 52 km, collegamento Italia-Corsica-Sardegna 410 km, collegamento Svezia-Gotland 100 km).
Esistono due metodi distinti per poter misurare correnti elettriche continue di grande intensità:
I trasduttori magnetici sono dei trasformatori di misura per corrente continua. In particolare, essi normalmente consistono di due nuclei magnetici identici, dove nel primario circola la corrente da misurare (di solito si tratta di una barra infilata all'interno dei due nuclei) e i due secondari hanno le spire avvolte nel verso contrario. Nei due secondari si fa circolare una corrente alternata di adeguata intensità: dalla misura del valore massimo della corrente alternata si può ricavare il valore della corrente continua. Con questo metodo si possono raggiungere anche delle discrete precisioni (1,0%). La presenza di un trasduttore implica anche che la tensione della barra dove circola la corrente continua (tensione che può essere anche molto elevata) non può venire in contatto con l'operatore: i due circuiti sono accoppiati magneticamente ma separati elettricamente (il circuito della corrente continua da quello della corrente alternata).
Per poter misurare correnti continue di elevata intensità si può utilizzare il generatore di Hall. La corrente continua, grazie ad un elettromagnete, produce un flusso magnetico che investe una sottile piastra di un materiale idoneo (antimoniuro di indio, arseniuro di indio, tellurio, bismuto, ecc). Tale piastra è anche attraversata da una piccola corrente continua (circa 200 - 300 milliampere) ortogonale al flusso magnetico, denominata corrente di controllo. L'effetto Hall induce una tensione continua direttamente proporzionale alla corrente di controllo, al flusso che investe la sonda e ad una costante dipendente dalle dimensioni della sonda medesima. Tale tensione, normalmente dell'ordine di grandezza dei decimi di volt, può essere misurata con un voltmetro (preferibilmente potenziometrico o elettrostatico per evitare effetti di second'ordine), usualmente tarato in ampere in modo da evitare i calcoli per poter risalire alla nostra corrente da misurare.
La bobina fissa di questo strumento può essere costituita da una sola spira, permettendo di raggiungere portate dirette di oltre 100 ampere, o di molte spire di filo di piccola dimensione se invece la portata dell'amperometro non è molto elevata.
L'autoconsumo (ovvero la potenza assorbita dall'amperometro) di questi strumenti è in generale molto piccolo. Molto spesso questi amperometri possiedono un commutatore per eseguire il cambio di portata senza interrompere il circuito dove sono installati.
In genere questi amperometri hanno portate che variano da un minimo di 0,5 ampere ad un massimo di 200 ampere.
Per il principio di funzionamento vedi Strumenti di misura per grandezze elettriche.
Come si è descritto nel "Principio di funzionamento", gli strumenti elettrodinamici possiedono "due" bobine, quando si utilizzano come amperometri queste due bobine possono essere collegate sia in serie, sia in parallelo.
Nel caso di collegamento in serie, visto che nella bobina mobile la corrente ci viene portata dalle molle antagoniste, la portata dell'amperometro è di pochi milliampere (millesimi di ampere). Con correnti maggiori, si distruggerebbe l'amperometro medesimo. Le correnti che attraversano le due bobine sono, ovviamente, in fase tra loro.
Se vogliamo un amperometro che possa misurare correnti più intense dobbiamo collegare le due bobine in parallelo. Anche gli amperometri elettrodinamici sono sensibili alle variazioni di temperatura, per poter minimizzare tali effetti si inserisce in serie ad ogni bobina una opportuna resistenza di manganina. Se ben costruito questo strumento è insensibile alla frequenza della corrente elettrica che lo attraversa, visto che è possibile fare in modo che le due correnti (nonostante i circuiti siamo in parallelo) siano in fase tra loro. Questo amperometro può essere utilizzato sia per correnti continue sia per correnti alternate. Un amperometro così costruito è in grado di misurare il valore efficace della corrente anche se la corrente stessa contiene armoniche che ne modificano la forma dell'onda. La portata normale di questi amperometri è di solito di 5 o 10 ampere. Anche con questi strumenti è possibile variare la portata dello strumento senza interrompere la corrente nel circuito.
Utilizzare uno shunt (deviatore), quando si vuole misurare correnti alternate ancora più grandi, come è usuale negli amperometri magnetoelettrici, non è proponibile per la difficoltà di costruire un adeguato deviatore con un rapporto tra resistenza ed induttanza identico all'amperometro stesso. In questi casi si preferisce ricorrere all'impiego di trasformatori di corrente (TA) con rapporto di trasformazione che può variare da 50/5 sino a 250/5.
Per il principio di funzionamento vedi Strumenti di misura per grandezze elettriche.
Negli amperometri a dilatazione termica la corrente da misurare percorre la resistenza che va a scaldare la spirale bimetallica. Questo amperometro può essere utilizzato per misurare ogni tipo di corrente elettrica indipendentemente se continua, alternata o a qualunque forma d'onda essa sia. Un amperometro così costruito è meccanicamente molto robusto ed è di grande utilità nel caso non sia desiderata una grande prontezza di risposta nell'indicazione. Spesso e volentieri può servire, nelle applicazioni pratiche, l'indicatore di massima corrente come è spiegato nel principio di funzionamento.In passato erano utilizzati degli amperometri a dilatazione termica denominati "amperometri a filo caldo" dove un filo di platino veniva percorso dalla corrente da misurare. Questa corrente provocava il riscaldamento del filo di platino che si deformava e, con una serie di rinvii, comandava l'indice dello strumento. Oggi, questi strumenti, sono completamente abbandonati.
Un esempio di applicazione pratica di un amperometro a dilatazione termica si ha quando, volendo misurare la corrente utilizzata da una macchina funzionante a carico variabile utilizzando uno strumento di altro tipo, vedremmo l'ago dello strumento oscillare continuamente senza poter giudicare il carico medio della nostra macchina.
Questo strumento ha il vantaggio di mostrare direttamente il valore efficace termicamente equivalente della corrente.
Gli strumenti a termocoppia si possono suddividere in due grandi famiglie.
Gli "amperometri a termocoppia a riscaldamento diretto" sono concettualmente costruiti da un milliamperometro magnetoeletrico (vedi amperometri magnetoelettrici) e da una coppia termoelettrica riscaldata dalla corrente da misurare. La coppia termoelettrica è saldata sul filo attraversato dalla corrente da misurare. Al passaggio della corrente si genera, sulla saldatura, un aumento di temperatura e tra i due capi della coppia termoelettrica conseguentemente si genera una tensione continua misurabile con un milliamperometro magnetoelettrico.
Gli "amperometri a termocoppia a riscaldamento indiretto" sono concettualmente identici a gli amperometri a termocoppia a riscaldamento diretto, differendo da questi per il fatto che la termocoppia non è saldata sul filo che si scalda. La termocoppia è riscaldata per conduzione termica o per irraggiamento termico.
Questi strumenti a termocoppia sono tarati in modo empirico e sono ideali per misurare (visto la grande sensibilità degli strumenti magnetoelettrici) anche correnti ad alta frequenza. Nel caso si debba misurare una corrente ad alta frequenza è, però, molto meglio utilizzare gli amperometri a termocoppia a riscaldamento indiretto perché la capacità parassita dello strumento è inferiore.
Tutti gli strumenti a termocoppia non possono sopportare nessun sovraccarico, visto la già grande temperatura della termocoppia.
Per il principio di funzionamento vedi Strumenti di misura per grandezze elettriche.
Poiché il principio elettrostatico consente di misurare esclusivamente tensioni possiamo, in linea di principio, costruire un amperometro elettrostatico andando a misurare la tensione che si genera quando una resistenza (shunt) di valore noto è attraversata dalla nostra corrente da misurare. In pratica però, visto che il principio elettrostatico consenta la costruzione di voltmetri con elevata resistenza interna ma purtroppo con scarsa sensibilità, vengono preferiti amperometri con altri principi di funzionamento in base alle nostre esigenze; infatti per rendere sensibile un amperometro elettrostatico bisognerebbe che la resistenza di shunt fosse elevata, ma questo influirebbe notevolmente sulla corrente che si intendeva misurare alterando il funzionamento del circuito sotto esame. Per questo motivo gli amperometri elettrostatici, praticamente, non esistono o sono rarissimi.
Per il principio di funzionamento vedi Strumenti di misura per grandezze elettriche.
Per poter avere un amperometro che funzioni con il principio dell'induzione bisogna avere due flussi sfasati tra loro (se sono in fase la coppia è nulla). Esistono vari modi per poter creare sul disco due flussi e conseguentemente due correnti indotte. 
Sono costituiti da un voltmetro digitale che visualizza la tensione misurata ai capi di una resistenza di precisione (shunt) attraversata dalla corrente in esame. Un microprocessore può eseguire calcoli sul segnale campionato per determinare il vero valore efficace ("True RMS") della corrente, così come il valore di picco o altri parametri.
Si tratta di strumenti portatili che misurano il campo magnetico indotto dal passaggio della corrente in un cavo conduttore. Hanno in genere la forma di pinze, o anelli apribili, al cui interno viene collocato il filo in esame senza necessità di interrompere il circuito. Le pinze sono realizzate in materiale ferromagnetico e costituiscono un circuito magnetico che concentra il flusso magnetico. All'interno del dispositivo una bobina avvolta sul circuito magnetico genera ai suoi capi una tensione che può essere misurata da uno strumento integrato. Questo tipo di amperometro misura sia la corrente alternata, sia la corrente continua con discreta precisione.
Esistono anche esecuzioni fisse da pannello, dove un trasformatore di corrente è fissato sul cavo e lo strumento indicatore è fissato sul pannello frontale del quadro elettrico.
Apparecchi più sofisticati in grado di misurare anche la corrente continua si basano su un sensore ad effetto Hall, il quale permette di effettuare misure con elevata precisione dell'intensità di un flusso di corrente, in un range di frequenza che spazia dalla corrente continua fino a 100 MHz.
</text>
</doc>
<doc id="132097" url="https://it.wikipedia.org/wiki?curid=132097">
<title>Accelerazione di gravità</title>
<text>
L'accelerazione di gravità, o "accelerazione gravitazionale" è l'accelerazione che un corpo subisce quando è lasciato libero di muoversi in caduta libera in un campo gravitazionale. 
Di solito si studia il valore che ha questa accelerazione sul suolo terrestre, che costituisce uno standard per le applicazioni tecniche più diffuse. Si può misurare in moltissimi modi, e si può derivare dai valori dei parametri che compaiono in leggi fisiche più generali, come la legge di gravitazione universale.
Per avere un'idea della grandezza o meno di un valore di accelerazione, si usa confrontarla con il valore di accelerazione che il campo gravitazionale terrestre provoca sugli oggetti che si trovano superficie terrestre. Infatti tutti gli uomini, gli animali, le piante e gli oggetti sulla Terra sono sottoposti a questa accelerazione, chi consapevolmente e chi inconsapevolmente.
Per questo parametro è stato fissato un valore convenzionale, che nelle unità di misura del Sistema Internazionale risulta pari a:
Notevolmente, questa scelta sul valore della costante è rimasta invariata dalla terza CGPM nel 1901.
Il valore standard di cui sopra è indicato con formula_1 o formula_2, talvolta anche con formula_3, e viene spesso impropriamente riportato tra le costanti "fisiche", per quanto sia più propriamente una costante tecnica, o costante definita (in inglese: "defined constant").
Il simbolo deve essere scritto con formula_4 minuscolo per distinguerlo dalla costante gravitazionale formula_5 che compare nella equazione di Newton.
Quando si valuta per esempio l'effetto di accelerazioni importanti su persone e strutture, per esempio nei terremoti, è una ottima consuetudine rapportare il valore ottenuto con questo valore standard esatto.
Si tratta di un valore medio, che approssima il valore dell'accelerazione di gravità presente al livello del mare a una latitudine di 45,5°. Il valore dell'accelerazione di gravità sulla superficie terrestre (formula_4) in effetti varia molto leggermente attorno al valore formula_1 a seconda del luogo. In particolare è influenzato dalla latitudine e dall'altitudine, ma viene influenzato per esempio anche dal tipo di rocce sottostanti.
L'accelerazione di gravità è misurabile semplicemente guardando un corpo in caduta libera, trascurando la resistenza dell'aria.
Il vettore dell'accelerazione di gravità terrestre ha sempre la direzione verticale ed è orientato verso il centro della Terra.
L'effettiva accelerazione che la Terra produce su un corpo in caduta varia al variare del luogo in cui questa è misurata.
Il valore dell'accelerazione aumenta con la latitudine per due ragioni:
La combinazione di questi due effetti rende il valore di formula_4 misurato ai poli circa lo 0,5% più grande di quello misurato all'equatore.
Il valore di formula_4 cui è sottoposto un corpo che si trova in aria ad altezza formula_11 sul livello del mare è calcolabile con la "formula tecnica" (che contiene cioè delle implicite unità di misura):
dove:
L'ultimo termine, h è una correzione dovuta all'altezza.
Se il corpo è sulla verticale della terraferma, viene aggiunta un'ulteriore correzione dovuta alla maggiore massa di un volume di terra rispetto all'acqua; tale maggiore massa può essere approssimata con una superficie orizzontale infinita dando luogo a un fattore di correzione (la "correzione di Bouguer", si veda Anomalia di Bouguer) pari a formula_18 volte la massa per unità di area, ovvero m·s·kg.
La gravità al di sotto della superficie terrestre viene invece calcolata sottraendo dalla massa totale della Terra la massa del guscio esterno al punto di misurazione. La forza di gravità diminuisce progressivamente all'aumentare della profondità e al centro della Terra è zero perché l'intera massa del pianeta attira il corpo in tutte le direzioni attorno a esso.
Anche variazioni locali nella composizione delle rocce e delle superfici possono alterare localmente l'accelerazione di gravità; queste anomalie sono generalmente misurate e mappate.
La costante di accelerazione gravitazionale terrestre trova inoltre grande impiego dal punto di vista fisico nello studio dei comportamenti dei corpi sottoposti a certe condizioni.
L'accelerazione standard formula_1 è spesso usata come unità di misura tecnica. Non è in effetti una unità di misura accettata dal Sistema internazionale, ma è molto comoda nella quotidianità per dare un'idea pratica della grandezza di una accelerazione. 
In questo contesto il parametro tecnico "formula_1" viene indicato semplicemente con la lettera "g", e utilizzata come unità di misura tecnica. Veniva in passato chiamata dai tecnici anche con l'espressione impropria di "forza g". Viene impiegata così anche in campo aerospaziale, per esprimere le accelerazioni alle quali sono sottoposti i velivoli, i veicoli spaziali e gli eventuali passeggeri.
Con l'espressione colloquiale forza g utilizzata in aeronautica ci si riferisce invece al fattore di carico lungo l'asse verticale di un aeromobile, unità di misura delle accelerazioni a cui astronauti e piloti sono soggetti, moltiplicato per l'accelerazione di gravità terrestre, con simbolo appunto "g".
La forza g non va quindi confusa con l'accelerazione di gravità sulla superficie terrestre. Nonostante il nome, non è una forza, ma una accelerazione; sebbene, nei casi di cui sopra sia una accelerazione relativa al riferimento considerato, determina una forza fittizia (es. la forza centrifuga). 
In questo contesto, quando ci si riferisce a "1 g" si indica un'accelerazione pari all'accelerazione di gravità media misurata sulla Terra, che vale 9,80665 m·s.
Una persona normale mediamente riesce a sopportare accelerazioni verticali di circa 5 g positivi e 2÷3 g negativi. 
Per g positivo si intende una accelerazione che produce lo stesso effetto soggettivo causato dalla gravità terrestre su un soggetto in posizione eretta; questo effetto è prodotto da una accelerazione nel senso che va dai piedi verso la testa, quindi di senso contrario alla forza di gravità cui si è soggetti stando in piedi. Per g negativi si intendono accelerazioni ed effetti soggettivi di senso inverso. I g positivi, quindi, causano il deflusso del sangue dalla testa verso i piedi, i negativi viceversa. Si calcola che un'accelerazione di 5 "g", se prolungata per vari secondi, provochi perdita di conoscenza e valori superiori possono danneggiare il corpo umano anche mortalmente, se non adeguatamente protetto.. A valori critici di g negativi infatti uno dei primi effetti è che il campo visivo diventa rosso, a causa del maggiore apporto di sangue nei capillari nei globi oculari dovuta all'aumento della pressione sanguigna. 
Con la combinazione di speciali tute anti-g e di forze applicate ai muscoli per tenerli in tensione, entrambi con lo scopo di ridurre il deflusso sanguigno dal cervello, i piloti moderni possono sopportare oltre 10 "g positivi" (100 m·s). La resistenza a "g negativi" rimane invece molto inferiore, e comunque nell'intervallo fra i 2 e i 3 "g".
L'accelerazione standard g è usata anche in campo automobilistico. In particolare, si usa per esprimere le accelerazioni che agiscono sul veicolo in curva, accelerazione, frenata, e per l'analisi delle collisioni.
L'accelerazione di gravità sulla Terra si può anche misurare in modo indiretto servendosi di un pendolo preciso, a patto che ci si limiti a misurare delle piccole oscillazioni. Per le piccole oscillazioni infatti vale la formula del pendolo "matematico" (il più semplice):
formula_21
dove:
In fisica il valore di g è deducibile come un caso particolare dalla legge della gravitazione universale.
Il valore dell'accelerazione corrisponde infatti al prodotto di alcuni dei termini nell'equazione di gravitazione:
Inserendo i valori della costante di gravitazione universale formula_5, della massa, e del raggio della Terra si ottiene infatti:
Questa è una buona approssimazione del valore medio dell'accelerazione di gravità formula_4, ma si vede subito che non è il valore che si è scelto come standard. Le differenze del valore appena calcolato col valore standard sono dovute a diversi fattori, tra cui:
</text>
</doc>
<doc id="147608" url="https://it.wikipedia.org/wiki?curid=147608">
<title>Riflessione (fisica)</title>
<text>
In fisica la riflessione è il fenomeno per cui un'onda, che si propaga lungo l'interfaccia tra differenti mezzi, cambia di direzione a causa di un impatto con un materiale riflettente.
In acustica la riflessione causa gli echi ed è utilizzata nel sonar. In geologia è importante nello studio delle onde sismiche.
Assorbimento, riflessione e trasmissione sono i fenomeni che avvengono quando la luce interagisce con la materia: quando l'energia radiante incide su un corpo, una parte viene assorbita, una parte viene riflessa e una parte trasmessa e per la legge di conservazione dell'energia, la somma delle quantità di energia rispettivamente assorbita, riflessa e trasmessa è uguale alla quantità di energia incidente.
Per indicare il tipo di riflessione di cui si tratta si usano gli aggettivi:
La riflessione può avvenire:
La riflettanza ("reflectance") è il rapporto tra flusso riflesso e flusso incidente valutato per ogni lunghezza d'onda. Essendo definita come rapporto di grandezze omogenee, la riflettanza è una grandezza adimensionale e viene espressa in percentuale (0-100%) o come fattore (0.0-1.0). Inoltre riguarda il flusso e quindi la totalità della radiazione riflessa nella emisfera. Il materiale artificiale con minor rifflettanza è il Vantablack.
La riflettanza non è solo funzione della lunghezza d'onda ma anche dell'illuminazione, della geometria di irradiamento e della geometria di visione (cioè della geometria con cui si illumina il corpo e della geometria con cui si misura la quantità riflessa), per cui è necessario definire una grandezza più generale della riflettanza, cioè il fattore di riflessione.
Si fa riferimento al "diffusore riflettente ideale". Si tratta di un corpo (ideale, cioè teorico) che non assorbe e non trasmette, ma riflette diffusamente la radiazione ricevuta con radianza o luminanza uguale per ogni angolo di riflessione e indipendentemente dalla direzione della radiazione incidente. Come prima applicazione del concetto di diffusore riflettente ideale si definisce il fattore di radianza ("radiance factor") o il fattore di luminanza ("luminance factor") come il rapporto tra la radianza di un'area e quella del diffusore ideale riflettente irradiato nello stesso modo.
Con riferimento a questo corpo ideale, il fattore di riflessione ("reflectance factor" o "reflection factor") di un corpo è il rapporto tra il flusso riflesso dal corpo in un dato cono il cui vertice è sul corpo considerato e il flusso riflesso dal diffusore riflettente ideale.
Il fattore di riflessione è dunque una grandezza generica che corrisponde:
Un tipico spettrofotometro è in grado di misurare il fattore di riflessione spettrale ad intervalli di 10 nm nell'intervallo da 380 a 730 nm.
La riflessione di onde elettromagnetiche è regolata da due leggi fondamentali, ricavabili dal principio di Fermat e dal principio di Huygens-Fresnel:
Un'onda elettromagnetica riflessa può subire uno sfasamento. Questo dipende dagli indici di rifrazione del mezzo nel quale viaggia la luce (formula_1) e del mezzo oltre la superficie riflettente (formula_2):
</text>
</doc>
<doc id="207577" url="https://it.wikipedia.org/wiki?curid=207577">
<title>Mescolanza sottrattiva</title>
<text>
La mescolanza sottrattiva di due stimoli di colore, nota anche come sintesi sottrattiva o miscela sottrattiva è la mescolanza di stimoli di colore che giungono modificati all'occhio.
L'esempio classico è quello della sovrapposizione di due inchiostri, per esempio inchiostro giallo sovrapposto ad inchiostro ciano, su un foglio bianco. In questo caso i due strati di inchiostro si comportano come filtri per la luce. L'inchiostro giallo assorbe una parte della luce. La parte rimanente attraversa l'inchiostro ciano che ne assorbe un'altra parte. La parte rimanente viene riflessa dalla carta bianca e costituisce lo stimolo di colore che arriva al nostro occhio.
Altri esempi di mescolanza sottrattiva: un fascio di luce che attraversa una soluzione di coloranti, la mescolanza di due pitture. Ogni colorante assorbe una parte di luce e la luce emergente, cioè lo stimolo di colore, è ciò che rimane della luce iniziale. La sintesi sottrattiva è utilizzata nella fotografia tradizionale su pellicola e, in parte, nella stampa retinata.
</text>
</doc>
<doc id="1475788" url="https://it.wikipedia.org/wiki?curid=1475788">
<title>Rigidezza</title>
<text>
In meccanica dei materiali, la rigidezza è la capacità che ha un corpo di opporsi alla deformazione elastica provocata da una forza applicata. Il suo inverso è detto cedevolezza o flessibilità.
La rigidezza è determinata da:
Il concetto di rigidezza deriva dalla teoria dell'elasticità, la cui equazione più comunemente utilizzata è la legge di Hooke. Nel metodo agli elementi finiti e nell'analisi dei sistemi elastici a più gradi di libertà la legge di Hooke viene formulata in termini tensoriali, permettendo l'estensione della legge costitutiva a tutti i possibili gradi di libertà. Nel caso della formulazione più generale possibile, si ha che la rigidezza è quantificata dal "tensore di elasticità" formula_1 e la cedevolezza dal "tensore di cedevolezza" formula_2:
inoltre, si ha che l"'energia di deformazione" formula_4 e l"'energia complementare" formula_5 sono pari a:
dove formula_7 è la tensione formula_8 e la deformazione. I tensori di elasticità e cedevolezza sono tensori del quarto ordine, pertanto hanno 81 coefficienti, di cui 36 sono indipendenti. Se il materiale in esame è iperelastico, i coefficienti indipendenti si riducono a 21 e, nel caso in cui sia anche omogeneo e isotropo essi saranno soltanto due.
Facendo uso della notazione di Voigt, i tensori di elasticità e cedevolezza vengono rappresentati da matrici simmetriche semidefinite positive:
Nel caso monodimensionale, la formulazione tradizionale della relazione che esprime una sollecitazione longitudinale lungo un asse formula_10 è:
pertanto, in questo caso, la rigidezza di un corpo che subisce un allungamento formula_12 causato da una forza applicata formula_13 è rappresentata dalla "costante elastica longitudinale" formula_14, di conseguenza la cedevolezza verrà espressa dalla "costante plastica longitudinale" formula_15. Nel Sistema Internazionale, la costante elastica longitudinale si misura in N·m "(newton su metro)", mentre la costante plastica longitudinale si misura in m·N ("metro su newton").
Per esprimere la sollecitazione longitudinale, la formulazione odierna del caso monodimensionale si serve del "coefficiente di dilatazione lineare" formula_16:
dove la rigidezza è rappresentata dal modulo di elasticità longitudinale di Young formula_18, mentre la cedevolezza sarà espressa dal modulo di plasticità longitudinale formula_19.
In modo del tutto analogo, nel caso monodimensionale, la formulazione tradizionale della relazione che esprime una sollecitazione tangenziale, a un piano a cui un asse formula_20 è ortogonale, è:
pertanto, in questo caso, la rigidezza di un corpo che subisce una variazione angolare formula_22 causato da una momento applicata formula_23 è rappresentata dalla "costante elastica tangenziale" formula_24, di conseguenza la cedevolezza verrà espressa dalla "costante plastica tangenziale" formula_25. Nel Sistema Internazionale, la costante elastica tangenziale si misura in N·m "(newton per metro)", mentre la costante plastica tangenziale si misura in (N·m) ("uno su newton per metro").
Per esprimere la sollecitazione tangenziale, la formulazione odierna del caso monodimensionale si serve del coefficiente di scorrimento angolare formula_5:
dove la rigidezza è rappresentata dal modulo di elasticità tangenziale formula_28, mentre la cedevolezza sarà espressa dal modulo di plasticità tangenziale formula_29. 
La costanti e i moduli di elasticità, usate entrambe per esprimere la rigidezza, sono strettamente legate tra loro. Dalle relazioni precedenti si può dedurre che formula_30 e che formula_31, dove formula_32 è la sezione, formula_33 è la dimensione longitudinale e formula_34 è il braccio della forza che causa il momento. Tuttavia esiste una profonda differenza tra le due categorie di grandezze; infatti, i moduli di elasticità sono proprietà costitutive del materiale, invece le costanti elastiche sono proprietà relative al corpo elastico. Ovvero, i moduli di elasticità dipendono soltanto dal materiale, mentre le costanti elastiche dipendono dal corpo e dalle condizioni di vincolo.
In generale si dovrebbe usare il termine "rigidità" quando si parla di un materiale, di "rigidezza" quando si parla di una struttura. La rigidezza di una struttura è di fondamentale importanza in molte applicazioni ingegneristiche, infatti essa rappresenta un parametro di scelta fondamentale del materiale. Un'alta rigidità si ricerca quando si vogliono basse deformazioni, una bassa rigidità quando è richiesta flessibilità. Lo spostamento può, in generale, riferirsi ad un punto distinto da quello di applicazione della forza, ed una struttura complicata non si deformerà solamente nella stessa direzione della direzione di applicazione della forza. Attraverso il tensore di rigidezza si può caratterizzare la rigidezza delle struttura in tutte le direzioni. Per un semplice sistema di punti collegati da molle, la matrice di rigidezza descrive la connettività tra i gradi di libertà del sistema stesso. Un semplice esempio è la matrice di rigidezza di una trave. Le stesse grandezze usate per esprimere la rigidezza rotazionale, vengono usate per esprimere anche la rigidezza al taglio, il rapporto fra deformazione di taglio per unità di forza applicata, e la rigidezza torsionale, ovvero il rapporto fra il momento di torsione applicato e l'angolo di rotazione.
In fisiologia, con il termine rigidezza (in inglese "stiffness") si indica la resistenza meccanica, la densità e la rigidità strutturale dei tendini e delle strutture di tessuto connettivo del muscolo. Sostanzialmente, maggiore è la rigidezza di questi tessuti, maggiore è l'energia che può essere immagazzinata durante un movimento eccentrico, per essere poi restituita e liberata durante la fase concentrica.
</text>
</doc>
<doc id="1637157" url="https://it.wikipedia.org/wiki?curid=1637157">
<title>Campo magnetico</title>
<text>
In fisica, in particolare nel magnetismo, il campo magnetico è un campo vettoriale solenoidale generato nello spazio dal moto di una carica elettrica o da un campo elettrico variabile nel tempo. Insieme al campo elettrico esso costituisce il campo elettromagnetico, responsabile dell'interazione elettromagnetica.
In realtà, le equazioni relative al campo elettrico e quelle relative al campo magnetico sono solo in apparenza completamente separate: le stesse cariche elettriche, quando sono in movimento, danno luogo a una densità di corrente e divengono dunque sorgente di un campo magnetico. 
Tuttavia, poiché il fatto che le cariche siano ferme o si muovano è un fatto relativo (cioè dipendente dal sistema di riferimento scelto per descrivere il fenomeno), diviene ugualmente relativo il fatto che si abbia a che fare con un campo elettrico o con un campo magnetico. Appare dunque naturale interpretare il campo elettrico e il campo magnetico come manifestazioni diverse di un'unica entità fisica, detta campo elettromagnetico.
La scoperta della produzione di campi magnetici da parte di conduttori percorsi da corrente elettrica si deve a Ørsted nel 1820: sperimentalmente si verifica che la direzione del campo è la direzione indicata dalla posizione d'equilibrio dell'ago di una bussola immersa nel campo; lo strumento per la misura del campo magnetico è il magnetometro.
Il campo magnetico agisce su un oggetto elettricamente carico con la forza di Lorentz (nel caso di una carica elettrica in movimento) oppure tramite il momento torcente che agisce su un dipolo magnetico. L'evoluzione spaziale e temporale del campo magnetico è governata dalle equazioni di Maxwell, un sistema di quattro equazioni differenziali alle derivate parziali lineari che sta alla base della descrizione formale dell'interazione elettromagnetica.
In fisica il campo induzione magnetica (anche detto impropriamente campo magnetico) in un punto di un mezzo è individuato dal vettore formula_1 composto da una prima componente indicata con formula_2 e una seconda componente indicata con formula_3 dovuta a fenomeni microscopici che avvengono nel mezzo come tipicamente un determinato allineamento degli spin atomici. formula_1 si misura in tesla (T) o in Wb/m² ed è anche detto densità di flusso magnetico o induzione magnetica; formula_5 è detto "campo magnetizzante" e si misura in A/m (o anche in Oe); formula_6 è il "vettore di magnetizzazione", anch'esso in A/m; formula_7 è la permeabilità magnetica del vuoto pari a formula_8. In definitiva: formula_9.
formula_6 tiene conto del fatto che i momenti magnetici intrinseci (spin) degli elettroni legati si allineano mediamente in una certa direzione, spesso quella del campo applicato esternamente, e inoltre compiono dei moti medi di precessione attorno a tale direzione in senso orario o antiorario a seconda del segno della loro carica elettrica. Si tratta di moti rotatori nello stesso senso e con la stessa direzione perpendicolare, che forniscono un contributo alla corrente elettrica macroscopica soltanto sulla superficie del materiale: al suo interno i moti delle cariche affiancate tra di loro si compensano a vicenda in quanto ruotano tutte nello stesso senso e da ciò deriva il fatto che le correnti delle cariche legate agli atomi sono esprimibili come il rotore della magnetizzazione. Il legame tra formula_6 e formula_5 è generalmente spiegabile con delle trattazioni quantistiche della materia, che caratterizzano le proprietà magnetiche dei materiali come il paramagnetismo, il diamagnetismo, il ferromagnetismo, l'antiferromagnetismo, il ferrimagnetismo e il superparamagnetismo.
formula_5 è un campo magnetico che ha quattro possibili contributi: la corrente dovuta a cariche libere nel materiale, un campo magnetico applicato esternamente, la variazione nel tempo del campo elettrico e il campo demagnetizzante formula_14 che è sempre opposto in verso alla magnetizzazione infatti esso nasce qualora la magnetizzazione abbia dei punti di non uniformità lungo la propria direzione, ovvero quando formula_6 ha divergenza non nulla. L'esempio più caratteristico di necessità del campo demagnetizzante in assenza di campi magnetici applicati esternamente, di correnti elettriche libere e variazioni del campo elettrico è il fatto che in un ferromagnete formula_6 può essere comunque presente ma essendo nulla fuori dal materiale ha una discontinuità al bordo che la rende non solenoidale quindi se formula_5 fosse nullo anche formula_1 sarebbe non solenoidale e ciò contraddirebbe la seconda equazione di Maxwell :formula_19.
In ambito ingegneristico viene spesso utilizzata una convenzione diversa: le quantità fondamentali (campo elettrico e campo magnetico) sono rappresentate dalla coppia duale formula_20, mentre le induzioni corrispondenti, ovvero la coppia duale formula_21, vengono considerate la risposta del mezzo all'eccitazione elettromagnetica. Grazie a questa convenzione esiste una dualità sia a livello di unità di misura (ampere è duale di volt, weber è duale di coulomb), sia a livello di notazione. Difatti, introducendo le quantità fittizie "densità di carica magnetica" formula_22 e "densità di corrente magnetica" formula_23, è possibile scrivere delle equazioni di Maxwell perfettamente simmetriche, e ciò consente di enunciare il teorema di dualità elettromagnetica.
Sia data una carica elettrica puntiforme formula_24 in moto con velocità istantanea formula_25 in una regione caratterizzata dalla presenza di un campo elettrico formula_26 e un campo magnetico formula_1. La forza di Lorentz è la forza formula_28 esercitata dal campo elettromagnetico sulla carica, ed è proporzionale a formula_24 e al prodotto vettoriale tra formula_25 e formula_1 secondo la relazione:
dove formula_33 è la posizione della carica, formula_34 la sua velocità e formula_35 è il tempo.
Una carica positiva viene accelerata nella direzione di formula_26 e viene curvata nella direzione perpendicolare al piano formato da formula_25 e formula_1.
Si consideri il caso in cui sia presente il solo campo magnetico. La formula può essere applicata al caso di un circuito filiforme di lunghezza formula_39 percorso dalla corrente elettrica formula_40:
e sapendo che per definizione:
con formula_43 la densità di corrente, si può estendere al caso più generale di un volume formula_44 percorso da una corrente descritta dalla densità di corrente, per il quale si ha:
Dal momento che la forza di Lorentz è legata al campo magnetico tramite il prodotto vettoriale, la forza e il campo non hanno la stessa direzione, essendo perpendicolari. Come conseguenza di ciò, la forza di Lorentz non compie lavoro, infatti:
L'ultimo integrando è nullo perché è il prodotto misto di tre vettori, di cui due paralleli.
Una serie di evidenze sperimentali, tra le quali l'esperimento di Oersted del 1820, ha portato a concludere che il campo magnetico nel generico punto formula_47 generato nel vuoto da un elemento infinitesimo formula_48 di un circuito percorso da una corrente formula_40 è dato da:
dove formula_51 è la distanza tra la posizione formula_33 dell'elemento infinitesimo formula_48 del circuito e il punto formula_47 in cui è calcolato il campo, e formula_7 è la permeabilità magnetica nel vuoto.
L'integrazione su tutto il circuito della precedente espressione produce la Legge di Biot-Savart:
che rappresenta il campo magnetico totale generato dal circuito in formula_47. Nel caso più generale, in cui l'approssimazione di circuito filiforme non viene applicata, si ricorre alla densità formula_43 della corrente che attraversa una sezione di conduttore. L'espressione del campo diventa:
dove formula_60 è il volume infinitesimo, di lunghezza formula_61 e sezione formula_62, del conduttore nel punto formula_33.
Calcolando la divergenza del campo generato da un circuito si dimostra che essa è sempre nulla:
Questa proprietà costituisce la seconda equazione di Maxwell:
Il fatto che si tratti di un campo a divergenza nulla implica che il campo magnetico sia un campo solenoidale. Da questo fatto segue che, applicando il teorema del flusso di Gauss, il flusso formula_66 di formula_1 attraverso qualsiasi superficie chiusa formula_68 che contiene al suo interno il circuito è nullo:
dove formula_44 è il volume racchiuso dalla frontiera formula_68. Inoltre, il campo magnetostatico non è conservativo e quindi non è irrotazionale, cioè il suo rotore non è nullo ovunque. Partendo dalla più generale formulazione del campo magnetico, nella quale si sfrutta la densità di corrente, si dimostra che:
dove formula_43 indica il vettore densità di corrente. Questa espressione costituisce la quarta equazione di Maxwell nel caso stazionario. Applicando alla precedente espressione il teorema del rotore si ottiene la Legge di Ampère:
ovvero, la circuitazione lungo una linea chiusa del campo magnetostatico è pari alla somma algebrica delle correnti concatenate con essa.
Il potenziale vettore del campo magnetico, indicato solitamente con formula_75, è un campo vettoriale tale che formula_1 sia uguale al rotore di formula_75:
La definizione non è tuttavia univoca, dal momento che formula_5 resta invariato se ad formula_75 si somma il gradiente di una qualsiasi funzione scalare:
Il potenziale vettore definito in questo modo risulta soddisfare automaticamente le equazioni di Maxwell nel caso statico.
Nel caso elettrodinamico bisogna modificare le definizioni dei potenziali in modo da ottenere che due equazioni di Maxwell risultino immediatamente soddisfatte. Per quanto riguarda formula_75, si verifica ancora che è definito in modo che il suo rotore sia formula_1, mentre formula_44 è definito in modo che:
L'elettrostatica e la magnetostatica rappresentano due casi particolari di una teoria più generale, l'elettrodinamica, dal momento che trattano i casi in cui i campi elettrico e magnetico non variano nel tempo. In condizioni stazionarie i campi possono essere infatti trattati indipendentemente l'uno dall'altro, tuttavia in condizioni non stazionarie appaiono come le manifestazioni di una stessa entità fisica: il campo elettromagnetico.
Più precisamente, le leggi fisiche che correlano i fenomeni elettrici con quelli magnetici sono la legge di Ampere-Maxwell e la sua simmetrica legge di Faraday.
La legge di Faraday afferma che la forza elettromotrice indotta in un circuito chiuso da un campo magnetico è pari all'opposto della variazione del flusso magnetico del campo concatenato con il circuito nell'unità di tempo, ovvero:
Per la definizione di forza elettromotrice si ha, esplicitando la definizione integrale di flusso:
applicando il teorema di Stokes al primo membro:
e per quanto detto si giunge a:
Uguagliando gli integrandi segue la terza equazione di Maxwell:
Si noti che nel caso non stazionario la circuitazione del campo elettrico non è nulla, dal momento che si genera una forza elettromotrice che si oppone alla variazione del flusso del campo magnetico concatenato col circuito.
L'estensione della legge di Ampère al caso non stazionario mostra come un campo elettrico variabile nel tempo sia sorgente di un campo magnetico. Ponendo di essere nel vuoto, la forma locale della legge di Ampère costituisce la quarta equazione di Maxwell nel caso stazionario:
Tale relazione vale solamente nel caso stazionario poiché implica che la divergenza della densità di corrente sia nulla, contraddicendo in questo modo l'equazione di continuità per la corrente elettrica:
Per estendere la legge di Ampère al caso non stazionario è necessario inserire la prima legge di Maxwell nell'equazione di continuità:
Il termine
è detto corrente di spostamento, e deve essere aggiunto alla densità di corrente nel caso non stazionario.
Inserendo la densità di corrente generalizzata così ottenuta nella legge di Ampère:
si ottiene la quarta equazione di Maxwell nel vuoto. Tale espressione mostra come la variazione temporale di un campo elettrico sia sorgente di un campo magnetico.
Per descrivere il comportamento del campo magnetico nella materia è sufficiente introdurre nelle equazioni di Maxwell un termine aggiuntivo formula_96, che rappresenta la densità di corrente associata alla magnetizzazione del materiale:
Tuttavia, tale termine non è in generale noto: questo ha portato all'introduzione del vettore "intensità di magnetizzazione", anche detto vettore di "polarizzazione magnetica" e indicato con formula_6, una grandezza vettoriale macroscopica che descrive il comportamento globale del materiale soggetto alla presenza del campo magnetico. Il vettore rappresenta il momento di dipolo magnetico per unità di volume posseduto dal materiale. Definito come la media del valore medio del momento magnetico proprio formula_99 di "N" particelle contenute in un volume infinitesimo formula_100, è espresso dalla relazione:
Nel Sistema internazionale di unità di misura il vettore di polarizzazione magnetica si misura in Ampere su metro (A/m), e nella definizione il limite vale per un volume che contenga un numero significativo di atomi tale da poterne calcolare una proprietà media.
Nel caso in cui la polarizzazione atomica all'interno del materiale sia uniforme, le correnti di magnetizzazione sono descritte dalla "corrente di magnetizzazione superficiale" formula_102, data da:
ovvero la corrente di magnetizzazione è pari al flusso del vettore "densità di corrente di magnetizzazione superficiale" formula_104 attraverso una superficie formula_105. Nel caso in cui la polarizzazione atomica all'interno del materiale non sia uniforme, invece, si introduce la "corrente di magnetizzazione volumica" formula_102, data da:
ovvero la corrente di magnetizzazione volumica è pari al flusso del vettore "densità di corrente di magnetizzazione volumica" formula_108 attraverso una superficie formula_105. Le relazioni che legano la densità di corrente di magnetizzazione con il vettore di magnetizzazione sono:
dove nella prima equazione formula_111 è il versore che identifica la direzione normale alla superficie del materiale.
La presenza di materia costringe a tenere conto delle correnti amperiane nelle equazioni di Maxwell per il campo magnetico:
e porta a definire il vettore campo magnetico formula_113 nella materia come:
L'equazione di Maxwell può essere riscritta in modo equivalente:
La densità di corrente formula_43 presente nella precedente equazione si riferisce esclusivamente alle correnti elettriche, date dal moto dei soli elettroni liberi, e non alle correnti atomiche di magnetizzazione. Nel caso non stazionario, inoltre, la quarta equazione ha l'espressione:
La permeabilità magnetica è una grandezza fisica che esprime l'attitudine di una sostanza a polarizzarsi in seguito all'applicazione di un campo magnetico e si misura in henry al metro (H/m), equivalente a newton all'ampere quadrato (N/A). Nel caso in cui il materiale sia omogeneo e isotropo e la sua risposta sia lineare, i vettori formula_118 e formula_113 sono paralleli, e questo implica che la relazione tra di essi è di semplice proporzionalità:
dove formula_121 è la permeabilità magnetica del materiale considerato.
Dal momento che non tutti i materiali hanno una reazione lineare tra formula_1 e formula_5, i materiali magnetici si distinguono in tre categorie:
L'energia magnetica è l'energia associata al campo magnetico, e nel caso di materiali in cui la relazione tra formula_1 e formula_5 sia lineare l'energia magnetica contenuta in un volume formula_131 è data da:
dove il prodotto scalare:
è la densità di energia magnetica.
Per un circuito percorso da corrente la densità di energia magnetica può essere definita a partire dal potenziale vettore formula_75 del campo magnetico e il vettore densità di corrente formula_43:
Il campo elettromagnetico è dato dalla combinazione del campo elettrico formula_26 e del campo magnetico formula_1, solitamente descritti con vettori in uno spazio a tre dimensioni. Il campo elettromagnetico interagisce nello spazio con cariche elettriche e può manifestarsi anche in assenza di esse, trattandosi di un'entità fisica che può essere definita indipendentemente dalle sorgenti che l'hanno generata. In assenza di sorgenti il campo elettromagnetico è detto onda elettromagnetica, essendo un fenomeno ondulatorio che non richiede di alcun supporto materiale per diffondersi nello spazio e che nel vuoto viaggia alla velocità della luce. Secondo il modello standard, il quanto della radiazione elettromagnetica è il fotone, mediatore dell'interazione elettromagnetica.
La variazione temporale di uno dei due campi determina il manifestarsi dell'altro: campo elettrico e campo magnetico sono caratterizzati da una stretta connessione, stabilita dalle quattro equazioni di Maxwell. Le equazioni di Maxwell, insieme alla forza di Lorentz, definiscono formalmente il campo elettromagnetico e ne caratterizzano l'interazione con oggetti carichi. Le prime due equazioni di Maxwell sono omogenee e valgono sia nel vuoto sia nei mezzi materiali, e rappresentano in forma differenziale la Legge di Faraday e la legge di Gauss per il campo magnetico. Le altre due equazioni descrivono il modo in cui il materiale, nel quale avviene la propagazione, interagisce polarizzandosi con il campo elettrico e magnetico, che nella materia sono denotati con formula_139 e formula_5. Esse mostrano in forma locale la Legge di Gauss elettrica e la Legge di Ampère-Maxwell.
Le equazioni di Maxwell sono formulate anche in elettrodinamica quantistica, dove il campo elettromagnetico viene quantizzato. Nell'ambito della meccanica relativistica, i campi sono descritti dalla teoria dell'elettrodinamica classica in forma covariante, cioè invariante sotto trasformazione di Lorentz. Nell'ambito della teoria della Relatività il campo elettromagnetico è rappresentato dal tensore elettromagnetico, un tensore a due indici di cui i vettori campo elettrico e magnetico sono particolari componenti.
</text>
</doc>
<doc id="1768865" url="https://it.wikipedia.org/wiki?curid=1768865">
<title>Principi della dinamica</title>
<text>
I principi della dinamica sono i principi alla base della omonima branca della meccanica classica, che descrive le relazioni tra il moto di un corpo e gli enti che lo modificano. Sono validi in sistemi di riferimento inerziali e descrivono bene la fisica dei corpi che si muovono a velocità molto minore di quella della luce.
Sono anche detti "Principi di Newton" perché enunciati da Isaac Newton nel suo testo fondamentale "Philosophiae Naturalis Principia Mathematica", anche se sono il frutto di una lunga evoluzione da parte di numerosi scienziati che ne ha preceduto e seguito la pubblicazione. All'interno della formalizzazione logico-matematica della meccanica newtoniana svolgono il ruolo di assiomi.
Aristotele nella sua "Fisica" del IV secolo a.C. asseriva che lo stato naturale dei corpi fosse la quiete, ossia l'assenza di moto, e che qualsiasi oggetto in movimento tende a rallentare fino a fermarsi, a meno che non venga spinto a continuare il suo movimento.
Nel Medioevo, Guglielmo di Ockham e gli occamisti, e poi, nel Quattrocento, Nicola Cusano, nell'opera "Il gioco della palla", e Leonardo da Vinci ripensarono la meccanica aristotelica: cominciarono a sviluppare una diversa dinamica, fondata su diversi principi fisici e presupposti filosofici.
Il principio di inerzia non è di banale osservazione sulla Terra, dominata dagli attriti, anzi, è letteralmente impossibile. Infatti, considerando, per esempio, una biglia che rotola su una superficie piana orizzontale molto estesa, l'esperienza comune riporta che con il passare del tempo la biglia rallenta fino a fermarsi. Questo è dovuto al fatto che essa interagisce con il piano e con l'aria. Si può osservare, comunque, che facendo diminuire progressivamente questi attriti, ad esempio rarefacendo l'aria e lisciando il piano per diverse volte, la biglia percorre uno spazio sempre maggiore prima di fermarsi. Generalizzando, l'idea che sta alla base del primo principio è che, teoricamente, diminuendo gli attriti fino a renderli nulli, il corpo non rallenti più e quindi non si fermi mai, cioè persista nel suo stato di moto rettilineo uniforme. Riferendosi invece alla tendenza di ogni corpo a mantenere lo stato di quiete o di moto si usa parlare di "inerzia" e questo concetto può esser visto come una diretta conseguenza del principio di relatività galileiana.
Ciò viene dettagliatamente descritto da Galileo in due sue opere, rispettivamente, nel 1632 e nel 1638: il "Dialogo sopra i due massimi sistemi del mondo" e "Discorsi e dimostrazioni matematiche intorno a due nuove scienze attenenti alla meccanica e i movimenti locali". Scrive Galileo:
Bisogna aggiungere, ad onor del vero, che, com’è noto, Galileo riteneva che un moto inerziale avrebbe assunto una direzione circolare e non rettilinea, come, invece, dedusse Newton. Infatti, secondo Galilei, i pianeti si muovono di moto circolare uniforme attorno al Sole, senza subire alcun tipo di effetto, gravitazionale o di altro tipo. Tuttavia, la sua prima enunciazione formale è nei "Principia" di Isaac Newton, che pur ne riconosce (impropriamente, come visto) la paternità galileiana. Newton chiarisce inoltre il concetto nella terza definizione:
La formula esplicita dell'uguaglianza fra la forza e il prodotto della massa inerziale per l'accelerazione apparve per la prima volta negli scritti di Eulero nel 1752.
I principi furono presentati tutti assieme da Newton nel 1687 nell'opera "Philosophiae Naturalis Principia Mathematica" ("I principi matematici della filosofia naturale"). Newton stesso chiamò i suoi principi "Axiomata, sive leges moti" ("Assiomi o leggi del moto"), a rimarcare che questi rappresentano la base fondante della meccanica, come gli assiomi di Euclide lo sono per la geometria, la cui validità può essere testata solo con esperimenti e a partire dai quali è possibile ricavare ogni altra legge sui moti dei corpi.
Il primo principio, detto d'inerzia, ha tradizionalmente origine con gli studi sulle orbite dei corpi celesti e sul moto dei corpi in caduta libera di Galileo. Il principio di inerzia si contrappone alla teoria fisica di Aristotele, il quale riteneva che lo stato naturale di tutti i corpi fosse quello di quiete e un agente esterno fosse necessario ad indurre il moto. Galileo ideò una serie di esperimenti, anche mentali, volti a dimostrare la non correttezza di questa assunzione. A simili conclusioni giunse anche Cartesio, nei suoi scritti riguardo alla fisica.
Il secondo principio della dinamica si deve a Newton, e introduce il concetto di forza come origine e causa del cambiamento dello stato di moto dei corpi. Nei secoli si sono susseguite numerose discussioni su come e su cosa di preciso Newton intendesse con "forza" e "cambio dello stato di moto", in relazione in particolare alla formulazione odierna del secondo principio della dinamica.
Il terzo principio esprime una importante proprietà delle forze e fu usato da Newton per dimostrare la conservazione della quantità di moto. Secondo il premio nobel Richard Feynman, il terzo principio ha una importante rilevanza nello sviluppo della meccanica:
I principi di Newton nella sua originaria formulazione sono validi per i corpi puntiformi, in quanto non considerano gli effetti che possono derivare dalla dimensione finita degli oggetti, come in particolare le rotazioni. I principi furono poi estesi ai corpi rigidi e ai corpi deformabili da Eulero nel 1750.
Nei "Principia" l'enunciato della "Lex I" è il seguente:
Questo principio, noto anche come "principio d'inerzia" o "principio di Galileo", afferma che se un corpo è fermo o si muove di moto rettilineo uniforme, allora la somma vettoriale delle forze che agiscono su di esso è nulla. Quindi, se la risultante delle forze agenti su un corpo è nulla, allora esso mantiene il proprio stato di moto. Nella realtà di tutti i giorni, si osserva che un corpo in moto tende lentamente a rallentare fino a fermarsi. Questo tuttavia non è in contraddizione con il primo principio, in quanto la forza di attrito, per esempio con l'aria o il terreno, sta agendo sul corpo modificando il proprio stato di moto. Se fosse possibile fare un esperimento in cui tutti gli attriti e le interazioni vengano annullate, ad esempio nello spazio vuoto lontano dalle galassie, allora si osserverebbe che il corpo continuerebbe a muoversi indefinitamente a velocità costante lungo una linea retta.
Gli esempi portati da Newton a proposito del cerchio in rotazione e del moto dei pianeti sono in realtà esempi di conservazione del momento angolare e rappresentano l'integrazione del principio di inerzia nel principio della conservazione della quantità di moto.
Il principio di inerzia rappresenta un punto di rottura con la fisica aristotelica in quanto l’assenza di forze è messa in relazione non solo con la quiete, ma anche con il moto rettilineo uniforme. Poiché la particolarità del moto rettilineo uniforme è che la velocità è vettorialmente costante, cioè in modulo, direzione e verso, si desume che la presenza di forze sia collegata alle variazioni di velocità. Ciò porta al secondo principio della dinamica.
Nei "Principia" l'enunciato della "Lex II" è il seguente:
Pertanto, il secondo principio, detto anche "principio di proporzionalità" o "principio di conservazione", afferma che:
Sia la forza che l'accelerazione sono dei vettori e sono indicati in grassetto nella formula. Nel testo, Newton prosegue affermando:La forza netta, o forza risultante, agente su un corpo è la somma vettoriale di tutte le forze applicate ad esso. L'accelerazione causata quindi dalle forze avrà come effetto una modifica del vettore velocità nel tempo. Questa modifica si può manifestare come un cambio della direzione della velocità, oppure come un aumento o diminuzione del suo modulo.
La massa che compare nel secondo principio della dinamica è chiamata massa inerziale, cioè misura quantitativamente la resistenza di un corpo ad essere accelerato. Infatti la stessa forza agente su un corpo di piccola massa, come ad esempio una spinta data ad un tavolo, produce un'accelerazione molto maggiore che su un corpo di grande massa, come un'automobile che con la stessa spinta cambierebbe la propria velocità di poco.
Se la massa inerziale del corpo non è costante, allora la seconda legge della dinamica può essere generalizzata con l'introduzione della quantità di moto. Ovvero, un punto materiale, cioè un corpo di dimensioni trascurabili rispetto al sistema di riferimento in esame e contemporaneamente dotato di massa, al quale sia applicata una forza, varia la quantità di moto in misura proporzionale alla forza e lungo la direzione della stessa. In altre parole, secondo una formulazione analoga a quella di Eulero: il tasso di aumento della quantità di moto è uguale e parallelo alla forza impressa:
cioè in base alla definizione di quantità di moto e di accelerazione e alla regola di Leibniz:
Per un sistema chiuso quindi il rapporto fra i moduli della forza applicata e dell'accelerazione è costante e pari alla massa inerziale:
Il secondo principio della dinamica fornisce una spiegazione per il fatto che tutti i corpi cadono con una velocità, che è indipendente dalla loro massa. Simile risultato fu raggiunto, secondo Newton, da Galileo Galilei con lo studio del piano inclinato e l'esperimento della caduta dei gravi. Tuttavia, ogni conoscitore delle opere galileiane sa che Galileo non giunse mai alla distinzione del concetto di massa da quello di peso. D’altra parte, ciò è comprensibile se si considera l’avversione galileiana nei confronti di ogni riferimento ad un'azione “a distanza” tra i corpi, come quella, per esempio, teorizzata da Keplero.
Nei "Principia" l'enunciato della "Lex III" è il seguente:Il terzo principio, detto anche "principio di azione e reazione", dove il termine "azione" deve essere inteso nell'accezione generale di forza o momento "reali", può essere riformulato come:
In termini matematici il terzo principio può essere riassunto come:
Nel proseguire del testo, Newton porta i seguenti esempi:Il terzo principio della dinamica in termini moderni implica che tutte le forze hanno origine dall'interazione di diversi corpi, in base al terzo principio se solo un corpo singolo si trovasse nello spazio, questo non potrebbe subire alcuna forza perché non vi sarebbe alcun corpo su cui la corrispondente reazione possa essere esercitata.
Un esempio chiaro è l'applicazione al sistema Terra-Luna, di cui sono sottosistemi la Terra e la Luna. La forza totale esercitata dalla Terra sulla Luna deve essere uguale, ma di senso opposto alla forza totale esercitata dalla Luna sulla Terra, in accordo con la legge di gravitazione universale.
Un esempio tipico che si può fare di applicazione controintuitiva del principio, è quello della semplice camminata: nella situazione noi imprimiamo forza al suolo all'indietro tramite il piede, il suolo reagisce con una forza uguale e contraria che poi è quella che ci spinge in avanti. Ma il suolo invece sembra non subire alcuna forza, poiché non accelera: la contraddizione si risolve considerando che la massa inerziale della Terra è enorme in confronto a quella dell'individuo, e perciò la forza si traduce in un'accelerazione piccola al punto da essere inosservabile.
Per un sistema fisico di "n" punti materiali (o corpi), il terzo principio della dinamica assieme al secondo implica la conservazione della quantità di moto e quindi la simmetria delle leggi fisiche rispetto a traslazioni spaziali. Considerando, ad esempio, due corpi isolati che interagiscono, allora in base al secondo principio della dinamica il terzo può essere riscritto come:
dove formula_11 e formula_12 sono rispettivamente le quantità di moto del corpo formula_5 e formula_6. Dato che gli incrementi possono essere sommati allora si ha:
da cui si ricava che è costante nel tempo la grandezza formula_16, che equivale alla quantità di moto totale del sistema formato dai corpi formula_5 e formula_6 considerati assieme. Questo ragionamento può essere esteso ad un numero arbitrario di corpi.
Nel caso del singolo punto materiale, la conservazione della quantità di moto deriva direttamente dal secondo principio della dinamica
Infatti è sufficiente che sul punto materiale non agisca alcuna forza esterna perché si conservi la quantità di moto. Si pensi, ad esempio, ad un razzo in volo nel vuoto spaziale. Consumando combustibile, questo riduce la sua massa e di conseguenza la sua velocità cresce di modo che il prodotto formula_20 sia costante, istante per istante.
Il testo "La fisica di Berkeley" riporta come principi fondanti la meccanica classica le seguenti (cit.):
Citando sempre dallo stesso libro, le 3 leggi di Newton sono così formulate:
Da quest'ultimo principio, integrando rispetto al tempo, discende il principio della conservazione della quantità di moto e viceversa.
"La fisica di Feynman" ha una impostazione "sui generis" che non consente di estrarre agevolmente un "corpus" di principi della dinamica espressi in maniera formale, poiché ha l'intento di costruire una visione unitaria della fisica, "filtrandola" col criterio della validità nella moderna teoria dei campi per non introdurre come invece si fa solitamente con l'approccio storico dei concetti che risultano in una teoria più ampia falsificati o particolari. Tuttavia riportiamo alcuni brani che a nostro avviso sono quanto più si avvicina ad una formulazione di tali principi. Citiamo quindi:
Per quanto riguarda il terzo principio della dinamica, Feynman lo considera, al pari della legge di gravitazione universale, una delle due sole cose sulla natura delle forze che Newton disse:
Secondo Feynman, Newton caratterizzò il concetto di forza tramite l'enunciazione di un principio generale, il terzo principio della dinamica appunto, e tramite la formulazione di una legge di forza particolare, ovvero quella gravitazionale.
I principi della dinamica non valgono in sistemi di riferimento non inerziali. Per studiare anche questi ultimi, infatti, è necessaria l'introduzione delle interazioni apparenti, ovvero forze e momenti dovuti alle accelerazioni del sistema di riferimento. Le forze apparenti, quali la forza centrifuga e la forza di Coriolis, non hanno alcuna reazione corrispondente, in altre parole il terzo principio della dinamica smette di essere vero nei sistemi di riferimento non inerziali.
La meccanica classica può essere vista come l'approssimazione a basse velocità rispetto a quella della luce della teoria della relatività ristretta. Il secondo principio della dinamica ad esempio non è più in grado di descrivere correttamente gli eventi che occorrono quando invece le velocità dei corpi sono vicine a quella della luce, dato che permette sempre di incrementare la velocità di un corpo con l'azione di una forza senza alcun limite. Inoltre, il terzo principio della dinamica richiede che l'azione e la reazione siano sempre opposte in ogni momento, generando un vincolo istantaneo fra punti lontani al di fuori dei rispettivi coni luce.
Per estendere la validità dei principi della dinamica, allargandoli ai sistemi "non inerziali" , il concetto di "azione" viene ristretto soltanto a forze e momenti, in meccanica razionale si parla di forze generalizzate, "reali" per cui vale questo principio, cioè che implicano la "reazione". Infine, per la simmetria tra i due concetti che scaturisce da questo principio si preferisce oggi parlare di interazione: ""l'interazione tra i corpi è reciproca, e unica sorgente di forza reale e momento meccanico reale. Una forza generalizzata applicata su un corpo formula_31 è "reale", se dovuta all'influenza di un qualsiasi altro corpo formula_37, e solo allora si manifesta su formula_32 con orientazione antiparallela"." Ricordando che un sistema inerziale è definito proprio in base a questo principio come sistema di riferimento in cui si manifestano solo interazioni tra i corpi, ovvero interazioni reali, e le interazioni apparenti sono appunto quelle che non provenendo dai corpi in quanto non reciproche, vengono imputate al sistema di riferimento, e non sono "reali" solo nel senso che non sono "assolute", e non nel senso di "ininfluenti" sui corpi quando presenti.
Nel 1981 Mordehai Milgrom propose una sua modifica volta a spiegare il problema delle curve di rotazione delle galassie a spirale in modo alternativo all'introduzione della materia oscura, denominata MOND dall'acronimo inglese per "Dinamica Newtoniana Modificata" che teneva conto dello strappo, che però gode di scarso consenso presso la comunità scientifica attuale, anche se le si può riconoscere di essere, popperianamente parlando, falsificabile al pari delle teorie a base di materia ed energia oscura.
</text>
</doc>
<doc id="1808740" url="https://it.wikipedia.org/wiki?curid=1808740">
<title>Leva (fisica)</title>
<text>
Una leva è una macchina semplice che non trasforma l'energia, ma reindirizza le forze; è costituita da un'asta rigida capace di muoversi attorno a un punto fisso, chiamato fulcro; è un'applicazione del principio di equilibrio dei momenti.
La condizione di equilibrio si verifica quando il momento della forza motrice è uguale al momento della forza resistente, ovvero la somma dei momenti meccanici ad essa applicati deve essere uguale a zero, come la risultante delle forze. Poiché nella leva l'asse di rotazione è fisso e sono applicate solo due forze, è sufficiente uguagliare i due momenti:
dove:
Nel caso in cui le forze siano perpendicolari all'asta, la formula si semplifica
formula_6
Segue che
ovvero il braccio e la forza su di esso applicata sono inversamente proporzionali.
Dalla condizione di equilibrio, segue che, imprimendo all'estremità del braccio lungo della leva un movimento con una determinata forza, l'estremità del suo braccio corto si muoverà con una forza moltiplicata di un fattore "b"/"b", anche se percorrerà un cammino ridotto dello stesso fattore, e viceversa se l'azione viene invece compiuta sul braccio corto. Il rapporto tra le dimensioni dei bracci determina quindi il rapporto tra forza resistente e forza da applicare.
In base al rapporto tra forza resistente e forza applicata (o potenza) le leve si distinguono in tre tipi:
In base alla posizione reciproca del fulcro e delle forze le leve si distinguono in:
La tabella seguente riporta alcuni semplici esempi di leve, indicando il fulcro, i punti di applicazione delle forze, il tipo di leva.
Il caso del remo di una barca è un tipo di leva che richiede particolare attenzione: può essere considerata come una leva di primo o secondo tipo a seconda della fase di utilizzo. Lo scalmo è il fulcro quando la pala sta per aria e la resistenza è rappresentata dal peso del remo fuori dalla barca; la pala è il fulcro quando sta immersa e quindi la resistenza (massa inerte della barca e forze di attrito) agisce sullo scalmo. Quando la pala sta per aria la leva è I tipo, quando la pala è immersa è II tipo.
Il caso della pagaia da kayak è ulteriormente complicato dal fatto che il fulcro, la resistenza e la potenza non giacciono sullo stesso asse: in un sistema di riferimento solidale alla terra infatti il fulcro è sempre l'acqua, la potenza è la sommatoria delle due mani e la resistenza è applicata alla chiglia della canoa, mentre in un sistema di riferimento solidale al kayak, il fulcro è rappresentato dalle mani.
Nel caso del sollevamento sugli avampiedi, anche se viene definita come leva di secondo genere e quindi sempre vantaggiosa, non bisogna farsi trarre in inganno a pensare che i muscoli gemelli debbano generare una forza minore rispetto al peso corporeo. Infatti, andando a calcolare la forza di reazione data dal terreno, troveremo un valore minore rispetto alla forza peso data dal corpo. Questo risultato può essere dimostrato assurdo in maniera empirica molto semplicemente dato che, se fosse vero tale risultato, sollevandoci sugli avampiedi sopra una bilancia, dovremmo veder diminuire il nostro peso. La considerazione che la caviglia sia una leva favorevole è data dal fatto che erroneamente si considera come unica forza agente sull'articolazione la forza peso del corpo, mentre su di essa agisce anche una forza di compressione data dalla contrazione muscolare (in questo caso dalla contrazione del gastrocnemio) tale da aumentare la forza resistente agente sulla leva. In questa situazione, per poter valutare in maniera più semplice il guadagno meccanico, conviene vedere la caviglia come una leva di primo genere, con il fulcro posizionato nell'articolazione della caviglia e spostando la forza resistente sull'avampiede, con intensità pari al peso del corpo.
Il tagliaunghie, in effetti, combina solitamente ad una leva di terzo genere (la parte dello strumento che di fatto taglia l'unghia) una leva di secondo genere (la parte dello strumento su cui si agisce con le dita).
</text>
</doc>
<doc id="1709357" url="https://it.wikipedia.org/wiki?curid=1709357">
<title>Laser</title>
<text>
Il laser (acronimo dell'inglese «light amplification by stimulated emission of radiation», in italiano "amplificazione della luce mediante emissione stimolata di radiazione") è un dispositivo optoelettronico in grado di emettere un fascio di luce coerente. Il termine si riferisce oltre che al dispositivo anche al fenomeno fisico dell'amplificazione per emissione stimolata di un'onda elettromagnetica. 
Nel 1917 Albert Einstein formulò le basi teoriche del laser e del maser nell'articolo "Zur Quantentheorie der Strahlung" (sulla teoria quantistica delle radiazioni) attraverso una riderivazione delle leggi sulla radiazione di Max Planck. Nel 1928 Rudolf W. Ladenburg dimostrò l'esistenza dell'emissione stimolata e dell'assorbimento negativo. Nel 1939, Valentin A. Fabrikant predisse l'uso dell'emissione stimolata per amplificare onde corte. Nel 1947, Willis E. Lamb and R. C. Retherford effettuarono la prima dimostrazione dell'emissione stimolata. Nel 1950 Alfred Kastler (vincitore del Nobel per la fisica nel 1966) propose il metodo per il pompaggio ottico confermato sperimentalmente due anni dopo da Brossel, Kastler e Winter.
Il primo maser venne costruito da Charles Hard Townes, J. P. Gordon, e H. J. Zeiger alla Columbia University nel 1953. L'apparecchio era simile ad un laser, ma concentrava energia elettromagnetica in un campo di frequenza notevolmente inferiore: utilizzava infatti l'emissione stimolata per produrre l'amplificazione delle microonde invece che di onde infrarosse o visibili. Il maser di Townes poteva erogare solo una minima potenza, circa 10 nW, ma Nikolay Basov e Aleksandr Prokhorov risolsero il problema teorizzando e sviluppando un "metodo di pompaggio" con più di due livelli di energia. Charles H. Townes, Nikolay Basov e Aleksandr Prokhorov ricevettero il nobel per la fisica nel 1964, ""per il lavoro fondamentale nel campo dell'elettronica quantistica, che ha portato alla costruzione di oscillatori e amplificatori basati sul principio maser-laser.""
La paternità dell'invenzione del laser non è stata attribuita con certezza ed il laser è stato oggetto di un trentennale contenzioso brevettuale.
Il 16 maggio 1960, Theodore H. Maiman azionò il primo laser funzionante a Malibù in California presso i laboratori della Hughes Research. Era un laser a stato solido che sfruttava il cristallo di rubino in grado di produrre un raggio laser rosso con una lunghezza d'onda di 694 nm. Sempre nel 1960 Ali Javan, William R. Bennett e Donald Herriott costruirono il primo laser utilizzando l'elio ed il neon, definito maser ottico a gas, in grado di produrre un raggio infrarosso. Nel 1963 K. Patel dei Bell Laboratories mette a punto il laser ad anidride carbonica. Tre anni prima Gordon Gould, che aveva incontrato e discusso con Townes, si era annotato vari appunti sull'utilizzo ottico dei maser e sull'utilizzo di un risonatore aperto, dettaglio poi successivamente comune in molti laser. Ritenendosi inventore del laser, Gordon Gould aveva depositato presso un notaio i suoi appunti, ma nel contenzioso legale che ne nacque, non gli venne riconosciuta dall'ufficio brevetti la paternità dall'invenzione. Nel 1971 Izuo Hayashi e Morton B. Panish dei Bell Laboratories disegnano il primo laser a semiconduttori (diodo laser) in grado di operare in continua a temperatura ambiente. Nel 1977 viene attribuito un brevetto per il "pompaggio ottico" a Gordon Gould e nel 1979 un brevetto descrive una grande varietà di applicazioni del laser, incluso riscaldamento e vaporizzazione dei materiali, saldatura, foratura, taglio, misurazione delle distanze, sistemi di comunicazione, sistemi di fotocopiatura oltre a varie applicazioni fotochimiche. Anche se non è mai stata attribuita a Gordon Gould l'invenzione del laser, per i suoi brevetti successivi, ha incassato, da chi ha sviluppato sistemi laser per applicazioni o con soluzioni da lui inventate, royalties milionarie.
La coerenza spaziale e temporale del raggio laser è correlata alle sue principali proprietà: 
L'emissione unidirezionale e coerente comporta la possibilità di raggiungere una irradianza o densità di potenza elevatissima a paragone di quella delle sorgenti luminose tradizionali.
Queste proprietà sono alla base del vasto ventaglio di applicazioni che i dispositivi laser hanno avuto e continuano ad avere nei campi più disparati:
Il laser è essenzialmente composto da 3 parti:
Nel laser si sfrutta il mezzo attivo, il quale possiede la capacità di emettere radiazioni elettromagnetiche (fotoni) quando attivato. Dal mezzo attivo dipende la lunghezza d'onda dell'emissione. Il mezzo attivo può essere gassoso (ad esempio anidride carbonica, miscela di elio e neon, ecc..), liquido (solventi, come metanolo, etanolo o glicole etilenico, a cui sono aggiunti coloranti chimici come cumarina, rodamina e fluoresceina) o solido (rubino, neodimio, semiconduttori ecc...). Il sistema di pompaggio fornisce energia al mezzo attivo portandolo all'eccitazione con emissione di fotoni. L'eccitazione può avvenire tramite:
Le radiazioni emesse vengono normalmente concentrate attraverso una cavità ottica con pareti interne riflettenti, ed una zona di uscita semiriflettente. Questa ultima superficie è l'unica che permette la fuoriuscita del raggio, il quale viene successivamente lavorato e riposizionato attraverso una serie di lenti e specchi per far sì che il raggio risultante abbia la posizione, concentrazione nonché ampiezza desiderate.
Come dice la stessa sigla (LASER → Light Amplification by Stimulated Emission of Radiation), la radiazione laser proviene dal processo di emissione stimolata:
Normalmente la luce che attraversa un materiale viene assorbita dal materiale stesso man mano che avanza, cioè cede energia agli atomi che incontra, eccitandoli, perché li trova in uno stato energetico "basso". Se però interveniamo eccitando gli atomi del materiale con una fonte di energia esterna, allora secondo l'analisi di Einstein le probabilità che avvengano l'emissione stimolata e l'assorbimento sono date dalla percentuale di atomi eccitati a fronte di quella di atomi nello stato energetico base:
dove B è il coefficiente di Einstein, N è la popolazione dello stato a energia E e N è la popolazione dello stato a energia E; (E &gt; E); ρ(ν) è la densità del campo di radiazione alla frequenza ν = (E - E)/h; Da questo si vede che se riusciamo a ottenere una inversione di popolazione, cioè se ci sono più atomi eccitati che atomi normali, la luce che attraversa il materiale "guadagnerà potenza" invece di perderla: cioè verrà amplificata dall'emissione stimolata degli atomi.
In condizioni di equilibrio N è sempre maggiore di N (perché le popolazioni dei due livelli sono descritte dalla distribuzione di Boltzmann formula_1, da notare l'esponente negativo) e quindi per ottenere prevalenza dell'emissione stimolata è necessario mantenere il sistema lontano dall'equilibrio, attuando l'inversione di popolazione.
La stimolazione o pompaggio di un laser può avvenire otticamente o elettricamente. La stimolazione ottica può essere effettuata da una lampada che avvolge il materiale attivo il tutto all'interno di uno specchio. In alternativa si può utilizzare una lampada lineare, ma il materiale attivo e la lampada devono essere posti nei fuochi di uno specchio ellittico in modo da far convergere tutti i raggi luminosi sul materiale attivo. La stimolazione elettrica invece avviene mediante l'applicazione di una differenza di potenziale ed è applicabile solo a materiali conduttori come, ad esempio, vapori di metalli.
I laser sono classificati in funzione del pericolo per la salute umana. La classificazione viene effettuata dal produttore secondo le norme IEC 60825 armonizzate nell'unione europea con le norme Cenelec EN 60825-1. Prima del 2007 i laser erano classificati in 5 classi (1,2,3a,3b,4) dipendenti dalla potenza e dalla lunghezza d'onda, considerando che le emissioni nella banda del visibile erano considerate meno pericolose grazie al riflesso palpebrale. Le norme attualmente in vigore dividono i laser in 7 classi, introducendo i parametri di:
La attuale classificazione introduce 2 classi M (M per magnificazione) a significare che il rischio è diverso se si utilizzano lenti, binocoli o strumenti ottici in grado di focalizzare sulla cornea il fascio laser. La classificazione attuale considera inoltre il limite massimo di emissione accessibile in funzione della durata della esposizione considerando il rischio connesso alla esposizione a impulsi o treni di impulsi ad alta potenza ma di durata molto breve.
Alcune ricerche hanno rilevato come l'esistenza del riflesso palpebrale per proteggere gli occhi non può essere assunto come regola.
L'ordinanza 16 luglio 1998 pubblicata nella Gazzetta Ufficiale n. 167 del 20 luglio 1998 vieta, su tutto il territorio nazionale, la commercializzazione di puntatori laser o di oggetti con funzione di puntatori laser di classe pari o superiore a 3 (&gt;1 mW), secondo la norma CEI EN 60825. L'ordinanza redatta quando erano in vigore le vecchie norme tecniche e classificazioni dei dispositivi laser, viene applicata vista l'analogia nella classificazione di rischio tra la vecchia e nuova normativa tecnica.
In relazione all'uso a cui è destinato, spesso è necessario poter disporre di un laser che, piuttosto che produrre una emissione continua di radiazione di una data lunghezza d'onda, produca invece brevi impulsi di intensità elevata. Per ottenere tale genere di laser si ricorre al Q-switching e al Mode-locking.
Il Q-switching è una tecnica che prende nome dal fattore Q, un parametro che esprime la qualità delle cavità risonanti, e permette di ottenere laser con impulsi dell'ordine dei nanosecondi (10 s). Il principio sfruttato, in pratica, consiste nell'inficiare momentaneamente l'effetto delle cavità con il risultato di ottenere una concentrazione di energia in un ristretto intervallo di tempo.
Il mode-locking è una tecnica grazie alla quale, modulando opportunamente le onde che pervengono nelle cavità risonanti, è possibile ottenere una intensa interferenza costruttiva con produzione di un raggio laser molto intenso a impulsi dell'ordine del picosecondo (10 s) e del femtosecondo (10 s).
Successivamente alla sua invenzione nel 1960, il laser è stato usato diffusamente per scopi medici. La funzione e risposta terapeutica dipendono in maniera complessa dalla scelta della lunghezza d'onda, dalla durata di irradiazione e dalla potenza del laser. Combinazioni diverse di questi parametri sono impiegate per trasformare l'energia luminosa in energia meccanica, termica o chimica. Generalmente gli effetti meccanici sono prodotti dall'applicazione di brevi impulsi (dell'ordine dei nanosecondi) ed alte energie.
In questo modo onde di stress meccanico possono essere prodotte con sufficiente forza per disintegrare calcoli urinari. Gli effetti termici si ottengono in funzione della energia assorbita dai diversi tessuti. Brevi impulsi laser vengono usati per ablare sottili strati di tessuto in chirurgia rifrattiva, utilizzando luce laser che penetra solo alcuni micrometri nel tessuto. La lunghezza d'onda della luce laser può essere scelta in modo tale che la luce sia assorbita selettivamente dal bersaglio. La coagulazione selettiva delle vene varicose in chirurgia estetica può essere compiuta usando una lunghezza d'onda assorbita selettivamente dall'emoglobina. L'impulso è scelto allora sufficientemente breve così da non arrecare danno al tessuto normale circostante, ma anche lungo a sufficienza da permettere la coagulazione sull'intero diametro del vaso. Con la criolaserforesi si sfrutta la permeazione della barriera cutanea per favorire la l'immissione di principi attivi per via cutanea.
Un altro importante uso medico del laser consiste nella correzione dei difetti refrattivi: miopia, astigmatismo e ipermetropia. In tutti questi casi il profilo della cornea - la superficie oculare trasparente - viene 'modellato' con varie tecniche (PRK e LASIK). Infatti, la cornea funziona come una lente naturale: modificandone la curvatura si varia il fuoco (il punto in cui i raggi luminosi convergono) e si può fare in modo che le immagini arrivino nitide sulla retina.
È importante sottolineare tuttavia che, quando ci si opera con il laser, il difetto visivo si corregge ma non si elimina: a livello organico un occhio miope, in quanto più lungo del normale in senso antero-posteriore, rimane della medesima lunghezza, ma questo difetto viene compensato da una correzione artificiale (è un po' come se si portassero delle lenti a contatto naturali permanenti). Anche se l'intervento generalmente ha buon esito, come tutti gli interventi chirurgici non può raggiungere il 100% dei successi. Ciò significa che talvolta può essere necessario portare ancora occhiali o lenti a contatto, sebbene di gradazione inferiore. L'eventuale insuccesso in genere non dipende tanto da un'imprecisione del macchinario, quanto piuttosto dal fatto che la cornea del paziente ha una cicatrizzazione anomala. 
L'intervento ha successo in più del 90% dei casi; non può provocare cecità incurabile; spesso si può rinunciare agli occhiali o alle lenti a contatto. In alcuni casi è necessario un secondo intervento. Arrivati alla quarta generazione di macchinari attuale, gli "effetti collaterali" sono: sensazione di corpo estraneo (da secchezza degli occhi), fastidi alla visione notturna, fotofobia, sdoppiamento delle immagini da astigmatismo, aloni, bruciore nei locali chiusi, occhi frequentemente arrossati. Le complicanze possono essere gravi in rari casi, tanto da impedire la guida notturna o il lavoro in ambienti a forte luminosità. Le complicazioni possono manifestarsi fin dai primi giorni dopo l'intervento e possono aggravarsi con gli anni. Se il trattamento è stato intenso per correggere forti difetti di vista e il lembo da rimuovere per l'incisione laser è cicatrizzato definitivamente, oppure se l'operatore del laser ha commesso errori nel sollevare e riporre il lembo superficiale della cornea, il danno è permanente. Talora, per rimediare a un intervento laser errato, è necessario ricorrere al trapianto di cornea.
Il laser retinico viene usato generalmente per cicatrizzare zone di retina malata, al fine di eliminarle o di fissare meglio la retina sana intorno a zone patologiche. L'obiettivo è quello di ottenere delle cicatrici che rinforzino l'adesione della retina agli strati sottostanti (la retina è simile alla pellicola di una macchina fotografica tradizionale su cui si imprimono le immagini). Per l'operazione si può impiegare un tipo particolare di strumento, l'"argon laser", il cui fascio luminoso con lunghezza d'onda dell'ordine dei 488 nm e 514 nm comporta un maggiore riscaldando superficiale dei tessuti ricchi di cromofori. Il forte riscaldamento provoca un'infiammazione a cui segue una risposta cicatriziale.
L'utilizzo del laser sulla superficie cutanea può avere finalità dermatologiche o estetiche.
Le lesioni vascolari superficiali (spider venosi, teleangectasie, emangiomi ecc. ) possono essere trattate con sorgenti laser che emettono a lunghezze d'onda che possono essere assorbite selettivamente dall'emoglobina e ossiemoglobina più che dai tessuti circostanti. Sono utilizzati laser KTP o "potassium titanyl phosphate " (532 nm), PDL o "pulsed dye"(585–595 nm), alessandrite (755 nm), diodici (800–810, 940 nm), Nd-YAG (1060 nm) a seconda della lesione vascolare da trattare. I primi 2 modelli sono preferiti nel trattamento di vasi di diametro inferiore al millimetro mentre gli ultimi tre modelli possono essere preferiti per lesioni di dimensioni superiori.
Il laser può essere utilizzato con funzione ablativa, quasi o non ablativa sulle lesioni cutanee che comportano una produzione irregolare di collagene.
I più comuni ablativi sono il laser CO (10600 nm) ed il laser erbio o Er-YAG (2640 nm). In origine è stato utilizzato anche il laser PDL (585 nm). Non ablativi o quasi ablativi invece le tecnologie a impulsi del laser Nd-YAG (1060 nm) e diodico (1450 nm). Recentemente è stata introdotta anche la tecnologia del laser frazionale (FRAXEL) .
Approccio analogo è stato sperimentato anche su cicatrici atrofiche da acne e strie distense.
Melasma, discromie cutanee, macchie iperpigmentate ma anche rimozione tatuaggi sono l'obiettivo di vari trattamenti con laser.
Si tende ad utilizzare laser ad impulso molto breve, normalmente con tecnologia Q-switching. Impulsi brevi o molto brevi comportano una pari efficacia ma un rischio minore di cicatrici e iperpigmentazioni rispetto a fluenze continue.
Sono privilegiati laser con impulsi da 10 a 300 ms che emettono nella lunghezze d'onda dove maggiore è la differenza relativa di assorbimento della luce della melanina del pelo rispetto all'emoglobina (circa da 650 a 1050 nm) e melanina della pelle. I più diffusi sono laser diodici (808 nm) in grado di erogare da 10 a 60 J/cm.
La tecnica corrente, chiamata dall'inglese "resurfacing", prevede un insulto termico prodotto dal laser che comporta la rimozione degli strati più superficiali che vengono sostituiti nel giro di poche settimane da strati completamente nuovi, dove normalmente sono meno evidenti i segni dell'invecchiamento. Il trattamento introdotto attorno al 1995 con laser CO ablativi ha visto l'utilizzo anche di laser Er-YAG, Nd-YAG e più recentemente di laser FRAXEL e laser non ablativi per ridurre il rischio cicatrici e discromie.
Il trattamento laser delle emorroidi con tecnica HeLP ("Hemorrhoidal Laser Procedure") è un intervento mininvasivo, eseguito senza alcun tipo di anestesia con un decorso post-operatorio rapido e indolore. Consiste nella chiusura, con un laser a diodi da 980 nm di lunghezza d'onda ed attraverso il canale dell'anoscopio, delle 12 arteriole che irrorano direttamente il plesso venoso emorroidario che va quindi incontro gradualmente ad ostruzione. Le arterie da chiudere vengono individuate in fase intraoperatoria con una sonda Doppler, specifica per questo tipo di tecnica.
Il laser viene utilizzato come tecnica non invasiva per la completa rimozione di tumori allo stadio iniziale. Nei tessuti viene iniettato una sostanza fotosensibile con un assorbimento selettivo nei tessuti malati. Al passaggio di un fascio di luce ad una determinata lunghezza d'onda, il farmaco attiva una reazione che ha per protagonista l'ossigeno, ossida e distrugge le sole cellule malate. Il fatto eccezionale è che il farmaco agisce selettivamente e le cellule sane non vengono intaccate, come purtroppo avviene durante un'asportazione chirurgica. Per tumori più estesi, serve a circoscrivere la metastasi, ma non guarisce la malattia.
Particolari laser argon cloruro eccimeri emettono nella banda dei 308 nm considerata ottimale per la fototerapia della psoriasi.
Analogo trattamento è considerato efficace per la vitiligine.
In fisioterapia sono diffusi laser con una irradianza tale da sviluppare un limitato calore sulla superficie corporea. Si tratta in genere di laser con emissione nell'infrarosso.
Il è un laser a stato solido che sfrutta un cristallo di ittrio e alluminio (YAG) drogato al neodimio (Nd:YAlO) emette normalmente a 1060 nm o 940 nm. Può essere utilizzato anche il laser a CO, che emette normalmente a 10600 nm. In fisioterapia sono possibili anche trattamenti laser a bassa potenza, cioè che non sviluppano alcun effetto termico sensibile, di cosiddetta: biostimolazione.
L'utilizzo militare delle tecnologie laser ha avuto immediata applicazione in sistemi di puntamento, telemetria ed accecamento. Nel 1980, il IV Protocollo della Convenzione delle Nazioni Unite su certe armi convenzionali, proibisce espressamente armi laser destinate all'accecamento dell'uomo. Nonostante sia entrato in forza il 30 luglio 1998, non ne vengono specificate le sanzioni per la sua violazione, e a marzo 2016 vi hanno aderito 106 nazioni. Il IV Protocollo non contempla il rischio di accecamento di umani che utilizzano strumenti di visione e l'accecamento di sistemi di visione elettronica.
Le ricerche sulla possibilità di danneggiare con un raggio laser proiettili, missili o aerei hanno ricevuto ingenti fondi, ma i risultati ottenuti hanno mostrato specifiche limitazioni della tecnologia. Nebbia, nuvole o tempeste di sabbia normalmente assorbono gran parte della potenza del raggio laser, inoltre dimensioni e peso del sistema d'arma per ottenere irradianze adeguate lo rendono poco maneggevole ed anche difficilmente trasportabile. La potenza necessaria ad alimentare un sistema d'arma da 100 KW va oltre i 400 KW con conseguenti problematiche di raffreddamento. Un altro limite intrinseco delle armi laser di alta potenza consiste nella defocalizzazione indotta dal calore dell'aria attraversata; fenomeno chiamato ""thermal blooming"".
Dagli anni 2000, le tecnologie laser hanno ricevuto spesso ingenti fondi, ma i risultati ottenuti sono sempre stati, almeno inizialmente, piuttosto modesti. I comandi militari hanno richiesto sistemi laser di elevata potenza (100 kW almeno) e maneggevoli, cioè apparecchiature trasportabili su mezzi cingolati o su gomma. I ricercatori sono stati in grado di realizzare laser di notevole potenza (anche diversi megawatt) e laser portatili, ma non sono stati in grado di realizzare sistemi che riunissero entrambe le caratteristiche. Già nel 2005, Il Pentagono aveva annunciato il progetto HELLADS ("High Energy Liquid Laser Area Defense System"): si trattava di un congegno in grado di combinare laser a stato solido e liquido, riducendo enormemente dimensioni e peso, così da permetterne il montaggio sugli aerei da caccia. L'arma era in grado di sprigionare una potenza di circa 1 kW, anche se per la fine dell'anno il DARPA aveva previsto di aumentare la potenza a più di 15 kW. Nel giro di 2 anni al massimo quindi, l'USAF avrebbe dovuto avere a disposizione un'arma laser per aerei da poco più di 1600 libbre (circa 750 kg).
Nel febbraio 2007 utilizzando un laser SSHCL ("Solid State Heat Capacity Laser") ricercatori statunitensi hanno dichiarato di aver raggiunto potenze di 67 kW con un dispositivo trasportabile. L'arma era in grado di sparare 200 volte al secondo un raggio di luce la cui lunghezza d'onda sarebbe stata di un micron, anche se si stava studiando la possibilità di sparare raggi con continuità verso un obiettivo da distruggere. I ricercatori sostennero inoltre che, dai 6 agli 8 mesi a quella parte, avrebbero potuto produrre un congegno in grado di sprigionare i tanto richiesti 100 kW. Sistemi come questi, sprigionano una grande potenza che può essere erogata continuamente per 2 minuti, al massimo, e richiedono 20 minuti per ricaricarsi.
Il 18 marzo del 2009 la Northrop Grumman Corporation affermò che i suoi ingegneri avevano costruito e testato con successo a Redondo Beach un laser trasportabile capace di raggiungere potenze sopra i 105 kW.
Tuttavia armi laser in grado di distruggere o danneggiare un obiettivo in combattimento, di cui si è molto discusso negli anni 2000, nell'ambito della strategia del surclassamento tecnologico, sono stati abbandonati; anche il sistema Tactical High Energy Laser sviluppato congiuntamente da Stati Uniti d'America ed Israele per intercettare proiettili di artiglieria o razzi è stato abbandonato nel 2006, nonostante la relativa efficacia contro razzi Katyusha o Qassam.
Nel novembre 2014, la United States Navy effettuò il primo montaggio e sperimentazione di un suo cannone laser. L'arma, denominata "LaWS", con la potenza di 30 kW, è in grado di danneggiare elicotteri, droni e piccole imbarcazioni, ma anche di far detonare materiale esplosivo e accecare i sistemi di puntamento dei bombardieri e delle navi nemiche. Il suo costo unitario era di circa 28 milioni di dollari ma il costo del singolo colpo laser è molto inferiore a quello di altri sistemi di intercettazione. Integrandole con altri sistemi d'arma, che non hanno le stesse limitazioni meteorologiche, si pensa che le armi laser di alta potenza avranno nei prossimi anni un importante sviluppo e non solo negli Stati Uniti d'America.
Contemporaneamente alla United States Navy, varie industrie come la Northrop, Raytheon Company e la Lockheed Martin hanno iniziato nei primi mesi del 2014 a produrre cannoni laser, con potenze e prestazioni sempre superiori. Nel giro di un anno, nel marzo 2015 la Lockheed affermò che "Athena", nome dell'arma, era in grado, pur con poco più di 30 kW di potenza, di perforare e sciogliere come burro la lastra del cofano di un pick-up da quasi un miglio di distanza, ovvero circa 1,6 km.
Il laser viene utilizzato nella tecnica in una gran varietà di apparecchiature: nelle telecomunicazioni e nelle reti di computer viene utilizzato per trasferire enormi quantità di dati attraverso le fibre ottiche nelle rispettive comunicazioni ottiche. Viene utilizzato come elemento di lettura nei player di CD e DVD e per la scrittura nei masterizzatori. È inoltre alla base di visioni di ologrammi nell'ambito della tecnica di foto 3D detta olografia.
In ambito industriale il laser viene utilizzato per tagliare o saldare lamiere in metallo anche di elevati spessori. Nel settore del packaging è utilizzato (generalmente in abbinamento ad una testa galvanometrica) per marcare date di scadenza, codici a barre e altre informazioni o per realizzare tagli ed incisioni. In metrologia grazie ai laser si possono effettuare delle misure di estrema precisione nel campo che va dai micron alle decine di metri. In campo edile vengono utilizzate sempre più spesso livelle laser. Si realizzano puntatori per sistemi d'arma, o più pacificamente, come indicatori per conferenzieri. Enormi laser permetteranno forse in un prossimo futuro di ottenere reattori nucleari a fusione efficienti. Lo SLAC-National Accelerator Laboratory presso l'Università di Stanford ha realizzato il più potente laser a raggi X al mondo, e i risultati sono stati pubblicati sulla rivista "Nature" a gennaio 2012.
Il Laser viene utilizzato anche per manipolare la materia a livello atomico. Il laser può essere utilizzato per saldare, dividere o forare elementi a livelli atomici, inoltre viene spesso utilizzato per raffreddare i composti a temperature prossime allo zero assoluto (qualche milionesimo di kelvin). Il raffreddamento si ottiene illuminando la materia con i fotoni, sotto opportune condizioni gli atomi assorbono il fotone e ne emettono uno a energia superiore perdendo di conseguenza energia. Si sta studiando la possibilità di utilizzare queste tecniche per raffreddare i semiconduttori.
Il laser può essere infine utilizzato nel mondo dello spettacolo per realizzare show, far comparire scritte o figure, animazioni. Un utilizzo che si presta a utilizzi in spazi interni, e soprattutto esterni (come nello spettacolo serale di fronte all'area tematica della valle dei re a Gardaland). Basti pensare che il più importante show italiano si è svolto il 10 marzo 2006 nello stadio Olimpico di Torino in occasione della Cerimonia di apertura dei IX Giochi Paralimpici invernali.
Il laser può tagliare i materiali in base a tre principi diversi: per vaporizzazione, per fusione o per combustione. In tutti e tre i casi, il processo di taglio si innesca e si mantiene grazie all'energia che il raggio laser può concentrare in un punto molto piccolo. A seconda del tipo di laser, del tipo di materiale e delle potenze in gioco può prevalere l'uno o l'altro meccanismo.
, laser ad argon, laser Q-switch e in generale tutti i laser che funzionano ad impulsi: taglio di metalli di piccolo spessore, taglio di plastica e materiali non ferrosi, marcatura, incisione, laser medicali. Ogni impulso scalda istantaneamente il materiale oltre il punto di vaporizzazione, asportandone un piccolo strato (si hanno centinaia o migliaia di impulsi al secondo). Il materiale intorno alla zona di taglio viene riscaldato molto poco. Un discorso a parte vale per i trapani laser per dentisti: questi usano una lunghezza d'onda che viene facilmente assorbita dalle molecole d'acqua. L'acqua presente nei tessuti o sulla superficie del dente assorbe l'energia dell'impulso laser e vaporizza istantaneamente, provocando una serie di microesplosioni che erodono smalto e dentina in modo più sicuro, più preciso, meno traumatico e doloroso di un trapano meccanico.
Laser CO2 ad onda continua di grande potenza, taglio di metalli di grande spessore. Il laser viene usato per portare a fusione un piccolo punto del metallo; il metallo fuso viene soffiato.
Laser CO2 a bassa potenza, bisturi laser. I laser a infrarossi a onda continua in uso in medicina tagliano per combustione: il raggio scalda il tessuto fino a far evaporare l'acqua contenuta in esso, e poi provoca la combustione del tessuto secco, che viene distrutto. La combustione del materiale asportato è spesso presente anche nei processi di taglio per fusione, dove può fornire un notevole contributo energetico. Questo tipo di taglio viene usato per fermare forti emorragie, poiché il laser a infrarossi causa la cauterizzazione della ferita.
I laser possono essere usati per la segnalazione di una emergenza, puntando il puntatore in cielo e usando un fascio a intermittenza.
Nel 2014 il laser entra a far parte dei vari sistemi di illuminazione utilizzati sulle automobili (alogeno, xeno, led). Le prime case automobilistiche ad usare questo sistema sono l'Audi, montandolo prima sulla vettura da competizione Audi R18 e-tron Quattro Laserlight ovvero l'auto utilizzata dal team Audi nel campionato Endurance e poi successivamente sulla Audi R8 che è stata anche la prima vettura di serie, e dalla BMW, che monta le luci al laser sulla BMW i8, autovettura ibrida della casa bavarese. Esse illuminano fino a 600 metri con un consumo di circa 10 Watt garantendo così una visibilità ottimale della strada fino a 250 km/h.
</text>
</doc>
<doc id="1706822" url="https://it.wikipedia.org/wiki?curid=1706822">
<title>Magnetismo</title>
<text>
In fisica il magnetismo è quel fenomeno per cui alcuni materiali sono in grado di attrarre il ferro nonché trasmettere tale capacità ad altri materiali.
Per estensione semantica, il magnetismo è anche la branca della fisica concernente il suddetto fenomeno. In particolare per fenomeni stazionari, ovvero non variabili nel tempo, si parla più specificatamente di magnetostatica (che presenta alcune analogie formali con l'elettrostatica allorché si sostituiscano alle distribuzioni di carica elettrica le densità di corrente elettrica).
Per fenomeni dipendenti dal tempo invece i campi elettrici e magnetici si influenzano a vicenda ed è necessario ricorrere ad una descrizione unificata dei due campi ottenuta nel 1864 dallo scienziato britannico James Clerk Maxwell all'interno della teoria dell'elettromagnetismo classico ovvero l'elettrodinamica classica.
L'esistenza di un magnetismo naturale era noto già agli antichi greci (V - VI secolo a.C.), ma probabilmente ancora precedentemente era stato scoperto nell'antica Cina dove, si dice, fosse in uso un rudimentale prototipo di bussola magnetica.
Pare che Archimede (287-212 a.C.) abbia cercato di magnetizzare le spade dell'esercito siracusano al fine di disarmare più facilmente i nemici.
Cristian Cardone attribuisce l'etimologia del termine "magnete" ad un pastore cretese di nome "Magnes", il quale scoprì casualmente le proprietà della magnetite appoggiandovi sopra il suo bastone con la punta in ferro.
Quello che è certo, comunque, è che gli antichi avevano scoperto la capacità di alcuni minerali (ad esempio la magnetite) di attrarre la limatura di ferro o piccoli oggetti ferrosi. Allo stesso modo, Tito Lucrezio Caro (99 a.C. - 55 a.C.) nel "De rerum natura" racconta un curioso esperimento elettromagnetico osservato a Samotracia (VI,or 1042 - 1048):
«Avviene anche, talora, che da questa pietra s'allontani la natura del ferro, solita a fuggirla e seguirla a vicenda. Ho visto anche sobbalzare anelli ferrei di Samotracia, e limatura di ferra infuriare entro bronzei bacili, sotto cui fosse stato posto il magnete: tanto il ferro si mostra impaziente di fuggir dalla pietra. Per il frapporsi del bronzo si crea tanta discordia […].»
Questa capacità di esercitare una forza a distanza ha dato fin dagli albori un particolare significato nei secoli al magnetismo. Tuttora nel XXI secolo si sente ancora talvolta parlare di "forze magnetiche" lasciando sottintendere un significato arcano e misterioso.
Il più importante studio medievale sull'argomento è certamente la "epistola de magnete" di Pietro Peregrino di Maricourt (del 1296), che tra l'altro introduce il concetto e la terminologia dei due poli (Nord e Sud) della calamita, spiega come determinarne con precisione la posizione, ne descrive le interazioni reciproche, attrattive e repulsive, e propone l'esperimento della calamita spezzata.
Nel 1600 apparve il "De magnete" di William Gilbert, che rimase a lungo il testo di riferimento sul tema del magnetismo.
I primi studi quantitativi sui fenomeni magnetostatici si possono far risalire alla fine del Settecento - inizio dell'Ottocento ad opera dei francesi Biot e Savart e, successivamente, di Ampère sempre in Francia.
Analogamente al caso elettrostatico anche nel magnetismo si individuano due sorgenti di campo di natura opposta che vengono convenzionalmente definiti poli. Come due cariche opposte si attraggono e due cariche simili si respingono lo stesso è per i poli magnetici.
Usando come magnete di riferimento la Terra si parlerà allora di polo nord e sud, in particolare il polo nord geografico corrisponde grossomodo al polo sud magnetico e viceversa.
Una proprietà interessante dei magneti naturali è che essi presentano sempre sia un polo nord che un polo sud. Se si divide in due parti un magnete, tentando di "separarne" i due poli, si ottengono due magneti del tutto simili (ciascuno con una coppia di poli opposti).
Poiché il processo può concettualmente proseguire all'infinito è ipotizzabile che il magnetismo naturale abbia origine nelle proprietà atomiche della materia. Considerando infatti ogni elettrone orbitale come una microscopica spira percorsa da corrente e tenendo anche conto del momento di spin si può intuire che collettivamente questi possano contribuire, in un mezzo materiale, a presentare un campo magnetico macroscopicamente osservabile.
In realtà occorre tenere conto del fatto che i moti di agitazione termica tendono, in generale, a disporre casualmente tutti questi microscopici dipoli magnetici, così che normalmente l'effetto magnetico complessivo è nullo. Solo in taluni minerali, i magneti naturali, i micromagnetini si "autodispongono" secondo direzioni comuni formando su scala macroscopica le cosiddette regioni o domini di Weiss con dipoli tutti orientati nella stessa direzione.
Nel Sistema Internazionale l'unità di misura del campo induzione magnetica B è il tesla (simbolo T), mentre per il campo magnetico H si usa l'ampere/metro (A/m). È invalso l'uso anche del sistema cgs in cui l'intensità del campo magnetico si misura in oersted ed il campo densità di flusso magnetico si misura in gauss.
I monopoli magnetici liberi, a tutt'oggi, non sono mai stati osservati sperimentalmente, sebbene previsti teoricamente negli anni '30 da Dirac e Majorana. Ciò conferisce una particolare proprietà alle linee di forza del campo magnetico: esse sono sempre chiuse e il flusso del campo attraverso qualsiasi superficie chiusa è nullo. Si può dimostrare che da ciò discende che il campo magnetico ha il medesimo flusso attraverso tutte le superfici che si "appoggiano" alla medesima curva chiusa. Un campo vettoriale con questa interessante proprietà è detto anche solenoidale.
Nel settembre 2009 tuttavia, è stato isolato in una struttura molecolare cristallina, un quasi-monopolo magnetico.
Particolarmente rilevante è l'esistenza di un magnetismo terrestre. Il nostro pianeta presenta infatti un debole magnetismo (circa 0,5 gauss) con distribuzione del campo grosso modo equivalente a quella generata da un dipolo magnetico disposto lungo la direttrice Polo Nord - Polo Sud lentamente variabile nel tempo. Il Polo Nord magnetico è spostato di circa 1.000 km da quello geografico e si trova attualmente in territorio canadese.
La definizione di poli nord e sud è legata alla proprietà di un ago magnetico libero di ruotare senza attriti attorno al suo baricentro di disporsi lungo le linee del suddetto campo di forze. Pertanto definendo il polo magnetico tipo "nord" quello dell'ago della bussola che si rivolge a Nord ne segue che il polo Nord terrestre è in realtà un polo Sud magnetico e viceversa.
</text>
</doc>
<doc id="1706832" url="https://it.wikipedia.org/wiki?curid=1706832">
<title>Energia</title>
<text>
L'energia è la grandezza fisica che misura la capacità di un corpo o di un sistema fisico di compiere lavoro, a prescindere dal fatto che tale lavoro sia o possa essere effettivamente svolto.
Il termine "energia" deriva dal tardo latino "energīa", a sua volta tratto dal greco ἐνέργεια ("enérgeia"), derivato di ἐνεργής (o l'equivalente ἐνεργός), 'attivo', composto dalla particella intensiva "en" e ἔργον ("ergon", 'lavoro', 'opera'). Il termine è stato introdotto da Aristotele in ambito filosofico per distinguere la δύναμις ("dýnamis"), la possibilità, la "potenza" propria della materia informe, dalla reale capacità (ἐνέργεια) di far assumere in atto realtà formale alle cose.
La parola italiana "energia" non è direttamente derivata dal latino, ma è ripresa nel XV secolo dal francese "énergie". «In Francia "énergie" è usato dal XV secolo nel senso di "forza in azione", con vocabolo direttamente derivato dal latino, mai con significato fisico. In Inghilterra nel 1599 "energy" è sinonimo di "forza o vigore di espressione". Thomas Young è il primo a usare, nel 1807, il termine "energy" in senso moderno»
Il concetto di energia può emergere intuitivamente dall'osservazione sperimentale che la capacità di un sistema fisico di compiere lavoro diminuisce a mano a mano che questo viene prodotto. In questo senso l'energia può essere definita come una proprietà posseduta dal sistema che può essere scambiata fra i corpi attraverso il lavoro (vedi Trasferimento di energia).
Il termine "energia" fu usato per la prima volta per indicare una grandezza fisica da Keplero nel suo "Harmonices Mundi" del 1619, tuttavia il termine "energia" fu introdotto sistematicamente nella letteratura scientifica in termini moderni solo a partire dalla fine del XIX secolo. Prima di allora si alternarono a seconda del contesto e dell'autore anche i termini "vis viva", "forza" o "lavoro". Il primo si conserva come tradizione storica ancora oggi nel nome di alcuni teoremi, mentre gli ultimi due termini hanno acquisito nella fisica moderna un significato completamente differente da quello dell'energia.
Storicamente, la prima grandezza simile a quella oggi indicata come energia cinetica apparve negli studi di Gottfried Leibniz nel 1686, chiamata con il nome di "vis viva" ("forza viva") in contrapposizione alla "vis mortua" ("forza morta") usata per designare l'inerzia. Il dibattito principale nella fisica del XVII e XVIII secolo era incentrato concettualmente non su un principio di conservazione, piuttosto sulla ricerca di una grandezza fisica che fosse in grado di misurare gli effetti dell'azione di una forza sui corpi, o in termini moderni di una interazione fra questi. Una forza che agisce su un corpo avrà l'effetto di modificare la sua velocità, così facendo cambieranno sia l'energia cinetica sia la quantità di moto formula_1 definita come:
A partire da queste due diverse possibilità nacque lo scontro fra Leibniz, che riteneva più adeguata come misura di una forza la "vis viva", e i sostenitori della teoria cartesiana, che utilizzavano invece la quantità di moto. Nella formulazione odierna della meccanica classica, entrambe le grandezze hanno la stessa importanza: come fu chiaro a partire da d'Alembert, il problema era unicamente legato all'uso di due punti di vista differenti. Infatti è possibile considerare gli effetti di una forza sommati rispetto a intervalli di tempo formula_3, da cui si ricava la variazione della quantità di moto direttamente in base al primo principio della dinamica:
Oppure è possibile considerare gli effetti di una forza sommati rispetto allo spazio, avendo in mente come esempio la compressione di una molla che frena un corpo in moto. Il risultato che si ottiene è che il lavoro formula_5 di una forza compiuto su un corpo è uguale al cambiamento dell'energia cinetica del corpo stesso:
In questo senso la differenza di energia cinetica o della quantità di moto finale e iniziale sono solo due misure diverse degli effetti dell'azione di una forza.
L'energia è una grandezza fisica estensiva (l'energia di due corpi è semplicemente la somma delle energie dei corpi presi singolarmente), che ha una importanza centrale nella formulazione di molte teorie, dalla meccanica classica alla termodinamica, dalla teoria della relatività alla meccanica quantistica.
Una precisa definizione di energia non è semplice da fornire, l'energia non ha alcuna realtà materiale ma è piuttosto un concetto matematico astratto che esprime un vincolo rispetto ai processi possibili e una simmetria temporale delle leggi fisiche. Non esiste quindi nessuna sostanza o fluido corrispondente all'energia pura. Come scrisse Feynman:
Un corpo può incrementare o diminuire la sua energia in seguito a una interazione con altri corpi: la variazione di energia riflette quindi i cambiamenti occorsi nelle sue proprietà microscopiche. Esistono numerose possibili interazioni; dal punto di vista qualitativo si possono distinguere la meccanica, con ad esempio urti fra corpi rigidi o forze fra particelle puntiformi, dalla termodinamica, dove si considerano ad esempio le reazioni fra gas a temperature differenti. Dal punto di vista del tipo di interazione, esistono in natura diversi tipi di forze, come quella gravitazionale, quella nucleare o quella elettrica. Tuttavia, tutti questi possibili processi lasciano invariata la quantità totale di energia, che quindi diviene la grandezza fisica costante per sistemi chiusi o isolati.
In ambito tecnologico l'energia permette, tramite il suo sfruttamento a livello industriale, la trasformazione di materie prime in prodotti o beni finali o direttamente la fornitura di servizi utili all'uomo e alla società.
La società moderna è estremamente dipendente dall'energia (in particolare nelle sue forme di energia meccanica, energia elettrica, energia chimica ed energia termica) in tutti i suoi processi produttivi e gestionali (ad esempio autotrazione, trasporto marittimo e aereo, riscaldamento, illuminazione, funzionamento di apparecchiature elettriche e processi industriali). Grande interesse e preoccupazione riveste dunque il problema energetico globale riguardo l'esaurimento nel tempo delle fonti fossili, la principale fonte di energia primaria, il cui utilizzo intensivo ha permesso il notevole sviluppo economico dalla prima rivoluzione industriale fino ai giorni nostri.
L'unità di misura derivata del Sistema Internazionale per l'energia è il joule (simbolo: J); in termini di unità fondamentali del SI, 1 J è pari a 1 kg·m·s. Nel CGS l'unità di misura per l'energia è l'erg, equivalente a 1 dyne·centimetro e in termini di unità base CGS a 1 g·cm·s (corrisponde a 10 J).
A seconda dell'ambito, altre unità di misura sono adottate per misurare l'energia:
L'energia meccanica è la somma di energia cinetica ed energia potenziale attinenti allo stesso sistema, da distinguere dall'energia totale del sistema E in cui rientra anche l'energia interna.
L'energia cinetica è l'energia che dipende unicamente dallo stato di moto del sistema preso in considerazione e da quello delle sue relative componenti. Per un corpo puntiforme l'energia cinetica formula_7 è uguale alla metà del prodotto della massa del corpo per il quadrato della sua velocità:
L'energia cinetica è una grandezza che può assumere solo valori positivi. Considerando corpi rigidi estesi non puntiformi, l'energia cinetica dipenderà anche dalla velocità angolare attraverso un termine aggiuntivo chiamato energia rotazionale.
La variazione dell'energia cinetica a seguito dell'azione di una forza è legata al lavoro, cioè al prodotto scalare della forza per la distanza dello spostamento effettuato. Il lavoro formula_5 di una forza compiuto su un corpo è infatti uguale al cambiamento dell'energia cinetica del corpo stesso:
in base al teorema energia-lavoro o teorema delle forze vive.
L'energia potenziale è un tipo di energia che dipende unicamente dalla configurazione o dalla posizione dei corpi e delle particelle in interazione.
A seconda del tipo di interazione e di forza considerata esistono numerosi tipi di energia potenziale. L'esempio più semplice di energia potenziale è quella posseduta da un corpo di massa formula_11 posto a un'altezza formula_12 nel campo gravitazione terrestre, uguale a:
dove formula_14 è l'accelerazione di gravità. Questo tipo di energia dipende solo dalla posizione di un corpo e quando questo viene lasciato cadere l'energia potenziale cambia durante il tempo la propria forma diventando cinetica. L'energia potenziale è definita a meno di una costante additiva, in questo esempio a meno della possibile scelta del punto rispetto a cui misurare l'altezza formula_12.
Il calore e il lavoro non possono essere definiti come "forme di energia", sebbene abbiano le sue stesse unità di misura, dato che non sono proprietà di un singolo corpo ma piuttosto sono proprietà della trasformazione termodinamica presa in considerazione. In altre parole, il calore e il lavoro non sono posseduti da un sistema e non sono quindi una variabile di stato, ma sono invece "energia in transito", la manifestazione sperimentale dello scambio di energia che avviene attraverso due sistemi. Il calore e il lavoro possono tuttavia essere misurati e utilizzati nella pratica per prevedere la differenza di energia posseduta da un corpo fra la fine e l'inizio del processo o della trasformazione.
In termodinamica il principio di conservazione dell'energia è contenuto nel primo principio della termodinamica, secondo il quale la variazione di energia di un sistema formula_16 è uguale alla somma del calore formula_17 e del lavoro formula_5 rispettivamente ceduto e compiuto dall'ambiente esterno al sistema:
Non tutta l'energia di un sistema è in grado di produrre lavoro in una trasformazione termodinamica, per via del secondo principio della termodinamica. La quantità di energia di un sistema disponibile per produrre lavoro può essere infatti molto minore di quella totale del sistema. Il rapporto tra l'energia utilizzabile e l'energia fornita da una macchina viene chiamato rendimento.
L'invarianza della quantità totale dell'energia è espressa dal principio di conservazione dell'energia, secondo il quale la variazione di energia in una regione di spazio è uguale al flusso netto di energia che fluisce verso lo spazio esterno. Sebbene l'espressione esatta dell'energia possa variare a seconda dei casi considerati, finora non è stato scoperto nessun processo in grado di incrementare o diminuire globalmente l'energia, questa può solo cambiare forma trasformandosi.
Il principio di conservazione ha guidato la scoperta di nuove forme di energia e ha permesso di scoprire nuovi tipi di processi fisici e perfino nuove particelle. Agli inizi del XX secolo furono scoperti alcuni decadimenti nucleari con emissione di elettroni che non sembravano soddisfare il principio di conservazione dell'energia. Per risolvere il problema nel 1924 Niels Bohr avanzò l'idea che a livello atomico l'energia non fosse strettamente conservata, proponendo una teoria che si rivelò errata. Wolfgang Pauli nel 1930 ed Enrico Fermi nel 1934, ritenendo fondamentale e tenendo ferma la conservazione dell'energia, postularono invece l'esistenza di nuove interazioni e di una nuova particella mai osservata prima che fosse in grado di trasportare l'energia che risultava mancante negli esperimenti. In questo modo, guidati dal principio di conservazione dell'energia, riuscirono a scoprire il neutrino, una particella priva di carica elettrica, effettivamente osservata sperimentalmente nel 1959.
Il principio di conservazione dell'energia riflette la simmetria temporale delle leggi fisiche rispetto a traslazioni temporali, il fatto cioè che queste non cambiano con lo scorrere del tempo. Un esperimento condotto a un tempo formula_20 fornirà lo stesso risultato dello stesso esperimento fatto nelle stesse medesime condizioni ma al tempo formula_21. Nella teoria della relatività, la conservazione dell'energia e la conservazione della quantità di moto sono riuniti in un'unica legge che corrisponde globalmente alla simmetria delle traslazioni nello spaziotempo quadridimensionale.
Il principio, nato nell'ambito dell'energia meccanica, può essere esteso anche a tutte le altre forme di energia a partire dal calore, dal momento che questo si ottiene per dissipazione di energia meccanica a livello macroscopico e trattasi di energia cinetica a livello molecolare, mentre tutte le altre forme di energia degradano inevitabilmente in calore.
Nella fisica classica, l'energia è una proprietà scalare continua immagazzinata da un sistema.
Nella meccanica quantistica invece per i sistemi legati (cioè i sistemi in cui l'energia della particella non supera le barriere di potenziale) è "quantizzata", cioè può assumere un numero discreto di valori (o "livelli energetici").
La celebre equazione di Einstein E=mc², diretta derivazione della teoria della relatività ristretta, mostra come massa ed energia siano due "facce della stessa medaglia" di un sistema fisico. Da questa semplice equazione si evince infatti che l'energia contribuisce all'inerzia di un corpo come la massa, cioè anche l'energia contribuisce alla resistenza di corpo a essere accelerato.
Le leggi quantistiche hanno mostrato che la massa può essere trasformata in energia e viceversa, nei processi nucleari ad esempio il decadimento dei metalli pesanti come l'uranio in elementi più leggeri comporta un difetto di massa corrispondente alla liberazione di energia sotto forma di radiazione.
Rispetto quindi alla meccanica classica, dove la massa e l'energia sono separatamente conservate, in relatività ristretta i due principi fisici possono essere fusi in un principio unico sotto la denominazione di principio di conservazione della massa/energia.
L'energia esiste in varie forme, ognuna delle quali ha una propria espressione in termini dei dettagli del sistema considerato, come la velocità o la distanza relativa fra particelle. Le principali forme di energia sono:
L'energia potenziale è quella posseduta da un materiale elastico sottoposto a deformazione. L'energia luminosa o radiante è l'energia trasportata dei fotoni che compongono la luce, quindi l'energia della radiazione elettromagnetica.
Le principali fonti di energia attraverso le quali è possibile produrre energia elettrica, energia termica o direttamente energia meccanica sono:
Con il termine "energie rinnovabili" si intendono quelle fonti di energia che non si esauriscono o si esauriscono in tempi che vanno oltre la scala dei tempi "umani" (ad esempio: energia solare, eolica, geotermica, mareomotrice, fusione nucleare), altrimenti si parla di "energie non rinnovabili" (ad esempio petrolio e carbone), mentre con il termine "energie alternative" si intendono le fonti di energia che possono essere impiegate in sostituzione dell'energia chimica prodotta dai classici combustibili o fonti fossili.
Si parla di "conversione" quando si passa da una forma di energia a un'altra, mentre si parla di "trasformazione" quando la forma di energia resta la stessa, ma se ne modificano alcuni parametri caratteristici.
Ad esempio una pila permette di convertire l'energia chimica in energia elettrica, mentre un trasformatore permette di trasformare l'energia elettrica variandone la tensione e l'intensità di corrente.
Ogni volta che avviene una conversione, una parte di energia (più o meno consistente) viene inevitabilmente convertita in energia termica; si parla in questo caso di "effetti dissipativi".
Nell'ambito della chimica degli alimenti, si parla di "valore energetico" per riferirsi all'energia che l'organismo umano può ricevere attraverso il consumo di un alimento.
Siccome parte dell'energia contenuta in un alimento può essere persa durante i processi digestivi e metabolici, il valore energetico può risultare minore rispetto al valore sperimentale ottenuto bruciando l'alimento in un calorimetro a bomba. Per tale motivo, sono stati messi a punto dei metodi sperimentali che tengono in conto tale perdite energetiche. Uno di questi metodi è l'utilizzo dei cosiddetti fattori di Atwater, grazie ai quali il valore energetico di un alimento viene calcolato a partire dal valore energetico associato ad alcuni dei suoi macronutrienti più importanti dal punto di vista energetico, in particolare: grassi, alcoli, proteine e carboidrati.
In Europa, il valore energetico è riportato per legge nell'etichetta nutrizionale dei prodotti alimentari, dove viene indicato in kcal o kJ per quantità di prodotto.
</text>
</doc>
<doc id="444218" url="https://it.wikipedia.org/wiki?curid=444218">
<title>Velocità di fase</title>
<text>
In fisica la velocità di fase è la velocità con cui si propaga la fase di un'onda, sia essa elettromagnetica o meccanica. La velocità di fase può essere visualizzata come la velocità di propagazione di una cresta dell'onda ma non coincide necessariamente con la velocità di propagazione di un segnale (che è più propriamente descritta dalla velocità di gruppo) e quindi può essere più alta della velocità della luce senza violare la relatività ristretta. Questo fenomeno è stato verificato sperimentalmente per le radiazioni elettromagnetiche e le onde sonore che in particolari circostanze riescono a superare la velocità della luce all'interno della cresta d'onda.
La velocità di fase viene definita come:
formula_1
Dove formula_2 è la pulsazione e formula_3 il vettore d'onda. Il grafico che descrive l'andamento della velocità di fase in funzione della frequenza (o della lunghezza d'onda) prende il nome di "curva di dispersione".
La differente velocità di fase in diversi mezzi fisici è responsabile del fenomeno della rifrazione, i cui effetti quantitativi sono espressi dalla legge di Snell.
</text>
</doc>
<doc id="338662" url="https://it.wikipedia.org/wiki?curid=338662">
<title>Intensità acustica</title>
<text>
L'intensità acustica o sonora è una grandezza fisica definita come il rapporto tra la potenza di un'onda sonora e l'area della superficie che da essa viene attraversata; oppure come l'energia che nell'unità di tempo attraversa l'unità di superficie posta in un punto perpendicolarmente alla direzione di propagazione del suono.
Nel Sistema Internazionale l'intensità acustica si misura in watt al metro quadrato, in simboli W/m.
L'intensità acustica è legata in modo indiretto al volume sonoro, ossia la qualità che distingue i suoni in deboli da quelli forti, grandezza legata alla psicoacustica.
L'orecchio umano è in grado di percepire intensità acustiche che variano in un intervallo molto grande (12 ordini di grandezza): si definisce soglia di udibilità il valore I = 10 W/m al di sotto del quale non è più possibile percepire alcun rumore, mentre si chiama soglia del dolore il valore I = 1 W/m al di sopra del quale si inizia a provare dolore fisico.
Vista l'ampia escursione delle intensità acustiche dei suoni udibili, si utilizza convenzionalmente una scala logaritmica (che possiede come punto di riferimento il valore della soglia dell'udibilità) definita livello di intensità acustica (Intensity level, IL), o livello sonoro, spesso misurata in decibel:
formula_2
Essendo l'intensità definita come il rapporto tra la potenza propagata dall'onda e la superficie che essa attraversa, nel caso di onde sferiche l'intensità è definita come:formula_3
L'intensità acustica, indicata con I, è definita dalla formula
dove
Sia I che v sono vettori, quindi hanno entrambi una "direzione" oltre che un modulo. La direzione dell'intensità acustica è la direzione media dell'energia sonora in questione.
L'intensità acustica media durante il tempo "T" è data da
</text>
</doc>
<doc id="418362" url="https://it.wikipedia.org/wiki?curid=418362">
<title>Sistema di riferimento non inerziale</title>
<text>
Un sistema di riferimento non inerziale è un sistema di riferimento nel quale la descrizione della dinamica dei corpi non vede verificato il principio di inerzia. Tutti e soli i sistemi di riferimento che si muovono di moto accelerato rispetto ad un qualsiasi sistema di riferimento inerziale presentano questa particolarità e possono essere quindi definiti non inerziali.
Un sistema di riferimento non inerziale è un sistema di riferimento in cui un corpo soggetto a una risultante delle forze esterne nulla si muove comunque di moto non uniforme, ovvero accelerato.
La descrizione di un evento di un sistema fisico può risultare differente, se operata da sistemi di riferimento differenti.
Le trasformazioni di Galileo stabiliscono le equazioni che permettono di passare dalla descrizione di un evento formula_1 da un sistema di riferimento inerziale formula_2 ad un altro inerziale formula_3: le grandezze che cambiano, da un sistema all'altro, sono la posizione e la velocità dei singoli corpi, ma l'eventuale accelerazione di un corpo risulta essere un'invariante per tutti i sistemi di riferimento inerziali.
Se si considera il punto di vista di un osservatore solidale con formula_4, ogni corpo solidale con un sistema di riferimento inerziale appare dotato di un'accelerazione pari a formula_5. Questa descrizione solidale con formula_4 non è simmetrica a quelle solidali con formula_7 e formula_2, perché l'osservatore "agganciato" a formula_4 non è in grado di individuare alcuna forza che sia responsabile dell'accelerazione dei corpi suddetti: questo osservatore è dunque costretto a "rinunciare" al principio di inerzia, e a constatare che oggetti e persone, se descritti dal proprio sistema di riferimento, possono subire variazioni della propria velocità senza che vi sia un'azione esterna a causarla.
Per "reintegrare" i principi della dinamica, l'osservatore nel sistema di riferimento non inerziale può fare appello alle cosiddette interazioni apparenti, postulare cioè ad hoc l'esistenza di forze e momenti legati ai corpi accelerati dal sistema di riferimento stesso.
È un esempio di forza apparente la forza centrifuga, percepita da un osservatore situato su di un sistema di riferimento in moto non rettilineo, che osservi un corpo solidale allo stesso.
È da notare infine come non abbia senso affermare che la descrizione della dinamica dei corpi in un sistema inerziale sia "più corretta" di quella effettuata in un sistema non inerziale: semplicemente, assumere la prospettiva del primo sistema è più funzionale ai fini di una rigorosa descrizione matematica dell'evento, perché permette di legare causalmente le forze con l'interazione con altri corpi, riguardo agli scambi di energia o di quantità di moto, ma ciò non toglie che in taluni casi sia invece più pratico considerare la prospettiva non inerziale.
</text>
</doc>
<doc id="436560" url="https://it.wikipedia.org/wiki?curid=436560">
<title>Riflessione interna totale</title>
<text>
La riflessione è il fenomeno, governato dalla legge della riflessione, per cui un'onda elettromagnetica che colpisce una superficie di separazione tra due mezzi, in parte prosegue il suo percorso deviandolo al di là della superficie, mentre in parte torna nella direzione da cui proveniva. In particolare, secondo la nota legge, detto formula_1 l'angolo di incidenza del raggio luminoso e detto formula_2 l'angolo formato dal raggio riflesso con la normale alla superficie, si ha che formula_3. Se invece si chiama formula_4 l'angolo formato dal raggio rifratto con la normale alla superficie, secondo la legge di Snell si ha che formula_5, detti formula_6 e formula_7 gli indici di rifrazione dei mezzi.
Si ha una riflessione interna totale quando l'angolo formula_4 raggiunga l'ampiezza di formula_9, cioè quando non esista più onda rifratta. Questo fenomeno può avvenire nel passaggio da un mezzo più denso a uno meno denso (ovvero, "n" &gt; "n") e l'angolo formula_1 tale per cui non esiste onda rifratta è detto angolo critico:
Quando θ &gt; θ non appare alcun raggio rifratto: la luce incidente subisce una riflessione interna totale ad opera dell'interfaccia. Si genera un'onda di superficie, o onda evanescente ("leaky wave"), che decade esponenzialmente all'interno del mezzo con indice di rifrazione "n".
La formula precedente è stata ottenuta ponendo nella legge di Snell formula_12 perché formula_13.
</text>
</doc>
<doc id="350632" url="https://it.wikipedia.org/wiki?curid=350632">
<title>Plasticità (fisica)</title>
<text>
In fisica e nella scienza dei materiali la plasticità è la capacità di un solido di subire grandi cambiamenti irreversibili di forma in risposta alle forze applicate. Esempi di materiali che esibiscono un comportamento plastico sono l'argilla e l'acciaio quando viene superato il limite di elasticità.
Per molti metalli, bassi livelli di carico applicati ad un campione di materiale determinano in questo un comportamento elastico: ad ogni incremento del carico corrisponde un aumento proporzionale della deformazione e, quando il carico viene rimosso, il campione ritorna esattamente alla sua configurazione originaria. Tuttavia, una volta che il carico eccede una certa soglia di resistenza (tensione di snervamento), la deformazione aumenta più sensibilmente rispetto al regime elastico e, rimuovendo il carico, una parte di questa continua a permanere sul campione scarico: è quello che definisce il comportamento plastico di un materiale. La fase di passaggio tra deformazione elastica e plastica è chiamata snervamento. La fase di snervamento è seguita tipicamente da una fase di incrudimento che porta alla rottura del materiale.
Le tre fasi descritte (elastica, snervamento ed incrudimento) sono pressoché sempre presenti nel comportamento di tutti i materiali, ma possono avere diversa estensione. In alcuni casi la fase di snervamento è molto estesa formula_1: si parla in tal caso di "materiali duttili "(acciaio dolce, rame, alluminio, ecc.). In altri lo snervamento può mancare del tutto: si parla di "materiali incruditi " (acciaio ad alta resistenza, ecc.). In altri ancora, anche la fase di incrudimento è molto ridotta e la fase elastica è seguita immediatamente dalla rottura: si parla in tal caso di "materiali fragili "(vetro, roccia, ecc.).
I materiali duttili sono pertanto quei materiali dove il fenomeno della plasticità acquista maggiore rilevanza, potendo questi sopportare grandi deformazioni prima che possano insorgere fenomeni di rottura. In particolare, si parla di comportamento "plastico perfetto" quando, nello snervamento, i materiali esibiscono grandi deformazioni plastiche irreversibili senza incrementi dello stato di sollecitazione.
Il fenomeno della plasticità è influenzato sia dalla temperatura che dalla velocità di applicazione dei carichi: bassi valori della temperatura (ed elevate velocità di deformazione) tendono a ridurre l'entità dei fenomeni plastici, mentre valori alti di temperatura (e deformazioni lente) tendono ad accentuare la presenza di tali fenomeni.
La plasticità, e quindi lo snervamento, sono rappresentativi, a livello macroscopico, di fenomeni che trovano spiegazione a livello microscopico, sulla scala dell'organizzazione molecolare del materiale. Nei materiali metallici, la plasticità può essere spiegata sulla base di modificazioni irreversibili del loro reticolo cristallino, cioè in termini della teoria delle dislocazioni (a tale conclusione pervennero contemporaneamente già nel 1934 gli studiosi Egon Orowan, Michael Polanyi e Geoffrey Ingram Taylor).
In altri materiali, come i polimeri, il comportamento plastico non può essere spiegato con la suddetta teoria delle dislocazioni, in quanto mancano di una regolarità di struttura molecolare (la struttura cristallina). In tal caso, la plasticità è spiegata come un effetto della sollecitazione che induce una regolarità, orientata secondo la sollecitazione, nel caos delle catene molecolari del polimero.
Il problema del comportamento nonlineare dei materiali (e delle strutture) era presente fin dai primordi della Meccanica, già ai tempi di Leonardo e Galileo. Tuttavia lo sviluppo di una moderna teoria matematica della plasticità ha incontrato in passato notevoli difficoltà a causa della complessità del fenomeno da rappresentare. Tale complessità è dovuta sia al carattere irreversibile del fenomeno plastico, sia al suo carattere anolonomo, nel senso che la deformazione finale raggiunta dipende non solo dal valore finale del carico, ma anche dal percorso di carico, cioè dalla storia passata della modalità di applicazione del carico stesso.
I primi studi moderni sul comportamento elasto-plastico delle strutture risalgono alla seconda metà del XIX secolo. Fra gli autori più importanti in questa fase si ricordano Tresca, S. Venant e Levy. Un nuovo sviluppo della teoria si è avuto poi agli inizi del XX secolo, in special modo ad opera di von Mises e von Karman.
Attorno al 1940 è stata sviluppata, particolarmente ad opera della scuola russa di Nadai ed Iliushin, una teoria della plasticità in termini finiti nota come "deformation theory". Tale teoria si basa essenzialmente sull'assunzione di un legame tra tensioni formula_2 e deformazioni formula_3 in termini globali del tipo
formula_4
e riferendosi essenzialmente a processi di carico che non comportino ritorni in fase elastica di parti della struttura precedentemente plasticizzate. In tal modo il problema elasto-plastico veniva trattato come una sorta di problema elastico nonlineare.
Più recentemente una teoria diversa si è imposta nel panorama degli studi meccanici della plasticità. Essa, nota come "Flow theory" o "Teoria incrementale della plasticità", è essenzialmente legata ai nomi di Melan, Prager (1930-40), Hodge, Hill, Drucker, Budiansky, Koiter (1950-60), Maier, Mandel (1960-70). Tale teoria riflette un punto di vista incrementale, studia cioè le relazioni tra gli incrementi infinitesimi di carico formula_5 e i corrispondenti incrementi della soluzione in termini di tensioni, deformazioni e spostamenti formula_6, nota la situazione preesistente in termini di carico, deformazioni e tensioni. Tale approccio si è rivelato più significativo ed efficace nel cogliere la natura anolonoma del comportamento elasto-plastico.
Alcuni materiali, specialmente quelli predisposti a subire transizioni martensitiche, si deformano in modi che non sono descrivibili in termini delle teorie classiche della plasticità e dell'elasticità. Uno tra gli esempi più noti è il nitinol, che presenta pseudoelasticità: le sue deformazioni sono reversibili nel quadro meccanico, ma irreversibili in termini termodinamici.
</text>
</doc>
<doc id="344985" url="https://it.wikipedia.org/wiki?curid=344985">
<title>Onda stazionaria</title>
<text>
Un'onda stazionaria è una perturbazione periodica di un mezzo materiale, le cui oscillazioni sono limitate nello spazio: in pratica non c'è propagazione lungo una certa direzione nello spazio, ma solo un'oscillazione nel tempo.
Pertanto, è soltanto il profilo dell'onda stazionaria a muoversi, oscillando "su e giù" in alcuni punti. I punti ove l'onda raggiunge ampiezza massima sono detti "antinodi" (o "ventri"), i punti che invece rimangono fissi (ove l'onda è sempre nulla) sono detti "nodi".
All'equazione d'onda unidimensionale devono essere fornite opportune condizioni al contorno, che limitano il moto. Se formula_1 è lo spazio di propagazione dell'onda, l'equazione diventa:
alla quale imponiamo le condizioni al contorno: formula_3. La soluzione più generale è della forma:
Le onde stazionare possono essere viste, in base alla prima formula di Werner, come l'interferenza tra due onde sinusoidali contrarie della stessa frequenza e di ampiezza dimezzata, come illustrato in figura:
Una caratteristica delle onde stazionarie è che ad esse non è associato alcun trasporto di energia, corrispondentemente al fatto che l'onda non si propaga nello spazio.
Un esempio di onda stazionaria è la corda di una chitarra, cioè una corda fissata a due estremi e messa in vibrazione. Dopo una fase transitoria, nella corda in vibrazione si sovrappongono, punto per punto, due "movimenti". Vi è una certa simultaneità negli eventi che, come detto, si sovrappongono. Il primo movimento si verifica spostando la corda verso l'alto o verso il basso (lungo un asse perpendicolare alla corda), per esempio pizzicandola come nel caso di una chitarra. Poiché la corda, elasticamente, tende a tornare nella posizione iniziale, questo spostamento perpendicolare si propaga per tutta la lunghezza della corda, finché giunge ad un estremo. Il "secondo movimento", allora, rimbalza e torna indietro. Intanto, però, la corda possiede ancora il primo movimento, per inerzia; allora, lo spostamento che "ritorna" si sovrappone a quello che "arriva". Ecco che due onde uguali si propagano lungo la corda in sensi opposti. Sovrapponendosi, esse possono produrre un'interferenza distruttiva, fino ad annullarsi, oppure costruttiva, fino a raggiungere un'ampiezza di oscillazione massima.
Le due onde hanno caratteristiche (periodo, lunghezza d'onda…) identiche. A causa della loro uguaglianza e degli estremi della corda fissi, esse si sovrappongono in un modo ben determinato: allora i punti in cui si annullano sono sempre gli stessi e allo stesso modo risultano stabiliti anche quelli in cui l'ampiezza può risultare massima. La forma d'onda che si ottiene, cioè l'onda risultante, non si propaga verso l'uno o l'altro estremo: si è ottenuta un'onda stazionaria.
Un altro esempio è quello di un secchio (o una vasca) pieno d'acqua, dove un'onda incidente riflette contro la superficie verticale del bordo, provocando un'onda riflessa: questa non si può distinguere da quella incidente, perché (come prima) sono sovrapposte ed hanno i nodi in comune.
Nel 1890 il fisico tedesco Otto Wiener scopre sperimentalmente che la luce può formare onde stazionarie. A partire dalle esperienze del 1888 di Hertz, egli riesce ad imprimere gli antinodi luminosi su di una sottilissima pellicola fotografica, posta fra una sorgente luminosa e uno specchio metallico. Suppone poi che, all'interno della teoria elettromagnetica ancora in evoluzione in quegli anni, essi siano gli antinodi del campo elettromagnetico: campo elettrico e campo magnetico, infatti, in un'onda luminosa stazionaria risultano sfasati di mezza lunghezza d'onda l'uno con l'altro, a causa dei diversi comportamenti nella riflessione.
Questo esperimento si trovò dunque in accordo con i risultati di Hertz e fu, così, dimostrato che le radiazioni elettromagnetiche provenienti dai circuiti (dipolo hertziano) e la luce hanno gli stessi comportamenti (rifrazione, riflessione, formazione di onde stazionarie): la luce è una radiazione elettromagnetica.
L'onda elettromagnetica stazionaria assume particolare importanza pratica nelle radio trasmissioni, in quanto
il rapporto di onda stazionaria è una misura del disaccoppiamento di impedenza tra la linea di trasmissione ed il suo carico. Quanto più questo rapporto si discosta da 1 (valore ideale), tanta più energia erogata dal trasmettitore viene riflessa indietro, piuttosto che trasmessa, con la possibilità concreta di danneggiare irrimediabilmente il trasmettitore stesso.
In idrodinamica un liquido può configurarsi con onde di pressione stazionarie (come si vede per esempio sul ciclone esagonale di Saturno).
</text>
</doc>
<doc id="346668" url="https://it.wikipedia.org/wiki?curid=346668">
<title>Scorrimento viscoso</title>
<text>
Lo scorrimento viscoso (talvolta chiamato impropriamente col termine inglese: "creep") è la deformazione di un materiale sottoposto a sforzo costante che si verifica nei materiali mantenuti per lunghi periodi ad alta temperatura. Tale fenomeno è presente nei materiali viscoelastici (tra cui l'acciaio, il calcestruzzo e le materie plastiche).
Il fenomeno duale, cioè la diminuzione nel tempo delle tensioni inizialmente create, a deformazione costante, è detto rilassamento degli sforzi.
Lo scorrimento viscoso si manifesta al di sopra della temperatura di scorrimento (Ts), coincidente indicativamente con la temperatura di ricristallizzazione e approssimabile, in media, alla metà della temperatura di fusione misurata in kelvin.
Si possono distinguere tre fasi principali quando il processo avviene a T &gt; Ts e sollecitazione costante:
In caso di T &lt; T, lo sforzo costante induce una deformazione elastica e plastica senza che questa continui fino a rottura: non vi è infatti abbastanza energia per muovere le dislocazioni meno favorevolmente orientate, quindi ad un certo punto la deformazione si arresta.
Il meccanismo che origina lo scorrimento viscoso è dovuto ad una competizione continua tra processi di incrudimento e di restaurazione strutturale del materiale, che regolano il moto delle dislocazioni.
Nel primo stadio prevale l'incrudimento e quindi la velocità di deformazione diminuisce nel tempo; nel secondo stadio i due processi si bilanciano e la deformazione prosegue in quanto il rilassamento strutturale riduce il tasso di incrudimento. Infine nel terzo stadio avviene la rottura del materiale, in seguito ad uno scorrimento tra grani.
Lo scorrimento viscoso è un fenomeno termicamente attivato e la velocità di deformazione può essere descritta da una legge di Arrhenius.
Dato che T è circa pari a 0,4 T, per la costruzione di componenti che debbano essere utilizzati a temperatura elevata si preferiscono materiali altofondenti.
Escludendo il terzo stadio, il creep è influenzato dalla presenza di numerosi piani di facile scorrimento; quindi sono svantaggiati i metalli con reticolo C.F.C. (cubico a facce centrate) mentre la resistenza al creep risulta importante per materiali ingegnerizzati (materiali con bordi colonnari) o ancora meglio, resistono al creep materiali monocristallini (per esempio palette per le turbine).
Lo slittamento delle dislocazioni può inoltre essere diminuito grazie all'inserimento di elementi alliganti; oligo-elementi quali boro e zirconio, invece, ostacolano lo slittamento dei bordi dei grani.
Per migliorare la resistenza allo scorrimento viscoso sono utili i trattamenti che contribuiscano a ridurre la superficie dei giunti, come ricottura, normalizzazione, solidificazione direzionale, creazione di oggetti in monocristallo.
La resistenza può essere aumentata anche da operazioni quali per esempio deformazioni plastiche preventive a freddo che ostacolino le dislocazioni, la precipitazione di particelle all'interno dei cristalli sotto alla temperatura di equicoesione o tra i cristalli per temperature superiori.
La produzione di metalli resistenti al creep avviene spesso sotto vuoto per eliminare le impurezze gassose (in particolare l'ossigeno, che potrebbe ossidare l'alluminio e il titanio quasi sempre presenti).
Il ritiro e il fluage rappresentano le deformazioni differite del calcestruzzo; nel caso dello scorrimento viscoso si parla di "deformazione differita nel tempo a struttura carica".
Il fenomeno dello scorrimento viscoso del calcestruzzo dipende dalla parziale migrazione dell'acqua chimicamente non combinata verso i vuoti disponibili, il che ha come conseguenza una contrazione volumetrica del gel di cemento.
È quindi legato alla composizione del calcestruzzo, alle dimensioni dell'elemento e all'umidità relativa dell'ambiente ma anche all'entità dei carichi di lunga durata applicati alla struttura e alla maturazione del calcestruzzo al momento dell'applicazione dei carichi.
Nel diagramma deformazione-tempo di un provino di calcestruzzo, in fase di carico la deformazione totale al tempo t è formata di una parte elastica ε, che non varia nel tempo, e che si manifesta immediatamente all'atto dell'applicazione dei carichi, e di una parte ε che rappresenta la deformazione viscosa in funzione del tempo e del carico applicato.
Il valore di ε con il tempo tende asintoticamente ad un valore ε che è pari a circa 2 - 3 volte ε.
Se si procede allo scarico del provino in corrispondenza di un generico istante t, si osserva un ritorno elastico istantaneo ε inferiore a ε.
A tale ritorno elastico fa seguito, a provino scarico, un ritorno elastico differito ε.
Permane una deformazione residua ε che rappresenta la deformazione irreversibilmente acquisita dal provino.
Nella comune prassi progettuale non si effettuano specifici calcoli per valutare l'effetto dello scorrimento viscoso.
Di esso però si tiene conto implicitamente in numerosi casi:
</text>
</doc>
<doc id="3223566" url="https://it.wikipedia.org/wiki?curid=3223566">
<title>Riflessione speculare</title>
<text>
La riflessione speculare consiste nella riflessione osservata quando un singolo raggio incidente che forma un angolo formula_1 con la normale produce un singolo raggio riflesso con angolo formula_2 rispetto alla normale con il verificarsi dell'uguaglianza formula_3, in accordo con la legge di riflessione. Raggio incidente, normale e raggio emergente giacciono sullo stesso piano.
In genere tutti i materiali sono in grado di riflettere specularmente la luce, purché sia possibile "lucidare" la loro superficie, cioè eliminarne tutte le irregolarità che siano confrontabili con lunghezza d'onda della luce (da 0,4 a 0,7 micrometri). Spontaneamente regolari, tra i materiali comuni, sono le superfici dei liquidi, che si dispongono con una superficie perfettamente piana, o comunque liscia, e il vetro, che in fondo è anch'esso un "liquido immobilizzato", perché la sua struttura amorfa fa sì che le molecole, libere dalle rigide geometrie dettate dalla struttura cristallina, durante la solidificazione possano seguire le tensioni superficiali, che invece impongono superfici microscopicamente lisce. Solo i metalli, però, possono riflettere con efficienza una parte sostanziale della luce che li colpisce: nei normali specchi il materiale riflettente, infatti, è alluminio o argento. Tutti gli altri materiali comuni, anche se "lucidati a specchio", riflettono solo piccole frazioni di luce, che dipendono dall'angolo di incidenza della luce, ma che in genere non superano il 5-10%. Tranne che in condizioni particolari, come nei prismi di vetro usati in "riflessione totale"; o in materiali complessi, appositamente strutturati, come la pelle argentea di molti pesci. 
</text>
</doc>
<doc id="3353371" url="https://it.wikipedia.org/wiki?curid=3353371">
<title>Grado di cristallinità</title>
<text>
Il grado di cristallinità di un materiale (spesso indicato con "ϰ") è una grandezza che indica la percentuale di materiale che si trova allo stato cristallino rispetto alla quantità totale.
A seconda del modo in cui esprimiamo la quantità di materiale, si parla più precisamente di grado di cristallinità "massico" (ϰ) oppure "volumetrico" (ϰ).
I valori del grado di cristallinità dipendono sia dalla natura chimica del materiale sia dal modo in cui esso è prodotto, lavorato e utilizzato.
In generale, il grado di cristallinità varia da 0 (per materiali completamente amorfi) a 1 (per materiali completamente cristallini). I materiali che presentano valori del grado di cristallinità compresi tra 0 e 1 sono detti "semicristallini".
Per la stima del grado di cristallinità si possono utilizzare le seguenti metodologie:
In particolare la densimetria è una tecnica che consiste nell'indagare la variazione del volume specifico al variare della temperatura; dai risultati di tale metodica è possibile stabilire se un materiale è amorfo, semicristallino o cristallino:
Le materie plastiche sono costituite da lunghe catene polimeriche; il grado di cristallinità dei materiali polimerici è maggiore se tali catene polimeriche sono allineate tra loro. La presenza nella macromolecola di gruppi funzionali stericamente ingombranti e la mancanza di tassia ostacolano l'allineamento delle catene polimeriche, quindi abbassano il grado di cristallinità.
Quindi i polimeri sindiotattici presentano maggiore cristallinità rispetto ai polimeri atattici.
Nel caso delle materie plastiche, il grado di cristallinità è in genere intorno al 20÷80%. Nel caso del polietilene ad alta densità (HDPE) si possono raggiungere valori intorno al 95%. Anche le fibre aramidiche (tra cui Nomex e Kevlar) presentano elevati valori di cristallinità.
In ambito geologico, le rocce possono essere classificate nelle seguenti categorie, in base al loro grado di cristallinità:
Materiali con un alto grado di cristallinità sono in genere più duri, più densi, meno trasparenti e presentano un coefficiente di diffusione di materia minore.
Ad esempio, il PET in forma cristallina presenta una densità di 1,499 g/cm, mentre in forma amorfa presenta una densità di 1,336 g/cm.
</text>
</doc>
<doc id="3177027" url="https://it.wikipedia.org/wiki?curid=3177027">
<title>Polo (elettrotecnica)</title>
<text>
Un polo, in elettrotecnica, si indica ciascuno dei morsetti elettrici di un componente elettrico. 
Un componente elettrico dotato di 2 poli viene detto bipolo, mentre un componente elettrico dotato di 4 poli viene detto quadripolo. 
In un bipolo (ad esempio una cella galvanica) operante in regime di corrente continua, si distingue tra un "polo positivo" (in cui si ha un potenziale elettrico più elevato e una carica positiva, ovvero carenza di elettroni) e un "polo negativo" (in cui si ha un potenziale elettrico più basso e una carica negativa, ovvero eccesso di elettroni).
Nel caso di componenti elettrici operanti in regime di corrente alternata, ogni polo può essere alternativamente positivo e negativo.
Nelle celle elettrochimiche:
Collegando i morsetti di una cella elettrochimica con un circuito esterno, il passaggio di elettroni al di fuori della cella avviene dal polo negativo al polo positivo nel caso delle celle galvaniche, mentre nel caso delle celle elettrolitiche avviene in senso inverso (dal polo positivo al polo negativo).
Nelle batterie, il polo positivo è riconoscibile dal segno "+" impresso su un lato della batteria, mentre il polo negativo è riconoscibile dal segno "-" e la corrente per convenzione scorre sempre dal polo positivo al polo negativo.
In particolare, nel caso di pile di sezione circolare (quali ad esempio le stilo e le mini-stilo) il polo negativo è piatto, mentre il polo positivo è più rialzato. In altre parole, "mettendo in piedi" la batteria il polo positivo è l'estremità più alta, mentre il polo negativo è l'estremità su cui si poggia.
</text>
</doc>
<doc id="2387746" url="https://it.wikipedia.org/wiki?curid=2387746">
<title>Pigmento</title>
<text>
Un pigmento è una sostanza utilizzata per modificare il colore di un materiale. Ciò che distingue un pigmento da un colorante è l'incapacità dei pigmenti di sciogliersi sia nei comuni solventi (come l'acqua) sia nella superficie da colorare, per cui nel caso dei pigmenti si parla di "dispersione".
La classificazione dei pigmenti si basa sulla loro natura ed origine, per cui i pigmenti possono essere suddivisi principalmente in:
La classificazione chimica più adottata è quella della AATCC ("American Association of Textile Chemists and Colorists") nel Colour Index, insieme ai coloranti, in base alla struttura chimica.
Le caratteristiche principali di un pigmento sono:
I pigmenti sono commercializzati in polvere, pasta o dispersi in un mezzo appropriato, ma si tratta in ogni caso di particelle estremamente fini, dell'ordine dei micrometri o anche meno.
In natura i pigmenti inorganici si trovano in rocce e minerali e spesso richiedono lunghe lavorazioni per essere purificati; un esempio noto a tutti è il blu oltremare che nel passato veniva estratto dal prezioso lapislazzuli.
In biologia, il pigmento è ogni materiale colorato presente nelle cellule vegetali e animali. Molte strutture biologiche, come pelle, peli, occhi e capelli contengono pigmenti, come la melanina, presenti in cellule specializzate dette cromatofori.
Nei vegetali, i pigmenti sono contenuti nei plastidi, specificatamente nei cloroplasti, e sono rappresentati da clorofille (verde), carotenoidi e flavonoidi (giallo-arancione), tannini, presenti nella corteccia, e pigmenti florali, tra cui gli antociani.
Una qualunque sostanza ci appare colorata solo in presenza di luce perché è formata da molecole in grado di assorbire selettivamente la luce incidente a ben determinate fasce di lunghezza d'onda, riflettendo il resto. Nel caso del bianco tutte le lunghezze d'onda vengono riflesse, mentre nel nero tutte le lunghezze d'onda vengono assorbite.
Un fotone che colpisce una molecola del pigmento, eccita un elettrone facendolo passare da uno stato fondamentale ad uno stato eccitato su di un orbitale più esterno. Solo i fotoni di una ben determinata energia sono in grado di farlo e sono quelli corrispondenti alle lunghezze d'onda assorbite. La luce riflessa sarà priva di queste lunghezze d'onda e ci apparirà colorata. L'energia assorbita dall'elettrone viene normalmente restituita a lunghezze d'onda che cadono all'esterno della fascia visibile (ad esempio nell'infrarosso) e quindi non sarà percepibile dal nostro occhio.
Il principale campo di applicazione dei pigmenti è quello delle vernici, ma trovano largo impiego anche nei seguenti campi: 
I pigmenti sono inoltre impiegati nella realizzazione delle opere d'arte e alla loro stabilità si deve la resistenza al tempo.
Esistono anche pigmenti luminescenti che si caricano quando sono esposti alla luce e la rilasciano al buio.
La chimica dei coloranti e dei pigmenti immette sul mercato nuovi prodotti in misura inferiore rispetto al campo farmaceutico. 
La ragione principale risiede nei costi di ricerca, brevetto, e produzione, che non sempre vengono ripagati in tempi accettabili. 
Tuttavia, esistono molteplici pubblicazioni accademiche e private che indagano lo sviluppo di nuovi pigmenti in applicazioni innovative, come nell'optoelettronica, o nei settori più maturi, come la tintura di materiali plastici (aspirinato di rame).
</text>
</doc>
<doc id="2577458" url="https://it.wikipedia.org/wiki?curid=2577458">
<title>Differenza di potenziale elettrico</title>
<text>
La tensione elettrica (impropriamente indicata con il termine "voltaggio", francesismo derivato da "voltage") indica l'energia o il lavoro necessari a separare cariche elettriche di segno opposto. 
Coincide con la differenza di potenziale elettrico, ovvero con la differenza tra il potenziale elettrico di due punti dello spazio. Si tratta della differenza tra l'energia potenziale elettrica posseduta da una carica nei due punti a causa della presenza di un campo elettrico, divisa per il valore della carica stessa. In condizioni stazionarie è pari al lavoro compiuto per spostare una carica unitaria attraverso il campo da un punto all'altro, cambiato di segno.
Si misura con un voltmetro, in genere integrato in un "tester" elettrico. Nell'ambito del sistema internazionale di unità di misura, l'unità di misura della differenza di potenziale elettrico è il volt (V).
La definizione di "tensione elettrica" si deve ad Alessandro Volta, che, accanto ai concetti di "capacità elettrica" e di "carica elettrica", usa per la prima volta il concetto di "tensione elettrica" per rendere conto delle proprietà intensive ed estensive dell'elettricità. Volta ne parla nei suoi studi relativi alla scoperta della pila voltaica (la prima batteria elettrochimica).
In un circuito elettrico alimentato da un generatore di tensione ideale la differenza di potenziale elettrico tra i due poli del generatore è pari alla forza elettromotrice. Essa aumenta quanto più crescono: carica elettrica totale, distanza e resistenza elettrica, intercorrenti tra le cariche elettriche. Nel caso si consideri un generatore reale, la tensione agli estremi del generatore è minore a causa della caduta di potenziale relativa alla resistenza interna del generatore.
L'energia erogata dal generatore può venire dissipata nel circuito in diversi modi, ad esempio attraverso carichi resistivi o sovratensioni, nel caso in cui siano presenti celle elettrochimiche.
Facendo un'analogia con un circuito idraulico, alla differenza di potenziale si può associare la differenza di pressione che si genera in un tubo chiuso pieno di liquido con le estremità poste ad altezze differenti: alla tensione fra due punti del circuito elettrico corrisponde la differenza di pressione fra due punti del circuito idraulico. La differenza di potenziale tra i poli del generatore elettrico può essere vista come la differenza di altezza dei serbatoi dell'analogo circuito idraulico, e le dissipazioni di energia elettrica come conseguenti dell'attrito del liquido con le pareti interne del tubo. Infine, l'intensità di corrente elettrica che scorre nel conduttore può essere messa in analogia con la portata di liquido nel tubo.
In questa analogia, come il flusso d'acqua può compiere del lavoro scorrendo da un punto ad alta pressione ad uno a bassa pressione, azionando ad esempio una turbina, allo stesso modo le cariche che si muovono tra due punti posti a potenziale differente costituiscono una corrente elettrica, che può alimentare, ad esempio, un motore elettrico o fornire comunque energia sotto altre forme.
La tensione elettrica ai capi di un percorso è definita come la quantità di lavoro per unità di carica sviluppato dal campo elettrico per muovere una carica elettrica, ed equivale quindi all'integrale di linea del campo elettrico lungo la curva considerata come percorso. Essendo il campo conservativo in condizioni stazionarie, esso ammette potenziale, e quindi l'integrale di linea del campo elettrico dipende solo dagli estremi di integrazione. In questo caso la tensione equivale alla differenza di potenziale, e l'integrale è nullo su qualsiasi linea chiusa.
Esplicitamente, la differenza di potenziale formula_1 tra due punti "a" e "b" è l'integrale del campo elettrico E lungo una qualunque linea formula_2 che congiunga i due punti:
dove formula_4 rappresenta il prodotto scalare e "φ" l'angolo compreso tra il vettore campo elettrico e il vettore spostamento formula_5.
In una spira che racchiude una superficie attraversata da un flusso magnetico, si genera una fem proporzionale alla velocità di variazione di flusso nel tempo.
Una differenza di potenziale è anche generata tra gli estremi di un conduttore elettrico che si muove perpendicolarmente ad un campo magnetico.
Alla base del comportamento dei circuiti con carico puramente resistivo, vi è la legge di Ohm. Essa stabilisce che se si applica una tensione formula_1 ai capi di una resistenza formula_7, l'intensità formula_8 della corrente elettrica risultante che la attraversa, è direttamente proporzionale alla tensione ed inversamente proporzionale alla resistenza:
La corrente elettrica che passa attraverso un componente resistivo (R), genera una dissipazione di potenza formula_10 il cui valore è dato dal prodotto dell'intensità (I) per la differenza di potenziale (V):
Tale fenomeno è detto Effetto Joule.
La tensione tra due o più rami del circuito posti in serie è pari alla somma tra le tensioni dei singoli rami, mentre due punti che in un circuito sono connessi da un conduttore ideale (cioè avente resistenza nulla) hanno differenza di potenziale pari a zero.
Il corpo umano è uno scarso conduttore di elettricità, se messo in confronto ad un pezzo di rame delle stesse dimensioni ed in genere presenta una resistenza interna con un valore di diverse migliaia di Ohm, che può cambiare anche parecchio in base a diversi fattori. Applicare quindi una tensione elettrica qualsiasi ad un corpo umano, in teoria non provoca danni, in quanto è l'intensità della corrente elettrica ad essere pericolosa per l'uomo. Infatti, un generatore ideale di tensione con intensità di corrente nulla, potrebbe ipoteticamente generare anche 50kV (ddp) che non recherebbe ugualmente alcun danno all'uomo.
Tuttavia, la resistenza interna di qualsiasi generatore reale di tensione è sempre maggiore di zero e ciò significa che, anche se di pochi milliampere (ad esempio), è pur sempre presente una certa intensità di corrente che potrebbe interagire col corpo umano, danneggiandolo anche mortalmente. Ad esempio, con tensioni di 50VAC o 120VDC, una intensità di corrente che supera i 10mA, diventa pericolosa per l'uomo.
La classificazione della tensione elettrica è differente a seconda dell'ambito a cui si fa riferimento e al tipo di corrente (alternata o continua).
In particolare secondo quanto dettato dalla norma CEI EN 50110-1 ""Esercizio degli impianti elettrici"", la tensione elettrica viene classificata come indicato nella seguente tabella:
La norma CEI EN 50160 (a cui fa riferimento l'AEEG) riporta invece i seguenti valori:
</text>
</doc>
<doc id="2577462" url="https://it.wikipedia.org/wiki?curid=2577462">
<title>Fisica</title>
<text>
La fisica () è la scienza della natura nel senso più ampio. 
Ha lo scopo di studiare i fenomeni naturali, ossia tutti gli eventi che possono essere descritti, ovvero quantificati o misurati, attraverso grandezze fisiche opportune, al fine di stabilire principi e leggi che regolano le interazioni tra le grandezze stesse e le loro variazioni, mediante astrazioni matematiche. Quest'obiettivo è raggiunto attraverso l'applicazione rigorosa del metodo scientifico, il cui scopo ultimo è fornire uno schema semplificato, o modello, del fenomeno descritto.
L'insieme di principi e leggi fisiche relative a una certa classe di fenomeni osservati definiscono una teoria fisica deduttiva, coerente e relativamente autoconsistente, costruita tipicamente a partire dall'induzione sperimentale.
La storia della fisica abbraccia certamente un lungo arco temporale, ma non vi è accordo sulla data di nascita della fisica. 
Tuttavia la fisica propriamente detta nasce con la Rivoluzione scientifica nel XVII secolo per opera di Niccolò Copernico, Keplero, Tycho Brahe, Galileo Galilei e il suo metodo scientifico, Leibniz e Newton che diedero contributi alla meccanica celeste e ai principi della meccanica classica fornendo anche gli strumenti matematici adattati allo scopo, come i fondamenti del calcolo infinitesimale.
La fisica rappresenta dunque la prima disciplina scientifica nella storia della scienza da cui nascerà nel XVIII secolo la chimica, nel XIX secolo la biologia e le scienze della Terra ecc... Nel XVIII e XIX secolo nascono e si sviluppano poi teorie come la termodinamica e l'elettromagnetismo. Sempre a livello storico si suole suddividere la fisica in fisica classica che comprende la meccanica classica, la termodinamica e l'elettromagnetismo fino alla fine del XIX secolo e la fisica moderna dall'inizio del XX secolo a partire dalla teoria della relatività, la meccanica quantistica e tutte le altre teorie fisiche della seconda metà del Novecento.
Conosciuta anche come la "regina delle scienze", originariamente branca della filosofia, la fisica è stata chiamata almeno fino al XVIII secolo "filosofia naturale". Solo in seguito alla codifica del metodo scientifico di Galileo Galilei, negli ultimi trecento anni si è talmente evoluta e sviluppata e ha conseguito risultati di tale importanza da conquistarsi piena autonomia e autorevolezza. Essa si è distinta dalla filosofia per ovvie ragioni di metodo di indagine.
L'indagine fisica viene condotta seguendo rigorosamente il metodo scientifico, anche noto come il "metodo sperimentale": all'osservazione del fenomeno segue la formulazione di ipotesi interpretativa, la cui validità viene messa alla prova tramite degli esperimenti. Le ipotesi consistono nella spiegazione del fenomeno attraverso l'assunzione di principi fondamentali, in modo analogo a quanto viene fatto in matematica con assiomi e postulati. L'osservazione produce come conseguenza diretta le leggi empiriche.
Se la sperimentazione conferma un'ipotesi, la relazione che la descrive viene detta "legge fisica". Il ciclo conoscitivo prosegue con il miglioramento della descrizione del fenomeno conosciuto attraverso nuove ipotesi e nuovi esperimenti.
Un insieme di leggi possono essere unificate in una "teoria" fondata su principi che permettano di spiegare il maggior numero possibile di fenomeni: questo processo permette anche di prevedere nuovi fenomeni che possono essere scoperti sperimentalmente.
Le leggi e le teorie fisiche, come tutte le leggi scientifiche, in quanto costruite a partire da processi conoscitivi di tipo induttivo-sperimentali, sono in linea di massima sempre provvisorie, nel senso che sono considerate "vere" finché non vengono in qualche modo "confutate", ossia finché non viene osservato il verificarsi di un fenomeno che esse non predicono o se le loro predizioni sui fenomeni si dimostrano errate. Infine ogni teoria può essere sostituita da una nuova teoria che permetta di predire i nuovi fenomeni osservati con un'accuratezza superiore ed eventualmente in un più ampio contesto di validità.
Cardine della fisica sono i concetti di grandezza fisica e misura: le grandezze fisiche sono ciò che è misurabile secondo criteri concordati (è stabilito per ciascuna grandezza un "metodo di misura" e un'unità di misura). Le misure sono il risultato degli esperimenti. Le leggi fisiche sono quindi generalmente espresse come relazioni matematiche fra grandezze, verificate attraverso misure. I fisici studiano quindi in generale il comportamento e le interazioni della materia attraverso lo spazio e il tempo.
Per queste sue caratteristiche, cioè il preciso rigore di studio dei fenomeni analizzati, è unanimemente considerata la scienza dura per eccellenza tra tutte le scienze sperimentali o scienze esatte grazie al suo approccio teso alla comprensione non solo "qualitativa", ma anche "quantitativa" con la stesura delle suddette leggi universali di natura matematica in grado di fornire una "previsione" sullo stato futuro di un fenomeno o di un sistema fisico.
Il "metodo scientifico" è la modalità con cui la scienza procede per raggiungere una conoscenza della realtà "oggettiva", "affidabile", "verificabile" e "condivisibile". Differisce dal metodo aristotelico, presente prima del 1600, per la presenza della sperimentazione. Esso consiste, da una parte, nella raccolta di evidenza empirica e misurabile attraverso l'osservazione e l'esperimento; dall'altra, nella formulazione di ipotesi e teorie da sottoporre nuovamente al vaglio dell'esperimento.
Esso fu applicato e codificato da Galileo Galilei nella prima metà del XVII secolo: precedentemente l'indagine della natura consisteva nell'adozione di teorie che spiegassero i fenomeni naturali senza una verifica sperimentale delle teorie stesse considerate vere in base al principio di autorità.
Il metodo sperimentale moderno richiede, invece, che le teorie fisiche debbano fondarsi sull'osservazione dei fenomeni naturali, debbano essere formulate come relazioni matematiche e debbano essere messe alla prova tramite esperimenti:
Il percorso seguito per arrivare alla stesura di una legge scientifica (e in particolare di una legge fisica) a partire dall'osservazione di un fenomeno si articola nei seguenti passi, ripetuti ciclicamente:
Dato che le condizioni in cui si svolge l'esperimento non sono mai ideali, al contrario di quanto supposto dalle ipotesi, è spesso necessario svolgere un elevato numero di misure e analizzare i risultati con metodi statistici.
Nel caso in cui l'ipotesi sia confermata la relazione che essa descrive diviene una legge fisica, ulteriormente sviluppabile attraverso:
Ogni osservazione di un fenomeno costituisce un caso a sé stante, una particolare istanza del fenomeno osservato. Ripetere le osservazioni vuol dire moltiplicare le istanze e raccogliere altri fatti, cioè altre "misure". Le diverse istanze saranno certamente diverse una dall'altra nei dettagli (ad esempio a causa di errori sperimentali), anche se nelle loro linee generali ci indicano che il fenomeno, a parità di condizioni, tende a ripetersi sempre allo stesso modo.
Per ottenere un risultato di carattere generale occorre sfrondare le varie istanze dalle loro particolarità e trattenere solo quello che è rilevante e comune ad ognuna di esse, fino a giungere al cosiddetto modello fisico.
Se l'ipotesi è smentita allora è rigettata ed è necessario formulare una nuova ipotesi e ripercorrere il percorso precedente.
Il ciclo conoscitivo proprio del metodo scientifico è di tipo induttivo: un procedimento che partendo da singoli casi particolari cerca di stabilire una legge universale.
Nella prima metà del XX secolo, il filosofo e logico inglese Bertrand Russell e il filosofo austriaco Karl Popper sollevarono obiezioni sul metodo dell'induzione. L'induzione non ha consistenza logica perché non si può formulare una legge universale sulla base di singoli casi; ad esempio, l'osservazione di uno o più cigni dal colore bianco non autorizza a dire che tutti i cigni sono bianchi; esistono cigni neri. Popper osservò che nella scienza non basta "osservare": bisogna saper anche che cosa osservare. L'osservazione non è mai neutra ma è sempre intrisa di quella teoria che, appunto, si vorrebbe mettere alla prova. Secondo Popper, la teoria precede sempre l'osservazione: anche in ogni approccio presunto "empirico", la mente umana tende inconsciamente a sovrapporre i propri schemi mentali, con le proprie categorizzazioni, alla realtà osservata.
Il metodo sperimentale non garantisce quindi che una legge fisica possa essere verificata in modo definitivo, ma si può limitare solamente a fornire la prova della falsità di un'ipotesi.
La misura è il processo che permette di conoscere una qualità di un determinato oggetto (ad esempio la lunghezza o la massa) dal punto di vista quantitativo, tramite un'unità di misura, cioè una grandezza standard che, presa "N" volte, associ un valore univoco alla qualità da misurare. La branca della fisica che si occupa della misurazione delle grandezze fisiche è chiamata metrologia. Il suo scopo è quello di definire alcune grandezze fisiche indipendenti, dette "fondamentali", dalle quali è possibile ricavare tutte le altre (che sono dette "derivate"), di definire i corretti metodi di misurazione e di costruire i campioni delle unità di misura adottate, in modo da avere un valore "standard" a cui fare riferimento in qualsiasi momento.
Il sistema di unità di misura universalmente accettato dai fisici è il Sistema Internazionale (SI): esso è basato su sette grandezze fondamentali, dalle quali derivano tutte le altre, ovvero:
Questo sistema di misurazione deriva direttamente dal sistema MKS, il quale ha come grandezze fondamentali solamente il metro, il secondo e il chilogrammo ed è stato sostituito con il sistema attuale poiché non sono considerati anche i fenomeni termodinamici, elettromagnetici e fotometrici
Altri sistemi usati in passato sono stati il sistema CGS, in cui le unità fondamentali sono il centimetro, il grammo e il secondo e il Sistema imperiale britannico (o anglosassone). Inoltre negli USA si utilizza attualmente il Sistema consuetudinario statunitense, derivato dal Sistema imperiale britannico.
In ogni procedimento di misura di una grandezza fisica, la misura è inevitabilmente accompagnata da un'incertezza o "errore" sul valore misurato. Una caratteristica fondamentale degli errori che influenzano le misure di grandezze fisiche è la sua "ineliminabilità", ossia una misura può essere ripetuta molte volte o eseguita con procedimenti o strumenti migliori, ma in ogni caso l'errore sarà sempre presente. L'incertezza fa parte della natura stessa dei procedimenti di misura. In un esperimento, infatti, non è mai possibile eliminare un gran numero di fenomeni fisici che possono causare dei disturbi alla misura, cambiando le condizioni nelle quali si svolge l'esperimento.
Una misura può quindi fornire solamente una "stima" del "valore vero" di una grandezza coinvolta in un fenomeno fisico.
Le incertezze che influenzano una misura sono solitamente suddivise a seconda delle loro caratteristiche in:
Nell'immagine a lato è rappresentato l'effetto delle incertezze su di una misura per analogia con il gioco delle freccette. Il valore vero della grandezza è il centro del bersaglio, ogni tiro (puntini blu) rappresenta una misura.
Quando si fa una misura, quindi, si deve procedere alla "stima dell'incertezza" ad essa associata, o, in altre parole, alla stima dell'errore sulla misura. Ogni misura deve essere quindi presentata accompagnata dalla propria incertezza segnalata dal segno di ± e dalla relativa unità di misura:
formula_1
In cui formula_2 è il simbolo relativo alla grandezza misurate, formula_3 è la "stima" del valore della misura, formula_4 è l'incertezza e formula_5 è l'unità di misura.
Quando una misura viene ripetuta molte volte è possibile valutare le incertezze casuali calcolando la deviazione standard delle misure (di solito indicata con la lettera greca, formula_4), la stima del valore vero si ottiene invece calcolando la media aritmetica dei valori delle misure. Se le misure sono ripetute poche volte si utilizza come incertezza la risoluzione dello strumento. L'incertezza deve fornire un intervallo di valori in cui, secondo la misura condotta dallo sperimentatore, cade il valore vero della misura secondo un certo livello di confidenza.
Ci si può servire dell'incertezza assoluta per quantificare la precisione della misura, il valore dell'incertezza con la relativa unità di misura è detto "incertezza assoluta", l'"incertezza relativa" si calcola come il rapporto fra l'incertezza assoluta e il valore vero della grandezza, in genere stimato dal valor medio delle misure effettuate. L'incertezza relativa è un numero adimensionale (ossia senza unità di misura). È possibile esprimere l'incertezza relativa anche con una percentuale.
Le incertezze si propagano quando i dati afflitti da incertezze vengono utilizzati per effettuare successivi calcoli (come ad esempio il calcolo dell'area di un tavolo a partire dalla lunghezza dei suoi lati), secondo delle precise regole dette della propagazione delle incertezze.
Infine, bisogna notare che in Fisica classica in linea di principio gli errori possono essere sempre ridotti fino alla sensibilità tipica dello strumento di misura, peraltro idealmente o in linea teorica sempre migliorabile, mentre in meccanica quantistica questo non è possibile a causa del principio di indeterminazione di Heisenberg.
Il tempo e lo spazio sono delle grandezze fondamentali della fisica, assieme a massa, temperatura, quantità di sostanza, intensità di corrente, e intensità luminosa: tutte le grandezze della fisica sono riconducibili a queste ultime.
L'unità di misura del tempo è il secondo, che è definito come la durata di 9 192 631 770 periodi della radiazione corrispondente alla transizione tra due livelli iperfini, da (F=4, MF=0) a (F=3, MF=0), dello stato fondamentale dell'atomo di cesio-133, mentre il metro è l'unità fondamentale dello spazio ed è definito come la distanza percorsa dalla luce nel vuoto in un intervallo di tempo pari a 1/299 792 458 di secondo.
Prima del Novecento i concetti di spazio e di tempo erano considerati assoluti e indipendenti: si pensava che lo scorrere del tempo e le estensioni spaziali dei corpi fossero indipendenti dallo stato di moto dell'osservatore che le misurava, ovvero dal sistema di riferimento scelto. Dopo l'avvento della teoria della relatività di Einstein i fisici dovettero cambiare opinione: le lunghezze e gli intervalli temporali misurati da due osservatori in moto relativo l'uno rispetto all'altro, possono risultare più o meno dilatati o contratti, mentre esiste un'entità, l'intervallo di Minkowski, che è invariante e se misurata da entrambi gli osservatori fornisce il medesimo risultato; quest'entità è costituita dalle 3 coordinate spaziali più una quarta, quella temporale, che rendono questo oggetto appartenente ad uno spazio a 4 dimensioni. Così facendo, lo spazio e il tempo non sono più due quantità fisse e indipendenti tra loro, ma sono correlate tra loro e formano un'unica e nuova base su cui operare, lo spazio-tempo.
Con la relatività generale, poi, lo spazio-tempo viene deformato dalla presenza di oggetti dotati di massa o energia (più in generale, di energia-impulso, vd. tensore energia impulso).
La massa è una grandezza fisica fondamentale. Essa ha come unità di misura nel Sistema internazionale il chilogrammo e viene definita nella meccanica newtoniana come la misura dell'inerzia offerta dai corpi al cambiamento del proprio stato di moto. Nella teoria della gravitazione universale di Newton svolge inoltre il ruolo di carica della forza gravitazionale. Questa doppia definizione della massa viene unita nella teoria della relatività di Einstein, tramite il principio di equivalenza, e inoltre essa viene legata all'energia di un corpo tramite la formula E = mc². La massa resta sempre costante a differenza del peso. Esempio: sulla luna la massa resta costante, mentre il peso diventa un sesto.
Nell'ambito della fisica, la forza viene definita come la rapidità di variazione della quantità di moto rispetto al tempo. Nel caso in cui la massa del corpo sia costante, la forza esercitata su un corpo è pari al prodotto della massa stessa per l'accelerazione del corpo.
In formule:
La forza esprime quantitativamente l'interazione di due corpi. L'interazione tra i corpi può avvenire attraverso una cosiddetta "area di contatto" (spesso assimilabile ad un punto) oppure può manifestarsi a distanza, attraverso quello che viene definito campo di forze.
Il concetto di campo di forze può essere chiarito se si pensa alla natura vettoriale della forza: la forza infatti viene descritta dal punto di vista matematico da un vettore, per cui un campo di forze è descritto in matematica come un campo vettoriale, cioè il campo di forze indica punto per punto la direzione, il verso e il modulo (o intensità) della forza che viene esplicata tra due corpi. Il campo di forze può essere visualizzato tramite le sue linee di campo o le linee di flusso.
Alcuni esempi di campi di forze sono: il campo magnetico, il campo elettrico e il campo gravitazionale.
Il modello fisico è una versione approssimata del sistema effettivamente osservato. Il suo impiego indiscriminato presenta dei rischi, ma ha il vantaggio di una maggiore generalità e quindi dell'applicabilità a tutti i sistemi simili al sistema in studio.
La costruzione del modello fisico è la fase meno formalizzata del processo conoscitivo, che porta alla formulazione di leggi quantitative e di teorie. Il modello fisico ha la funzione fondamentale di ridurre il sistema reale, e la sua evoluzione, ad un livello astratto ma traducibile in forma matematica, utilizzando definizioni delle grandezze in gioco e relazioni matematiche che li leghino. Tale traduzione può essere portata a termine anche attraverso l'uso del calcolatore, con programmi detti di simulazione, con i quali si studiano i fenomeni più disparati.
Il modello matematico, che ovviamente si colloca ad un livello di astrazione ancora superiore a quello del modello fisico, ovvero al massimo livello di astrazione nel processo conoscitivo, è costituito normalmente da equazioni differenziali che, quando non siano risolvibili in maniera esatta, devono essere semplificate opportunamente o risolte, più o meno approssimativamente, con metodi numerici (al calcolatore). Si ottengono in questo modo delle relazioni analitiche o grafiche fra le grandezze in gioco, che costituiscono la descrizione dell'osservazione iniziale.
Tali relazioni, oltre a descrivere l'osservazione, possono condurre a nuove previsioni. In ogni caso esse sono il prodotto di un processo che comprende diverse approssimazioni:
La soluzione del modello matematico va quindi interpretata tenendo conto delle varie approssimazioni che sono state introdotte nello studio del fenomeno reale, per vedere con quale approssimazione riesce a rendere conto dei risultati dell'osservazione iniziale e se le eventuali previsioni si verificano effettivamente e con quale precisione. Questo può venire confermato solo dall'esperienza, creando una sorta di schema in retroazione, che è il ciclo conoscitivo.
La fisica si compone di più branche che sono specializzate nello studio di diversi fenomeni oppure che sono caratterizzate dall'utilizzo estensivo delle stesse leggi di base. 
In base alla prima classificazione si possono distinguere quattro classi principali di fenomeni fisici: 
Ciascuna classe di fenomeni osservabili in natura è interpretabile in base a dei principi e delle leggi fisiche che insieme definiscono una teoria fisica deduttiva, coerente e relativamente autoconsistente. Benché ogni teoria fisica sia intrinsecamente falsificabile per la natura tipicamente induttiva del metodo di indagine scientifico, allo stato attuale esistono teorie fisiche più consolidate di altre seguendo il percorso storico di evoluzione della fisica stessa.
In base alla seconda classificazione si può invece distinguere tra fisica classica e fisica moderna, poiché quest'ultima fa uso continuamente delle teorie relativistiche, della meccanica quantistica e delle teorie di campo, che non sono invece parte delle teorie cosiddette classiche.
La fisica classica studia tutti i fenomeni che possono essere spiegati senza ricorrere alla relatività generale e alla meccanica quantistica. Le teorie principali che la compongono sono la meccanica classica (in cui si ricomprende l'acustica), la termodinamica, l'elettromagnetismo (in cui si ricomprende l'ottica) e la teoria newtoniana della gravità. Sostanzialmente tutte le teorie che sono state prodotte prima dell'inizio del XX secolo fanno parte della fisica classica. Le leggi della fisica classica, nonostante non siano in grado di spiegare alcuni fenomeni, come la precessione del perielio di Mercurio, o l'effetto fotoelettrico, sono in grado di spiegare gran parte dei fenomeni che si possono osservare sulla Terra. Le teorie, invece, falliscono quando è necessario spingersi oltre i limiti di validità delle stesse, ovvero nelle scale atomiche e subatomiche, o in quello dei corpi molto veloci, per cui è necessario fare ricorso alle leggi della fisica moderna.
La fisica classica utilizza un numero relativamente ridotto di leggi fondamentali che a loro volta si basano su una serie di principi assunti alla base della teoria. Fra questi quelli più importanti sono i concetti di spazio assoluto e tempo assoluto che sono poi alla base della relatività galileiana. Molto importanti sono anche i principi di conservazione.
Se la fisica classica aveva di per sé esaurito brillantemente quasi del tutto lo studio dei fenomeni fisici macroscopici, con il successivo passo, ovvero con la fisica moderna, lo studio fisico si incentra su tutti quei fenomeni che avvengono a scala atomica e subatomica o con velocità prossime a quelle della luce; le teorie principali che costituiscono questa nuova fisica sono la meccanica quantistica e la relatività generale. Più precisamente fanno parte di questa categoria tutte le teorie che sono state prodotte a partire dal XX secolo per cercare di spiegare alcuni fenomeni che le teorie classiche non riuscivano a dimostrare.
Queste nuove teorie rappresentarono una "spaccatura" netta nel disegno teorico tracciato dalla fisica classica precedente in quanto ne hanno completamente rivisto idee e concetti di fondo in cui l'uomo aveva sempre creduto fin dai tempi più antichi: 
Un'altra classificazione vuole la distinzione tra fisica sperimentale e fisica teorica in base alla suddivisione del processo di indagine scientifica rispettivamente nella fase dell'osservazione dei dati dell'esperimento e della loro successiva interpretazione ed elaborazione all'interno di teorie fisico-matematiche: stretto è dunque il loro legame di collaborazione. Entrambe queste distinzioni possono essere fatte all'interno sia della fisica classica che della fisica moderna.
La fisica matematica è quella disciplina scientifica che si occupa delle applicazioni della matematica ai problemi della fisica e dello sviluppo di metodi matematici adatti alla formulazione di teorie fisiche e alle relative applicazioni. È una branca della fisica tipicamente teorica.
In tempi recenti l'attività dei fisici-matematici si è concentrata principalmente sulle seguenti aree:
L'evoluzione della fisica in questo senso va verso la cosiddetta teoria del tutto ovvero una teoria omnicomprensiva che spieghi la totalità dei fenomeni fisici osservati in termini delle interazioni fondamentali a loro volta unificate.
La "fisica atomica" è invece la branca della fisica che studia l'atomo nella sua interezza ovvero comprendendo nucleo ed elettroni. Si tratta di un campo della fisica studiato all'inizio del XX secolo con la fornitura dei vari modelli atomici fino al modello attuale ritenuto più verosimile ovvero con nucleo interno ed elettroni esterni di tipo orbitale. Si tratta di un campo assestato già nella prima metà del XX secolo.
La più ampia branca della fisica della materia condensata (comunemente detta fisica della materia) è la fisica dello stato solido e riguarda lo studio delle proprietà dei solidi, sia elettroniche, che meccaniche, ottiche e magnetiche.
Il grosso della ricerca teorica e sperimentale della fisica dello stato solido è focalizzato sui cristalli, sia a causa della loro caratteristica struttura atomica periodica, che ne facilita la modellizzazione matematica, che per il loro ampio utilizzo tecnologico.
Con il termine "stato solido" in elettronica ci si riferisce in generale a tutti i dispositivi a semiconduttore. A differenza dei dispositivi elettromeccanici, quali ad esempio i relè, i dispositivi a stato solido non hanno parti meccaniche in movimento. Il termine è utilizzato anche per differenziare i dispositivi a semiconduttore dai primi dispositivi elettronici: le valvole e i diodi termoionici.
Il punto di partenza di gran parte della teoria nell'ambito della fisica dello stato solido è la formulazione di Schrödinger della meccanica quantistica non relativistica. La teoria si colloca generalmente all'interno dell'approssimazione di Born - Oppenheimer e dalla struttura periodica del reticolo cristallino si ricavano le condizioni periodiche di Born-von Karman e il Teorema di Bloch, che caratterizza la funzione d'onda nel cristallo. Le deviazioni dalla periodicità sono trattate ampiamente tramite approcci perturbativi o con altri metodi più innovativi, quali la rinormalizzazione degli stati elettronici.
Appartiene alla fisica dello stato solido anche la fisica delle basse temperature la quale studia gli stati della materia a temperature prossime allo zero assoluto e i fenomeni ad essi connessi (ad es. condensato di Bose-Einstein, superconduttività ecc..).
La "fisica nucleare" è la branca della fisica che studia il nucleo atomico nei suoi costituenti protoni e neutroni e le loro interazioni.
La fisica nucleare si distingue dalla fisica atomica che invece studia l'atomo, sistema composto dal nucleo atomico e dagli elettroni.
La fisica nucleare si distingue a sua volta dalla fisica delle particelle o fisica subnucleare che invece ha come oggetto lo studio delle particelle più piccole del nucleo atomico. La fisica delle particelle o subnucleare è stata per molto tempo considerata una branca della fisica nucleare. Il termine fisica subnucleare sta cadendo in disuso poiché si riferiva allo studio di particelle interne al nucleo, mentre oggi la maggior parte delle particelle note non sono costituenti nucleari.
L'energia nucleare è la più comune applicazione della fisica nucleare, ma il campo di ricerca è anche alla base di molte altre importanti applicazioni, come in medicina (medicina nucleare, risonanza magnetica nucleare), in scienza dei materiali (implantazioni ioniche) o archeologia (radiodatazione al carbonio).
La "fisica delle particelle" è la branca della fisica che studia i costituenti fondamentali e le interazioni fondamentali della materia; essa rappresenta la fisica dell'"infinitamente piccolo". Talvolta viene anche usata l'espressione fisica delle alte energie, quando si vuole far riferimento allo studio delle interazioni tra particelle elementari che si verificano ad altissima energia e che permettono di creare particelle non presenti in natura in condizioni ordinarie, come avviene con gli acceleratori di particelle.
In senso stretto, il termine "particella" non è del tutto corretto. Gli oggetti studiati dalla Fisica delle particelle, obbediscono ai principi della meccanica quantistica. Come tali, mostrano una dualità onda-corpuscolo, in base alla quale manifestano comportamenti da particella sotto determinate condizioni sperimentali e comportamenti da onda in altri. Teoricamente, non sono descritte né come onde né come particelle, ma come vettori di stato in un'astrazione chiamata spazio di Hilbert.
È una branca relativamente recente della fisica moderna che studia appunto il comportamento fisico di sistemi complessi come ad esempio il sistema economico (econofisica) o il sistema climatico assunti come sistemi dinamici non lineari.
Questa branca della fisica (la fisica cibernetica), nata nella seconda metà del XX secolo, si è sviluppata a tal punto che è ora ricompresa all'interno di varie discipline tecnico-applicative quali l'automatica, la meccatronica e l'informatica (intelligenza artificiale).
La "fisica medica" o "fisica sanitaria" è un'attività che riguarda, in generale, tutti i settori della fisica applicata alla medicina e alla radioprotezione. Più in particolare, le strutture di fisica sanitaria ospedaliere si occupano, in prevalenza, dell'impiego delle radiazioni ionizzanti e non ionizzanti (diagnostica per immagini, radioterapia, medicina nucleare, ...), ma anche di informatica, di modellistica, ecc.
L'astrofisica è una scienza che applica la teoria e i metodi delle altre branche della fisica per studiare gli oggetti di cui è composto l'universo, quali ad esempio le stelle, i pianeti, le galassie e i buchi neri.
L'astrofisica si differenzia dall'astronomia in quanto l'astronomia si pone come obiettivo la comprensione dei movimenti degli oggetti celesti, mentre l'astrofisica tenta di spiegare l'origine, l'evoluzione e il comportamento degli oggetti celesti stessi, rappresentando quindi la fisica dell'"infinitamente grande". Un'altra disciplina con cui l'astrofisica è intimamente correlata è la cosmologia, che ha come oggetto di studio l'origine dell'universo.
I telescopi spaziali (tra cui va ricordato il telescopio spaziale Hubble) sono strumenti indispensabili alle indagini dell'astrofisica: grazie ad essi gli astrofisici hanno trovato conferma di molte teorie sull'universo.
La "geofisica" (anche detta "fisica terrestre") è in generale l'applicazione di misure e metodi fisici allo studio delle proprietà e fenomeni fisici tipici del pianeta Terra.
La geofisica è una scienza di tipo preminentemente sperimentale, che condivide il campo di applicazione sia con la fisica che con la geologia e comprende al suo interno diverse branche, quali ad esempio:
La "geofisica applicata" studia la parte solida più superficiale della Terra e rivolge il suo campo di ricerche all'individuazione di strutture idonee per l'accumulo di idrocarburi, nonché alla risoluzione di problemi nel campo dell'ingegneria civile, ingegneria idraulica, ingegneria mineraria e per l'individuazione di fonti di energia geotermica.
Le prospezioni geofisiche (prospezioni sismiche, elettriche, elettromagnetiche, radiometriche, gravimetriche) rappresentano alcuni metodi fisici utilizzati nel campo dell'esplorazione geologica.
I principi fisici sono alla base di numerose discipline tecnico-scientifiche sia teoriche sia più vicine al campo applicativo (tecnica). Allo stesso tempo la fisica si avvale degli strumenti tecnici e matematici messi a disposizione da queste discipline per aiutarsi nel suo continuo processo di indagine scientifica dei fenomeni dell'universo.
Nel testo Il Saggiatore del 1623, Galileo Galilei afferma:
In generale, gli elementi che caratterizzano il modello matematico di un sistema fisico sono due: lo spazio degli stati e la dinamica. Il primo è un insieme che contiene tutti i possibili stati in cui il sistema si può trovare, dove per stato si intende una collezione di grandezze fisiche che, se conosciute in un certo istante, sono sufficienti per predire come evolverà il sistema, cioè quali stati saranno occupati negli istanti futuri; ad esempio, per un sistema meccanico di "n" particelle libere di muoversi nello spazio, uno stato è un insieme di 6"n" numeri reali, 3"n" per le posizioni (3 coordinate per ogni particella), e i restanti 3"n" per le velocità (3 componenti per ogni particella). Lo spazio degli stati può essere molto complicato, sia geometricamente (ad esempio nella meccanica dei sistemi vincolati e nella teoria della relatività generale, dove in genere è una varietà differenziale, i.e. uno spazio "curvo") che analiticamente (ad esempio in meccanica quantistica, dove è uno spazio di Hilbert proiettivizzato). La dinamica, invece, è la legge che, dato uno stato iniziale, descrive l'evoluzione del sistema. Solitamente, è data in forma differenziale, cioè collega lo stato in un certo istante a quello in un istante successivo "infinitamente vicino" nel tempo.
Le più grandi rivoluzioni della fisica moderna (la teoria della relatività generale, la meccanica quantistica e la teoria quantistica dei campi) si possono ricondurre all'inadeguatezza della fisica classica a descrivere i nuovi fenomeni sperimentali riscontrati verso la fine dell'Ottocento e l'inizio del Novecento (l'esperimento di Michelson-Morley e i vari esperimenti in cui si presentano fenomeni quantistici, tra cui, l'esperimento della doppia fenditura, il corpo nero, l'effetto fotoelettrico e l'effetto Compton).
Le maggiori aree della matematica che forniscono strumenti utili allo studio sia della forma dello spazio degli stati che della dinamica sono:
Gli strumenti della statistica sono utilizzati durante la fase di rilevamento dei dati a partire dal modello fisico e nella fase successiva di trattamento dei dati.
Particolarmente utile nella prima fase di rilevamento dei dati è la metodica del campionamento statistico (in inglese "sampling"), che consiste nel selezionare una particolare serie di dati all'interno dell'intervallo di condizioni studiate.
Una volta ottenuti i dati, viene effettuata la cosiddetta analisi di regressione, che permette di ottenere dall'insieme di dati più o meno sparsi (in quanto affetti da errori di varia natura) una relazione matematica precisa. Nel caso più semplice in cui la relazione matematica tra i dati venga rappresentata da una retta, si parla di regressione lineare.
Molti concetti statistici sono poi presi a prestito dalla fisica statistica laddove non è possibile avere informazioni deterministiche sui sistemi o fenomeni a molti gradi di libertà e variabili.
I computer vengono utilizzati in più fasi del processo conoscitivo: durante la fase di osservazione possono essere utilizzati ad esempio per effettuare un campionamento delle misurazioni, ovvero il valore della grandezza da misurare viene letto ad intervalli determinati, in modo da avere più misure in un ristretto lasso di tempo. Il calcolatore può svolgere anche la funzione di strumento registratore: i dati relativi all'osservazione vengono ad essere archiviati per lo svolgimento di operazioni successive di valutazione e/o confronto con altri dati. L'intero sistema per la misurazione, il trattamento e la registrazione dei dati, costituito dal calcolatore e da strumentazioni specifiche ad esso interfacciate, viene denominato sistema di acquisizione dati (o DAQ).
Gli strumenti informatici possono quindi fungere da "strumento" durante le diverse fasi dell'esperienza, ma possono anche andare oltre, costituendo un vero e proprio sistema virtuale, che sostituisce e "imita" il sistema fisico reale; si parla in questo caso di simulazione del processo in esame. Il sistema simulato presenta il vantaggio rispetto al sistema reale di avere un controllo su tutti gli elementi di disturbo che influenzano il fenomeno studiato; d'altra parte è necessaria una precedente conoscenza del modello matematico associato al modello fisico per la creazione del modello simulato. La simulazione quindi affianca "in primis" l'osservazione diretta durante il processo conoscitivo, con lo scopo di convalidare il modello matematico ipotizzato, e una volta che la corrispondenza tra modello fisico e modello simulato è stata accertata, è possibile utilizzare la simulazione per effettuare delle stime in condizioni contemplate dal modello matematico, ma che sono differenti da quelle in cui è avvenuta la precedente osservazione diretta.
La fisica è strettamente connessa alla chimica (la scienza delle molecole) con cui si è sviluppata di pari passo nel corso degli ultimi due secoli. La chimica prende molti concetti dalla fisica, soprattutto nei campi di termodinamica, elettromagnetismo, e meccanica quantistica. Tuttavia i fenomeni chimici sono talmente complessi e vari da costituire una branca del sapere distinta.
Nella chimica, come nella fisica, esiste il concetto di forza come "interazione tra i corpi". Nel caso della chimica "i corpi" hanno dimensioni dell'ordine dell'Ångström, e sono appunto le molecole, gli atomi, gli ioni, i complessi attivati, e altre particelle di dimensioni ad essi confrontabili. Le forze di interazione tra questi corpi sono i legami chimici (legami intramolecolari) e altre forze di interazione più blande (ad esempio le forze di Van der Waals, il legame a idrogeno e le forze di London).
È probabilmente la disciplina che più di ogni altra si avvale dei principi della fisica per sviluppare teorie proprie dedicate all'ideazione, progettazione, realizzazione e gestione di sistemi utili alle esigenze dell'uomo e della società: nel campo dell'ingegneria edile e dell'ingegneria civile strutture edili e opere civili (case, strade, ponti) sfruttano le conoscenze nel campo della statica e sulla resistenza meccanica dei materiali sottoposti a stress o sollecitazioni meccaniche e/o termiche; l'ingegneria meccanica e l'ingegneria motoristica sfruttano le conoscenze offerte dalla termodinamica per la progettazione e la realizzazione delle macchine termiche; l'ingegneria energetica sfrutta le conoscenze fisiche per la realizzazione di sistemi di produzione e distribuzione dell'energia (energia nucleare, energie rinnovabili, energia da combustibili fossili); l'ingegneria dell'informazione sfrutta i segnali e le onde elettromagnetiche emesse dalle sorgenti per il trasporto dell'informazione a distanza.
L'approccio metodologico utilizzato nel campo della fisica è applicato dall'inizio degli anni novanta anche a problematiche di tipo economico nell'ambito della disciplina denominata econofisica come tentativo di superamento dell'approccio classico economico di tipo semi-quantitativo.
Ad esempio vengono studiate le fluttuazioni dei mercati finanziari e i crash del mercato azionario a partire da modelli normalmente utilizzati per studiare fenomeni di tipo fisico quali: modelli di percolazione, modelli derivati dalla geometria frattale, modelli di arresto cardiaco, criticalità auto-organizzata e previsione dei terremoti, tipicamente modelli per sistemi complessi e caotici ovvero non-lineari.
Prima dell'avvento del metodo scientifico, l'interpretazione dei fenomeni naturali era riservata alla filosofia, per cui per lungo tempo la fisica fu denominata "filosofia naturale".
Tra i primi tentativi di descrivere la materia in ambito filosofico, si ricorda Talete.
Successivamente Democrito tentò di descrivere la materia attraverso i concetti di vuoto e atomo.
Ad oggi la fisica mantiene stretti rapporti con la filosofia attraverso branche come l'epistemologia e la filosofia della scienza.
Come in ogni altra disciplina scientifica i contributi scientifici alla nascita ed allo sviluppo di teorie fisiche avvengono attraverso pubblicazioni scientifiche su riviste scientifiche soggette ai ben noti e rigorosi processi di revisione paritaria.
</text>
</doc>
<doc id="2574744" url="https://it.wikipedia.org/wiki?curid=2574744">
<title>Diagramma di corpo libero</title>
<text>
Un diagramma di corpo libero è la rappresentazione schematica delle forze agenti su un corpo "libero", utilizzata spesso in fisica e ingegneria. Questo tipo di diagramma può semplificare la comprensione delle forze e dei momenti agenti su un corpo, e suggerire i concetti adeguati da applicare per risolvere le equazioni del moto.
Un diagramma di corpo libero è innanzi tutto uno schema del corpo considerato, su cui sono disegnate delle frecce che rappresentano le forze ad esso applicate. La scelta del corpo da disegnare può essere la prima decisione importante nel processo di risoluzione di un problema meccanico. Ad esempio, per determinare le forze agenti sul fulcro di un paio di pinze, è sufficiente disegnare un diagramma di corpo libero di una sola delle metà di cui sono composte le pinze, rimpiazzando la seconda metà con le stesse forze che andrebbero applicate alla prima.
Nella rappresentazione del corpo non si devono aggiungere più dettagli di quanto sia strettamente necessario. Spesso è sufficiente disegnare soltanto la sagoma del corpo considerato. Tutti i contatti esterni, i vincoli, e le forze agenti sul corpo sono indicati da frecce rappresentanti vettori, identificate tramite delle descrizioni appropriate. Ai fini pratici, le frecce devono indicare il punto di applicazione, la direzione e il verso delle forze che rappresentano.
In un diagramma di corpo libero devono essere disegnate tutte e sole le forze agenti sull'oggetto considerato. Queste possono essere le forze di gravità, attriti, reazioni normali o, semplicemente, forze di contatto dovute a una spinta. Quando si è in presenza di un sistema di riferimento non inerziale, è necessario disegnare anche le interazioni apparenti, come la forza centrifuga, che è una forza apparente.
Solitamente va incluso anche un sistema di riferimento. Questo potrebbe semplificare la definizione dei vettori nella scrittura delle equazioni del moto. Ad esempio, in un piano inclinato, la direzione formula_1 potrebbe essere scelta lungo la superficie della rampa, in modo che la forza di attrito abbia solo componenti formula_1, e la reazione normale del piano abbia solo componenti formula_3. La forza di gravità in questo caso avrà componenti lungo entrambi gli assi: formula_4 lungo la direzione formula_3, e formula_6 lungo la direzione formula_1, dove formula_8 è l'angolo di inclinazione della rampa.
Tutti i contatti esterni e i vincoli non sono rappresentati nel disegno. Questi elementi vanno sostituiti da frecce rappresentanti le forze, come descritto in precedenza.
Le forze causate dal corpo rigido ad altri oggetti non vanno incluse. Ad esempio, se una palla è ferma sopra un tavolo, la palla applica una forza alla tavola, e subisce una forza uguale ed opposta. Il diagramma di corpo libero della palla rappresenterà soltanto la forza normale causata dal tavolo.
Le forze interne, le forze tra le varie parti che compongono il sistema che viene trattato come un corpo rigido, sono omesse. Per esempio, se si sta analizzando una trave per determinare le reazioni ai punti di contatto, le forze tra gli elementi che compongono la trave non vanno rappresentate.
I vettori velocità e accelerazione non vanno disegnati. Questi possono invece essere rappresentati in un diagramma associato, chiamato "diagramma cinetico", "diagramma di risposta inerziale", o in altri modi, a discrezione dell'autore.
Il diagramma di corpo libero riflette le assunzioni esemplificative, o ipotesi, introdotte per analizzare il sistema.
Se ad esempio il corpo in questione è un satellite in orbita, di cui interessa unicamente determinare la velocità, allora la rappresentazione migliore potrebbe semplicemente essere un punto. D'altra parte, il sistema di sospensioni di una motocicletta non può essere rappresentato da un singolo punto, e sarà necessario disegnare un diagramma più complicato che tiene conto delle dimensioni estese.
I vettori di forza devono essere posizionati ed etichettati con attenzione per evitare assunzioni che presuppongono già un determinato risultato.
Ad esempio, nel diagramma di un corpo su un piano inclinato, la posizione esatta della risultante della forza esercitata dalla rampa sul corpo può essere determinata solo dopo averne analizzato il moto: una determinazione a priori richiede l'ulteriore assunto che il corpo sia in equilibrio.
Un semplice diagramma di corpo libero, illustrato all'inizio di questa voce, mostra le seguenti caratteristiche:
</text>
</doc>
<doc id="2585115" url="https://it.wikipedia.org/wiki?curid=2585115">
<title>Spettroscopia di assorbimento</title>
<text>
Le spettroscopie di assorbimento sono un insieme di tecniche spettroscopiche. 
Questo tipo di tecniche si basano sull'assorbimento di radiazione elettromagnetica a diverse lunghezze d'onda dei materiali producendo uno spettro espresso solitamente in numeri d'onda e assorbanza o trasmittanza.
Tra le tecniche ad assorbimento si trova la spettroscopia di assorbimento atomico.
</text>
</doc>
<doc id="264440" url="https://it.wikipedia.org/wiki?curid=264440">
<title>Equazione del moto</title>
<text>
In meccanica classica, un'equazione del moto è un'equazione che descrive il moto di un sistema fisico in funzione della posizione nello spazio e del tempo. In particolare, l'equazione che caratterizza l'andamento della posizione in funzione del tempo è detta legge oraria.
Un sistema meccanico con formula_1 gradi di libertà viene solitamente descritto attraverso un insieme di coordinate generalizzate formula_2. La conoscenza in un dato istante temporale delle coordinate generalizzate e delle velocità generalizzate formula_3, che sono le derivate rispetto al tempo delle coordinate generalizzate, consente una caratterizzazione completa dello stato meccanico del sistema. Con tali informazioni si possono determinare univocamente le accelerazioni formula_4, ed è quindi possibile prevedere l'evoluzione del sistema ad un tempo successivo a quello considerato. L'equazione del moto mette in relazione le quantità formula_5, formula_6 e formula_7, e se l'incognita è formula_5, come spesso accade, si tratta di un'equazione differenziale del secondo ordine le cui soluzioni sono le possibili leggi orarie formula_9 di un punto materiale, o un corpo, soggetto ad una interazione nota. Le equazioni del moto sono completate dalla definizione dei parametri iniziali, che definiscono il problema di Cauchy e che sotto opportune ipotesi consentono di determinare univocamente la soluzione.
Solitamente la legge oraria di un oggetto in moto è un'equazione che si ricava dall'applicazione al sistema delle leggi della dinamica di Newton o di leggi di conservazione, quali ad esempio la legge di conservazione dell'energia meccanica o del momento angolare. La legge oraria di un punto materiale può essere data sia rispetto ad un sistema di riferimento sia rispetto ad una ascissa curvilinea. Per esempio, se un punto materiale è vincolato su una guida per definirne la posizione si può sia indicare i valori della proiezione del punto sugli assi, sia la distanza da un punto di riferimento preso sulla guida.
Nella meccanica newtoniana un'equazione del moto è una funzione formula_10 che ha la forma di un'equazione differenziale ordinaria rispetto alla funzione che descrive la posizione in funzione del tempo formula_11:
Il problema di Cauchy è dato assegnando un valore alla posizione e alla sua derivata nell'istante formula_13:
Il secondo principio della dinamica può essere formulato sia attraverso la legge di Newton che con la prima equazioni di Eulero. Quest'ultima rappresenta la sua forma più generale:
dove formula_16 è la Lagrangiana del sistema. Le equazioni di Eulero-Lagrange:
si ottengono direttamente a partire dal principio variazionale, e sono equazioni del moto. Esse descrivono il moto di un oggetto che obbedisce al secondo principio della dinamica, mettendo in relazione la posizione e la velocità di ogni elemento che compone il sistema.
Le soluzioni dell'equazione del moto si rappresentano attraverso orbite nello spazio delle fasi. Una costante del moto è una funzione costante lungo ogni orbita del sistema. Dato un sistema di equazioni differenziali del primo ordine:
una funzione scalare formula_19 è una costante del moto o "quantità conservata" se per tutte le condizioni iniziali si ha:
La soluzione del sistema è tangente al campo vettoriale formula_21, che può essere ad esempio un campo di velocità, ed è l'intersezione di due superfici: esse sono gli integrali primi del sistema di equazioni differenziali. Utilizzando la regola della catena si mostra che il campo vettoriale formula_21 è ortogonale al gradiente della quantità conservata formula_23.
Un caso semplice di legge oraria è quello della traiettoria di una particella puntiforme vincolata a stare su una retta. Presa come sistema di riferimento la retta stessa, orientata e con un'origine, la legge oraria è una funzione formula_24 che associa ad ogni istante formula_25 un punto formula_26 della retta (in questo caso il sistema di riferimento ortonormale e l'ascissa curvilinea coincidono). Per esempio, si supponga di avere una particella di massa formula_27 spinta da una forza costante formula_28 nella direzione positiva della retta. Applicando il secondo principio della dinamica si ha l'equazione del moto:
da cui, integrando due volte (o ricordando la formula per il moto rettilineo uniformemente accelerato) si ha la legge oraria:
Un caso meno banale, nel quale si vede anche la differenza tra sistema di riferimento cartesiano e ascissa curvilinea, è quello di un corpo puntiforme su un piano inclinato liscio, con inclinazione formula_31, sottoposto alla forza di gravità, come in figura. Il sistema di riferimento è preso con l'asse formula_26 orizzontale da sinistra a destra e l'asse formula_33 verticale orientato verso l'alto.
Il secondo principio della dinamica, una volta sommate tutte le forze, reazione vincolare inclusa, fornisce le due seguenti equazioni:
che si risolvono indipendentemente come due moti uniformemente accelerati lungo gli assi formula_26 e formula_33:
L'insieme di queste due funzioni è la legge oraria cercata: dato un valore del tempo formula_25 si può conoscere la posizione del punto tramite le sue coordinate cartesiane. Un'altra espressione della posizione può però essere data nei termini di un'ascissa coincidente col piano e diretta verso il basso: in questo modo il moto, che prima era bidimensionale, si riduce ad un moto unidimensionale lungo il piano. Con questo sistema di riferimento, l'equazione del moto è:
e la legge oraria:
Per chiarire il formalismo vettoriale si può definire un vettore posizione formula_11 come:
e la legge oraria è espressa come una funzione vettoriale:
Questo vettore è ambientato nel piano verticale formato dagli assi formula_26 e formula_33 ed indica istante per istante la posizione della particella. Lo spostamento della particella tra due istanti formula_46 e formula_47 è dato semplicemente da:
</text>
</doc>
<doc id="299780" url="https://it.wikipedia.org/wiki?curid=299780">
<title>Urto anelastico</title>
<text>
L'urto anelastico è l'urto in cui l'energia meccanica totale non si conserva. Nel caso poi sia anelastico totale, i corpi, dopo la collisione, restano a contatto e possono essere considerati come un unico corpo ed essi viaggiano con la stessa velocità, come può essere il caso di un'automobile che urta contro un camion e rimane incastrata in esso: nel sistema, dopo l'urto, automobile e camion si fondono in un unico corpo, che continua a viaggiare con una velocità formula_1 diversa dalla velocità iniziale dell'automobile e da quella del camion.
La legge di conservazione della quantità di moto del sistema è:
formula_2
per gli "urti anelastici totali", si può scrivere 
formula_3 
dove formula_4 e formula_5 rappresentano le quantità di moto prima dell'urto rispettivamente del primo corpo di massa formula_6 e del secondo corpo di massa formula_7, mentre formula_8 è la quantità di moto dell'intero sistema dopo l'urto, cioè quando i due corpi si fondono in un unico corpo di massa pari alla somma delle precedenti, formula_9
formula_1, ricavabile dalla precedente espressione, rappresenta la velocità con cui si muovono i due corpi insieme dopo l'urto.
Se si suppone per semplicità che non vi siano variazioni di energia potenziale (caso più comune), allora la perdita di energia meccanica è dovuta alla sola variazione di energia cinetica.
L'energia cinetica dissipata durante l'urto completamente anelastico, è
formula_11
dove:
formula_12
si dice massa ridotta del sistema.
È possibile dimostrare che se l'urto è totalmente anelastico, l'energia cinetica dissipata è la massima possibile.
</text>
</doc>
<doc id="221853" url="https://it.wikipedia.org/wiki?curid=221853">
<title>Elasticità (meccanica)</title>
<text>
In fisica, l"'elasticità" è la proprietà che permette ad un corpo di deformarsi sotto l'azione di una forza esterna e di riacquisire, se le deformazioni non risultano eccessive, la sua forma originale al venir meno della causa sollecitante. Se il corpo, cessata la sollecitazione, riassume esattamente la configurazione iniziale è detto perfettamente elastico. 
L'elasticità riguarda sia i corpi solidi che i fluidi. I primi possiedono sia elasticità di forma che di volume, reagiscono cioè elasticamente alle sollecitazioni che tendono a deformare il volume del corpo e a cambiare i suoi angoli; i fluidi invece presentano solo elasticità di volume, in quanto reagiscono elasticamente a una compressione o espansione ma non oppongono resistenza al cambiamento di forma, che dipende dal recipiente.
La sollecitazione massima che garantisce il comportamento elastico del materiale è detta limite di elasticità e, nel caso venga superata, si entra nella regione di comportamento plastico del pezzo, che consiste nel cedimento o nel flusso del materiale, a seconda che sia fragile o duttile rispettivamente. Il limite di elasticità trattandosi di una pressione è misurato in Pascal, ovvero una forza per unità di superficie: 
formula_1
Se il materiale è duttile, ovvero che permette plasticizzazioni, il limite elastico è la tensione di snervamento, mentre nel caso di materiali fragili, che sono privi del campo plastico, il limite elastico è la rottura del materiale.
Il modello matematico più semplice di rappresentazione del comportamento elastico è quello lineare della legge di Hooke (e della legge di Hooke generalizzata nel caso di stati tensionali pluriassiali), che nel caso di stato di sforzo monoassiale, tipico delle prove di trazione, è: formula_2 dove formula_3 è lo sforzo agente nel provino raffigurato in figura con formula_4 forza applicata alle sue estremità ed formula_5 superficie della sezione trasversale iniziale, formula_6 la deformazione del provino ovvero il suo allungamento relativo, con formula_7allungamento assoluto del provino, ossia la differenza fra la lunghezza finale formula_8 e quella iniziale formula_9 ed formula_10 il modulo di Young (o elastico), che è la costante di proporzionalità fra gli sforzi e le deformazioni in campo elastico.
Tale modello riveste un aspetto fondamentale sia in ambito teorico, per la possibilità di pervenire ad uno studio matematico completo dei problemi formulati, sia in ambito ingegneristico, per la ricaduta che esso ha nella modellazione e risoluzione di problemi di interesse tecnico e scientifico. Altri più complessi modelli matematici di elasticità nonlineare, importanti per la rappresentazione del comportamento delle gomme, fanno riferimento al modello di materiale iperelastico, mentre per mezzi porosi il modello si declina nella poroelasticità.
Lo studio dei corpi solidi elastici è oggetto della teoria dell'elasticità, una branca della meccanica dei solidi.
Il comportamento elastico dei diversi materiali ha origini microscopiche che si distinguono in base al particolare tipo di materiale. Possiamo parlare infatti di "elasticità entalpica" ed "elasticità entropica".
L'elasticità entalpica è caratteristica dei materiali cristallini, e deriva da un fenomeno che avviene a livello atomico. Le proprietà elastiche di questi materiali derivano dal tipo di interazione che si instaura fra i loro atomi costituenti, quando questi sono sottoposti ad un carico esterno. Se tali interazioni determinano un dislocamento degli atomi contenuto, questi, una volta rimosso il carico, riescono a rioccupare la loro posizione iniziale ed il materiale è detto elastico; se perdipiù il dislocamento è sufficientemente piccolo, è garantita la diretta proporzionalità fra deformazione e carico ed è valida pertanto la legge di Hooke. 
Il fitto reticolo cristallino di questi materiali permette solamente piccole deformazioni e spostamenti locali, da cui derivano l'alto limite di elasticità ed il grande modulo elastico. Questo comporta la necessità di esercitare elevate tensioni per ottenere deformazioni rilevanti. Nel caso in cui si rimanga al di sotto dello sforzo di snervamento del materiale, il rapporto tra sforzo e deformazione è pari alla costante modulo elastico o modulo di Young, che rappresenta la proporzionalità fra sforzo e deformazione nel campo lineare del materiale, descritta dalla legge di Hooke, e determina la pendenza del tratto rettilineo nel diagramma sforzo-tensione della prova monoassiale rappresentata in figura. 
L'elasticità dipende quindi dalla struttura microscopica del materiale e dalle forze di interazione che agiscono fra gli atomi che lo compongono. In particolare va considerata l'energia potenziale esistente tra ogni coppia di atomi, che può essere espressa in funzione della loro distanza. A una certa distanza d i due atomi sono in equilibrio, ossia la risultante delle forze di interazione tra i due è nulla. La variazione di tali forze (a causa della sollecitazione esterna) fa variare la distanza reciproca tra le particelle (producendo a livello macroscopico la deformazione del corpo: nel caso di trazione, per esempio, si ha uno "stiramento" dei legami). Per livelli relativamente bassi delle sollecitazioni, il lavoro meccanico necessario viene accumulato come energia elastica, all'interno del materiale, e viene restituito interamente al venir meno della causa sollecitante mentre le particelle ritornano alla loro posizione iniziale (il corpo riacquista la sua forma e dimensioni originarie). L'energia immagazzinata nel materiale è quantificabile dalla seguente relazione: formula_11 che graficamente è rappresentata dall'area sottesa alla curva tensione-deformazione rappresentata in figura, dove formula_12 è il lavoro svolto di deformazione, immagazzinato nel materiale come energia elastica, formula_13è l'andamento dello sforzo in funzione della deformazione formula_14, ed formula_15 è la deformazione finale che si raggiunge applicando il carico esterno.
Questo meccanismo è alla base del comportamento elastico macroscopico dei diversi materiali, ma al variare del tipo di materiale, e dunque della struttura microscopica, si delineano comportamenti elastici differenti.
L'elasticità entropica è caratteristica dei materiali polimerici costituiti a livello molecolare da catene; tale elasticità scaturisce da un movimento delle catene da uno stato ad elevata entropia (lo stato più probabile, in cui le catene sono aggrovigliate) a uno stato a bassa entropia (uno stato meno probabile, più ordinato, in cui le catene sono allineate), che avviene durante l'allungamento del materiale.
Materiali polimerici come la gomma, essendo costituiti a livello microscopico da molecole a catena, permettono grandi scorrimenti e deformazioni, e pertanto sono caratterizzati da bassi limiti di elasticità e piccolo modulo di elasticità. Ciò significa che a sforzi e tensioni relativamente bassi corrispondono già deformazioni apprezzabili macroscopicamente, così come punti di snervamento o rottura molto bassi. Questi materiali sono detti elastomeri, con un comportamento cosiddetto ad ""alta elasticità"" rispetto alla ""vera elasticità"" dei cristallini. Inoltre, a causa del precoce stiramento delle catene, causato da un ulteriore allungamento quando queste sono già state allineate, gli elastomeri hanno un comportamento elastico non lineare.
I materiali cellulari, come il legno, reagiscono in modo differente alla compressione e alla trazione. Grazie alla presenza di cavità nel materiale, la compressione mostra completa rigidità fino a quando le pareti di tali cavità non sono soggette a inflessione elastica, che permette di avere una notevole deformazione senza grande incremento di sforzo. Tali deformazioni inoltre, sono in gran parte recuperabili, ma una volta avvenute riportano il corpo a uno stato di rigidità, essendosi annullate le cavità. D'altra parte, queste non hanno la stessa influenza sulla trazione, che non permette la flessione elastica delle pareti nello stesso modo.
Per studiarne il comportamento se sottoposti a sforzo, i materiali possono essere modellati come privi di struttura interna e costituiti da un continuo solido. Rappresentando il corpo in un sistema di riferimento cartesiano, si può indicare la posizione di ogni suo punto tramite il vettore posizione: formula_16ed il loro spostamento con il vettore formula_17. Il vettore spostamento descrive come si deforma il corpo sotto carico, infatti: formula_18 è la distanza cartesiana fra due punti del corpo e formula_19è la stessa distanza dopo che il corpo si sia deformato ed è chiaramente funzione di formula_20. Si introduce la grandezza formula_21 detta deformazione, che al variare di formula_22 forma un tensore di rango 2, detto tensore delle deformazioni: formula_23dove i termini diagonali formula_24con formula_25 sono dette deformazioni normali e descrivono gli allungamenti o le contrazioni, le restanti formula_26con formula_27 sono detti scorrimenti e descrivono la variazione di forma, quindi degli angoli, rispetto al riferimento cartesiano.
Lo stato di sforzo è generalmente, e nella maggior parte dei casi, tridimensionale. Per studiarlo si sfrutta il Teorema di Cauchy ponendo una terna cartesiana sul punto formula_28 sotto studio e tagliando il corpo con un piano inclinato di normale formula_29 a distanza infinitesima formula_30 da formula_28, che individua insieme ai tre piani di riferimento un tetraedro, detto di Cauchy, rappresentato in figura. La faccia di normale formula_29ha superficie pari a formula_33, mentre le altre, di normale formula_34formula_35e formula_36, hanno superficie rispettivamente pari a formula_37, formula_38e formula_39dove formula_40, formula_41e formula_42sono i coseni direttori di formula_29. Il generico sforzo agente sul piano di normale formula_29 è formula_45, e sulle altre facce formula_46, formula_47 e formula_48, che per convenzione sono considerati positivi se entranti e quindi il meno sta ad indicare che sono uscenti dal volume infinitesimo. Per studiare il generico stato di sforzo formula_45di un punto appartenente al corpo basta imporre l'equilibrio statico nel tetraedro (I equazione cardinale della statica): 
formula_50 
che nel caso siano noti i tre vettori formula_46, formula_47 e formula_48si può determinare lo sforzo formula_45in qualsiasi punto del corpo. 
Si possono ora proiettare tutti e tre i vettori formula_46, formula_47 e formula_48nelle tre direzioni formula_58, formula_59e formula_60ed il vettore formula_45nella direzione normale e tangenziale del piano di normale formula_29, ottenendo: 
formula_63 
formula_64 
dove due delle tre componenti saranno tangenziali alla faccia di applicazione dello sforzo e la rimanente sarà normale alla faccia. Si compone infine il tensore degli sforzi, che descrive il generico stato di sforzo: 
formula_65
La densità di energia di deformazione è l'energia elastica immagazzinata dal materiale per unità di volume, e vale la relazione: formula_66ovvero l'incremento della densità di energia di deformazione formula_67è pari al lavoro svolto dagli sforzi formula_68per alterare le deformazioni formula_26. Si ricava allora che: formula_70.
La relazione formula_66 può essere espansa in serie con Taylor nell'intorno di formula_72nel caso di solido lineare e di stato iniziale scarico ed indeformato, ovvero con formula_73, ottenendo:
formula_74 alla quale se applichiamo formula_70 otteniamo la legge di Hooke generalizzata: formula_76 che nel caso di materiale isotropo diviene:
formula_77
dove formula_10 è il modulo elastico e formula_79 è il coefficiente di Poisson.
</text>
</doc>
<doc id="294308" url="https://it.wikipedia.org/wiki?curid=294308">
<title>Circuiti in serie e in parallelo</title>
<text>
I circuiti in serie e in parallelo indicano componenti elettrici in un circuito elettrico (ad esempio resistori, condensatori, induttori e generatori di tensione) che possono essere collegati fra loro "in serie" oppure "in parallelo" per mezzo di conduttori elettrici (ad esempio fili metallici) che trasportano al loro interno gli elettroni per il funzionamento dei circuiti 
Si parla di collegamento in serie quando due o più componenti sono collegati in modo da formare un percorso unico per la corrente elettrica che li attraversa; nel caso di componenti elettrici a due terminali (detti bipoli) il collegamento in serie prevede che l'estremità di ciascuno di essi sia collegata solo con l'estremità di un altro, l'alimentazione viene fornita tramite i morsetti liberi del primo e dell'ultimo bipolo.
Conoscendo il valore della resistenza (in ohm) e l'intensità della corrente elettrica (in ampere) è possibile determinare la tensione elettrica (in volt) su una resistenza con la legge di Ohm (valida sia in corrente continua sia in corrente alternata)
Conoscendo la differenza di potenziale "V" ai capi del resistore e il valore della sua resistenza "R", è possibile calcolare l'intensità della corrente elettrica "I" che circola utilizzando la formula inversa:
Le resistenze in serie sono attraversate tutte dalla stessa corrente quindi si ha:
formula_3
La resistenza totale di "n" resistori in serie è data dalla somma delle resistenze di ciascun resistore:
Il collegamento in serie di resistori a volte si adotta per necessità. Un tipico esempio è il circuito che genera l'alta tensione nei vecchi oscilloscopi a tubo catodico; Un resistore da 50 Megaohm avente ai suoi capi una tensione di 2000 volt, va incontro a conseguenze distruttive nel caso si verifichi un arco voltaico; per prevenire questo rischio, il costruttore realizza il resistore da 50 Megaohm collegando in serie 5 resistori da 10 Megaohm, conseguentemente ciascun resistore vede ai suoi capi una tensione di 400 volt, un valore di tutta sicurezza.
La capacità totale di "n" condensatori in serie è data dalla seguente relazione:
La tensione di funzionamento di una serie di condensatori è data dalla somma delle singole tensioni di funzionamento.
All'interno di ogni condensatore le cariche positive si raggruppano su una armatura in modo uguale alle cariche negative su quella opposta; poiché ogni armatura "positiva" di un condensatore è collegata a quella "negativa" del vicino, la quantità di carica positiva su un'armatura dovrà essere uguale alla carica negativa di quella collegata, il che fa sì che su tutti i condensatori sia presente la stessa quantità di carica formula_6.
Di conseguenza la tensione elettrica ai capi di ciascun elemento è diversa: 
Ne discende che il condensatore di capacità più piccola deve sopportare la differenza di potenziale più grande.
Si parla di collegamento in parallelo quando i componenti sono collegati ad una coppia di conduttori in modo che la tensione elettrica sia applicata a tutti quanti allo stesso modo. Riprendendo l'esempio delle persone, queste sono disposte fra due corde distese e parallele, ed ogni persona stringe ciascuna corda con una mano in modo che ognuno tenga con la mano destra la stessa corda che tutti gli altri tengono con la mano destra, e lo stesso per la mano sinistra. Inoltre gli utilizzatori sono paralleli l'uno all'altro e svolgono funzione indipendente: se uno non funziona gli altri funzionano.
La resistenza totale di n resistori in parallelo è data dalla relazione:
ovvero è il reciproco della somma dei reciproci delle resistenze dei singoli resistori
Poiché alle resistenze in parallelo viene applicata la stessa tensione, si ha:
formula_9
La formula sopra citata si semplifica nel caso di due soli resistori. In questo caso si avrà:
La capacità totale di n condensatori in parallelo è in questo caso la somma delle singole capacità:
Un esempio di condensatori collegati in parallelo si può trovare negli alimentatori per computer e negli apparecchi Hi-Fi; se necessita una capacità di 20.000 microfarad, è più conveniente impiegare 5 condensatori da 4000 microfarad piuttosto di uno solo da 20.000. Una soluzione analoga si può verificare con i resistori; se in un circuito occorre dissipare in calore 100 watt tramite un resistore da 50 ohm, a volte risulta più conveniente impiegare due resistori da 100 ohm - 50 watt, collegati in parallelo.
</text>
</doc>
<doc id="286922" url="https://it.wikipedia.org/wiki?curid=286922">
<title>Isolatore</title>
<text>
Un isolatore è uno strumento, un materiale o un corpo che ha la funzione, per l'appunto, di isolare, ossia di impedire — o grandemente ridurre — il propagarsi di una grandezza fisica. In relazione al tipo di grandezza fisica che isolano, si possono avere
Gli isolatori elettrici sono dispositivi che permettono di separare elettricamente un conduttore elettrico da altri materiali conduttori. Un caso comune è l'utilizzo per sostenere meccanicamente dei conduttori sospesi (cavo nudo, ovvero senza isolante) sulla struttura di supporto (esempio un palo o un traliccio), evitando dispersioni di corrente sulla struttura stessa. Possono essere realizzati con dischi in vetro o altro materiale ad alta rigidità dielettrica (ovvero che resistono a forte differenza di tensione senza produrre perforazione elettrica). Forme e dimensioni sono estremamente variabili: dai piccoli rocchetti di circa 1 cm, usati una volta negli impianti elettrici delle abitazioni, alle lunghe catene di i. sospesi, pesanti diversi quintali e lunghe qualche metro, per le linee di alta ed altissima tensione. I materiali tradizionali per la realizzazione degli isolatori elettrici sono il vetro e la ceramica, cui si sono affiancati, negli anni '70 e '80 i cd. materiali compositi, di origine sintetica. Da ricordare comunque che gli i. in ceramica sono mediamente più resistenti meccanicamente, di quelli in vetro, che però risultano essere maggiormente usati, per i minori costi di fabbricazione, dovuti essenzialmente alla pressoché totale assenza di scarti di lavorazione. (nella foto due isolatori modello 1/1920 di fabbricazione italiana in vetro verde prodotti da MIVA o da BORGO/FIDENZA). Esistono diversi criteri per la classificazione degli i.: i. per interno o per esterno, passanti o portanti, rigidi o sospesi, per bassa, media, alta tensione, telegrafici, telefonici, terminali, ecc. Per ultimo, da sapere che in questi ultimi anni sì è sviluppato (ed istituzionalizzato, con la creazione di una apposita associazione) il collezionismo di i. elettrici.
Dispositivo per isolare una struttura o costruzione dalle vibrazioni provocate da un movimento sismico. Gli isolatori sismici utilizzano varie tecniche tutte orientate a ridurre l'intensità delle oscillazioni indotte sulla struttura dalle vibrazioni o movimenti del terreno.
Permette di variare la rigidezza equivalente della struttura modificandone il periodo proprio di vibrazione.
Componente introdotto in un apparato, motore, elettrodomestico, costruzione o altro dispositivo, al fine di minimizzare il trasferimento di calore, al fine di proteggere da alte o basse temperature.
Componente inserito tra una fonte di rumore e una zona sensibile al rumore stesso, al fine di limitare il trasferimento di potenza acustica.
Gli isolatori a microonde sono dispositivi elettronici passivi che tramite l'interazione di un segnale elettrico con un campo magnetico permanente (ottenuto con magneti) realizzano una funzione di trasferimento unidirezionale, permettendo cioè il passaggio del segnale solo in una direzione, introducendo una forte attenuazione ai segnali che si propagano in direzione opposta. Sono comunemente utilizzati nei sistemi di comunicazione a microonde (Ponti radio) per proteggere gli amplificatori dal ritorno di potenza per riflessione da disadattamento di impedenza dei circuiti a valle o dell'antenna.
Dispositivi basati sullo stesso principio di funzionamento sono i circolatori, dispositivi a tre porte che hanno la caratteristica di permettere il transito dei segnali in modo circolare (porto A-&gt;B, B-&gt;C, C-&gt;A) e che sono per esempio utilizzati per accoppiare alla stessa antenna un trasmettitore ed un ricevitore isolando al contempo il ricevitore dal trasmettitore (permessi con bassa perdita i transiti: Trasmettitore -&gt; Antenna, Antenna -&gt; Ricevitore, Ricevitore -&gt; Trasmettitore in cui quest'ultimo non ha rilevanza ma attenua fortemente il transito Trasmettitore-&gt;Ricevitore).
Un isolatore ottico, o 'diodo ottico', è un componente ottico che consente la trasmissione di luce in una sola direzione.
È tipicamente utilizzato per scongiurare retroriflessioni indesiderate in un oscillatore ottico, come una cavità laser. Il funzionamento del dispositivo si basa sull'effetto Faraday (a sua volta generato dall'effetto magneto-ottico), che è sfruttato nel componente principale dell'isolatore ottico, il rotatore di Faraday.
Dispositivo che permette di separare dal punto di vista elettrico due apparati elettronici, pur permettendo il trasferimento di segnali. In pratica l'isolatore ottico realizza una conversione di segnale elettrico in ottico e viceversa, trasferendo l'informazione senza realizzare contatto fra parti metalliche, ed evitando quindi ogni rischio di scariche dannose tra i due apparati in comunicazione. Sono particolarmente importanti in ambienti soggetti a forti campi elettrici e/o magnetici che possono provocare correnti indesiderate tra circuiti.´
Un museo di isolatori si trova in una ex-cabina del trasformatore (monumento nazionale) in Lohr am Main nella via “Haaggasse". Questo è il più piccolo museo della Germania.
</text>
</doc>
<doc id="261778" url="https://it.wikipedia.org/wiki?curid=261778">
<title>Campo gravitazionale</title>
<text>
In fisica, il campo gravitazionale è il campo associato all'interazione gravitazionale.
In meccanica classica, il campo gravitazionale è trattato come un campo di forze conservativo. Secondo la relatività generale esso è espressione della curvatura dello spazio-tempo creata dalla presenza di massa o energia, quindi la forza di gravità sarebbe una forza apparente, ed è rappresentato matematicamente da un tensore metrico legato allo spazio-tempo curvo attraverso il tensore di Riemann. 
Il campo gravitazionale generato dalla Terra, ad esempio, in prossimità della superficie terrestre assume valori prossimi a 9,8 m·s e per convenzione si adotta tale valore di riferimento per l'accelerazione di gravità.
Il campo gravitazionale è un campo di forze conservativo. Il vettore del campo gravitazionale generato nel punto formula_1 nello spazio dalla presenza di una massa nel punto formula_2, origine del riferimento, è definito come:
dove formula_4 è la costante di gravitazione universale e formula_5 la massa. È quindi possibile esprimere la forza esercitata sul corpo di massa "m" come:
L'unità di misura del campo gravitazionale nel Sistema internazionale è:
dove formula_8 è il modulo di formula_9.
Il campo gravitazionale è descritto dal potenziale gravitazionale, definito come il valore dell'energia gravitazionale rilevato da una massa posta in un punto dello spazio per unità di massa. L'energia gravitazionale della massa è il livello di energia che la massa possiede a causa della sua posizione all'interno del campo gravitazionale; pertanto il potenziale gravitazionale della massa è il rapporto tra l'energia gravitazionale e il valore della massa stessa, cioè: 
Essendo il campo gravitazionale conservativo, è sempre possibile definire una funzione scalare "V" il cui gradiente, cambiato di segno, coincida con il campo:
Per ogni campo gravitazionale è possibile definire delle superfici ortogonali al campo in ogni punto dello spazio, dette superfici equipotenziali. Il significato fisico di queste superfici è chiaro se si considera il lavoro della forza di gravità lungo un cammino appartenente alla superficie: dato che lo spostamento è punto per punto ortogonale alla forza, il lavoro lungo questo cammino è nullo. Ciò vuol dire che masse uguali sulla stessa superficie equipotenziale hanno la stessa energia potenziale. Per esempio, nel caso di una sorgente sferica, le superfici equipotenziali sono sfere concentriche e le linee di flusso sono l'insieme delle semirette entranti nel centro delle sfere.
Indicato il campo gravitazionale come formula_12, a meno di fattori moltiplicativi e traslazionali, con formula_13 vettore posizione, si osserva che la sua divergenza in tre dimensioni è nulla. Infatti:
Il campo gravitazionale assume nell'ambito della teoria della relatività generale di Einstein una struttura molto più complessa. Esso rappresenta la differenza tra il tensore metrico dello spazio-tempo e il tensore metrico dello spazio-tempo piatto, o spazio-tempo di Minkowski. La deformazione dello spazio-tempo data dal campo gravitazionale viene talvolta rappresentata graficamente come la deformazione di un materasso, o di un telo elastico, ad opera di una palla pesante posta su di esso: qui lo spazio-tempo piatto è rappresentato dal telo perfettamente teso e, appunto, piatto.
Il tensore metrico dello spazio-tempo deformato dalla presenza di masse, oppure semplicemente energia, viene calcolato attraverso l'equazione di campo di Einstein:
dove formula_16 è il tensore metrico, formula_17 e formula_18 sono rispettivamente la curvatura scalare e il Tensore di Ricci, ottenuti come contrazione dal Tensore di Riemann, legato alle derivate del tensore metrico e formula_4 è la costante di gravitazione universale.
</text>
</doc>
<doc id="210540" url="https://it.wikipedia.org/wiki?curid=210540">
<title>Linea di campo</title>
<text>
In fisica, una linea di campo di un campo vettoriale, anche detta linea di forza nel caso di un campo di forze, è una curva ideale che ha come tangente in ogni punto la direzione del vettore del campo stesso. Per ogni punto passa una sola linea di campo che perciò si può dire univocamente definita.
Generalmente le linee di campo, ad esempio nel caso di un campo elettrico, vengono disegnate radialmente rispetto alla carica che genera il campo. Esse sono orientate uscenti se la carica è positiva, entranti se la carica è negativa. Il verso del campo viene indicato tramite un'opportuna freccia. Questo caso non è valido, ad esempio, per un campo magnetico, dove le linee di campo sono chiuse e si dispongono in direzione dei poli magnetici.
Il numero di linee di campo che vengono disegnate è anche un indice quantitativo dell'intensità del campo stesso.
In un campo elettrostatico le linee di campo non possono essere chiuse. Se per assurdo ci fosse una linea di campo chiusa, la circuitazione del campo lungo tale linea sarebbe "non" nulla, in contrasto con la conservatività del campo elettrico.
In un campo elettrostatico le linee di campo sono dunque linee "aperte" e hanno inizio o fine su una carica. Il verso di tali linee è sempre quello che va dalla carica positiva a quella negativa.
</text>
</doc>
<doc id="275584" url="https://it.wikipedia.org/wiki?curid=275584">
<title>Urto elastico</title>
<text>
In meccanica classica un urto elastico è un urto durante il quale si conserva l'energia meccanica totale del sistema, ed in particolare l'energia cinetica.
Nel caso di corpi prossimi a velocità della luce un urto elastico è un urto nel quale si conserva il quadrivettore quantità di moto.
In generale, nella risoluzione di un problema d'urto completamente elastico, si parte dalla conservazione della quantità di moto e dell'energia cinetica prima e dopo l'urto.
Nel caso di urti monodimensionali tra due corpi, le equazioni sono 2 equazioni scalari, mentre nel caso di urti in un piano esse sono 3 (le due componenti della quantità di moto e l'energia). Per quanto riguarda urti nello spazio tridimensionale, per la maggior parte dei problemi è valida l'assunzione che l'urto si svolga in un piano, perciò con un opportuno cambio di coordinate è possibile ricondursi al caso precedente; altrimenti si hanno 4 relazioni scalari.
Per problemi unidimensionali il numero di equazioni permette di risolvere completamente il moto, trovando cioè le velocità dei due corpi dopo l'urto; per problemi nel piano con corpi estesi queste non sempre bastano, ed è possibile trovare una soluzione solo per alcuni casi notevoli con geometrie semplici, come per esempio un urto elastico tra due sfere, per i quali si possono usare altre relazioni quali, per esempio, simmetrie del sistema.
Consideriamo due corpi approssimabili come punti materiali che urtino frontalmente. Mettiamoci in un sistema di riferimento "S". 
Indichiamo con:
Imponiamo la conservazione dell'energia cinetica "K" e della quantità di moto "P": otteniamo il sistema:
cioè:
Queste equazioni si risolvono facilmente raggruppando in ognuna di esse in un membro i termini con formula_5 e nell'altro membro i termini con formula_6, dividendo la prima equazione per la seconda e ricordando che formula_11.
Infatti dividendo la prima equazione per formula_12 e tenendo conto della proposizione precedente:
ovvero dividendo membro a membro:
formula_14
Da qui basterà risolvere un semplice sistema lineare per trovare le nostre due velocità finali:
ovvero:
Dalla soluzione appena trovata si vede nel caso di formula_17 e di un elevato valore di formula_18, il valore di formula_19 è piccolo se le masse sono approssimativamente uguali: colpendo un corpo con una particella molto più leggera non si modifica in maniera significativa la velocità, colpendolo con una particella di massa molto superiore, si induce la particella veloce a rimbalzare con velocità diminuita. Per questo motivo un moderatore (un mezzo che rallenta i neutroni veloci, trasformandoli in neutroni termici capaci di sostenere una reazione a catena) è costituito da un materiale formato da atomi con nuclei leggeri (con la proprietà aggiuntiva che non assorbano facilmente i neutroni): l'idrogeno, il più leggero, ha circa la stessa massa di un neutrone.
Dato che durante gli urti il sistema è isolato, il centro di massa si muove di moto rettilineo uniforme, con velocità media ponderata "&lt;v&gt;":
Di conseguenza abbiamo:
cioè:
Usando l'energia cinetica si può scrivere
Dividendo per la (*) otteniamo:
che si riscrive come
Da ciò notiamo che "la velocità relativa di una particella rispetto all'altra è invertita dall'urto". Nel caso di particelle con masse differenti, la particella più pesante si muove lentamente verso il centro di massa, e rimbalza con la stessa bassa velocità, mentre la particella più leggera si muove rapidamente verso il centro di massa e dopo l'urto se ne allontana con eguale velocità.
Si dimostra anche che "rispetto al centro di massa entrambe le velocità appaiono invertite dopo l'urto". Infatti, nel sistema di riferimento del centro di massa si ha:
da cui:
e analogamente:
Per evitare di ricadere nel caso banale di assenza d'urto avevamo imposto formula_35, che implica formula_36. Perciò otteniamo:
CVD.
La meccanica classica fornisce una buona approssimazione quando tratta oggetti con dimensioni macroscopiche che si muovono a velocità molto minore della velocità della luce. Oltre i limiti classici, fornisce dei risultati erronei. La quantità di moto totale di due corpi che si urtano è dipendente dal sistema di riferimento. Le equazioni che governano gli urti nel caso relativistico discendono dalla conservazione del quadrimpulso, o quadrivettore quantità di moto (per approfondimenti sulle leggi della dinamica relativistiche vedi "Teoria della relatività ristretta"). Distinguendo tra parte temporale e spaziale abbiamo:
formula_38
dove formula_39 è il fattore di Lorentz.
La prima equazione rappresenta la conservazione della parte spaziale della quantità di moto formula_40, alla quale diventa formalmente identica utilizzando la "massa relativistica", ora in disuso. Moltiplicando la seconda equazione per "c" riconosciamo invece la conservazione dell'energia relativistica formula_41. Possiamo quindi riscrivere il sistema come:
In generale risolvere direttamente le equazioni sovrastanti è molto difficile dal momento che il grado dell'equazione è troppo elevato. Come per il caso classico, un aiuto può venire da un cambio di sistema di riferimento, avendo cura di comporre le velocità non con la composizione galileiana ma con il loro equivalente nella relatività ristretta. Un buon sistema di riferimento può essere, ad esempio, quello del centro di massa; avendo chiamato con "v" la velocità di trascinamento tra un sistema e l'altro, dalla regola di composizione delle velocità in relatività speciale le velocità prima dell'urto nel sistema di riferimento del centro di massa formula_43 e formula_44 sono:
da cui:
Quando formula_51 e formula_52,
e perciò otteniamo:
I calcoli della meccanica classica risultano quindi corretti quando la velocità di entrambi i corpi è molto minore della velocità della luce (circa 3 x 10 m/s).
Un modo per schematizzare le interazioni tra particelle subatomiche è quello di considerare l'interazione come urto elastico. Supponiamo di avere due particelle entrambe di massa a riposo "m", supposta nota, che si muovono l'una contro l'altra a velocità formula_61 e formula_62. Dopo l'urto si forma un'unica particella. Troviamo la massa "M" e la velocità "v" di questa nuova particella. Le equazioni (IV) portano al sistema:
Ricaviamo i fattori γ delle due velocità iniziali:
Dalla conservazione dell'energia ricaviamo "M":
Sostituiamo nell'altra equazione:
Nota "v" sostituiamo nell'equazione per "M", che risulta essere:
Abbiamo scoperto che nell'interazione la massa della nuova particella non è uguale alla somma delle masse delle altre due, ma superiore. Infatti parte dell'energia dovuta alla velocità delle particelle si è tramutata in massa; come è noto, nella teoria della relatività la massa e l'energia sono intercambiabili (la celebre formula E=mc²): infatti durante le interazioni delle particelle subatomiche questi scambi si verificano continuamente.
Un'importante applicazione degli urti relativistici è l'effetto Compton, che collega l'angolo di deflessione di un fotone che interagisce con un'altra particella con la variazione di energia del fotone stesso, cioè della sua lunghezza d'onda. L'interazione è schematizzata come urto elastico, nel quale valgono le (IV). L'urto avviene sul piano, e la conservazione della quantità di moto implica la conservazione delle sue proiezioni lungo gli assi. Mettendo a sistema con la conservazione dell'energia, si ottiene:
dove formula_71 è l'angolo di deflessione del fotone e 
è detta "lunghezza d'onda di Compton".
La legge di Newton (come la conservazione della quantità di moto) si applica alle componenti della velocità risolte lungo le comuni superfici normali dei corpi collidenti al punto di contatto. Nel caso di due sfere le componenti della velocità coinvolte saranno le componenti risolte lungo la linea congiungente i centri nell'istante dell'urto. Di conseguenza, le componenti della velocità perpendicolari a questa linea resteranno invariate durante l'urto.
Per risolvere un'equazione che coinvolge due corpi che collidono in un sistema bidimensionale, la velocità complessiva di ciascun corpo deve essere scomposta in due velocità ortogonali: una tangente alla superficie comune normale dei due corpi collidenti nel punto di contatto, l'altra lungo la linea di collisione. Siccome l'urto imprime forze solo lungo la linea di collisione, le velocità tangenti al punto di collisione non cambiano. Per calcolare le velocità lungo la linea d'urto si possono utilizzare le stesse equazioni di un urto monodimensionale. Le velocità finali possono essere calcolate dalle due nuove componenti e dipenderanno dal punto di collisione. Sono stati condotti degli studi sugli urti bidimensionali per molti corpi nella struttura di un gas bidimensionale.
La quantità di moto di due corpi dipende dalle loro velocità effettive e masse, per cui non si può prevedere la quantità di moto di due corpi se le energie cinetiche dei due sono eguali.
Per le due monete in figura la componente che cambia si può trovare tramite il prodotto scalare della velocità con un versore che indica la direzione diretta dell'urto.
</text>
</doc>
<doc id="272315" url="https://it.wikipedia.org/wiki?curid=272315">
<title>Mezzo trasmissivo</title>
<text>
Il mezzo trasmissivo indica nelle telecomunicazioni il canale a livello fisico entro il quale viaggiano i segnali rappresentativi dell'informazione; ne consegue quindi anche la sostanziale equivalenza tra i termini "mezzo trasmissivo" e "canale trasmissivo".
Affinché l'informazione viaggi a distanza, cioè da e verso entità dislocate in luoghi diversi, necessita di una elaborazione che la trasformi in segnali elettrici e/o elettromagnetici i quali, a loro volta, devono essere adattati ai canali utilizzati per il trasporto.
Un canale di trasmissione "ideale" dovrebbe possedere una banda sufficientemente larga ed uniforme per contenere lo spettro del segnale di informazione senza distorcerlo e dovrebbe poterlo trasferire a qualsivoglia distanza senza introdurre degradamenti nella qualità elettrica di origine; la realtà risulta diversa in quanto sono presenti fattori di degradazione tipici quali:
I mezzi trasmissivi possono classificarsi secondo il seguente schema: 
</text>
</doc>
<doc id="221400" url="https://it.wikipedia.org/wiki?curid=221400">
<title>Durezza</title>
<text>
La durezza è un valore numerico che indica le caratteristiche di deformabilità plastica di un materiale. È definita come "la resistenza alla deformazione permanente".
Le prove di durezza determinano la resistenza offerta da un materiale a lasciarsi penetrare da un altro ("penetratore").
Esistono diverse scale per misurare la durezza dei materiali. Le più usate sono infatti:
Le prove di durezza si eseguono con macchine provviste di penetratori con forme diverse e con diverse metodologie.
Si basa, nel calcolo della durezza, sulla misura del diametro dell'impronta lasciata dal penetratore. Anche la Vickers si basa sullo stesso principio e la prova Vickers viene chiamata anche prova di microdurezza.
Rapida, economica, non distruttiva (oggetto riutilizzabile), possibilità di impiegare carichi particolarmente alti.
Se si moltiplica per 3,3 la durezza Brinell di un acciaio normalizzato si ottiene il suo carico di rottura.
Sono difficili i confronti tra diverse misure Brinell (a parità di carico applicato, il penetratore può affondare in misure diverse, cambiando anche l'inclinazione delle facce e la distribuzione degli sforzi).
Infatti è possibile confrontare prove eseguite con parametri diversi a patto che il rapporto formula_2 risulti verificato.
Si basa sulla misura dell'area dell'impronta lasciata dal penetratore. Viene chiamata prova di microdurezza, per via dei piccoli carichi applicati al penetratore.
Angolo di apertura: formula_6
L'inclinazione delle facce è costante; si usano anche carichi piccoli per fare misure di durezza ravvicinate, precisione della misurazione.
La scala è unica per tutti i materiali.
Costosa, notevole perdita di tempo nella lettura delle impronte che si può fare solo al microscopio.
Prova di durezza messa a punto da Steven Rockwell. Si basa sull'affondamento diretto dell'impronta e non sulla durezza misurata come pressione. Le scale di durezza ottenute sono convenzionali.
Velocità (usata in campo industriale).
È una misura di durezza solo convenzionale. L'unità di misura è rappresentata dai punti della scala utilizzata (HRC per la scala Rockwell, HV per la Vickers, HB per la Brinell, ecc.)
I valori di durezza ottenuti con le diverse scale non si possono confrontare perché utilizzano unità di misura diverse. Confronti empirici possono essere effettuati ma con molta attenzione (ad es. materiali simili, dimensione di impronte simili, ecc.) (vedi norma ISO 18265)
Misura la durezza in senso fisico, resistenza che il campione offre all'abrasione, si stabilisce la scala di durezza o misurando la larghezza del solco o il peso applicato. Per i metalli però non può fornire utili indicazioni.
Il principio su cui si basa lo Scleroscopio di Shore (durometro Shore) è il rimbalzo di una sfera d'acciaio che cade da una determinata altezza sul saggio. Il valore della durezza è definito dall'altezza del rimbalzo. 
La prova è impiegata per il collaudo di cilindri per laminatoi a freddo.
</text>
</doc>
<doc id="218922" url="https://it.wikipedia.org/wiki?curid=218922">
<title>Legge di Snell</title>
<text>
Nell'ottica geometrica la legge di Snell, nota anche come legge di Descartes o legge di Snell-Descartes (o legge di Cartesio o legge di Snell-Cartesio), descrive le modalità di rifrazione di un raggio luminoso nella transizione tra due mezzi con indice di rifrazione diverso, e deriva dall'equazione iconale.
Il nome della legge di Snell rispetta la legge dell'eponimia di Stigler. La legge è documentata per la prima volta in un manoscritto scritto intorno al 984 del matematico arabo Ibn Sahl, che la usò per ottenere i profili delle lenti asferiche (lenti che fuocheggiano la luce senza indurre aberrazioni geometriche). Fu poi scoperta di nuovo da Thomas Harriot nel 1602, che però non pubblicò il suo lavoro. Nel 1621, fu scoperta ancora una volta da Willebrord Snell, in una forma matematicamente equivalente, ma rimase inedita fino alla sua morte. René Descartes derivò indipendentemente la legge in termini di funzioni sinusoidali nel suo trattato "Discorso sul metodo" del 1637 e la usò per risolvere diversi problemi di ottica. In francese la legge di Snell è chiamata "di Descartes" o "di Snell-Descartes".
La luce si propaga nel vuoto alla velocità costante c.
La figura mostra due mezzi trasmissivi con indice di rifrazione "n" (a sinistra) e "n" (a destra) in contatto tra loro attraverso una superficie, che viene chiamata interfaccia (linea verticale in figura). Nel caso "n" &gt; "n", la luce ha una velocità di fase più bassa nel secondo mezzo.
Il raggio luminoso PO proveniente dal mezzo di sinistra colpisce l'interfaccia nel punto O. A partire da tale punto O tracciamo una retta perpendicolare all'interfaccia stessa, che viene chiamata "normale" all'interfaccia (linea orizzontale in figura). L'angolo tra la normale e il raggio luminoso PO viene chiamato "angolo d'incidenza", θ.
Il raggio attraversa l'interfaccia e prosegue nel mezzo di destra, indicato come OQ. L'angolo che tale raggio (rifratto) forma con la normale si chiama "angolo di rifrazione", θ.
La legge di Snell fornisce la relazione tra gli angoli θ e θ:
formula_1
Si noti che nel caso θ = 0° (ovvero il raggio risulta perpendicolare all'interfaccia) la soluzione è θ = 0° per qualunque valore di "n" e "n". In altri termini, un raggio che entra in un mezzo in modo perpendicolare alla sua superficie non viene mai deviato.
Quanto detto sopra vale anche nel caso di un raggio luminoso che passa da un mezzo più denso a uno meno denso; la simmetria della legge di Snell mostra che gli stessi percorsi luminosi sono validi anche nella direzione opposta.
Una regola di carattere qualitativo per determinare la direzione della rifrazione è che il raggio luminoso è sempre più vicino alla normale dal lato del mezzo più denso.
La legge di Snell è valida in generale solo per mezzi isotropi, come il vetro. Nel caso di mezzi anisotropi (ad esempio alcuni cristalli) il fenomeno della birifrangenza può dividere in due il raggio rifratto. Si vengono allora ad avere due raggi, uno "ordinario" (raggio "o") che segue la legge di Snell, e uno "straordinario" (raggio "e") che può non essere complanare con quello incidente.
Si consideri l'equazione iconale nella forma:
formula_2
dove x è la ascissa curvilinea lungo il cammino ottico, n è l'indice di rifrazione e formula_3 il versore del raggio ottico ed effettuando il prodotto vettoriale per il versore formula_4 della posizione si ottiene:
formula_5
dato che la derivata è un operatore lineare, si può trasportare dentro nel modo più semplice il prodotto vettoriale:
formula_6
si arriva quindi alla legge di Snell:
formula_7
che si esprime nella forma più comune chiamando θ l'angolo fra la direzione della posizione e quella del raggio ottico:
formula_8
e considerando il fatto che la derivata parziale nulla equivale a un argomento costante rispetto alla variabile di derivazione:
formula_9
Il caso più semplice è quello in cui l'indice di rifrazione è uniforme lungo l'ascissa curvilinea:
formula_10
In questo caso si vede immediatamente che la traiettoria del raggio risulta rettilinea, con inclinazione uniforme:
formula_11
Nel caso invece l'indice di rifrazione sia lineare con l'ascissa curvilinea:
formula_12
il raggio risulta inclinato in ogni punto della sua traiettoria con legge:
formula_13
questa comporta uno smorzamento della deviazione lungo la traiettoria: col limite all'infinito si vede che l'inclinazione asintotica risulta nulla, qualunque sia l'inclinazione iniziale:
formula_14
quindi questo mezzo riesce ad allineare dei raggi ottici con qualsiasi direzione iniziale, anche se sempre in modo incompleto poiché fisicamente non può essere realizzato con un'estensione infinita, con efficienza proporzionale al cammino ottico e all'intensità dell'indice di rifrazione.
Nel passaggio da un mezzo più denso a uno meno denso (ovvero, "n" &gt; "n") si può verificare facilmente che l'equazione formula_15 sia priva di soluzioni quando θ supera un valore che viene chiamato angolo critico:
Quando θ &gt; θ non appare alcun raggio rifratto: la luce incidente subisce una riflessione interna totale ad opera dell'interfaccia. Si genera un'onda di superficie, o onda evanescente ("leaky wave"), che decade esponenzialmente all'interno del mezzo con indice di rifrazione "n".
Dal versore s del raggio luminoso incidente, e il versore p, normale all'interfaccia, è possibile ricavare i versori associati al raggio riflesso e rifratto:
La legge di Snell può essere legata al Principio di Fermat: 
Infatti si può verificare che il cammino seguito dalla luce è un punto stazionario per il cammino ottico, vale a dire che in corrispondenza di esso la derivata del cammino ottico si annulla. In alternativa, la relazione può essere ottenuta considerando l'interferenza di tutti i possibili percorsi che l'onda di luce può percorrere dalla sorgente all'osservatore - risulta che l'interferenza è distruttiva ovunque, eccetto che negli estremi di fase (dove è costruttiva) - che diventa il percorso effettivo.
In una classica analogia della brachistocrona proposta da Feynman una spiaggia è una regione a indice di rifrazione più basso del mare; il modo più rapido per un bagnino sulla spiaggia di raggiungere a velocità costante una persona che sta affogando è percorrere il cammino ottico ovvero seguire la legge di Snell.
</text>
</doc>
<doc id="229547" url="https://it.wikipedia.org/wiki?curid=229547">
<title>Fatica (scienza dei materiali)</title>
<text>
La fatica è un fenomeno meccanico di progressiva degradazione di un materiale sottoposto a carichi variabili nel tempo (in maniera regolare o casuale) che può portare alla sua rottura (cedimento a fatica o rottura per fatica) anche se sia rimasto nel suo limite d'elasticità, cioè nonostante durante la vita utile del materiale l'intensità massima dei carichi in questione si sia mantenuta ad un valore sensibilmente inferiore alla tensione di rottura o di snervamento statico (in assenza di cicli di sforzo) del materiale stesso.
Storicamente scoperta e studiata come fenomeno prettamente metallurgico (quindi nell'ambito dei materiali metallici), in seguito il termine "fatica" è stato usato anche per le altre classi di materiali, come i materiali polimerici o i materiali ceramici.
Si stima che la fatica sia il fenomeno responsabile della grande maggioranza dei cedimenti degli organi di macchine in materiale metallico in fase di esercizio: approssimativamente il 90% delle rotture segue i tratti caratteristici del cedimento a fatica.
A seconda della causa scatenante il fenomeno della fatica si può distinguere in:
I primi studi intorno alla fatica vennero compiuti alla fine del secolo XIX, in seguito a una serie di rotture "inspiegabili" di assi ferroviari progettati per resistere a carichi (statici) ben superiori a quelli cui invece avveniva la loro rottura improvvisa in esercizio. August Wöhler intuì che il fenomeno era dovuto alla natura ciclica del carico cui l'assale era sottoposto (flessione rotante) e tentò di ricostruire lo stato di sollecitazione in laboratorio, mettendo in relazione l'ampiezza massima del ciclo di sollecitazione con il numero di cicli che il pezzo sopportava prima della rottura: ne ricavò una serie di curve su base statistica che sono chiamate "diagrammi di Wöhler" e costituiscono lo strumento base per la progettazione di componenti meccanici sollecitati a fatica.
Ad esempio un pezzo in grado di resistere a 50 kg/mm² in maniera statica (cioè in assenza di cicli di sforzo) può scendere a soli 10–12 kg/mm² se sottoposto a oltre 100 milioni di cicli. La curva di Wöhler divide il piano in due parti: per qualsiasi condizione al di sopra della curva il pezzo va incontro a rottura per fatica, mentre per qualsiasi condizione al di sotto della curva il pezzo non va incontro a rottura per fatica.
Da questi diagrammi si evidenzia per alcuni materiali l'esistenza di un limite inferiore di sforzo massimo al di sotto del quale il materiale non si rompe per effetto di fatica nemmeno per un numero "molto alto" (idealmente infinito) di cicli. Questo valore dello sforzo è detto limite di fatica del materiale. Nella pratica ingegneristica, il limite di fatica viene determinato per un numero ben preciso di cicli, che anche se non è infinito, corrisponde ad un numero particolare elevato. Ad esempio nel caso di leghe di rame il limite di fatica si riferisce a 100 milioni di cicli.
Va sottolineato che i valori ottenuti tramite la curva di Wöhler sono relativi ai provini adoperati durante la prova; per lo specifico componente meccanico, si introducono diversi coefficienti correttivi che tengono conto di processi di lavorazione (e possibili stati tensionali residui), dimensioni, condizioni di esercizio, intensificazione delle tensioni in corrispondenza di intagli, e altri fattori che possono influenzare la resistenza a fatica del componente stesso.
Nel caso dei materiali metallici, la fatica è legata ai fenomeni di micro-deformazioni plastiche cicliche locali indotte dal ciclo di sollecitazioni. Esse sono dovute al fatto che, per effetto di vari tipi di microintagli e/o discontinuità (bordi di grano, inclusioni non metalliche, composti interstiziali, rugosità superficiali), il valore dello sforzo può superare localmente il carico di snervamento, anche se il carico macroscopico esterno rimane sempre al di sotto di esso.
In particolare il danneggiamento per fatica procede attraverso i seguenti stadi:
La propagazione della frattura è facilmente osservabile in qualunque oggetto rotto a fatica. Un oggetto rotto a fatica presenterà infatti due superfici diverse in corrispondenza della sezione di rottura:
Il limite di fatica è correlato alla tensione di rottura R e indirettamente ai fattori che la modificano, che sono:
Dunque le strutture non omogenee e lamellari creano maggiori concentrazioni di sforzi nel materiale e sono quindi più rischiose; ad esempio la perlite ha una struttura che peggiora la resistenza alla fatica.
In genere gli acciai da bonifica sono più resistenti a frattura fragile.
Le inclusioni sono dannose se in quantità e con geometria lamellare. In linea di massima è quindi più resistente un pezzo ottenuto per solidificazione sottovuoto rispetto ad un pezzo ottenuto per colata.
Si considerano come fattori meccanici tutti quelli legati all'esercizio e al dimensionamento del pezzo metallico.
È necessario eliminare i solchi lasciati dagli utensili di lavorazione, in quanto in essi si crea una concentrazione di tensioni. Comunque una superficie ben levigata apporta significativi vantaggi solo su pezzi in acciai ad alta resistenza, per i quali è quindi indispensabile una accurata lavorazione. Si sottolinea poi che è importante pure evitare che una successiva corrosione crei irregolarità superficiali.
La cosa più importante comunque è la finitura superficiale.
Si ha una diminuzione notevole del limite di fatica man mano che aumentano le irregolarità superficiali.
Un'azione molto accentuata nell'abbassare il limite di fatica è svolta da una corrosione che sia contemporanea alla sollecitazione di fatica tanto è vero che il danneggiamento continua a crescere con il numero di cicli qualunque sia la sollecitazione applicata.
Anche la forma del pezzo ha importanza sulla vita a fatica: ogni lieve variazione di sezione, determinando delle concentrazioni di tensioni e localizzando le deformazioni, agisce sempre nel senso di una netta diminuzione del limite di fatica, per questo hanno un'azione dannosa fori, intagli e spigoli vivi.
Le cricche di fatica nucleano quasi sempre (eccetto alcuni casi tipici, come la fatica per contatto ciclico negli ingranaggi) su una superficie libera del pezzo in questione: questo per un concorso di cause (in superficie sono in genere massimi gli sforzi dovuti a flessione o torsione; in superficie sono in genere presenti difetti microscopici come la rugosità superficiale che fungono da microintagli e favoriscono l'innesco...). Per prevenire il danneggiamento per fatica o per migliorare la resistenza ad esso in genere si ricorre a trattamenti quali:
È inoltre necessario, in fase di progettazione di un componente che dovrà resistere a fatica, curare bene il disegno dello stesso in modo che non presenti intagli o brusche variazioni di sezione che possano amplificare localmente gli sforzi e in tal modo favorire la nucleazione di cricche di fatica.
Un pistone in alluminio di un motore diesel ad iniezione diretta, presenta una cricca all'interno di una cava del segmento; in prossimità si trova pure un foro di lubrificazione.
La causa di innesco della cricca può ragionevolmente essere la concentrazione degli sforzi, infatti localmente sono presenti diversi spigoli vivi.
Sezionando il pezzo in questione ed osservando la superficie fessurata si possono notare i segni caratteristici di una rottura per fatica: le linee di cresta e le linee di arresto frontali (linee di spiaggia).
Nell'immagine sono evidenziate in rosso le linee di cresta, tipiche della nucleazione: infatti hanno origine proprio dalla gola, e si nota un'intensificazione in prossimità del foro (effetto intaglio). 
Le linee nere, dette linee di arresto frontali, rappresentano l'avanzamento del fronte di frattura.
Un ingrandimento al microscopio metallografico permette di visualizzare ancora più in dettaglio le linee di cresta alla periferia, mentre le linee di arresto si possono notare come variazione di luminosità della superficie (dovuta al cambio di piano).
La zona di cedimento fragile non si nota in quanto non si è avuta rottura completa del pezzo: infatti la sezione resistente a geometria toroidale ha conservato la sua integrità per un settore ben maggiore di quello fessurato.
Evidentemente i carichi statici non hanno superato il limite di resistenza della sezione residua, quindi non si è avuto una rottura di schianto.
Per i materiali polimerici, il fenomeno della fatica è complicato dalla loro natura viscoelastica. La dipendenza dal tempo della risposta del materiale in termini di deformazioni ad uno sforzo applicato fa sì che, sottoposto ad un carico ciclico variabile sinusoidalmente, a regime, un materiale polimerico presenterà uno sfasamento "delta" tra l'andamento degli sforzi applicati e quello delle deformazioni. L'entità di questo sfasamento dipende da quanto il comportamento sia elastico piuttosto che viscoso: nei casi limite, lo sfasamento sarà nullo (comportamento perfettamente elastico) o pari a un quarto di periodo (comportamento perfettamente viscoso).
Questo sfasamento fa sì che, ad ogni ciclo di sollecitazione, il materiale rilasci energia a causa dell'isteresi viscoelastica: questa energia viene dissipata sotto forma di calore, che può anche essere considerevole e portare il polimero, localmente, a superare la temperatura di transizione vetrosa.
L'insorgere di fenomeni termici di questa entità sposta il problema dal piano puramente meccanico della fatica a quello dei fenomeni termici: il materiale scaldandosi sempre di più cede per creep, o per rammollimento locale, piuttosto che per fatica. In questo caso si parla di ""cedimento termico per fatica"".
Se invece il ciclo di isteresi è sufficientemente ridotto o il materiale ha proprietà termiche tali da consentirgli di dissipare il calore prodotto in maniera stabile, raggiungendo una temperatura di equilibrio non troppo elevata, allora a prevalere è l'aspetto meccanico del fenomeno, e si parla di ""cedimento meccanico per fatica"".
</text>
</doc>
<doc id="213526" url="https://it.wikipedia.org/wiki?curid=213526">
<title>Diffrazione</title>
<text>
In fisica la diffrazione è un fenomeno associato alla deviazione della traiettoria di propagazione delle onde (come anche la riflessione, la rifrazione, la diffusione o l'interferenza) quando queste incontrano un ostacolo sul loro cammino. È tipica di ogni genere di onda, come il suono, le onde sulla superficie dell'acqua o le onde elettromagnetiche come la luce o le onde radio; il fenomeno si verifica anche nelle particolari situazioni in cui la materia mostra proprietà ondulatorie, in accordo con il dualismo onda-particella.
Gli effetti di diffrazione sono rilevanti quando la lunghezza d'onda è comparabile con la dimensione dell'ostacolo: in particolare per la luce visibile (lunghezza d'onda attorno a 0,5 µm) si hanno fenomeni di diffrazione quando essa interagisce con oggetti di dimensione sub-millimetrica.
Qualunque deviazione di un raggio di luce non imputabile a riflessione o rifrazione è chiamato "diffrazione". Questa è la classica definizione riscontrata nel trattato classico di Ottica di Arnold Sommerfeld. È sorprendente notare che questa definizione ricalca quanto descritto per la prima volta dal Gesuita Francesco Maria Grimaldi (si veda l'originale definizione nella seconda figura che riproduce l'originale paragrafo nel trattato di F. M. Grimaldi), coniandone il termine che significa "frazionamento in più parti" nel 1665. Isaac Newton attribuì la causa del fenomeno a un "incurvamento" dei raggi luminosi (non osservando, come tutti gli Ottici Newtoniani, le frange all'interno dell'ombra di un capello). 
Il termine newtoniano che designa la diffrazione è "inflexion". Thomas Young studiò la diffrazione come sovrapposizione tra la luce direttamente trasmessa oltre una apertura in uno schermo (o un ostacolo) e un'onda avente origine dal bordo dell'apertura o dell'ostacolo. Lo stesso Augustin-Jean Fresnel adottò inizialmente il modello di Thomas Young, ma alcune esperienze atte ad evidenziare variazioni della figura di diffrazione dai parametri caratteristici del bordo (natura, geometria del bordo) e una inversione rispetto alla posizione prevista delle frange scure nella regione esterna all'ombra di un capello, lo indussero ad un abbandono della teoria dell'onda di bordo (stabilita da A. Fresnel in modo del tutto indipendente da Thomas Young), a favore della teoria basata sul principio di Huygens, riuscendo soprattutto a fornire una descrizione del fenomeno dal punto di vista matematico.
È da notare che la teoria dell'onda di bordo di Thomas Young ha precursori "Newtoniani" antecedenti a Thomas Young, la cui teoria è in alcuni punti non chiara e priva di supporto matematico. In genere la posizione di Thomas Young, cui si attribuisce il merito di avere per primo stabilito la natura "periodica" della luce, è in realtà incerta (il termine "lunghezza d'onda" non è "mai" usato) mentre è una costante delle sue ricerche l'analogia tra "suono" e "luce". Tuttavia, almeno all'epoca dei pionieri (T. Young e A. Fresnel) né la teoria dell'onda di bordo, né il principio di Huygens hanno un supporto teorico che giunge solo nel 1883 ad opera di G. Kirchhoff e, anche se inosservato, da G. A. Maggi nel 1886 per la teoria dell'onda di bordo.
Di fronte ad un fenomeno di diffrazione, nel caso ottico, si possono compiere alcune osservazioni preliminari.
Il caso generale del fenomeno è la "diffrazione di Fresnel" (o da campo vicino), dove la sorgente di luce e il piano di osservazione sono posti a distanza finita dalla fessura.
La "diffrazione di Fraunhofer" (o da campo lontano), invece, è un caso particolare della precedente, ma molto più semplice da analizzare: essa si ha infatti quando la sorgente e il piano sono posti a distanza infinita dal diaframma, così che i raggi incidenti possano essere considerati paralleli fra loro.
Un esempio di questo caso è quello di una sorgente di luce puntiforme (o rettilinea), come il tratto diritto del filamento di una lampadina o un fascio laser, vista da una distanza di un paio di metri attraverso due lamette distanti tra loro mezzo decimo di millimetro.
Le caratteristiche della diffrazione sono quindi che:
Fenomeni di diffrazione possono essere osservati quotidianamente, in particolare quelli che interessano la luce visibile: per esempio, le tracce incise sulla superficie di un CD o di un DVD agiscono come un reticolo di diffrazione, creando il familiare effetto arcobaleno; anche i piccoli ologrammi, ad esempio delle carte di credito, si basano sulla diffrazione. In natura, si possono osservare colori cangianti dovuti a diffrazioni interferenziali, come quelli delle piume del pavone, o della corazza di alcuni coleotteri, o delle ali di molte farfalle (figura a sinistra), che sono colorate grazie all'interferenza delle onde diffratte da parte di microscopiche scaglie disposte regolarmente.
La diffrazione atmosferica causata da microscopiche gocce d'acqua in sospensione è la responsabile degli anelli luminosi visibili attorno alle sorgenti di luce; la stessa ombra di un oggetto può mostrare deboli effetti di diffrazione sui bordi.
Una figura "policromatica" analoga alla farfalla nella foto si osserva tra le trame di un ombrello quando si guarda una luce lontana attraverso di esse. La diffrazione costituisce un limite nella risposta di qualunque strumento ottico e pertanto riguarda varie tecnologie: essa infatti pone un limite alla risoluzione di fotocamere, videocamere, telescopi e microscopi.
A causa della diffrazione le onde marine formano figure intricate quando incrociano un piccolo ostacolo, come un faro in mare, o attraversano una apertura stretta (figura a destra), come un canale o l'ingresso di un porto.
La diffrazione può venire "intuitivamente" "letta" come una richiesta di continuità da parte del fronte d'onda che subisce una discontinuità dal bordo (o dai bordi) di un ostacolo. La figura a fianco, che simula la diffrazione di un'onda piana attraverso la fenditura, ricorda quanto osservato in un'onda alla superficie dell'acqua quando passa attraverso una fenditura. Oltre la fenditura il fronte d'onda incidente è "tagliato" dai due bordi. La parte di fronte d'onda contigua a ciascun bordo piega attorno al bordo stesso fornendo così una perturbazione continua. 
Secondo la chiave di lettura della teoria dell'onda di bordo è come se l'ostacolo diventasse una sorgente (fittizia) di un'onda a simmetria cilindrica che si sovrappone tanto all'onda trasmessa secondo le leggi dell'ottica geometrica e, ovviamente, all'altra onda di bordo.
Secondo la chiave di lettura del principio di Huygens, il fronte d'onda incidente è l'inviluppo di onde elementari sferiche. Qui, le sorgenti ("fittizie") di tali onde sono nei punti della fenditura. L'inviluppo di tali onde sferiche in prossimità del bordo si propaga dando luogo a nuovi fronti d'onda successivi.
Nonostante la diversità nella descrizione del fenomeno, sia il modello dell'onda di bordo che il modello basato sul principio di Huygens sono pienamente equivalenti visto che la "matematizzazione" della teoria dell'onda di bordo discende dalla matematizzazione della teoria della propagazione secondo il principio di Huygens. Si vedano a questo proposito i riferimenti [6] e [7] della precedente sezione storica.
Per determinare gli effetti della diffrazione bisogna trovare innanzitutto la fase e l'intensità di ciascuna sorgente di Huygens in ogni punto dello spazio; ciò significa calcolare per ogni punto la sua distanza dal fronte d'onda: se la distanza di ciascun punto differisce a meno di un numero intero di lunghezze d'onda, tutte le sorgenti sono in fase e daranno luogo ad una "interferenza costruttiva"; se, al contrario, la distanza differisce di un numero intero più mezza lunghezza d'onda, l'interferenza sarà "distruttiva". In generale, è sufficiente determinare le posizioni di questi massimi e minimi per ottenere una completa descrizione del fenomeno.
La descrizione più semplice di diffrazione si ha nel caso di un problema in due dimensioni, come nel caso delle onde nell'acqua che si propagano solo sulla superficie del liquido; per quanto riguarda i raggi luminosi, si può trascurare una dimensione solo se la fenditura si estende in quella direzione per una distanza molto più grande della lunghezza d'onda della luce; nel caso di fenditure circolari, invece, si devono considerare tutte e tre le dimensioni.
Come esempio, si può ricavare un'equazione più precisa che leghi l'intensità delle bande di diffrazione all'angolo a cui si considerano, nel caso di una singola fenditura: partendo dalla rappresentazione matematica del principio di Huygens si considera un'onda monocromatica formula_1 sul piano complesso di lunghezza d'onda λ incidente su una fenditura di ampiezza "a"; se questa fenditura giace lungo il piano individuato dagli assi x′-y′ (con centro nell'origine), si può ipotizzare che la diffrazione generi un'onda complessa formula_2 che viaggia lungo una direzione radiale "r" rispetto alla fenditura e la cui equazione è:
Sia ora (x′,y′,0) un punto interno alla fenditura: se (x,0,z) sono le coordinate alle quali corrisponde l'intensità da misurare della figura di diffrazione, la fenditura si estenderà da formula_4 a formula_5 in un verso e da formula_6 a formula_7 nell'altro.
La distanza "r" dalla fenditura è:
Considerando il caso della diffrazione di Fraunhofer, risulterà che:
In altre parole, la distanza dello schermo è molto più grande dell'ampiezza della fenditura; con l'aiuto del teorema binomiale, questa distanza può essere ben approssimata come:
Sostituendo questo valore di "r" nella prima equazione si trova:
Per semplificare si possono raccogliere i termini costanti e chiamarli "C" ("C" può contenere numeri immaginari, anche se al termine ψ si potrà semplificare eliminando queste componenti). Ora, nella diffrazione di Fraunhofer formula_13 è molto piccolo, in modo da poter scrivere formula_14. Quindi, essendo formula_15, risulterà:
Si può notare con l'aiuto della formula di Eulero che formula_16 e formula_17:
formula_18
con la posizione: formula_19.
Infine, sostituendo in formula_20, l'intensità formula_21 delle onde diffratte a un dato angolo θ è data da:
Ripartendo dal principio di Huygens
si considerano ora "N" fenditure di uguale ampiezza ("a", formula_23, 0) distanti l'una dall'altra di una lunghezza "d" lungo l'asse x′. Come precedentemente trovato, la distanza "r" dalla prima fenditura sarà:
Per generalizzare questa situazione nel caso di "N" fenditure, si può innanzitutto osservare che mentre "z" e "y" restano costanti, x′ varia in questo modo:
Dunque si ha che:
e la somma di tutti gli "N" contributi all'onda è:
Di nuovo si può notare che formula_28 è trascurabile, 
in modo che formula_29; quindi risulta:
Ora si può usare la seguente identità
formula_30
per sostituire nell'equazione ed ottenere:
Di nuovo, sostituendo "k" e introducendo la variabile formula_31 al posto delle costanti non oscillanti, come nella diffrazione da una fenditura, si può semplificare il risultato; ricordandosi che:
si possono scartare gli esponenziali ed ottenere:
La diffrazione di un'onda piana incidente su un'apertura circolare dà come risultato il cosiddetto disco di Airy. La variazione dell'intensità dell'onda in funzione dell'angolo è data dall'espressione:
dove "a" è il raggio dell'apertura, "k" è pari a 2π/λ e J è una funzione di Bessel. Più piccola è l'apertura, più grande è la dispersione delle onde, a pari distanza.
Nel caso della diffrazione da un'apertura circolare, si rilevano una serie di anelli concentrici attorno al disco di Airy. L'analisi matematica di questo specifico caso è simile alla versione utilizzata per la diffrazione da una singola fenditura vista precedentemente.
Un'onda non deve necessariamente attraversare una fenditura per andare incontro a diffrazione: per esempio, anche un raggio di luce di ampiezza finita subisce un processo di diffrazione ed aumenta la propria ampiezza. Questo fenomeno limita l'ampiezza "d" dei dispositivi dove si raccoglie la luce, nel fuoco di una lente; ciò è conosciuto come "limite di diffrazione":
dove λ è la lunghezza d'onda della luce, "f" è la distanza focale della lente e "a" è il diametro del raggio di luce o (se il raggio di luce è più ampio della lente) il diametro della lente. L'ampiezza risultante contiene circa il 70% dell'energia della luce e corrisponde al raggio del primo minimo del disco di Airy, approssimato con il criterio di Rayleigh; il diametro del primo minimo, che contiene l'83.8% dell'energia della luce, è spesso utilizzato come "diametro di diffrazione".
Utilizzando il principio di Huygens, è possibile ricavare la superficie di diffrazione di un'onda che attraversa una fenditura di qualsiasi forma: se questa superficie viene osservata ad una certa distanza dall'apertura, risulterà essere la trasformata di Fourier in due dimensioni della funzione che rappresenta l'apertura.
La diffrazione da numerose fenditure descritta precedentemente è un fenomeno simile a ciò che si verifica quando un'onda viene diffusa da una struttura periodica, come il reticolo di atomi in un cristallo o le grate di un reticolo di diffrazione Ogni punto di diffusione, ad esempio ogni atomo del cristallo, agisce come una sorgente puntiforme di onde sferiche, le quali daranno luogo a fenomeni di interferenza costruttiva per formare un certo numero di onde diffratte. La direzione di queste onde è descritta dalla "Legge di Bragg":
dove λ è la lunghezza d'onda, "d" è la distanza tra ogni punto di diffusione, θ è l'angolo di diffrazione e "m" è un numero intero che indica l"'ordine" di ciascun onda diffratta.
La diffrazione di Bragg viene usata nella cristallografia a raggi X per ricavare la struttura di un qualsiasi cristallo analizzando gli angoli ai quali i raggi X vengono diffratti dal cristallo stesso: poiché l'angolo θ di diffrazione dipende dalla lunghezza d'onda λ, un reticolo di diffrazione causa una dispersione angolare di un raggio di luce.
L'esempio più semplice di diffrazione di Bragg è lo spettro di colori che si può vedere riflesso da un Compact disc: la breve distanza tra le tracce sulla superficie del disco costituisce un reticolo di diffrazione e ogni componente della luce bianca viene diffratta con differenti angoli, in accordo con la legge di Bragg.
La diffrazione di particelle materiali come gli elettroni è uno dei maggiori punti di forza della meccanica quantistica: osservare la diffrazione di un elettrone o di un neutrone consente di verificare l'esistenza della dualità onda-particella; questa diffrazione è anche un utile strumento scientifico: la lunghezza d'onda di queste particelle è sufficientemente piccola da essere usata nella scansione della struttura atomica dei cristalli.
La lunghezza d'onda associata ad una particella è la cosiddetta lunghezza d'onda di De Broglie:
dove "h" è la costante di Planck e "v" e "m" sono rispettivamente la velocità e la massa della particella; λ è caratteristica di qualsiasi oggetto materiale, anche se è rilevabile solo per entità con piccola massa, come gli atomi e altre particelle.
Recentemente, è stata osservata la diffrazione di particelle chiamate barioni e di un particolare tipo di fullereni chiamato "buckyball" ; il prossimo obiettivo della ricerca sarà quello di osservare la diffrazione dei virus, i quali, avendo molta più massa delle particelle elementari, hanno una lunghezza d'onda inferiore, cosicché devono attraversare molto lentamente una fenditura estremamente sottile affinché manifestino caratteri ondulatori.
Persino la Terra ha una sua lunghezza d'onda (in effetti, qualunque oggetto dotato di una quantità di moto la possiede): avendo una massa di circa 6×10 kg e una velocità orbitale media di circa 30000 ms, essa ha una lunghezza d'onda di De Broglie pari a 3.68×10 m.
La descrizione della diffrazione poggia, come detto in precedenza, sulla descrizione dell'interferenza tra onde generate dalla stessa sorgente che percorrono direzioni differenti, partendo dal medesimo punto; in questo modello, la differenza di fase tra le onde dipende solo dall'effettiva lunghezza del tragitto; può accadere però che due onde emesse in tempi diversi dalla sorgente arrivino sullo schermo in due punti diversi ma "allo stesso istante"; la fase iniziale con cui la sorgente genera le onde può anche cambiare nel tempo: onde emesse a intervalli di tempo sufficientemente lunghi non potranno quindi formare una stabile figura d'interferenza, dal momento che la loro differenza di fase non sarà più indipendente dal tempo.
La lunghezza correlata alla fase di un'onda elettromagnetica come la luce è detta lunghezza di coerenza: affinché si verifichi un'interferenza, la differenza dei tragitti di due onde deve essere inferiore alla lunghezza di coerenza.
Se le onde sono emesse da una sorgente estesa, ciò può produrre un'incoerenza lungo la direzione trasversale: osservando perpendicolarmente un raggio di luce, la lunghezza per la quale le fasi sono correlate è chiamata "lunghezza di coerenza trasversale"; nel caso della diffrazione dalla doppia fenditura, solo se questa lunghezza è minore della distanza tra le due aperture si osserverà il fenomeno della diffrazione.
Nel caso della diffrazione di particelle, la lunghezza di coerenza è legata all'estensione nello spazio della funzione d'onda che descrive tali particelle.
</text>
</doc>
<doc id="292637" url="https://it.wikipedia.org/wiki?curid=292637">
<title>Moto (fisica)</title>
<text>
In fisica il moto è il cambiamento di posizione di un corpo in funzione del tempo, misurato da uno specifico osservatore in un determinato sistema di riferimento. Fino al XIX secolo, le leggi di Newton, incluse tra gli assiomi e i postulati del famoso "Philosophiae Naturalis Principia Mathematica",
erano alla base di quella parte della meccanica classica nota come cinematica. Lo studio del moto a partire dalle cause che lo generano ovvero le forze è noto invece come dinamica. 
Storicamente il problema del moto è stato dunque il primo problema affrontato dalla fisica, direttamente applicato al moto dei corpi celesti con la meccanica celeste nell'ambito della rivoluzione scientifica. I calcoli delle traiettorie e delle forze esercitate dai corpi in moto basati sulle leggi newtoniane e delle fisica classica, si dimostrarono efficaci fintanto che i fisici non si occuparono di fenomeni molto rapidi, come quelli della fisica atomica agli inizi del XX secolo. La forza del moto si chiama forza d'inerzia.
Nello studio del moto di un corpo particolarmente utile risulta il cosiddetto "Principio di indipendenza dei moti simultanei" il quale afferma che ""il moto di un corpo lungo una certa traiettoria nello spazio è la risultante ovvero la composizione di singoli moti ciascuno lungo le direzioni degli assi cartesiani nello spazio"". 
Esistono vari tipi di moto: rettilineo; curvilineo; circolare; parabola e ellissi: tutte queste sono dette traiettorie
Il principio è utile per determinare la traiettoria del corpo nello spazio componendo i singoli moti.
Tra i moti della "fisica classica", si ricordano:
Tra i moti della "fisica atomica e subatomica" e dell"'astrofisica", si ricordano:
</text>
</doc>
<doc id="11260" url="https://it.wikipedia.org/wiki?curid=11260">
<title>Legge di Hooke</title>
<text>
In meccanica dei materiali, la legge di Hooke è la più semplice relazione costitutiva di comportamento dei materiali elastici. Essa è formulata dicendo che un corpo elastico subisce una deformazione direttamente proporzionale allo sforzo a esso applicato. La costante di proporzionalità che dipende dalla natura del materiale stesso.
I materiali per i quali la legge di Hooke è un'utile approssimazione del reale comportamento sono detti "materiali elastico-lineari". Definisce perciò un solido elastico allo stesso modo in cui la legge di Pascal definisce un fluido ideale.
Robert Hooke cominciò il suo studio sull'elasticità partendo dalla caratterizzazione del comportamento della "molla perfetta" o "ideale", cioè una molla priva di massa, di spessore trascurabile quando completamente compressa e in totale assenza di attrito e di altri fenomeni dissipativi; infatti, la molla ideale rappresenta il modello classico di elasticità lineare. La legge fu prima formulata nel 1675, nella forma dell'anagramma latino «"ceiiinosssttuv"», la cui soluzione fu pubblicata da Hooke nel 1678 come «"Ut tensio, sic vis"» («"come l'estensione, così la forza"»).
A partire dall'enunciato fornito originariamente da Hooke, l'equazione che esprime la forza elastica esercitata da una molla sollecitata longitudinalmente, in trazione o in compressione, lungo un asse formula_1 è:
quindi la forza formula_3 con cui la molla reagisce alla sollecitazione è direttamente proporzionale all'allungamento formula_4. La costante formula_5 rappresenta la "costante elastica longitudinale" della molla, espressa in formula_6.
In modo del tutto analogo, si ricava l'equazione che esprime il momento elastico, diretto lungo un asse formula_7 ortogonale al piano di torsione, esercitato da una molla torsionale sollecitata tangenzialmente:
quindi il momento meccanico formula_9 con cui la molla reagisce alla sollecitazione è direttamente proporzionale alla variazione dell'angolo formula_10. La costante formula_11 rappresenta la "costante elastica tangenziale" del corpo, espressa in formula_12.
Tuttavia, la formulazione odierna della legge di Hooke si serve di due grandezze vettoriali, la tensione formula_13 e la deformazione formula_14, legate tra loro da una relazione tensoriale.
Nel caso monodimensionale la relazione longitudinale diventa:
dove formula_16 è il "coefficiente di dilatazione lineare" e formula_17 è il modulo di elasticità longitudinale di Young, mentre la relazione inversa è:
dove l'inverso del modulo di Young è detto "modulo di cedevolezza longitudinale" formula_19.
Mentre il caso monodimensionale della relazione tangenziale diventa:
dove formula_21 è il c"oefficiente di scorrimento angolare" e formula_22 è il modulo di elasticità tangenziale.
Dalle relazioni precedenti si può dedurre che formula_23 e che formula_24, dove formula_25 è la sezione, formula_26 è la dimensione longitudinale e formula_27 è il braccio della forza che causa il momento.
Dato un sistema di riferimento cartesiano centrato un punto formula_28 appartenente ad un corpo deformabile, con formula_29 e detto formula_30, si ha che la cinematica del punto formula_31 è data dall'equazione:
mentre la trattazione statica di formula_31 la si ottiene attraverso il teorema di Cauchy-Poisson:
dove formula_35 è il vettore spostamento, formula_36 la giacitura e formula_37 e formula_38 sono, rispettivamente i tensori delle deformazioni e delle tensioni, che risultano entrambi simmetrici. Facendo uso della notazione di Voigt, a questi due tensori è possibile associare, rispettivamente, il vettore deformazione formula_14 e il vettore tensione formula_13.
In campo elastico, deformando un volume infinitesimo unitario formula_41, portandolo da uno stato formula_42 a uno stato formula_43, si applica un lavoro formula_44. Pertanto, il materiale rilascia tutta l'energia accumulata e ciò permette che si verifichi l'assenza di deformazioni residue.
Per i materiali iperelastici, l'energia di deformazione è definita come una funzione continua:
quindi essa rappresenta il "potenziale delle tensioni", mentre il "potenziale delle deformazioni" è rappresentato dall'energia complementare:
Essendo entrambe dei potenziali, entrambe le funzioni devono rispettare le condizioni di Schwarz.
A partire da queste considerazioni energetiche è possibile ricavare la legge di Hooke in termini tensoriali:
dove l'operatore lineare formula_48 è il "tensore di elasticità", la legge inversa, invece, è definita come:
dove l'operatore lineare formula_50 è il "tensore di cedevolezza". Pertanto si ha che:
Nonostante siano state ricavate per materiali iperelastici, queste leggi sono valide per tutti i tipi di materiali elastici.
Sia formula_48 che formula_50 sono tensori del quarto ordine, pertanto hanno 81 coefficienti scalari. In generale, entrambi i tensori hanno 36 coefficienti indipendenti, che si riducono a 21 nel caso di materiale iperelastico e a soli 2 nel caso il materiale sia anche omogeneo e isotropo. 
In tale ultimo caso il legame costitutivo è dato dalla relazione:
mentre, l'espressione inversa del legame costitutivo è la seguente:
dove formula_56 è la matrice identità e i due parametri scalari sono la costante di Lamé formula_57 e il modulo di elasticità tangenziale formula_22, che si legano al modulo di Young formula_17 e al modulo di Poisson formula_60 attraverso le relazioni:
La validità della legge di Hooke per una molla può essere verificata in laboratorio anche tramite semplici attrezzature. In genere, l'obiettivo dell'esperimento è la determinazione del valore della costante elastica longitudinale formula_5 di una molla. 
Per fare ciò occorre sottoporre la molla a carichi crescenti, misurando il relativo allungamento formula_4, pari alla differenza tra la lunghezza della molla sottoposta al carico, crescente, e la lunghezza della molla a riposo, ovvero non sottoposta ad alcun carico verticale, a meno del peso della molla stessa. Il rapporto tra la forza formula_3 applicata e l'allungamento formula_4 rappresenta esattamente il valore della costante elastica formula_5 di quella data molla. A questo punto occorre applicare forze verticali crescenti alla molla che, seguendo la legge di Hooke, produrrà allungamenti formula_4 direttamente proporzionali alle forze formula_3 applicate. I singoli valori di costante elastica formula_5 così determinati, se l'esperimento è svolto correttamente, risulteranno costanti, a meno di eventuali errori di misura da determinarsi con la teoria degli errori. 
Nel caso in cui formula_70 molle fossero poste in serie si può dimostrare e verificare sperimentalmente che il valore della costante elastica equivalente totale sarà pari a: 
</text>
</doc>
<doc id="39135" url="https://it.wikipedia.org/wiki?curid=39135">
<title>Spettro elettromagnetico</title>
<text>
Lo spettro elettromagnetico (abbreviato spettro EM) indica l'insieme di tutte le possibili frequenze delle radiazioni elettromagnetiche.
Pur essendo lo spettro continuo, è possibile una suddivisione puramente convenzionale ed indicativa in vari intervalli o "bande di frequenza", dettata a partire dallo spettro ottico. L'intero spettro è suddiviso nella parte di spettro visibile che dà vita alla luce e le parti di spettro non visibile a lunghezza d'onda maggiori e minori dello spettro visibile. Le onde di lunghezza nell'intervallo tra la luce visibile e le onde radio, a bassa intensità hanno poca energia e risultano scarsamente dannose, le radiazioni comprese tra l'ultravioletto e i raggi gamma invece hanno più energia, sono ionizzanti e quindi possono danneggiare gli esseri viventi.
Come l'orecchio ha dei limiti nella percezione del suono, l'occhio umano ha dei limiti nella visione della luce. 
In entrambi i casi, vi sono limiti superiori e inferiori.
Quantunque si distinguano varie zone nello spettro, non si può dire che esistano tra esse limiti netti.
La radiazione con una lunghezza d'onda inferiore a 400 nm è denominata luce ultravioletta. Questa zona scende fino a una lunghezza d'onda di circa 10 nm. Al di sotto di questa zona, si trova quella dei raggi X e si stende fino a una lunghezza d'onda di circa 0,006 nm. La parte inferiore dello spettro si compone di onde denominate raggi gamma. Questa zona si trova al di sotto della zona dei raggi X. Il campo di raggi gamma rappresenta il risultato della disintegrazione radioattiva.
Dalla parte dello spettro dove la luce ha lunghezza d'onda maggiore, cioè oltre il rosso, si trova la zona denominata infrarossa. Quest'ultima va da 0,7 µm a 0,4 mm. Quindi, viene la zona delle microonde, con lunghezze d'onda da 0,4 mm a 100 cm. Oltre a questa, vi sono tre campi di onde radio: onde corte da 1 m a 100 m; onde medie da 200 m a 600 m; onde lunghe superiori a 600 m. Le onde radio possono essere generate da scariche che producono onde elettromagnetiche.
È interessante rilevare che solo una parte assai limitata dello spettro contiene radiazioni visibili all'occhio.
L'occhio non può vedere la radiazione elettromagnetica oltre la zona violetta dello spettro e al di sotto della zona rossa. Lo spettro elettromagnetico si compone delle zone al di sopra e al di sotto di questi limiti, incluso il campo visibile. Anche se l'ultima lunghezza d'onda considerata nel campo visibile è di 0,4 µm, alcune persone possono vedere la radiazione con una lunghezza d'onda anche di solo 0,3 µm.
Per quanto le onde delle diverse zone abbiano tutte le stesse proprietà, si impiega il termine luce solo per la parte visibile dello spettro e le due zone circostanti. Le parti di luce visibile dello spettro sono emesse da corpi incandescenti, quali ad esempio il Sole o una lampadina. Vale la pena ricordare che, in base alla legge di Stefan-Boltzmann, ogni corpo a qualsiasi temperatura (purché superiore allo zero assoluto) emette radiazione elettromagnetica. Tuttavia i corpi che ci appaiono incandescenti sono appunto solo quelli che emettono un'apprezzabile quantità di radiazione nelle frequenze a cui il nostro occhio è sensibile.
Un utilizzo tipico applicativo dello spettro elettromagnetico è nelle telecomunicazioni per veicolare informazione attraverso segnali (portante modulata) sul canale di comunicazione tra mittente e destinatario, utilizzando la banda ottica e quella dell'infrarosso per le comunicazioni ottiche, quella a microonde e a radiofrequenza per le radiocomunicazioni (spettro radio).
Lo spettro infrarosso è coinvolto in tutti i processi di scambio di calore tra corpi per irraggiamento e quindi anche nei sistemi di riscaldamento, mentre un'altra applicazione è nei forni a microonde dove si utilizza appunto la banda delle microonde per la cottura dei cibi. Raggi X sono invece comunemente impiegati in diagnostica medica (radiografia).
</text>
</doc>
<doc id="21947" url="https://it.wikipedia.org/wiki?curid=21947">
<title>Interazione gravitazionale</title>
<text>
L'interazione gravitazionale (o gravitazione o gravità nel linguaggio comune) è una delle quattro interazioni fondamentali note in fisica.
Nella fisica classica newtoniana la gravità è interpretata come una forza conservativa di attrazione a distanza agente fra corpi dotati di massa, secondo la legge di gravitazione universale; la sua manifestazione più evidente nell'esperienza quotidiana è la forza peso. 
Nella fisica moderna l'attuale teoria più completa, la relatività generale, interpreta l'interazione gravitazionale come una conseguenza della curvatura dello spaziotempo creata dalla presenza di corpi dotati di massa o energia (una piccola massa a grande velocità o una grande massa in quiete hanno lo stesso effetto di deformazione sulla curvatura dello spaziotempo circostante). Il campo gravitazionale che ne deriva è rappresentato matematicamente da un tensore metrico legato alla curvatura dello spaziotempo attraverso il tensore di Riemann. In tale contesto la forza peso diventa una forza apparente, conseguenza della geometria dello spaziotempo indotta dalla massa terrestre.
Le prime spiegazioni di una forza agente capace di aggregare i corpi vennero formulate, nella filosofia greca, all'interno di una visione animistica della natura, come nella dottrina di Empedocle, in cui domina l'alternanza di due princìpi, Amore e Odio, o in quella di Anassagora, dove prevale l'azione ordinatrice di una Mente suprema ("Nous"). 
Platone riteneva che la materia fosse pervasa da una "dynamis", cioè un'energia intrinseca, che spinge il simile ad attrarre il simile; concezione ripresa da Aristotele, per il quale tutto l'universo anela alla perfezione del primo motore immobile (Dio). Questo anelito si esprime nel movimento circolare di stelle, Sole, Luna e pianeti, giungendo tuttavia a corrompersi progressivamente fino a diventare rettilineo nella dimensione terrestre sublunare. Soltanto in quest'ambito, dunque, alcuni corpi, quelli che Platone e Aristotele chiamavano "gravi", risultano soggetti alla gravità: si trattava di composti dei quattro elementi fondamentali (fuoco, aria, acqua, terra), mentre l'etere fluttuava al di sopra di essi. Secondo la teoria aristotelica dei luoghi naturali, tutto ciò che è terra tende a ritornare lì dove risiede la terra, ovvero al centro dell'universo; al di sopra vi è la sfera dell'acqua che attrae tutto ciò che è liquido; analogamente si comportano i cerchi dell'aria e del fuoco. 
Come i suoi contemporanei, Aristotele interpretava la fisica dell'universo deducendola dalla fisiologia umana, sostenendo ad esempio che oggetti di peso diverso cadessero a velocità diverse, in analogia all'esperienza dell'uomo che tenti di contrastare il peso di un sasso, adottando così una prospettiva che, seppur contraddetta nel VI secolo d.C. da Giovanni Filopono, continuerà ad essere insegnata fino all'epoca di Galileo. Con lo stoicismo lo studio della gravità portò a scoprire una relazione tra il moto delle maree e i movimenti del Sole della Luna: l'universo è infatti concepito dagli stoici come un unico organismo vivente, animato dal "pneuma", forza vitale che tutto pervade, e che si esprime nella reciproca azione di un elemento attivo ("heghemonikòn") e di uno passivo ("hypàrchon") che ne subisce l'attrazione.
Anche per la dottrina neoplatonica, ripresa dalla teologia cristiana, il cosmo è animato dal "Logos" divino, dal quale le stelle e i pianeti risultano attratti: nel Medioevo il loro movimento viene spiegato in particolare con l'azione di intelligenze motrici, ordinate gerarchicamente in un coro di angeli. Si tratta di un universo retto da un principio armonico che si irradia in ogni sua parte, e strutturato perciò in maniera concentrica secondo l'insegnamento aristotelico. A fondamento di quest'ordine geometrico è posto Dio, il quale lo governa attraverso un atto d'amore: la gravità, dunque, come forza d'amore, così descritta ad esempio da Dante nell'ultimo verso della "Divina Commedia".
L'analogia neoplatonica tra Dio e il Sole condurrà tuttavia la filosofia rinascimentale a fare di quest'ultimo il centro di attrazione della Terra e dei pianeti. In Keplero, il primo a descrivere in maniera ellittica le loro orbite, permane la concezione animistica e astrologica dell'universo, basata sulla corrispondenza armonica tra i cieli e la terra; egli interpretava la forza immateriale della gravità come una sorta di emanazione magnetica.
A partire dal Seicento la visione animistica della gravità verrà progressivamente sostituita da una puramente meccanicista; Galileo Galilei ne fornì una descrizione limitata all'aspetto quantitativo, e riprendendo l'antica idea di Filopono teorizzò che, facendo cadere due corpi di masse differenti nello stesso momento, entrambi sarebbero arrivati al suolo in contemporanea. 
Cartesio negò che la gravità consistesse in una forza intrinseca, spiegandola sulla base di vortici di etere, e riconducendo ogni fenomeno fisico al principio di conservazione del moto, dato dalla massa per la velocità ("mv"). Leibniz obietterà a Cartesio che la quantità di moto non bastava a definire l'essenza di una forza, e ripristinò il concetto vitalistico di energia o "vis viva", espressa dal prodotto della massa per la velocità al quadrato ("e=mv"): era questa per lui ad essere conservata in natura.
Un concetto di forza affine a quello di Cartesio era stato peraltro espresso da Newton, che fece della massa, cioè della quantità di materia (data dal volume per la densità) il concetto fondamentale della meccanica gravitazionale: quanto più è grande la massa di un corpo, tanto più potente è la sua forza di gravità. Newton capì che la stessa forza che causa la caduta di una mela sulla Terra mantiene i pianeti in orbita attorno al Sole, e la Luna attorno alla Terra. Nel libro "Philosophiae Naturalis Principia Mathematica", del 1687, egli enunciò la legge di gravitazione universale, che dimostrò con il "metodo delle flussioni", un procedimento analogo alla derivazione. In seguito Huygens, nel suo "Horologium oscillatorium", chiarificò la natura delle forze centrifughe che impediscono ai pianeti di cadere sul sole pur essendone attratti.
Restava aperto tuttavia il problema di spiegare l'azione a distanza tra i corpi celesti, priva di contatto materiale, al quale verrà data una soluzione soltanto ai primi del Novecento da parte di Einstein, che sostituì l'etere con la tessitura dello spazio-tempo.
In meccanica classica l'interazione gravitazionale è generata da un campo vettoriale conservativo e descritta da una forza, detta forza peso, che agisce sugli oggetti dotati di massa.
La legge di gravitazione universale afferma che due punti materiali si attraggono con una forza di intensità direttamente proporzionale al prodotto delle masse dei singoli corpi e inversamente proporzionale al quadrato della loro distanza. Questa legge, espressa vettorialmente, diventa:
dove formula_2 è la forza con cui l'oggetto 1 è attratto dall'oggetto 2, "G" è la costante di gravitazione universale, che vale circa 6,67 × 10 Nm²/kg, "m" e "m" sono le masse dei due corpi, formula_3
è il vettore congiungente i due corpi (supposti puntiformi) e formula_4 è il suo modulo; nella seconda espressione della forza (che evidenzia il fatto che il modulo della forza è inversamente proporzionale al quadrato della distanza) formula_5 rappresenta il "versore (unitario)" che individua la retta congiungente i due punti materiali.
Definito il vettore accelerazione di gravità:
la legge di gravitazione universale può essere espressa come:
In prossimità della superficie terrestre il valore di formula_8 è convenzionalmente:
anche espressa in newton su chilogrammo.
Il campo gravitazionale è un campo di forze conservativo. Il campo generato nel punto formula_10 nello spazio dalla presenza di una massa nel punto formula_11 è definito come:
dove "G" è la costante di gravitazione universale e "M" la massa. È quindi possibile esprimere la forza esercitata sul corpo di massa "m" come:
L'unità di misura del campo gravitazionale nel Sistema internazionale è:
Il campo gravitazionale è descritto dal potenziale gravitazionale, definito come il valore dell'energia gravitazionale rilevato da una massa posta in un punto dello spazio per unità di massa. L'energia gravitazionale della massa è il livello di energia che la massa possiede a causa della sua posizione all'interno del campo gravitazionale; pertanto il potenziale gravitazionale della massa è il rapporto tra l'energia gravitazionale e il valore della massa stessa, cioè:
Essendo il campo gravitazionale conservativo, è sempre possibile definire una funzione scalare "V" il cui gradiente, cambiato di segno, coincida con il campo:
Nel precedente paragrafo si è detto che il valore medio dell'accelerazione di gravità nei pressi della superficie terrestre è stimato in 9,81 m/s². In realtà questo valore è diverso da quello reale perché non tiene conto di fattori, come la forza centrifuga causata dalla rotazione terrestre e la non perfetta sfericità della terra (la terra ha la forma di un geoide). Il valore convenzionalmente assunto è quindi "g" = 9,80665 m/s², deciso nella terza CGPM nel 1901 e corrisponde all'accelerazione subita da un corpo alla latitudine 45,5°.
Per molte applicazioni fisiche e ingegneristiche è quindi utile utilizzare una versione approssimata della forza di gravità, valida nei pressi della superficie terrestre:
dove formula_18 è un versore diretto lungo la "verticale". In sostanza la forza di gravità è approssimata con una forza di modulo costante, indipendente dalla quota del corpo, e come direzione il "basso", nel senso comune del termine. Naturalmente anche in questa approssimazione corpi con masse diverse hanno la stessa accelerazione di gravità.
L'energia potenziale gravitazionale "U" è data da:
dove "h" è la quota del corpo rispetto a un riferimento fisso.
In questo caso approssimato è molto semplice ricavare le leggi del moto, mediante integrazioni successive: per un corpo in caduta libera, chiamando "z" l'asse verticale (sempre diretto verso il basso) e proiettando il moto su di esso, valgono le seguenti leggi:
Inoltre, dalla conservazione dell'energia meccanica si ottiene un risultato notevole per corpi in caduta libera inizialmente fermi. Scriviamo l'energia meccanica del sistema a un tempo generico:
dove "v" è la velocità del corpo e "z" la sua quota. Supponiamo ora che all'istante iniziale formula_24 il corpo si trovi a una quota formula_25 e all'istante finale formula_26 abbia una velocità formula_27 e si trovi a quota formula_28; scriviamo quindi l'energia del sistema ai due istanti:
Dato che l'energia meccanica si conserva possiamo uguagliare le due ultime equazioni e ricavarci il modulo della velocità dopo una caduta di una quota "h":
Il problema generale della gravitazione, cioè la determinazione del campo gravitazionale creato da un insieme di masse, si può esprimere con il teorema di Gauss e il teorema della divergenza.
Essendo la forza di gravità conservativa, si può esprimere formula_8 come:
dove formula_34 è proporzionale all'energia potenziale gravitazionale come segue:
Dal teorema di Gauss:
Per il teorema della divergenza, il primo integrale, cioè il flusso della forza gravitazionale, è esprimibile come integrale di volume della sua divergenza:
Sostituendo a formula_8 la sua espressione come gradiente:
che, dovendo valere per ogni volume di integrazione, implica:
Quest'ultima è una equazione differenziale alle derivate parziali del secondo ordine, detta equazione di Poisson, da completare con le opportune condizioni al contorno.
La teoria di Newton della gravitazione ha permesso di descrivere con accuratezza la grande maggioranza dei fenomeni gravitazionali nel Sistema Solare.
Tuttavia, da un punto di vista sperimentale essa presenta alcuni punti deboli, successivamente affrontati a partire dalla teoria della relatività generale: 
Einstein sviluppò una nuova teoria della gravitazione, denominata relatività generale, pubblicata nel 1915.
Nella teoria di Einstein, la gravità non è una forza, come tutte le altre, ma è la proprietà della materia di deformare lo spazio-tempo. Propriamente, la gravità non è un'interazione a distanza fra due masse, ma è un "fenomeno mediato" da una deformazione dello spazio-tempo. La presenza di massa (più in generale, di energia e impulso) determina una curvatura della geometria (più esattamente, della struttura metrica) dello spazio-tempo: poiché i corpi che si muovono in "caduta libera" seguono nello spazio-tempo traiettorie geodetiche, e queste ultime non sono rettilinee se lo spazio-tempo è curvo, ecco che il moto degli altri corpi (indipendentemente dalla loro massa) subisce le accelerazioni che classicamente sono attribuite alla "forza di gravità".
I pianeti del Sistema Solare quindi hanno orbite ellittiche non per effetto di una forza di attrazione esercitata direttamente dal Sole, ma perché la massa del Sole incurva lo spazio-tempo. Il campo gravitazionale attorno a una stella è rappresentato dalla soluzione di Schwarzschild delle equazioni di Einstein, soluzione che si ottiene semplicemente assumendo le proprietà di simmetria sferica nello spazio tridimensionale di indipendenza dal tempo. Le equazioni del moto geodetico nella metrica di Schwarzschild permettono di calcolare l'orbita di un pianeta attorno a una stella: per quasi tutti i pianeti del Sistema Solare, la differenza fra queste orbite e i moti descritti dalle leggi di Keplero (soluzioni delle equazioni di Newton) non è osservabile in quanto è molto più piccola degli effetti perturbativi dovuti all'interazione dei pianeti fra loro. L'unica eccezione è rappresentata dal moto di Mercurio, in cui la precessione dell'asse dell'orbita che si osserva è molto maggiore di quanto previsto dalla gravità newtoniana (anche tenendo conto dell'influenza degli altri pianeti), ed è invece in perfetto accordo con la previsione delle equazioni relativistiche. L'osservazione della precessione del perielio di Mercurio è quindi una delle evidenze a favore della relatività generale rispetto alla teoria gravitazionale newtoniana.
Un'ulteriore evidenza osservativa, riscontrata per la prima volta nel corso dell'eclissi solare del 1919 (ma definitivamente confermata da osservazioni su scala extragalattica a partire dal 1980), consiste nell'effetto detto lente gravitazionale: l'immagine di un corpo celeste visto dalla Terra appare spostata rispetto alla posizione reale del corpo (talvolta l'immagine è anche sdoppiata) a causa della deflessione che la luce subisce quando rasenta una regione dello spazio con alta densità di massa. Questo conferma il fatto che la gravitazione deforma lo spazio-tempo, e che tale deformazione è avvertita anche da particelle prive di massa (i fotoni).
Un diverso approccio meccanicistico della gravità è dato dalla teoria del "Loop Quantum Gravity" e, nell'ambito della teoria delle stringhe, dall'esistenza dei gravitoni.
Il fisico matematico Erik Verlinde propone, rivedendo idee già in circolazione, che la gravità sia interpretabile come la manifestazione di una forza emergente in senso entropico: citando le sue parole la gravità altro non è che un ""effetto collaterale della propensione naturale verso il disordine"". Verlinde, con assoluta moderazione suggerisce che si tratta "di idee che dovrebbero servire da guida per ulteriori studi". Allo stato attuale degli studi, la teoria di Verlinde si delinea come l'ultima e la più motivata delle ipotesi speculative tra, e per, gli addetti ai lavori. Nel luglio 2010 la sua teoria è passata al grande pubblico, tramite la diffusione mediatica e attraverso internet, esautorando la teoria gravitazionale propagandata dal motto: "la gravità non esiste".
Nel 2009, Erik Verlinde formalizzò un modello concettuale che descrive la gravità come una forza entropica, che suggerisce che la gravità è una conseguenza del comportamento statistico dell'informazione associata alla posizione dei corpi materiali. Questo modello combina l'approccio termodinamico della gravità con il principio olografico, e implica che la gravità non sia una interazione fondamentale, ma un fenomeno che emerge dal comportamento statistico dei gradi di libertà microscopici codificati su uno schermo olografico.
La legge di gravità può essere derivata dalla meccanica statistica classica applicata al principio olografico, che afferma che la descrizione di un volume di spazio può essere rappresentato come formula_41 bit d'informazione binaria, codificata ai confini della regione, una superficie di area formula_42. 
L'informazione è distribuita casualmente su tale superficie e ciascun bit immagazzinato in una superficie elementare dell'area.
dove formula_44 è la lunghezza di Planck.
Il teorema statistico di equipartizione lega la temperatura formula_45 di un sistema (espressa in joule, basandosi sulla costante di Boltzmann) con la sua energia media:
Questa energia può essere identificata con la massa formula_47 per la relazione di equivalenza di massa ed energia:
La temperatura effettiva sperimentata da un rivelatore uniformemente accelerato in un campo di vuoto o stato di vuoto è data dall'effetto Unruh.
Questa temperatura è:
dove formula_50 è la costante di Planck ridotta,
e formula_51 è l'accelerazione locale, 
che è legata alla forza formula_52 dalla seconda legge di Newton del moto:
Assumendo ora che lo schermo olografico sia una sfera di raggio formula_4, la sua superficie è data da:
Da questi principi si deriva la legge di gravitazione universale di Newton:
L'iter è reversibile: leggendolo dal basso, dalla legge di gravitazione, risalendo per i principi della termodinamica si ricava l'equazione che descrive il principio olografico.
</text>
</doc>
<doc id="15482" url="https://it.wikipedia.org/wiki?curid=15482">
<title>Colore</title>
<text>
Il colore è la percezione visiva delle varie radiazioni elettromagnetiche comprese nel cosiddetto spettro visibile.
Lo studio del colore coinvolge anche:
La varietà di pigmenti utilizzati oggigiorno era sconosciuta prima della scoperta dei colori sintetici, dalla loro limitata varietà negli utilizzi sia artistici che di tintura di stoffe risulta un limitato vocabolario.
Gli antichi Greci non utilizzavano dei nomi fissi per indicare i diversi tipi di colore, ma li distinguevano più che altro in base alla loro limpidezza o tenebrosità, così che soltanto il bianco e il nero erano adoperati in maniera definita, a differenza degli altri. Ad esempio il termine "xanthos" poteva indicare tanto il giallo lucente quanto il rosso vivo del fuoco, come pure le tinte purpuree e persino blu.
I colori fondamentali erano dunque anticamente ricondotti a due, il bianco e il nero, ossia la chiarezza e l'oscurità, dalla cui mescolanza derivavano tutti gli altri. In particolare Empedocle, nel trattato "Sull'origine", attribuiva il bianco al fuoco e il nero all'acqua.
Platone, nel "Timeo", oltre al bianco e al nero annoverava tra i colori primari anche il rosso e lo «splendente» ("lampron"). Per Aristotele il bianco e il nero si determinano in base alla presenza o meno del "diaphanes", ossia di un elemento trasparente in grado di far trasparire la luce (da "leukòs", cioè bianco): tale "diaphanes" è massimo nel fuoco, associato al caldo, e minimo nella terra, associata al freddo. Oltre che nel trattato "Sull'Anima", l'argomento è trattato da Aristotele anche nell'opuscolo "Sul senso e sui sensibili", appartenente al gruppo di brevi opere conosciute col titolo di "Parva naturalia", ossia «Piccoli scritti naturali»: in esso viene spiegata l'origine dei colori in termini di mescolanza a partire dal bianco e dal nero.
La dottrina greca che vedeva i colori originati dalle due opposte polarità, chiaro e scuro, rimase predominante durante il Medioevo, in cui soprattutto quella aristotelica continuò a essere discussa e commentata. Concezioni analoghe furono elaborate nel Rinascimento: ad esempio Leonardo da Vinci, artista e scienziato insieme, attingendo alle teorie aristoteliche vedeva nel bianco e nel nero gli estremi fondamentali della gamma cromatica, a partire dai quali egli studiò il modo in cui due colori complementari si pongono reciprocamente in risalto, distinguendo le tinte prodotte dalla luce, come il giallo e il rosso, dai colori delle ombre, spesso tendenti al verde e all'azzurro. Egli distingueva così sei colori fondamentali:
Parallelamente presero tuttavia a svilupparsi alcune teorie negatrici di una visione unitaria della luce, che avrebbero preparato il terreno all'atomismo di Newton. Marcus Marci nel Seicento studiò i colori da un punto di vista esclusivamente fisico, sperimentando la trasformazione della luce quando veniva riflessa da oggetti colorati. Il fisico e gesuita Francesco Maria Grimaldi spiegò la colorazione della luce, che ad esempio passasse attraverso un vetro rosso, sulla base di modificazioni intrinseche alla luce stessa, e non perché il rosso le si andasse ad aggiungere.
Isaac Newton pertanto, adottando un approccio meccanico e matematico, concluse che la luce, se diversamente riflessa, fa apparire la varietà dei colori a seconda del suo diverso grado di rifrazione, e che i colori sarebbero in origine contenuti nella luce quali suoi componenti, come gli atomi compongono la materia: egli ne enumerò in tutto sette, per ragioni più filosofiche che fisiologiche.
Queste conclusioni furono contestate ai primi dell'Ottocento da Goethe, secondo cui era tipica della mentalità newtoniana la trascuratezza di un approccio basato sui sensi, che conduceva ad astrazioni teoriche ed arbitrarie: Goethe rimproverava a Newton di aver effettuato i suoi esperimenti con l'ausilio di un prisma, e che era questo il responsabile dell'insorgere dei colori. Il prisma per Goethe non è uno strumento neutro, ma "sporca" la luce, offuscandola e producendo così i diversi effetti cromatici secondo la densità della sua figura. Per Goethe non è la luce a scaturire dai colori, bensì il contrario; i colori non sono «primari», ma consistono nell'interazione della luce ("urphänomen") con l'oscurità. Polarizzandosi a contatto col buio, la luce dà luogo a coppie di colori contrapposti e complementari, che Goethe dispose in un cerchio cromatico di sei colori.
Anche il filosofo idealista Hegel contestò le conclusioni di Newton, sottolineandone le contraddizioni nel fare della luce un composto di colori, cioè nel rendere scuro quel che è bianco:
Nel 1861 James Clerk Maxwell, contemporaneo del famoso fisico e ottico Hermann von Helmholtz, con un esperimento produsse la prima fotografia a colori.
Lo scienziato Edwin Land, inventore della Polaroid, confutò la teoria di Newton dimostrando che la percezione del colore di un punto è influenzata dal contesto limitrofo.
Dal punto di vista delle proprietà fisiche, la luce visibile appare complessivamente bianca se la si considera la somma di tutte le frequenze dello spettro ottico. A ciascuna frequenza del visibile è associato un determinato colore. In particolare la diversità di colore o semplicemente il colore dei corpi che non emettono o brillano di luce propria, percepito poi dall'occhio umano, deriva dal fatto che un certo corpo assorbe tutte le frequenze o lunghezze d'onda dello spettro visibile, ma riemette o riflette una o più componenti o frequenze della luce bianca che, eventualmente mescolate tra loro, danno vita al colore percepito dall'occhio umano. In particolare nei due casi estremi un corpo appare bianco quando assorbe tutte le frequenze riflettendole a sua volta tutte, viceversa un corpo appare nero quando assorbe tutte le frequenze e non ne riflette alcuna; in tutti gli altri casi intermedi si avrà la percezione tipica di un altro colore.
Ogni sorgente di luce emette fotoni di diverse lunghezze d'onda, per cui quello che appare come "tinta unica" è solo la lunghezza d'onda dominante o risultante e non quella in cui sono assenti altre.
Nel caso di corpi che emettono o brillano di luce propria (ad es. le stelle e il Sole), come è noto tutti i corpi al di sopra dello zero assoluto, emettono invece radiazione elettromagnetica con potenza che è proporzionale alla loro temperatura assoluta T secondo la legge di Stefan-Boltzmann e distribuita con buona approssimazione secondo lo spettro del corpo nero di Planck con il picco di emissione che si sposta secondo la Legge di Wien in funzione della temperatura T: se il corpo è sufficientemente caldo parte di questa radiazione elettromagnetica cade nella banda del visibile risultando così visibile ai nostri occhi passando dal rosso, al giallo, al bianco, azzurro e blu quanto più il corpo è caldo (vedi temperatura di colore).
La formazione della percezione del colore, non necessariamente corrisponde alle proprietà fisiche della luce, avviene in tre distinte fasi.
Nella prima fase una sorgente luminosa emette un flusso di fotoni di diversa frequenza. Questo flusso di fotoni può:
In ogni caso i fotoni che giungono all'occhio costituiscono lo stimolo di colore. Ogni singolo fotone attraversa la cornea, l'umore acqueo, la pupilla, il cristallino, l'umore vitreo e raggiunge uno dei fotorecettori della retina (un bastoncello, oppure un cono L, un cono M o un cono S) dal quale può essere o non essere assorbito. La probabilità che un tipo di fotorecettore assorba un fotone dipende dal tipo di fotorecettore e dalla frequenza del fotone.
Come risultato dell'assorbimento ogni fotorecettore genera un segnale elettrico in modulazione di ampiezza, proporzionale al numeri di fotoni assorbiti. Gli esperimenti mostrano che i segnali generati dai tre coni L, M e S sono direttamente collegati con la sensazione di colore, e sono detti segnali di tristimolo.
Nella seconda fase i segnali di tristimolo vengono elaborati e compressi con modalità non ancora completamente note. Questa elaborazione avviene nelle altre cellule della retina (cellule orizzontali, bipolari e gangliari) e termina con la generazione di altri tre segnali elettrici, questa volta in modulazione di frequenza, che sono chiamati segnali opponenti e vengono trasmessi al cervello lungo il nervo ottico.
I segnali elettrici opponenti che lungo i due nervi ottici (che sono costituiti dagli assoni delle cellule gangliari) raggiungono il cervello arrivano nei cosiddetti corpi genicolati laterali, che costituiscono una stazione intermedia per i segnali, che da qui vengono proiettati in apposite aree della corteccia visiva, dove si realizza infine la percezione del colore.
I contrasti cromatici si producono tramite l'accostamento di due o più colori diversi tra loro. È altresì vero che esistono processi fisiologici oculari che come per i contrasti luminosi permettono la visione al nostro occhio di due tipi di colore, quello reale e quello apparente. Con un colore, per esempio il giallo, si avranno delle percezioni diverse in base allo sfondo a cui lo sottoponiamo: questo perché tende alla tonalità complementare dello sfondo stesso. Se invece lo sottoponessimo ad uno sfondo che è il complementare del colore stesso, avremo maggiore luminosità per il principio del contrasto luminoso.
Consiste nell'accostare almeno tre colori al più alto grado di saturazione, cioè di intensità e di forza.
Il contrasto tra colori complementari si ottiene tramite l'accostamento di un colore primario e del colore risultante dall'unione degli altri due primari rimasti, tali contrasti sono: giallo-viola, rosso-verde, blu-arancione.
Ogni tinta presenta un diverso grado di luminosità, per cui se vogliamo creare un equilibrio percettivo è necessario stendere in modo molto proporzionale le varie zone di colore, ad esempio, un colore molto luminoso dovrà occupare un'area minore rispetto a un colore con un minore grado di luminosità.
Berlin e Kay nel 1969 studiarono il numero di nomi dedicati ai colori nelle diverse culture stabilendo che si può passare da un minimo di 2, chiaro e scuro, ad un massimo di 11. Dimostrarono inoltre che man mano che si procede con la definizione di più colori lo sviluppo è omogeneo in tutte le culture, ad esempio dopo il chiaro e lo scuro si indica come colore il rosso, poi il verde e il giallo e così via fino a giungere all'arancione che è il colore definito in meno culture.
La teoria dei due antropologi era che il numero di colori dipendesse dalla complessità della cultura, ma questa teoria venne criticata in quanto essi non consideravano che alle percezioni del colore erano legate delle sensazioni emotive e quindi la percezione del colore è legata alla cultura stessa. Al termine si lega quindi una connotazione, un alone di significati a seconda del contesto
Inoltre alcuni colori non vengono definiti se non associandoli al colore di un elemento naturale (es. "verde" diviene "foglia") così come accade quando noi definiamo un rosso come "ruggine". Inoltre dal rapporto fra colore e materia nascono due modi di interpretazione del colore, quali il "colore-qualità" nel quale il colore consente di qualificare la realtà, e di "colore-materia", nel quale l'artista è impegnato a creare un avvenimento nuovo.
Se con gli Impressionisti si instaura un nuovo rapporto tra l'immagine e la pittura e quindi nasce l'antitesi fra il colore e l'immagine, entrambi sfumati e non completati, i Puntinisti e i Divisionisti utilizzano le scoperte della scienza positivista e con Van Gogh la tensione cromatica corrisponde simbolicamente allo stato psicologico da descrivere; infine i pittori "gestuali" e "informali" spingono verso l'idea del colore-oggetto come informazione naturale.
Per quanto riguarda l'indagine dell'utilizzo del colore nell'arte, assumono grande importanza i riflessi emotivi, quali il calore e la profondità, dato che abitualmente i colori "caldi" avanzano verso l'osservatore, all'opposto di quelli "freddi".
Ogni colore, che sia primario o secondario, suscita e rappresenta un'emozione o uno stato d'animo e può essere legato in particolare ad un evento. L'esperienza del colore è soggettiva e può rimandare alla cultura di appartenenza che suggerisce le personali percezioni su un determinato colore. Per esempio il giallo suscita qualcosa che irradia, come la luce del sole, mentre il blu qualcosa che racchiude, come l'universo. Il rosso sembra invece in movimento ma su se stesso, come il fuoco o il sangue. Nella cromoterapia i colori sono associati alla persona per innalzare o modificare una sua caratteristica o una personale vibrazione del suo essere.
Nei differenti contesti socio-culturali i colori afferiscono a diversi significati ed occasioni in cui vengono utilizzati. Il bianco ad esempio può essere associato alla purezza, ma anche alla morte; il rosa che in origine in molte culture era legato alla sfera maschile poiché derivato dal rosso, colore che rimanda alla forza, è stato successivamente associato alla femminilità, come lo è stato il celeste per gli uomini, per motivi commerciali di marketing e vendita di prodotti.
Molti autori si sono posti il problema della razionalizzazione cromatica ideando vari modelli cromatici,tra i quali:
L'incapacità di percepire i colori viene chiamata acromatopsia, ed in alcune popolazioni raggiunge percentuali elevate.
L'Acromatopsia è differente dalla discromatopsia (o daltonismo) poiché quest'ultima è un'incapacità solo parziale di vedere alcuni colori ed è suddivisa in deuteranopia, tritanopia e protanopia.
Lo spettro visibile all'occhio umano si basa sulla visione tricromatica presente solo nelle scimmie antropomorfe e quindi non corrisponde a quello degli altri animali, ad esempio i mammiferi in prevalenza hanno una visione dicromatica, i serpenti vedono gli infrarossi e i pesci gli ultravioletti.
</text>
</doc>
<doc id="24282" url="https://it.wikipedia.org/wiki?curid=24282">
<title>Costante di gravitazione universale</title>
<text>
La costante di gravitazione universale (indicata con la lettera "G") è la costante fisica che compare come coefficiente di proporzionalità nella legge di gravitazione universale e nell’equazione di campo della relatività generale.
Il valore approssimato della costante di gravitazione universale è "G" = . In quanto costante, non dipende dalle masse usate per determinarlo sperimentalmente, né dal luogo né dal momento scelto per effettuare la misura.
La prima misura della costante di gravitazione universale fu dedotta da un esperimento condotto da Henry Cavendish nel 1798, atto a misurare la densità media della Terra, usando una bilancia di torsione. La debolezza dell'intensità dell'interazione gravitazionale rispetto alle altre forze, come quella elettromagnetica, ha reso la determinazione del valore di "G" particolarmente difficoltosa.
Il valore della costante di gravitazione universale, raccomandato nel 2014 dal CODATA, risulta essere nelle unità di misura del sistema internazionale pari a:
Tale costante è spesso approssimata a:
</text>
</doc>
<doc id="1770" url="https://it.wikipedia.org/wiki?curid=1770">
<title>Elettrone</title>
<text>
L'elettrone è una particella subatomica con carica elettrica negativa che si ritiene essere una particella elementare.
Insieme ai protoni e ai neutroni, è un componente dell'atomo e, sebbene contribuisca alla sua massa totale per meno dello 0,06%, ne caratterizza sensibilmente la natura e ne determina le proprietà chimiche: il legame chimico covalente si forma in seguito alla redistribuzione della densità elettronica tra due o più atomi..
La maggior parte degli elettroni presenti nell'universo è stata prodotta dal Big Bang, ma possono essere generati anche dal decadimento beta degli isotopi radioattivi e in collisioni ad alta energia, mentre possono essere annichilati dalla collisione con i positroni o assorbiti in un processo di nucleosintesi stellare.
Il moto dell'elettrone genera un campo magnetico, mentre la variazione della sua energia e della sua accelerazione causano l'emissione di fotoni; è inoltre responsabile della conduzione della corrente elettrica e del calore.
L'avvento dell'elettronica e il relativo sviluppo dell'informatica hanno reso l'elettrone protagonista dello sviluppo tecnologico del ventesimo secolo. Le sue proprietà vengono sfruttate in svariate applicazioni, come i tubi a raggi catodici, i microscopi elettronici, la radioterapia e il laser.
"Elettrone" deriva dalla parola greca "ήλεκτρον" (pronuncia "électron"), il cui significato è ambra. Tale nome è storicamente dovuto al fatto che l'ambra ebbe un ruolo fondamentale nella scoperta dei fenomeni elettrici: in particolare a partire dal VII secolo a.C. gli antichi Greci erano a conoscenza del fatto che strofinando un oggetto di ambra o ebanite con un panno di lana, l'oggetto in questione acquisiva la capacità di attirare a sé corpuscoli leggeri, quali ad esempio granelli di polvere. Queste evidenze sperimentali vennero riprese nel XVI secolo da William Gilbert, che individuò numerose sostanze, tra cui il diamante e lo zolfo, che presentavano lo stesso comportamento dell'ambra. Egli diede il nome di "forza elettrica" alla forza che attirava i corpuscoli, e chiamò "elettrizzati" i materiali che manifestavano tale proprietà.
Gli studi sull'elettricità e sul magnetismo furono continuati in epoca moderna fra gli altri da Benjamin Franklin e Michael Faraday, e in questo periodo nel contesto dell'atomismo fu avanzata l'idea che anche l'elettricità potesse essere costituita da piccoli corpuscoli indivisibili.
L'idea di una quantità fondamentale di carica elettrica fu introdotta dal filosofo Richard Laming nel 1838 per spiegare le proprietà chimiche dell'atomo..
Nel 1874 il fisico irlandese George Stoney introdusse il concetto di "unità di carica fondamentale". Nel 1891 ne stimò il valore e coniò il termine ""elettrone"" per riferirsi a tali "unità" (dalla combinazione del termine ""elettrico"" e del suffisso -one, che sarà utilizzato anche successivamente per designare altre particelle subatomiche, come il protone o il neutrone), scrivendo:
Le prime prove sperimentali dell'esistenza di questa particella si ebbero nel 1860, quando il fisico e chimico inglese Sir William Crookes effettuò esperimenti con il tubo di Geissler, inserendovi due lamine metalliche e collegandole a un generatore di corrente continua a elevato potenziale (circa ). Durante tale esperimento, Crookes si accorse che si generava una luce avente una colorazione differente a seconda del gas utilizzato. Tale emissione luminosa aveva origine dal catodo (polo negativo) e fluiva verso l'anodo (polo positivo).
In seguito all'esperienza di Crookes, anche il fisico tedesco Johann Wilhelm Hittorf nel 1869, mentre si stava dedicando ad uno studio sulla conduttività elettrica dei gas, evidenziò un bagliore emesso dal catodo e verificò che aumentava in intensità con il decrescere della pressione del gas. Nel 1876 il fisico tedesco Eugen Goldstein mostrò che i raggi di tale bagliore proiettano un'ombra e li chiamò "raggi catodici". Durante gli anni settanta del XIX secolo, Crookes sviluppò il primo tubo catodico con un vuoto spinto all'interno, dimostrando che i raggi luminescenti che appaiono all'interno del tubo trasportano energia e si muovono dal catodo all'anodo. Inoltre, applicando un campo magnetico, fu in grado di deflettere i raggi, dimostrando che il fascio si comporta come se fosse carico negativamente. Nel 1879, Crookes avanzò l'idea che queste proprietà potessero essere spiegate da quella che denominò "materia radiante""" e suggerì che si doveva trattare di un nuovo stato della materia, consistente di molecole cariche negativamente che sono espulse ad alta velocità dal catodo.
Il fisico inglese di origini tedesche Arthur Schuster proseguì gli esperimenti di Crookes posizionando delle piastre metalliche parallele ai raggi catodici e applicando un potenziale elettrico fra loro. Il campo deflesse i raggi verso la piastra carica positivamente, confermando che i raggi trasportano carica negativa. Misurando l'ammontare della deflessione per una data intensità di corrente elettrica, nel 1890 Schuster fu in grado di stimare il rapporto fra la massa e la carica dei componenti dei raggi catodici. Tuttavia, tale stima fu ritenuta poco attendibile dai suoi contemporanei poiché risultò migliaia di volte superiore alle attese.
Negli ultimi anni dell'Ottocento numerosi fisici sostennero la possibilità che l'elettricità fosse costituita da unità discrete, alle quali vennero conferiti vari nomi, ma delle quali non vi fu alcuna prova sperimentale convincente. Nel 1896, il fisico britannico J. J. Thomson, con i suoi colleghi John S. Townsend e H. A. Wilson, svolsero una serie di esperimenti che dimostrarono che i raggi catodici erano costituiti da singole particelle, piuttosto che onde, atomi o molecole come si riteneva in precedenza. Thomson stimò in maniera accurata la carica "e" la massa, trovando che le particelle dei raggi catodici, che lui chiamò "corpuscoli", avevano probabilmente una massa migliaia di volte inferiore a quella dell'idrogenione (H), lo ione più leggero che si conoscesse a quel tempo. Thomson mostrò come il rapporto carica/massa ("e"/"m"), uguale a e/g, fosse indipendente dal materiale del catodo. Inoltre mostrò come le particelle cariche negativamente prodotte dai materiali radioattivi, dai materiali riscaldati e dai raggi catodici fossero riconducibili tutte alla stessa entità. Il nome "elettrone" fu nuovamente proposto per identificare tali particelle dal fisico irlandese George F. Fitzgerald e da allora il nome venne universalmente accettato.
Mentre studiava i minerali naturalmente fluorescenti nel 1896, il fisico francese Henri Becquerel scoprì che essi emettono radiazione senza l'intervento di una sorgente di energia esterna. Tali materiali radioattivi divennero argomento di grande interesse da parte degli scienziati, fra cui anche il fisico neozelandese Ernest Rutherford, il quale scoprì che emettevano particelle, da lui chiamate particelle alfa e beta, sulla base della loro capacità di penetrare la materia. Nel 1900, Becquerel mostrò che i raggi beta emessi dal radio potevano essere deflessi da un campo elettrico e che il loro rapporto massa-carica era lo stesso dei raggi catodici. Tale evidenza sperimentale suggeriva che gli elettroni esistevano come componenti degli atomi.
La carica degli elettroni fu misurata con maggiore precisione dal fisici americani Robert Millikan e Harvey Fletcher nel loro esperimento della goccia d'olio del 1909, i cui risultati furono pubblicati nel 1911. In tale esperimento venne usato un campo elettrico per frenare la caduta, dovuta alla gravità, di una goccia d'olio elettricamente carica. Grazie a tale apparato strumentale, fu possibile misurare la carica elettrica prodotta da pochi ioni (tra 1 e 150) con un margine di errore inferiore allo 0,3%. Si ottenne un valore pari a e fu quindi possibile stimare che la massa dell'elettrone dovesse valere . Un simile esperimento era stato condotto in precedenza dal gruppo di Thomson, usando nubi di gocce di acqua cariche generate tramite l'elettrolisi, e nel 1911 da Abram Ioffe, che ottenne in maniera indipendente lo stesso risultato di Millikan usando microparticelle di metallo cariche, pubblicando i risultati nel 1913. Tuttavia, le gocce d'olio risultavano più stabili di quelle dell'acqua a causa della loro bassa velocità di evaporazione e quindi maggiormente adatte per svolgere esperimenti precisi per un lungo periodo di tempo.
Attorno all'inizio del ventesimo secolo, fu scoperto che sotto certe condizioni una particella carica che si muove ad elevata velocità causa una condensazione di vapore acqueo sovrassaturo lungo il suo cammino. Nel 1911, Charles Wilson sfruttò tale principio per sviluppare la prima camera a nebbia, uno strumento che permette di tracciare e fotografare il percorso seguito da particelle cariche, come gli elettroni veloci.
A partire dal 1914, gli esperimenti dei fisici Ernest Rutherford, Henry Moseley, James Franck e Gustav Hertz stabilirono definitivamente che l'atomo è formato da un nucleo massivo carico positivamente circondato da elettroni di massa minore. Nel 1913, il fisico danese Niels Bohr postulò che gli elettroni si trovano in stati di energia quantizzata, con l'energia determinata dal momento angolare delle orbite degli elettroni attorno al nucleo. La teoria avanzata da Bohr prevedeva inoltre che gli elettroni potessero muoversi tra questi stati (o orbite) in seguito all'assorbimento o all'emissione di un quanto di energia, un fotone di specifica frequenza. Tale teoria era in grado di spiegare la comparsa delle linee di emissione spettrale dell'idrogeno come conseguenza del suo contenuto energetico attraverso riscaldamento o facendolo attraversare da corrente elettrica. Nonostante ciò, il modello di Bohr non era in grado di predire l'intensità delle relative linee e di spiegare la struttura dello spettro di atomi più complessi.
La formazione di legami chimici tra atomi fu spiegata nel 1916 da Gilbert Newton Lewis, il quale asserì che il legame covalente sia generato dalla condivisione di una coppia di elettroni tra due atomi, mentre una descrizione completa sulla formazione di queste coppie e dei legami chimici venne fornita da Walter Heitler e Fritz London nel 1923 grazie alla meccanica quantistica. Nel 1919 il chimico statunitense Irving Langmuir rielaborò il modello statico dell'atomo di Lewis ipotizzando che tutti gli elettroni fossero distribuiti in una serie di gusci ("shell") sferici approssimativamente concentrici, tutti di uguale spessore"; tali gusci erano a loro volta suddivisi in celle, ognuna delle quali conteneva una coppia di elettroni. Tramite questo modello, Langmuir spiegò qualitativamente le proprietà chimiche di tutti gli elementi, le quali si ripetono secondo un ordine preciso stabilito dalla tavola periodica.
Nel 1924, il fisico austriaco Wolfgang Pauli osservò che la struttura a strati di un atomo poteva essere spiegata attraverso un insieme di quattro parametri che definivano univocamente lo stato quantico di un elettrone, e che un singolo stato non poteva essere occupato da più di un singolo elettrone (questa legge è nota come principio di esclusione di Pauli). Nonostante la sua intuizione, Pauli non riuscì a spiegare il significato fisico del quarto parametro, il quale poteva assumere solo due valori. La spiegazione teorica di tale parametro si deve invece ai fisici olandesi Samuel Goudsmit e George Uhlenbeck, i quali suggerirono che un elettrone, oltre al momento angolare associato alla sua orbita, può possedere un proprio momento angolare intrinseco. Fu così introdotto il concetto di spin e con questa scoperta era possibile spiegare anche la separazione delle linee spettrali osservata con uno spettrografo ad alta definizione.
Nel 1929, il fisico francese Louis de Broglie vinse il premio Nobel per la fisica per aver scoperto che anche gli elettroni, oltre alla luce, sono caratterizzati da una doppia natura, una corpuscolare e una ondulatoria. Questa nuova proprietà, presentata per la prima volta nella sua dissertazione del 1924 dal titolo "" (Ricerca sulla teoria dei quanti) è nota come dualismo onda-particella e comporta la possibilità di osservare fenomeni di interferenza fra elettroni sotto appropriate condizioni:
L'interferenza è una proprietà di tutte le onde: ad esempio nel caso della luce, se tra una sorgente luminosa e uno schermo illuminato da tale sorgente viene interposto un foglio con delle fessure parallele, la luce prodotta dalla sorgente attraversa tali fessure e si proietta sullo schermo producendo delle figure a bande in corrispondenza dello schermo. Nel 1927 furono osservati gli effetti dell'interferenza con un fascio di elettroni dal fisico inglese George Paget Thomson con una sottile pellicola metallica e dai fisici americani Clinton Davisson e Lester Germer, i quali studiarono il fenomeno di scattering degli elettroni incidenti su una lastra di nickel monocristallino. Niels Bohr nello stesso anno incluse l'ipotesi di de Broglie e queste evidenze sperimentali nel principio di complementarità, secondo il quale una descrizione completa dell'elettrone e della luce non può fare riferimento solo alla sua natura ondulatoria o solo alla sua natura particellare, ma deve necessariamente includerle entrambe.
Infatti la natura ondulatoria dell'elettrone si manifesta ad esempio nel fenomeno dell'interferenza, mentre la natura corpuscolare fa sì che un fascio di elettroni riesca a fare girare un piccolo mulinello posizionato lungo il suo tragitto.
Il successo della previsione di de Broglie portò alla pubblicazione dell'equazione di Schrödinger, formulata nel 1926 da Erwin Schrödinger, che descrive l'evoluzione temporale di uno stato quantico (e quindi della relativa funzione d'onda). Piuttosto che cercare una soluzione che determinasse la posizione di un elettrone nel tempo, questa equazione era usata per prevedere la probabilità di trovare un elettrone in un volume finito o infinitesimo dello spazio. Da questo approccio ebbe origine la branca della fisica denominata "meccanica quantistica", che garantì la possibilità di ricavare teoricamente i livelli energetici di un elettrone nell'atomo di idrogeno in buon accordo con i dati sperimentali. Una volta che vennero presi in considerazione lo spin e l'interazione fra più elettroni, la meccanica quantistica fu in grado di ricostruire l'andamento delle proprietà chimiche tipiche degli elementi nella tavola periodica.
Nel 1928, basandosi sul lavoro di Wolfgang Pauli, Paul Dirac formulò un modello dell'elettrone coerente con la teoria della relatività ristretta, applicando considerazioni relativistiche e di simmetria alla formulazione hamiltoniana della meccanica quantistica per un elettrone in un campo elettromagnetico; questa trattazione portò alla formulazione dell'equazione di Dirac. Per risolvere i problemi della sua equazione relativistica (in primo luogo l'esistenza di soluzioni a energia negativa), nel 1930 lo stesso Dirac sviluppò un modello del vuoto come un mare infinito di particelle con energia negativa, che fu poi chiamato mare di Dirac. Questo permise di prevedere l'esistenza del positrone, la corrispettiva antiparticella dell'elettrone, che fu scoperta sperimentalmente nel 1932 da Carl David Anderson. Anderson propose di chiamare gli elettroni "negatroni" e di usare il termine "elettroni" per indicare genericamente una delle varianti della particella sia a carica positiva che negativa. Questo uso del termine "negatroni" è occasionalmente utilizzato tuttora, anche nella sua forma abbreviata "negatone".
Gli elettroni nel mare di Dirac furono introdotti con lo scopo di impedire la perdita di energia senza limiti degli elettroni reali osservati. In questo contesto, i fotoni (cioè i quanti della radiazione elettromagnetica) possono essere assorbiti dagli elettroni del mare, permettendo a questi ultimi di uscire fuori da esso. Come risultato netto si generano degli elettroni a carica negativa e delle lacune di carica positiva nel mare. Una lacuna potrà essere rioccupata dall'elettrone che perde energia rilasciando in questo modo nuovamente un altro fotone.
Nel 1947 Willis Lamb, lavorando in collaborazione con lo studente Robert Retherford, trovò che certi stati quantistici dell'elettrone nell'atomo di idrogeno, che avrebbero dovuto avere la stessa energia, erano spostati uno rispetto all'altro e tale deviazione fu chiamata spostamento di Lamb. Circa nello stesso periodo, Polykarp Kusch, lavorando con Henry M. Foley, scoprì che il momento magnetico dell'elettrone è di poco più grande di quanto previsto dell'equazione di Dirac. Questa piccola differenza fu successivamente chiamata "momento magnetico di dipolo anomalo dell'elettrone". Per risolvere questo e altri problemi, una teoria avanzata chiamata elettrodinamica quantistica fu sviluppata da Sin-Itiro Tomonaga, Julian Schwinger e Richard P. Feynman alla fine degli anni quaranta.
Con lo sviluppo degli acceleratori di particelle nella prima metà del XX secolo, i fisici iniziarono ad approfondire le proprietà delle particelle subatomiche. Le proprietà di corpuscolo elementare puntiforme dell'elettrone hanno reso questa particella una sonda perfetta per esplorare la struttura dei nuclei atomici. Il primo tentativo riuscito di accelerare elettroni usando l'induzione elettromagnetica fu ad opera di Donald William Kerst nel 1942: il suo primo betatrone raggiunse energie di , mentre quelli successivi raggiunsero i . Nel 1947 fu scoperta la radiazione di sincrotrone con un sincrotrone di della General Electric; questa radiazione era causata dall'accelerazione degli elettroni che, in un campo magnetico, raggiungono velocità prossime a quelle della luce.
Il primo acceleratore di particelle ad alte energie è stato ADONE, con un fascio di particelle di energia pari a ; questa struttura, operativa a partire dal 1968, accelerava elettroni e positroni in direzioni opposte, raddoppiando in pratica l'energia prodottasi nelle loro collisioni se paragonata a quella ottenuta nelle collisioni degli elettroni con un bersaglio fisso. Il Large Electron-Positron Collider (LEP) al CERN, che operò dal 1989 al 2000, raggiunse energie di collisione pari a e fece importanti misure in merito al modello standard.
Il Large Hadron Collider (LHC), l'ultimo acceleratore del CERN, sostituisce gli elettroni con adroni, perché questi ultimi sono meno soggetti alla perdita di energia per radiazione di sincrotrone e quindi il rapporto fra energia acquisita dalla particella e l'energia spesa per ottenerla è maggiore.
L'elettrone possiede una massa a riposo di , pari a circa di quella del protone e una carica pari a (esatta). Si tratta della particella subatomica stabile più leggera che si conosca tra quelle dotate di carica elettrica. La carica elettrica è la carica elementare cambiata di segno e lo spin ha valore semi intero, per cui l'elettrone è un fermione. Appartiene alla prima generazione dei leptoni ed è soggetto all'interazione gravitazionale, a quella debole e a quella elettromagnetica. La sua antiparticella è il positrone, che si differenzia solo per la carica elettrica di segno opposto.
Nel modello standard della fisica delle particelle gli elettroni appartengono al gruppo delle particelle subatomiche chiamate leptoni, che si ritiene siano particelle elementari, ed hanno massa minore rispetto a ogni altra particella carica conosciuta. L'elettrone appartiene alla prima generazione di particelle fondamentali, mentre la seconda e la terza generazione contengono altri leptoni carichi, il muone e il tauone, che possiedono identica carica e spin, ma massa a riposo maggiore. L'elettrone e tutti i leptoni differiscono dagli altri componenti fondamentali della materia (che sono i quark, costituenti i protoni e i neutroni) per il fatto che non risentono della forza di interazione nucleare forte.
La massa a riposo di un elettrone è di approssimativamente o che, in base al principio di equivalenza massa ed energia, corrisponde a un'energia a riposo di , con un rapporto rispetto alla massa del protone di circa 1 a 1836. Misure astronomiche hanno mostrato che il rapporto fra le masse del protone e dell'elettrone è rimasto costante per almeno metà dell'età dell'universo, come è previsto nel modello standard.
L'elettrone ha una carica elettrica di , che viene chiamata "carica elementare" ed è usata come unità standard per la carica delle particelle subatomiche. Entro i limiti dell'errore sperimentale, il valore della carica dell'elettrone è uguale a quella del protone, ma con il segno opposto. Il valore della carica elementare è indicato con il simbolo "e", mentre l'elettrone viene comunemente indicato con il simbolo "e", dove il segno meno indica il fatto che tale particella presenta carica negativa; analogamente, per il positrone, che ha la stessa massa dell'elettrone e carica di segno opposto, è utilizzato come simbolo "e".
L'elettrone non ha sotto strutture conosciute e viene descritto come un punto materiale, dal momento che esperimenti effettuati con la trappola di Penning hanno mostrato che il limite superiore per il raggio della particella è di 10 metri. Esiste inoltre una costante fisica, il raggio classico dell'elettrone, a cui corrisponde un valore di ; questa costante deriva tuttavia da un calcolo che trascura gli effetti quantistici presenti.
Si ritiene che l'elettrone sia stabile poiché, dal momento che la particella possiede carica unitaria, il suo decadimento violerebbe la legge di conservazione della carica elettrica. Il limite inferiore sperimentale per la vita media dell'elettrone è di anni, con un intervallo di confidenza al 90%.
In meccanica quantistica l'elettrone può essere trattato sia come onda che come particella, in accordo con il dualismo onda-particella. Nel formalismo delle funzioni d'onda l'elettrone è descritto matematicamente da una funzione di variabile complessa, la funzione d'onda appunto. Il quadrato del valore assoluto della funzione d'onda rappresenta una densità di probabilità, cioè la probabilità che l'elettrone sia osservato nell'intorno di una determinata posizione.
Da tale distribuzione si può calcolare l'incertezza della posizione dell'elettrone. Un calcolo analogo si può fare sulla quantità di moto dell'elettrone. Le incertezze sulla posizione e la quantità di moto sono legate dal principio di indeterminazione di Heisenberg.
Gli elettroni sono particelle identiche, ovvero non possono essere distinte l'una dall'altra per le loro proprietà fisiche intrinseche: è possibile cambiare la posizione di una coppia di elettroni interagenti senza che si verifichi un cambiamento osservabile nello stato del sistema. La funzione d'onda dei fermioni, di cui gli elettroni fanno parte, è antisimmetrica: il segno della funzione d'onda cambia quando la posizione dei due elettroni viene scambiata, ma il valore assoluto non varia con il cambio di segno e il valore della probabilità resta immutato. Questo differenzia i fermioni dai bosoni, che hanno una funzione d'onda simmetrica.
Il momento angolare intrinseco è caratterizzato dal numero quantico di spin, pari a 1/2 in unità di ħ, e l'autovalore dell'operatore di spin è √3⁄2 ħ. Il risultato di una misura della proiezione dello spin su ognuno degli assi di riferimento può inoltre valere soltanto ±ħ⁄2.
Oltre allo spin, l'elettrone ha un momento magnetico intrinseco, allineato al suo spin, che ha un valore approssimativamente simile al magnetone di Bohr, che è una costante fisica che vale . La proiezione del vettore di spin lungo la direzione della quantità di moto definisce la proprietà delle particelle elementari conosciuta come elicità.
L'evoluzione temporale della funzione d'onda di una particella è descritta dall'equazione di Schrödinger, che nel caso di un sistema di elettroni interagenti mostra una probabilità nulla che una coppia di elettroni occupi lo stesso stato quantico: questo fatto è responsabile del principio di esclusione di Pauli, il quale afferma che due elettroni del sistema non possono avere i medesimi numeri quantici. Tale principio è alla base di molte proprietà dei sistemi con molti elettroni, in particolare genera la loro configurazione all'interno degli orbitali atomici.
Quando un elettrone si muove con velocità prossima a quella della luce è necessario ricorrere alla teoria della relatività speciale per descriverne il moto. Secondo tale teoria, la massa relativistica dell'elettrone aumenta dal punto di vista di un osservatore esterno, e di conseguenza è necessaria una forza sempre più intensa per mantenere costante l'accelerazione. In questo modo un elettrone non può mai raggiungere la velocità della luce nel vuoto "c", essendo richiesta un'energia infinita. Tuttavia, se un elettrone che si muove a una velocità prossima a quella della luce entra in un mezzo dielettrico, per esempio l'acqua, in cui la velocità della luce è significativamente minore di quella dell'elettrone, l'interazione con esso può generare un fronte d'onda di luce causato dall'effetto Čerenkov. Tale effetto è simile al boom sonico, che accade quando un oggetto supera la velocità del suono.
L'effetto della relatività speciale è descritto da una quantità nota come fattore di Lorentz, definita da:
dove formula_2 è la velocità della particella e l'energia cinetica formula_3 associata a un elettrone che si muove con velocità formula_2 è:
dove "m" è la massa a riposo dell'elettrone. Per esempio, l'acceleratore lineare di Stanford (SLAC) può accelerare un elettrone a circa 51 GeV. Questo fornisce un valore per formula_6 vicino a 100 000, dal momento che la massa a riposo dell'elettrone è circa 0,51 MeV/c. La quantità di moto relativistica è 100 000 volte la quantità di moto dell'elettrone prevista dalla meccanica classica alla stessa velocità.
Dal momento che l'elettrone ha anche un comportamento ondulatorio, a una data velocità esso ha una caratteristica lunghezza d'onda di de Broglie. Questa è data da "λ" = "h"/"p" dove "h" è la costante di Planck e "p" è la quantità di moto. Per un elettrone con energia di 51 GeV, come quelle raggiunte dall'acceleratore SLAC, la lunghezza d'onda è di circa , piccola a sufficienza per esplorare la scala infinitesima del nucleo atomico e dei protoni.
La teoria dei campi quantistica interpreta i fenomeni di interazione fra gli elettroni e la radiazione elettromagnetica in termini di scambi di particelle generate nel vuoto dalle fluttuazioni quantistiche. Ad esempio, secondo l'elettrodinamica quantistica, gli elettroni e il campo elettromagnetico interagiscono fra loro puntualmente tramite lo scambio di fotoni e particelle virtuali aventi vita breve e non direttamente osservabili. Le fluttuazioni quantistiche creano continuamente nel vuoto coppie di particelle virtuali, fra le quali vi sono l'elettrone e il positrone, che si annichilano in breve tempo senza poter essere misurate effettivamente. In base al principio di indeterminazione di Heisenberg, la variazione dell'energia necessaria a produrre la coppia di particelle e la loro vita media non si possono conoscere contemporaneamente, tuttavia se la vita media è estremamente breve l'incertezza riguardo all'energia è molto ampia, e il processo e la fluttuazione possono avvenire senza violare la conservazione dell'energia.
La presenza delle particelle virtuali, sebbene non direttamente osservabile, è responsabile tuttavia della differenza delle caratteristiche dell'elettrone al variare della scala di energie dei processi in cui è coinvolto. Le correzioni virtuali sono all'origine di correzioni divergenti di tipo logaritmico della massa dell'elettrone rispetto al valore nominale classico. La rimozione di queste divergenze, alla base della teoria della rinormalizzazione, comporta una ridefinizione del concetto di costante fisica, che viene ad assumere nel contesto quantistico un valore differente in base alla scala di osservazione. Per esempio la carica elettrica dell'elettrone non è costante ed aumenta lentamente all'aumentare dell'energia dei processi in cui è coinvolto.
Questo importante risultato delle teorie di campo quantistiche può essere interpretato come l'effetto di schermo prodotto dalle particelle virtuali. La presenza di un elettrone isolato permette attraverso il campo elettromagnetico di creare una coppia positrone-elettrone dal vuoto; il positrone virtuale appena creato, di carica positiva, sarà attratto dall'elettrone isolato, mentre l'elettrone virtuale ne sarà respinto. Questo fenomeno produce uno schermo positivo attorno all'elettrone isolato, la cui carica a grande distanza sarà quindi considerevolmente ridotta rispetto a quella a corta distanza. Una particella carica ad alta energia sarà in grado di penetrare lo schermo e per questo motivo entra in interazione con una carica elettrica efficace più alta. In base a processi analoghi, anche la massa dell'elettrone tende a crescere quando le scale di energie crescono. Questo tipo di comportamento delle costanti fisiche è caratteristico di tutte le teorie che presentano un polo di Landau, come l'elettrodinamica quantistica.
La mutua interazione fra fotoni e elettroni spiega anche la piccola deviazione dal momento magnetico intrinseco dell'elettrone dal magnetone di Bohr. I fotoni virtuali, responsabili del campo elettrico, possono permettere infatti all'elettrone di avere un moto agitato nell'intorno della sua traiettoria classica, che genera l'effetto globale di un moto circolare con una precessione. Questo moto produce sia lo spin che il momento magnetico dell'elettrone. Negli atomi, poi, la creazione di fotoni virtuali spiega lo spostamento di Lamb osservato nelle linee spettrali e il fenomeno del decadimento spontaneo di elettrone da uno stato eccitato a uno di energia inferiore. Questo tipo di polarizzazione è stata confermata sperimentalmente nel 1997 usando l'acceleratore giapponese TRISTAN.
L'elettrone è responsabile delle proprietà chimiche fondamentali degli atomi e delle molecole. L'interazione elettromagnetica fra gli elettroni è infatti all'origine dei legami fra gli atomi e della struttura macroscopica della materia, oggetto di studio della chimica e della fisica dello stato solido.
Gli elettroni sono i costituenti fondamentali degli atomi, assieme a protoni e neutroni. Essi sono confinati nella regione in prossimità del nucleo atomico e nel caso di un atomo neutro isolato sono in numero pari al numero atomico, cioè al numero di protoni contenuti nel nucleo. Se il numero di elettroni è differente dal numero atomico, l'atomo è detto ione e possiede una carica elettrica netta.
Secondo la meccanica classica, un elettrone in moto circolare uniforme attorno al nucleo, essendo accelerato, emetterebbe radiazione elettromagnetica per effetto Larmor, perdendo progressivamente energia e impattando infine sul nucleo. Il collasso degli atomi è smentito dall'osservazione sperimentale della stabilità della materia: per questo motivo il modello atomico di Bohr è stato introdotto nel 1913 per fornire una descrizione semiclassica nella quale un elettrone può muoversi soltanto su alcune determinate orbite non-radiative caratterizzate da precisi valori dell'energia e del momento angolare. Nello sviluppo successivo della meccanica quantistica, per rappresentare lo stato degli elettroni nell'atomo, la traiettoria classica è stata sostituita dalla funzione d'onda nota con il nome di orbitale atomico.
Ad ogni orbitale è associato uno degli stati energetici degli elettroni che interagiscono con il potenziale elettrico generato dal nucleo. Il valore della funzione d'onda associata a tali stati è fornito dalla soluzione dell'equazione d'onda di Schrödinger, che può essere risolta per l'atomo di idrogeno notando la simmetria radiale del potenziale elettrico indotto dal nucleo. Le soluzioni dell'equazione d'onda sono enumerate da numeri quantici che assumono un insieme discreto di valori, che rappresentano il valore di aspettazione dell'energia e del momento angolare, in particolare:
Gli atomi con più elettroni richiedono una descrizione degli stati più complessa di quella dell'atomo di idrogeno, in quanto è necessaria l'introduzione di approssimazioni a causa dell'impossibilità di risolvere esattamente l'equazione di Schrödinger per via analitica. Le approssimazioni più utilizzate sono il metodo di Hartree-Fock, che sfrutta la possibilità di scrivere la funzione d'onda degli elettroni come un determinante di Slater, l'accoppiamento di Russell-Saunders e l'accoppiamento jj, che invece riescono ad approssimare l'effetto dovuto all'interazione spin-orbita nel caso di nuclei rispettivamente leggeri e pesanti.
Per il principio di esclusione di Pauli, due o più elettroni non possono trovarsi nel medesimo stato, cioè non possono essere descritti dai medesimi numeri quantici. Questo fatto determina la distribuzione degli elettroni negli orbitali. Gli orbitali sono occupati dagli elettroni in modo crescente rispetto all'energia. Lo stato di momento angolare è definito dal numero quantico azimutale "l"; dove il quadrato del valore assoluto del momento angolare è formula_7. Il numero quantico magnetico può assumere valori interi compresi tra -"l" e +"l": il numero di tali valori è il numero delle coppie di elettroni, con valore di spin opposto, che possiedono il medesimo numero quantico azimutale. Ad ogni livello energetico corrisponde un numero crescente di possibili valori del numero quantico azimutale, a ogni valore del numero quantico azimutale corrispondono "2l + 1" valori di "m", e a ogni valore di "m" corrispondono i due valori possibili di spin.
All'interno della nuvola elettronica è possibile che un elettrone effettui una transizione da un orbitale a un altro principalmente attraverso l'emissione o l'assorbimento di fotoni (cioè di quanti di energia), ma anche in seguito alla collisione con altre particelle o tramite l'effetto Auger. Quando un elettrone acquista un'energia pari alla differenza di energia con uno stato non occupato all'interno degli orbitali, esso effettua una transizione in tale stato. Una delle applicazioni più importanti di tale fenomeno è l'effetto fotoelettrico, in cui l'energia fornita da un fotone è tale da separare l'elettrone dall'atomo. Inoltre, dal momento che l'elettrone è carico, il suo moto attorno al nucleo, che in una descrizione semiclassica è circolare uniforme, produce un momento di dipolo magnetico proporzionale al momento angolare orbitale. Il momento magnetico totale di un atomo è equivalente alla somma vettoriale dei momenti di dipolo magnetici e di spin di tutti i suoi elettroni e dei costituenti del nucleo. Il momento magnetico dei costituenti del nucleo è tuttavia trascurabile rispetto a quello degli elettroni. L'interazione tra il momento di dipolo magnetico e il momento di spin è descritto dall'interazione spin-orbita, mentre l'interazione con un campo magnetico esterno è descritta dai limiti di Paschen-Back e Zeeman, a seconda che l'interazione spin-orbita sia rispettivamente trascurabile o meno rispetto al campo applicato.
Nelle molecole gli atomi sono uniti dal legame chimico covalente, in cui uno o più elettroni sono condivisi fra due o più atomi. In una molecola gli elettroni si muovono sotto l'influenza attrattiva dei nuclei e il loro stato è descritto da orbitali molecolari, più grandi e complessi di quelli di un atomo isolato, che in prima approssimazione si possono ottenere attraverso la sommatoria di più orbitali degli atomi considerati singolarmente. Differenti orbitali molecolari hanno differenti distribuzioni spaziali di densità di probabilità: nel caso di una molecola costituita da due atomi, per esempio, gli elettroni che ne formano l'eventuale legame si troveranno con maggiore probabilità in una ristretta regione posta fra i due nuclei.
Un composto ionico può essere definito come un composto chimico formato da ioni, aventi ciascuno una carica elettrica positiva o negativa, ma l'insieme di tali ioni ha carica elettrica complessiva neutra. Alla base dei composti ionici vi è il legame ionico, di natura elettrostatica, che si forma quando le caratteristiche chimico-fisiche dei due atomi sono nettamente differenti e vi è una notevole differenza di elettronegatività. Per convenzione si suole riconoscere un legame ionico tra due atomi quando la differenza di elettronegatività Δχ è maggiore di 1,9. Al diminuire di tale differenza cresce il carattere covalente del legame.
L'elettrone genera un campo elettrico che esercita una forza attrattiva su particelle con una carica positiva (come il protone) e una forza repulsiva su particelle con carica negativa. L'intensità di tale forza è determinata dalla legge di Coulomb. Un elettrone in movimento genera un campo magnetico: tale proprietà prende il nome di "induzione elettromagnetica" ed è responsabile ad esempio della generazione del campo magnetico che permette il funzionamento del motore elettrico. Tramite la legge di Ampère tale movimento rispetto all'osservatore può essere messo in relazione al campo magnetico generato. In generale, i campi elettrici e magnetici prodotti da cariche o correnti elettriche sono calcolati risolvendo le equazioni di Maxwell. Il campo elettromagnetico di una particella carica in movimento è espresso tramite il potenziale di Liénard-Wiechert, anche quando la velocità della particella è prossima a quella della luce.
Quando un elettrone è in moto in corrispondenza di un campo magnetico è soggetto alla forza di Lorentz, la quale esercita una variazione della componente della velocità dell'elettrone perpendicolare al piano definito dal campo magnetico e dalla velocità iniziale dell'elettrone e la forza centripeta che viene generata costringe l'elettrone a seguire una traiettoria elicoidale. L'accelerazione che deriva da questo moto curvilineo, nel caso di velocità relativistiche, causa una radiazione di energia da parte dell'elettrone sotto forma di radiazione di sincrotrone. L'emissione di energia causa a sua volta un rinculo dell'elettrone, conosciuto come forza di Abraham-Lorentz-Dirac, che rallenta il moto dell'elettrone; questa forza è generata da un effetto di retroazione del campo dell'elettrone su sé stesso.
In elettrodinamica quantistica, l'interazione elettromagnetica tra le particelle è trasmessa dai fotoni: un elettrone isolato nello spazio vuoto che non subisce un'accelerazione non è in grado di emettere o di assorbire un fotone reale, poiché così facendo violerebbe le leggi di conservazione dell'energia e della quantità di moto. Invece i fotoni virtuali possono trasferire la quantità di moto tra due particelle cariche ed è questo scambio di fotoni virtuali che genera, per esempio, la forza di Coulomb. L'emissione di energia può avvenire quando un elettrone viene deviato da una particella carica, come per esempio un protone; l'accelerazione dell'elettrone porta all'emissione della radiazione di "bremsstrahlung", detta anche radiazione di frenamento.
Una collisione anelastica tra un fotone e un elettrone libero produce l'effetto Compton: questo urto è associato a un trasferimento dell'energia e della quantità di moto tra le particelle, che porta alla variazione della lunghezza d'onda del fotone incidente. Il valore massimo di questa variazione della lunghezza d'onda è "h"/"m"c ed è noto come lunghezza d'onda Compton e per l'elettrone vale . Se la lunghezza d'onda della luce incidente è sufficientemente lunga (come ad esempio quella della luce visibile, che ha una lunghezza d'onda che va da a ), la variazione della lunghezza d'onda dovuta all'effetto Compton diventa trascurabile e l'interazione tra radiazione e particelle può essere descritta tramite lo scattering Thomson.
La forza dell'interazione elettromagnetica tra due particelle cariche è data dalla costante di struttura fine α che è una quantità adimensionale formata dal rapporto di due contributi energetici: l'energia elettrostatica di attrazione o repulsione data dalla separazione di una lunghezza d'onda Compton e dall'energia a riposo della carica. Il suo valore è , che è possibile approssimare con la frazione 1/137.
Quando elettroni e positroni collidono si annichilano l'un l'altro, originando due o più fotoni dei raggi gamma. Se invece la quantità di moto dell'elettrone e del positrone è trascurabile si può formare il positronio prima che il processo di annichilamento porti alla formazione di due o tre fotoni dei raggi gamma con un'energia totale di . D'altra parte i fotoni molto energetici possono trasformarsi in un elettrone e in un positrone tramite un processo chiamato produzione di coppia, ma questo avviene solo in presenza di una particella carica nelle vicinanze, come un nucleo atomico.
Nella teoria dell'interazione elettrodebole la componente sinistrorsa della funzione d'onda dell'elettrone forma un doppietto di isospin debole con il neutrino elettronico, cioè a causa dell'interazione elettrodebole il neutrino si comporta come un elettrone. Ciascuna componente di questo doppietto può subire l'interazione della corrente debole carica tramite l'emissione o l'assorbimento di un bosone W e può essere trasformata nell'altra componente. La carica è conservata durante questo processo poiché anche il bosone W porta una carica che annulla ogni variazione netta durante la reazione. Le interazioni della corrente debole carica sono responsabili del decadimento beta negli atomi radioattivi. Sia l'elettrone che il neutrino possono subire l'interazione della corrente debole neutra tramite uno scambio di bosoni Z e questo è responsabile dello scattering elastico tra elettrone e neutrino.
Se un corpo ha un numero di elettroni maggiore o minore rispetto a quelli necessari per bilanciare la carica positiva dei nuclei, esso presenterà una carica elettrica netta: nel caso di un eccesso di elettroni, il corpo è carico negativamente, mentre nel caso di un difetto di elettroni, il corpo è carico positivamente; se invece il numero di elettroni e il numero di protoni sono uguali, le loro cariche si annullano a vicenda e il corpo è dunque elettricamente neutro. Un corpo macroscopico può sviluppare una carica elettrica ad esempio attraverso lo sfregamento, per via dell'effetto triboelettrico.
Gli elettroni indipendenti che si muovono nel vuoto sono detti "elettroni liberi" e anche gli elettroni nei metalli hanno un comportamento simile a quelli liberi. Il flusso di carica elettrica dovuto al moto degli elettroni liberi o in un materiale è detto corrente elettrica.
I materiali sono classificati in base alla resistenza che oppongono al passaggio di corrente: si dividono in conduttori, semiconduttori e isolanti (o dielettrici).
In generale, ad una data temperatura, ciascun materiale ha una conducibilità elettrica che determina il valore della corrente quando è applicato un potenziale elettrico. Esempi di buoni conduttori, cioè materiali capaci di far scorrere facilmente al proprio interno elettricità, sono i metalli come il rame e l'oro, mentre vetro e plastica sono cattivi conduttori.
I metalli sono spesso anche buoni conduttori di calore. Nonostante questo, al contrario della conducibilità elettrica, la conducibilità termica è quasi indipendente dalla temperatura; ciò è espresso matematicamente dalla legge di Wiedemann-Franz, la quale afferma che il rapporto fra la conduttività termica e la conduttività elettrica è proporzionale alla temperatura.
Le proprietà di conduzione di un solido cristallino sono determinate dagli stati quantistici degli elettroni, la cosiddetta struttura elettronica a bande. Nel caso di solidi amorfi, cioè senza struttura cristallina, la descrizione è più complessa.
Nei solidi cristallini gli atomi sono disposti regolarmente in un reticolo. La simmetria di tale distribuzione spaziale permette di semplificare il calcolo degli stati energetici degli elettroni nel cristallo e ricavare la struttura a bande. Con questa descrizione è possibile approssimare il comportamento degli elettroni nei solidi con quello di elettroni liberi, ma con una diversa massa, detta massa efficace. Un elettrone all'interno di un reticolo cristallino è descritto da una funzione d'onda detta funzione di Bloch, alla quale è associato un vettore detto "quasi-impulso" o "impulso cristallino", che è l'analogo della quantità di moto per gli elettroni liberi. L'analogia con gli elettroni liberi è particolarmente adeguata per alcuni valori di impulso cristallino, per i quali si ha una relazione di dispersione quadratica, come nel caso libero.
Nei solidi gli elettroni sono trattati come quasiparticelle poiché, a causa dell'interazione reciproca e con gli atomi del reticolo, assumono delle proprietà diverse da quelle degli elettroni liberi. Inoltre, nei solidi si introduce una quasiparticella, detta lacuna, che descrive la "mancanza" di un elettrone. Tale particella ha una sua massa efficace ed ha carica positiva, uguale in valore assoluto a quella dell'elettrone.
Nei materiali isolanti gli elettroni rimangono confinati in prossimità dei loro rispettivi nuclei. Al contrario, i metalli hanno una struttura elettronica a bande, alcune delle quali sono parzialmente riempite dagli elettroni. La presenza di queste bande permette agli elettroni nei metalli di muoversi come elettroni liberi o delocalizzati; essi non sono associati a uno specifico atomo e quindi, quando è applicato un campo elettrico, si muovono liberamente come un gas, chiamato gas di Fermi.
Un'altra categoria di materiali è quella dei semiconduttori, in cui la conducibilità può variare di molto fra i valori estremi di conduzione e isolante.
A causa delle collisioni fra elettroni e atomi la velocità di deriva degli elettroni in un conduttore è dell'ordine di pochi millimetri per secondo. Ciò nonostante, la velocità di propagazione di un segnale elettrico, cioè la velocità con la quale si propaga la variazione di corrente in un conduttore, è tipicamente di circa il 75% della velocità della luce. Questo accade perché i segnali elettrici si propagano come onde, con una velocità dipendente dalla costante dielettrica del materiale.
Il disordine termico nel reticolo cristallino del metallo causa un aumento della resistività del materiale, producendo quindi la dipendenza dalla temperatura per la corrente elettrica.
Quando alcuni materiali sono raffreddati al di sotto di una certa temperatura critica, avviene una transizione di fase a causa della quale essi perdono la resistività alla corrente elettrica, in un processo noto come superconduttività. Nella teoria BCS, gli elettroni sono legati in coppie che entrano in uno stato quantistico noto come condensato di Bose-Einstein. Tali coppie, dette coppie di Cooper, si accoppiano nel loro moto per mezzo delle vibrazioni di reticolo chiamate fononi, evitando le collisioni con gli atomi che normalmente causano la resistività elettrica (le coppie di Cooper hanno un raggio di circa , quindi si possono scavalcare a vicenda). La teoria BCS non descrive tutti i materiali superconduttori, e non esiste ancora un modello teorico in grado di spiegare completamente la superconduttività ad alta temperatura.
Gli elettroni all'interno dei solidi conduttivi, che sono a loro volta trattati come quasi-particelle, quando sono strettamente confinati intorno a temperature vicine alle zero assoluto si comportano globalmente come due nuove differenti quasi-particelle: gli spinoni e gli oloni. Il primo trasporta spin e momento magnetico, mentre il secondo la carica elettrica.
Gli elettroni possono, secondo la teoria di Eugene Paul Wigner, formare essi stessi una struttura cristallina, disponendosi nei punti di un reticolo. Tale stato della materia è detto cristallo di Wigner.
Per spiegare gli istanti iniziali dell'evoluzione dell'universo è stata sviluppata la teoria del Big Bang, che è la più accettata dalla comunità scientifica. Nel primo millisecondo dell'esistenza dell'universo noto, la temperatura era di circa un miliardo di kelvin e i fotoni avevano un'energia media nell'ordine del milione di elettronvolt; questi fotoni erano sufficientemente energetici da poter reagire l'un l'altro per formare coppie di elettroni e positroni:
dove formula_6 è il fotone, formula_10 è il positrone e formula_11 è l'elettrone. Contemporaneamente le coppie elettrone-positrone si annichilivano e producevano fotoni energetici. I due processi erano in equilibrio durante la prima fase di evoluzione dell'universo, ma dopo 15 secondi la temperatura dell'universo calò sotto la soglia di formazione delle coppie di elettroni-positroni. La maggior parte degli elettroni e positroni rimasti si annichilirono e produssero raggi gamma che in breve tempo irradiarono l'universo.
Per ragioni non ancora ben comprese, durante il processo di leptogenesi vi era un numero maggiore di elettroni rispetto a quello dei positroni, perciò circa un elettrone ogni miliardo sopravvisse durante il processo di annichilazione. Questo eccesso era analogo a quello dei protoni sugli antiprotoni, in una condizione nota come asimmetria barionica, perciò la carica netta presente nell'universo risultava nulla. I protoni e i neutroni superstiti iniziarono a interagire in un processo noto come nucleosintesi primordiale, durato fino a circa 5 minuti dopo l'istante iniziale, in cui si assistette alla formazione dei nuclei degli isotopi di idrogeno, elio e in minima parte litio. I neutroni rimasti subirono il decadimento beta, con una vita media di circa quindici minuti, con la formazione di un protone, un elettrone e un antineutrino:
dove formula_13 è il neutrone, formula_14 è il protone e formula_15 è l'antineutrino elettronico. Per i successivi - anni gli elettroni liberi erano troppo energetici per legarsi ai nuclei atomici; passato questo periodo, seguì un processo di ricombinazione, in cui gli elettroni si legarono ai nuclei atomici per formare atomi elettricamente neutri e a causa di ciò l'universo divenne trasparente alla radiazione elettromagnetica.
Circa un milione di anni dopo il Big Bang, si iniziò a formare la prima generazione di stelle; all'interno di queste stelle, la nucleosintesi portò alla produzione di positroni derivanti dalla fusione di nuclei atomici e queste particelle di antimateria si annichilirono immediatamente con gli elettroni formando raggi gamma. Ciò portò a una continua riduzione nel numero di elettroni e a un corrispettivo aumento di neutroni; nonostante questo il processo di evoluzione stellare portò alla sintesi di isotopi radioattivi i quali potevano decadere con un decadimento di tipo beta, emettendo in questo modo un elettrone e un antineutrino dal nucleo.
Alla fine della sua vita, una stella di massa superiore di 20 volte la massa solare può subire un collasso gravitazionale e formare un buco nero; in base alle leggi della fisica classica, questo oggetto stellare massivo esercita un'attrazione gravitazione così grande da impedire a qualsiasi cosa, anche alla radiazione elettromagnetica, di potergli sfuggire una volta che è stato superato il raggio di Schwarzschild. Si pensa tuttavia che gli effetti quantistici possano permettere l'emissione di una radiazione di Hawking a tale distanza, infatti si ritiene che sull'orizzonte degli eventi di questi oggetti vengano prodotte coppie virtuali di elettroni e positroni e quando esse vengono formate in prossimità dell'orizzonte degli eventi, la distribuzione spaziale casuale di queste particelle può permettere a una particella della coppia di apparire all'esterno dell'orizzonte grazie all'effetto tunnel. Il potenziale gravitazionale del buco nero può fornire l'energia sufficiente per trasformare la particella virtuale in una particella reale, facendo in modo da diffonderla nello spazio, mentre all'altra particella della coppia è stata fornita energia negativa e ciò comporta una perdita netta di energia del buco nero. La velocità della radiazione di Hawking cresce con il diminuire della massa e questo comporta l'evaporazione del buco nero che alla fine esplode.
Un altro modo di formazione degli elettroni è dato dall'interazione dei raggi cosmici con gli strati alti dell'atmosfera: i raggi cosmici sono particelle che viaggiano nello spazio con energie anche dell'ordine dei e, quando esse collidono con le particelle presenti nell'alta atmosfera terrestre, vi è la produzione di una cascata di particelle, tra le quali pioni e muoni, con questi ultimi che sono i responsabili di più della metà della radiazione cosmica osservata a Terra. Il decadimento del pione porta alla formazione dei muoni tramite il seguente processo:
mentre a suo volta il muone può decadere formando elettroni:
Le prime osservazioni degli elettroni come particella, hanno sfruttato fenomeni elettrostatici o la produzione di raggi catodici. Oggi si eseguono esperimenti in laboratorio in cui vengono osservati elettroni sia per lo studio delle proprietà di queste particelle, sia per studiare le proprietà di corpi macroscopici.
In condizioni di laboratorio, l'interazione di elettroni individuali possono essere osservate con l'uso di rilevatori di particelle, che permettono misure precise di specifiche proprietà come energia, spin e carica elettrica. Lo sviluppo della trappola ionica quadrupolare ha permesso di contenere particelle in piccole regioni dello spazio per lunghi periodi. Questo ha permesso la misura precisa delle proprietà particellari. Per esempio in una misurazione si è riusciti a contenere un singolo elettrone per un periodo di dieci mesi. Il momento magnetico di un elettrone fu misurato con una precisione di 11 cifre significative, che, nel 1980, è la misura migliore di una costante fisica.
La prima immagine video della distribuzione di energia di un elettrone è stata catturata da un team dell'università di Lund in Svezia, nel febbraio 2008. Gli scienziati hanno usato flash estremamente piccoli di luce, che hanno permesso di osservare il moto di un elettrone per la prima volta.
Tramite la misura dell'energia irradiata da elettroni, gran parte delle misure spettroscopiche sono collegati allo studio degli elettroni liberi o legati, misurando l'energia dei fotoni emessi.
Per esempio, nell'ambiente ad alta energia come la corona di una stella, gli elettroni liberi formano un plasma che emette energia per gli effetti di Bremsstrahlung. Il gas elettronico può formare delle oscillazioni di plasma, ovvero oscillazioni regolari della densità degli elettroni, e queste possono produrre emissioni di energia che possono essere rilevate usando i radiotelescopi.
Nel caso di atomi e molecole, un elettrone confinato a muoversi attorno a un nucleo può transire fra i diversi livelli energetici di questo consentiti, assorbendo o emettendo fotoni di frequenza caratteristica. Per esempio, quando un atomo è irraggiato da una sorgente con uno spettro continuo, appariranno delle distinte linee spettrali per la radiazione trasmessa. Ciascun elemento o molecola esibisce un insieme caratteristico proprio di serie di linee spettrali, che lo distinguono dagli altri atomi, come per esempio il noto caso delle serie dello spettro dell'atomo di idrogeno. Lo studio dell'intensità e la larghezza di queste linee permette di indagare le proprietà fisico-chimiche delle sostanza in analisi.
La distribuzione di elettroni nei materiali solidi può essere visualizzata dallo spettroscopio ARPES ("Angle resolved photoemission spectroscopy", ovvero spettroscopia fotoelettrica angolarmente risolta). Questa tecnica si basa sull'effetto fotoelettrico per misurare il reticolo reciproco, una rappresentazione matematica della struttura periodica di un cristallo. ARPES può essere usato per determinare la direzione, la velocità e la diffusione di elettroni nel materiale.
I fasci di elettroni sono usati nella saldatura di materiali, permettendo di raggiungere densità di energia superiori ai nello stretto diametro focale di e spesso non richiedono un materiale di riempimento. Questa tecnica di saldatura deve essere eseguita nel vuoto, in modo tale che gli elettroni non interagiscano con l'aria prima di raggiungere il bersaglio e può essere usata per unire materiali conduttori che altrimenti sarebbero difficili da saldare.
La litografia a fasci di elettroni (EBL) è un metodo per stampare i semiconduttori a risoluzioni più basse del micron. Questa tecnica è limitata dagli alti costi, basse performance, dalla necessità di operare con fascio nel vuoto e dalla tendenza degli elettroni a essere diffusi nei solidi. L'ultimo problema limita la risoluzione a circa . Per questa ragione, l'EBL è principalmente usata per la produzione di un piccolo numero di circuiti integrati specializzati.
La lavorazione con fasci di elettroni è usata per irradiare i materiali in modo da cambiare le loro proprietà fisiche o per la sterilizzazione medica e la produzione di cibo. Nella radioterapia, i fasci di elettroni generati da acceleratori lineari sono usati per il trattamento di tumori superficiali: dato che un fascio di elettroni può penetrare solamente uno spessore limitato prima di essere assorbito, tipicamente intorno a per elettroni di energia nel range 5–, la radioterapia è utile per il trattamento di lesioni della cute come il carcinoma basocellulare. Un fascio di elettroni può essere usato per integrare il trattamento di aree che sono state irraggiate da raggi X.
Gli acceleratori di particelle usano campi elettrici per far raggiungere agli elettroni e alle loro antiparticelle alte energie. Nel momento in cui queste particelle passano in una regione in cui c'è campo magnetico, questi emettono radiazione di sincrotrone. L'intensità di questa radiazione dipende dallo spin e questo può permettere la polarizzazione dei fasci di elettroni in un processo noto come effetto Sokolov-Ternov. La polarizzazione di fasci di elettroni può essere molto utile per numerosi esperimenti. La radiazione di sincrotrone può anche essere usata per raffreddare il fascio di elettroni, in modo da ridurre la quantità di moto persa dalle particelle. Una volta che le particelle sono state accelerate sino alla energia richiesta, i fasci separati di elettroni e positroni sono portati alla collisione e la risultante emissione di radiazione è osservata dai rivelatori di particelle ed è studiata dalla fisica particellare.
Gli elettroni possono essere utilizzati anche per ottenere immagini microscopiche grazie ai microscopi elettronici, che indirizzano un fascio focalizzato direttamente sul campione. A causa dell'interazione del fascio con il materiale, alcuni elettroni cambiano le loro proprietà, come una variazione della direzione, della fase relativa e dell'energia. Registrando questi cambiamenti del fascio elettronico, si possono produrre immagini a risoluzione atomica del materiale. Questa elevata risoluzione, maggiore dei microscopi ottici (che è di circa in luce blu), è possibile poiché i microscopi elettronici sono limitati dalla lunghezza d'onda di De Broglie degli elettroni (a titolo d'esempio, un elettrone ha una lunghezza d'onda di quando questo viene accelerato da un potenziale di ). Il microscopio elettronico a trasmissione corretto in aberrazione è in grado di avere una risoluzione inferiore a , che è sufficiente per risolvere i singoli atomi. Queste caratteristiche tecniche rendono il microscopio elettronico uno strumento di laboratorio utile per le immagini ad alta risoluzione; a fronte di questi vantaggi, i microscopi elettronici sono strumenti molto costosi da mantenere.
Vi sono due tipi di microscopi elettronici: a trasmissione e a scansione. Il primo funziona in maniera analoga a una lavagna luminosa, ovvero il fascio di elettroni passa attraverso una parte del campione e viene successivamente proiettato tramite lenti su diapositive o su un CCD. Nel secondo invece l'immagine è prodotta con un fascio elettronico molto fine che scansione riga per riga una piccola regione del campione; l'ingrandimento varia da 100× a o più per entrambi i microscopi. Un altro tipo di microscopio elettronico è quello a effetto tunnel sfrutta l'effetto tunnel quantistico degli elettroni che fluiscono da una punta conduttrice appuntita al materiale di interesse e può riprodurre immagini a risoluzione atomica delle superfici.
Altre tecniche permettono di studiare la struttura cristallina dei solidi; una tecnica che sfrutta questo principio è il Low Energy Electron Diffraction (LEED) che permette di visualizzare su uno schermo fluorescente la figura di diffrazione di un cristallo utilizzando un fascio collimato di elettroni avente un'energia tra i 20 e i . Un altro metodo che sfrutta la diffrazione è il Reflection high-energy electron diffraction (RHEED) che sfrutta la riflessione di un fascio di elettroni incidente a piccoli angoli in modo da caratterizzare la superficie del materiale di studio; l'energia tipica del fascio è tra 8 e , mentre l'angolo di incidenza varia tra 1° e 4°.
Nel laser a elettroni liberi, un fascio di elettroni a energia relativistica passa attraverso una coppia di ondulatori che contengono una serie di dipoli magnetici, i cui campi sono orientati in direzioni alternate; l'elettrone emette radiazione di sincrotrone che, a turno, interagisce coerentemente con lo stesso elettrone e ciò porta a un grosso aumento del campo di radiazione alla frequenza di risonanza. Il laser può emettere una radiazione elettromagnetica coerente ad alta radianza con un ampio intervallo di frequenze, che va dalle microonde ai raggi X morbidi. Questo strumento potrà essere utilizzato per l'industria, per le comunicazioni e per varie applicazioni mediche, come la chirurgia dei tessuti molli.
Gli elettroni sono fondamentali per il funzionamento dei tubi catodici, che sono largamente usati nei dispositivi come computer e televisori. In un tubo fotomoltiplicatore ogni fotone che colpisce il fotocatodo dà inizio a una cascata di elettroni che produce un impulso di corrente rivelabile. I tubi a vuoto sfruttano il flusso di elettroni per manipolare i segnali elettrici e svolgono un ruolo importante nello sviluppo nell'elettronica; nonostante ciò essi sono stati in gran parte soppiantati dai dispositivi a semiconduttori come i transistor.
</text>
</doc>
<doc id="17770" url="https://it.wikipedia.org/wiki?curid=17770">
<title>Grandezza fisica</title>
<text>
Secondo la terza edizione, del 2007, del "Vocabolario Internazionale di Metrologia" ("VIM 3"), in fisica una grandezza è la proprietà di un fenomeno, corpo o sostanza, che può essere espressa quantitativamente mediante un numero e un riferimento (ovvero che può essere misurata quantitativamente).
Nella seconda edizione del "Vocabolario Internazionale di Metrologia" (1993) una grandezza era definita come ""la proprietà misurabile di un fenomeno, corpo o sostanza, che può essere distinta qualitativamente e determinata quantitativamente""; pertanto, la misurazione non può essere applicata alle proprietà nominali, le quali non possono pertanto essere definite "grandezze". Nella definizione di "grandezza" del "VIM 3" il termine "riferimento" può essere una unità di misura, una procedura di misura, o un materiale di riferimento, o una loro combinazione. Sebbene in base a questa definizione, il concetto di "grandezza" coincida con quello di "grandezza fisica scalare", può essere considerato "grandezza" anche un vettore le cui componenti siano grandezze. Il concetto di grandezza, inoltre, può essere specificato progressivamente in vari livelli di concetti specifici. Per esempio, il concetto della grandezza "lunghezza" può essere specificato, per esempio, in quelle di:
Nel SI (Sistema internazionale di unità di misura), adottato per legge in Italia dal 1976 e obbligatorio negli atti pubblici, le grandezze si dividono in 7 "grandezze base" e numerose "grandezze derivate" dalle precedenti.
Condizione necessaria perché una (classe di equivalenza di) proprietà sia misurabile è quella di poter stabilire una relazione d'ordine fra quelle proprietà in sistemi diversi: poter giudicare quale sistema esibisce "più" proprietà dell'altro. Se tale confronto può essere basato sul "rapporto", espresso da un numero, fra le proprietà dei due sistemi, allora la classe di equivalenza di quelle proprietà costituisce una "grandezza fisica".
In questo caso, è possibile scegliere la proprietà di un particolare sistema ed eleggerla a "unità di misura" per quella grandezza fisica. Fissata l'unità di misura, la quantità di tale grandezza per un qualsiasi altro sistema potrà dunque essere univocamente specificata da un valore numerico ottenuto dal rapporto con la proprietà scelta come campione di riferimento.
Possiamo quindi esprimere il valore di una grandezza fisica formula_1 come il prodotto di un valore numerico {M} e un'unità di misura [M]:
Esistono anche "grandezze adimensionali", per le quali non è necessario definire un'unità di misura (ad esempio la frazione molare e il numero di Reynolds).
Come detto, essere capaci di confrontare proprietà omogenee significa semplicemente essere capaci di stabilire una relazione d'ordine fra quelle proprietà in sistemi diversi. La possibilità di valutare un "rapporto numerico" fra le due proprietà è invece una "condizione più forte".
Il tipico controesempio di proprietà fisica che "non" costituisce una grandezza fisica (propriamente detta) è dato dalla temperatura. Sebbene, dati due corpi, sia sempre possibile giudicare quale sia a temperatura maggiore o minore dell'altro (ad esempio misurando la direzione in cui fluisce il calore), tuttavia non avrebbe alcun significato fisico l'affermazione secondo cui un corpo si trova a temperatura, per dire, doppia di quella dell'altro. Nel caso della temperatura, ovvero nel caso di una proprietà fisica che manifesta soltanto una relazione d'ordine, è possibile applicare metodi quantitativi solo definendo una scala (di misura), che in questo caso diremo "termometrica". Anche se con abuso di linguaggio si parla di "unità di misura della temperatura", si tratta in realtà di una corrispondenza (arbitraria) fra la proprietà esibita da diversi fenomeni e una porzione dell'asse dei numeri reali.
L'arbitrarietà di una tale scelta è molto maggiore dell'arbitrarietà della scelta di un'unità di misura per una (vera) grandezza fisica: infatti una qualsiasi trasformazione monotòna di una particolare scala termometrica scelta, costituirebbe un'alternativa del tutto legittima al problema di quantificare la proprietà fisica in questione, la temperatura.
Il caso delle grandezze fisiche propriamente dette, in questo senso, è speciale, perché esiste una scala naturale di confronto, data appunto dal rapporto reciproco: se un sistema presenta "il doppio" di una data proprietà rispetto a un altro sistema, i valori numerici "Q" rispecchieranno tale rapporto, "qualsiasi sia l'unità di misura scelta" per tale grandezza.
La lunghezza del perineo di peppe di un oggetto può essere confrontata con quella di un altro oggetto. La lunghezza, in astratto, costituisce una grandezza fisica perché è possibile stabilire la proporzione, ovvero il rapporto fra la lunghezza specifica di due oggetti. Possiamo allora scegliere la lunghezza di un oggetto particolare, come il metro campione, e utilizzarla come unità di misura per la lunghezza di qualsiasi altro oggetto.
Le grandezze fisiche possono essere:
La scelta delle grandezze base è il punto di partenza di ogni analisi dimensionale.
Il Sistema internazionale considera fondamentali queste sette grandezze fisiche:
Ogni altra grandezza fisica è omogenea a un prodotto di potenze di grandezze fondamentali detto "dimensione (fisica)", e grandezze (unita di misura) con la stessa dimensione sono fra loro omogenee per transitivita, anche se solo alcune loro combinazioni hanno senso fisicamente.
</text>
</doc>
<doc id="334" url="https://it.wikipedia.org/wiki?curid=334">
<title>Accelerazione</title>
<text>
In fisica, in primo luogo in cinematica, l'accelerazione è una grandezza vettoriale che rappresenta la variazione della velocità nell'unità di tempo. In termini differenziali, è pari alla derivata rispetto al tempo del vettore velocità.
Nel SI l'unità di misura del modulo dell'accelerazione è il m/s², ovvero metro al secondo quadrato.
Quando non specificato, per "accelerazione" si intende l'"accelerazione traslazionale", sottintendendo che lo spostamento a cui si fa riferimento è una traslazione nello spazio. Il termine, "accelerazione", infatti, può essere utilizzato con un significato più generale per indicare la variazione di una velocità in funzione del tempo. Ad esempio, nella descrizione del moto rotatorio, per definire l"'accelerazione di rotazione" si usano l'accelerazione angolare e l'accelerazione areolare.
Le derivate temporali della velocità di ordine superiore al primo vengono studiate nel moto vario.
L'accelerazione di un punto materiale è la variazione della sua velocità rispetto al tempo. Il modo più immediato per quantificare tale variazione consiste nel definire l"'accelerazione media" formula_1 come il rapporto tra la variazione di velocità formula_2 al tempo finale formula_3 e iniziale formula_4 posseduta dall'oggetto, e l'intervallo finito di tempo formula_5 di durata del moto:
Un modo preciso per caratterizzare l'accelerazione si ottiene considerando la velocità in ogni istante di tempo, ovvero esprimendo la velocità in funzione del tempo e, ove la funzione è continua, calcolandone la derivata. Si definisce in questo modo l"'accelerazione istantanea":
Si tratta del limite per l'intervallo di tempo tendente a zero del rapporto incrementale che definisce l'accelerazione media:
L'accelerazione media coincide con l'accelerazione istantanea quando quest'ultima è costante nel tempo (formula_9), e si parla in tal caso di "moto uniformemente accelerato".
Nel moto del punto materiale su di una curva, il vettore accelerazione in un punto è orientato verso la concavità della traiettoria in quel punto. Può succedere che durante il moto il vettore velocità cambi soltanto in direzione e verso, restando costante in modulo, come ad esempio nel caso di moto circolare uniforme. La componente del vettore accelerazione nella direzione del moto è in questo caso nulla, e il vettore è quindi radiale (perpendicolare alla traiettoria). Data una traiettoria curvilinea arbitraria e continua, per individuare la direzione ed il verso dell'accelerazione di un oggetto che la percorre si utilizza il metodo del cerchio osculatore.
In un contesto più formale, sia formula_10 la lunghezza di un arco della curva percorsa dall'oggetto in moto. Se formula_11 è lo spostamento dell'oggetto nel tempo formula_12, la norma della velocità istantanea nel punto formula_13 è la derivata dello spostamento rispetto al tempo:
con il vettore velocità che è quindi scritto come:
dove formula_16 è il vettore unitario tangente alla curva. Il modulo dell'accelerazione istantanea è allora:
ed il vettore accelerazione è dato da:
dove formula_19 è la curvatura e si sono evidenziate la componente in direzione del moto e la componente in direzione perpendicolare, con formula_20 vettore unitario normale alla curva. In generale è possibile introdurre una terna di versori ortonormali, detta triedro di Frenet, costituita ortogonalizzando i vettori velocità, accelerazione ed un terzo vettore, generato dal prodotto vettoriale dei primi due. I versori così generati prendono il nome di "versore tangente", "normale" e "binormale". L'accelerazione giace sempre, per costruzione, nel piano individuato dal versore tangente e da quello normale. La geometria differenziale sfrutta il triedro di Frenet per permettere di calcolare in ogni punto la curvatura e la torsione della traiettoria.
In uno spazio a tre dimensioni si può scrivere l'accelerazione come:
dove formula_22, formula_23 e formula_24 sono i versori del sistema di riferimento cartesiano utilizzato. Poiché, nella sua definizione generale, l'accelerazione è il vettore che quantifica la variazione di direzione e modulo della velocità, data una traiettoria qualsiasi, è sempre possibile scomporre l'accelerazione del corpo in una componente ad essa tangente, detta "accelerazione tangenziale", e in una componente perpendicolare, detta "accelerazione normale":
L'accelerazione tangenziale descrive il cambiamento in norma della velocità, mentre quella normale è associata alla variazione della direzione della velocità.
Sapendo che la velocità lineare formula_26, che è sempre tangente alla traiettoria, è legata alla velocità angolare formula_27 dalla relazione:
dove formula_29 denota il prodotto vettoriale, formula_27 la velocità angolare e formula_31 il raggio di curvatura della traiettoria nel punto considerato. Pertanto formula_26 è ortogonale al piano formato da formula_27 e da formula_31, e viceversa, il vettore formula_27 è ortogonale al piano formato da formula_26 e da formula_31, cioè dal piano sul quale avviene il moto.
Data una traiettoria formula_38 giacente in un piano, e tracciato per un punto formula_39 in moto il cerchio osculatore, ovvero la circonferenza tangente in ogni istante alla traiettoria in "formula_39", la quale approssima al meglio la traiettoria in quel punto, si trova che:
dove formula_42 è l'accelerazione angolare. Considerando la derivata del vettore velocità formula_43, si ha:
Eguagliando quanto ottenuto dalle equazioni precedenti e identificando i termini si ha che le componenti sono:
In due dimensioni il versore normale è univocamente determinato, mentre in tre dimensioni bisogna specificarlo; infatti, esso risulta parallelo al raggio del cerchio osculatore.
Da quanto mostrato segue inoltre che se la componente normale dell'accelerazione è nulla, allora il moto si svolge su una retta; infatti, la direzione del vettore velocità è costante, e dato che la velocità è sempre tangente alla traiettoria, quest'ultima è rettilinea. Nel caso in cui l'accelerazione tangenziale sia costante si ha un moto rettilineo uniformemente accelerato. Se, invece, anche la componente tangenziale dell'accelerazione sia nulla, il vettore velocità è allora costante e si ha un moto rettilineo uniforme.
Viceversa, se a essere costante è la componente normale la traiettoria risulterà circolare. In questo caso, essa prenderà il nome di "accelerazione centripeta" perché punta istante per istante verso il centro della circonferenza. Se l'accelerazione angolare, quindi anche l'accelerazione tangenziale, è costante, si ha un moto circolare uniformemente accelerato. Invece, nel caso di moto circolare uniforme l'accelerazione angolare è nulla, per cui l'accelerazione si riduce alla sola componente centripeta, pertanto la velocità angolare sarà costante nel tempo. 
Un osservatore solidale a un sistema di riferimento non inerziale sperimenterà delle accelerazioni apparenti. Per il teorema delle accelerazioni di Coriolis, le accelerazioni apparenti dall'osservatore sono due: la prima detta "accelerazione centrifuga", avente modulo e direzione identici all'accelerazione centripeta, ma con verso opposto, e la seconda che prende il nome di "accelerazione complementare", o "accelerazione di Coriolis", il cui valore è: 
L'accelerazione media si rappresenta con il grafico velocità-tempo, dal quale si comprende come l'accelerazione media sia uguale alla pendenza della retta che congiunge i punti iniziale e finale del grafico velocità-tempo in cui andiamo a calcolare la media.
L'accelerazione istantanea è la tangente alla curva velocità-tempo nel punto fissato, così come è il significato geometrico della derivata prima. Essa è quindi uguale alla pendenza della retta tangente alla curva nel punto in cui viene calcolata.
Attraverso lo studio della curva nel grafico velocità-tempo si possono ricavare ulteriori importanti informazioni: dall'angolo che la tangente forma con l'asse del tempo si evince che l'accelerazione è negativa se la tangente forma un angolo superiore ai 90 gradi con l'asse delle ascisse, è positiva se rimane sotto i 90 gradi mentre è nulla se la tangente è parallela all'asse. Inoltre, si noti come a valori positivi della curva accelerazione-tempo corrispondano valori crescenti della curva velocità-tempo. Poiché l'accelerazione è la derivata seconda della posizione, si può anche ricavare l'andamento della relazione accelerazione-tempo anche studiando la concavità del grafico.
Se gli formula_47 punti materiali di un sistema sono in movimento, solitamente, la posizione del centro di massa varia. Pertanto, nell'ipotesi in cui la massa totale formula_48 sia costante, l'accelerazione del centro di massa sarà:
dove formula_50 la quantità di moto totale del sistema e formula_51 è la sommatoria delle forze esterne.
</text>
</doc>
<doc id="24912" url="https://it.wikipedia.org/wiki?curid=24912">
<title>Attrito</title>
<text>
In fisica l'attrito è una forza che si oppone al movimento o spostamento di un corpo su una superficie: se si manifesta tra superfici in quiete tra loro è detta di attrito statico, se invece si manifesta tra superfici in moto relativo si parla invece di attrito dinamico. Si tratta di un fenomeno macroscopico sempre presente nel mondo reale presentando vantaggi e svantaggi a seconda del contesto di analisi e la cui origine fisica è fatta risalire alle forze di adesione o coesione tra materiali in interazione tra loro, le quali a loro volta derivano in ultima analisi dall'interazione elettrostatica tra i materiali in questione.
Aristotele non isolò il fenomeno dell'attrito, legandolo inscindibilmente alla dinamica di un corpo: nel suo modello per principio un corpo tenderebbe naturalmente a fermarsi se non mosso da qualche forza, in accordo con le proprie osservazioni del mondo quotidiano.
Fu invece Galilei a rendersi conto grazie agli esperimenti sul piano inclinato che era un fenomeno variabile in base al tipo di contatto tra i corpi, e che non era quindi "proprio" dei corpi stessi.
Coulomb proseguì lo studio fino ad arrivare all'enunciazione di tre leggi classiche riguardanti in particolare l'attrito radente: questo dipende linearmente dal carico di compressione delle superfici, non dipende dall'estensione della superficie di contatto tra i due corpi, ed infine non dipende dalla velocità relativa di strisciamento di un corpo sull'altro.
Queste tre "leggi" sono in realtà approssimazioni, in quanto valide solo sotto particolari ipotesi riduttive.
In particolare l'ultima legge è valida solo per velocità di strisciamento piuttosto ridotte, poiché all'aumentare della velocità il coefficiente di attrito diminuisce con legge non lineare; la seconda legge è valida per superfici mediamente piane e non eccessivamente ridotte, mentre al loro tendere al minimo (forza concentrata) il coefficiente di attrito può diminuire; infine la prima legge (di linearità) è valida fintanto che i materiali a contatto siano sufficientemente isotropi, esibiscano comportamenti elastici e poco viscosi e che l'intervallo di tempo in cui la superficie è a contatto sia sufficientemente lungo, mentre in caso di materiale viscoso, anisotropo e/o plastico (come alcuni terreni) o con tempi di contatto ridotti (come per ruote pneumatiche che ruotino velocemente, anche se senza scorrimenti) il legame carico di compressione/attrito diviene non-lineare.
Un esempio quotidiano della non validità dell'ultima legge si manifesta oggi nei freni automobilistici: la forza frenante non dipende solo dal coefficiente μ e dalla forza che preme il tamburo sul cerchione o la pastiglia sul disco (mentre è sostanzialmente indipendente dall'estensione dell'area di quest'ultimo), ma dipende anche dalla velocità di strisciamento tra pastiglia e disco, e di conseguenza una frenata di intensità costante ha una efficacia che aumenta al diminuire della velocità, causando il classico effetto di "contraccolpo" al momento dell'arresto.
La causa dell'attrito radente fu però sempre individuata nelle asperità tra le superfici a contatto fino a Hertz, che invece dimostrò come l'attrito radente sia dovuto soprattutto a fenomeni di adesione (legami chimici) tra le superfici a contatto, e modificò quindi il modello matematico del fenomeno.
Si osserva in particolare che lastre metalliche lucidate a specchio in condizioni di vuoto spinto possiedono un coefficiente di attrito enorme.
Infine la spiegazione quantistica dell'attrito ne lega le cause all'interazione elettrostatica attrattiva tra le molecole delle superfici di contatto, come evidente nel modello di Tomlinson.
Secondo l'interpretazione classica, esistono tre diversi tipi di attrito:
L'attrito radente è dovuto allo strisciamento (ad esempio, l'interazione tra due superfici piane che rimangono a contatto mentre scorrono l'una rispetto all'altra).
Si esercita tra le superfici di corpi solidi a contatto ed è espresso dalla formula:
dove "F" è la forza di attrito radente, formula_2 il coefficiente di attrito radente e formula_3 la componente perpendicolare al piano di appoggio della risultante delle forze agenti sul corpo. Per un corpo appoggiato su un piano orizzontale formula_3 è semplicemente uguale a formula_5, forza peso del corpo; per un corpo appoggiato su un piano inclinato di un angolo formula_6 rispetto all'orizzontale risulta invece
Il coefficiente d'attrito è una grandezza adimensionale e dipende dai materiali delle due superfici a contatto e dal modo in cui sono state lavorate. Esso corrisponde al rapporto tra la forza di attrito tra due corpi (formula_8) e la forza che li tiene in contatto (formula_3). Il coefficiente di attrito statico formula_10 è sempre maggiore o uguale al coefficiente d'attrito dinamico formula_11 per le medesime superfici. Dal punto di vista microscopico, esso è dovuto alle forze di interazione tra gli atomi dei materiali a contatto. Questo implica che la forza necessaria al primo distacco (cioè per far sì che i corpi inizino a strisciare) è superiore a quella necessaria a tenerli in strisciamento. Il coefficiente di attrito statico è uguale alla tangente dell'angolo massimo raggiungibile tra le due forze prima che uno dei due corpi cominci a scivolare lungo l'altro (angolo di attrito).
La forza di attrito, definita dalla prima delle due formule scritte sopra, rappresenta la "forza di attrito massima" che si manifesta nel contatto tra due superfici. Se la forza motrice formula_12 è minore di formula_13, allora l'attrito è pari a formula_12 e il corpo non si muove; se formula_12 supera formula_13, il corpo inizia a muoversi; per valori di formula_12 ancora maggiori, l'attrito (dinamico) è sempre costante e pari a formula_18.
Si può calcolare il coefficiente d'attrito dinamico di un materiale su un altro attraverso un piano inclinato, facendolo strisciare su di esso. Lasciando andare il corpo, esso si muoverà di moto rettilineo uniformemente accelerato, con accelerazione pari a:
dove a è l'accelerazione, "s" è lo spazio percorso e "t" è il tempo trascorso. La forza che muove il corpo è pari a:
Dove formula_12 è la forza che muove il corpo, formula_22 è la forza parallela al piano inclinato, formula_23 è l'attrito dinamico, formula_24 è la massa, formula_25 è l'accelerazione di gravità, formula_6 è l'angolo d'inclinazione del piano e formula_11 è il coefficiente d'attrito dinamico. Dividendo la forza risultante per la massa del corpo si ottiene l'accelerazione:
Ora si mettono a confronto le due formule dell'accelerazione:
e risolvendo l'equazione, si trova:
Il rotolamento di norma è reso possibile dalla presenza di attrito radente statico tra la ruota e il terreno; se questo attrito non ci fosse, o fosse molto piccolo (come nel caso di un terreno ghiacciato), la ruota striscerebbe senza riuscire a compiere un rotolamento puro, nel qual caso entrerebbe subito in gioco l'attrito radente dinamico che si oppone allo slittamento, riducendo progressivamente la velocità relativa fra i corpi striscianti, tende a ripristinare le condizioni di puro rotolamento. Un caso in cui il puro rotolamento può avvenire senza l'aiuto dell'attrito statico si ha quando una ruota che sta già rotolando su un piano orizzontale con velocità angolare formula_31, dove formula_32 è la velocità del centro di massa della ruota, viene lasciata a sé stessa: in tal caso l'attrito statico assume il valore zero e solo l'attrito volvente può frenare il rotolamento, riducendo simultaneamente e armonicamente sia la velocità di traslazione sia quella di rotazione della ruota in modo che il puro rotolamento si conservi fino a fine corsa.
Se si applica un momento alla ruota, essa inizia a rotolare senza strisciare fintanto che il momento applicato è minore di formula_33, dove formula_34 è il raggio della ruota. Se il momento supera questo valore, la forza motrice applicata alla superficie della ruota supera l'attrito statico massimo e la ruota slitta mentre rotola; è la classica "sgommata" ottenuta accelerando da fermi in modo repentino.
L'effetto dell'attrito volvente si può descrivere spostando leggermente in indietro, nel senso opposto al moto, la reazione vincolare (in genere non perfettamente normale) esercitata dal piano di rotolamento sul corpo rotolante, di modo che tale reazione vincolare abbia non solo una componente contraria al moto traslatorio, ma anche un momento di forza rispetto all'asse di rotazione della ruota che si oppone al moto rotatorio. Una siffatta reazione vincolare è la sintesi schematica del campo di sforzi che sorgono e si distribuiscono sull'intera area di contatto (che non è mai veramente puntiforme o riducibile ad un segmento) tra la ruota e il terreno: la rotazione causa di fatto una deformazione dell'area di contatto e quindi una distribuzione delle forze di pressione, dovute alla forza peso, non uniforme su tutta la superficie di contatto; il risultato di queste interazioni si può riassumere dicendo che il piano di rotolamento esercita sulla ruota una forza vincolare quasi-normale, rivolta verso l'alto e all'indietro rispetto al moto, la cui linea di applicazione di norma non passa per l'asse della ruota, di modo che tale forza produce sia una debole resistenza al moto traslatorio sia un debole momento torcente opposto al senso del rotolamento in atto.
Quantitativamente, questo tipo di attrito è espresso da un'equazione simile alla precedente,
A parità delle altre condizioni, infatti, la resistenza opposta dall'attrito volvente è tanto minore quanto maggiore è il raggio di curvatura del corpo che rotola.
Il coefficiente d'attrito volvente ha le dimensioni di una lunghezza. Similmente all'attrito radente, è possibile calcolare il coefficiente d'attrito volvente di un materiale su un altro attraverso un piano inclinato, facendolo rotolare su di esso. Lasciando andare il corpo, esso si muoverà di moto rettilineo uniformemente accelerato, con accelerazione pari a:
La forza che muove il corpo è pari a:
</text>
</doc>
<doc id="18682" url="https://it.wikipedia.org/wiki?curid=18682">
<title>Telescopio rifrattore</title>
<text>
Un telescopio rifrattore è un telescopio ottico che, mediante l'utilizzo di lenti, sfrutta il fenomeno della rifrazione.
Il più grande rifrattore mai costruito è quello da 49,2 pollici (125 cm) dell'Esposizione universale di Parigi del 1900, ma venne smantellato dopo alcuni anni, senza trovare acquirenti (le lenti e il siderostato sono conservati all'Osservatorio di Parigi), segue il 40 pollici (101,60 cm) dell'Osservatorio Yerkes, costruito alla fine del XIX secolo.
La componente ottica di un telescopio rifrattore è costituita da un tubo lungo sulla cui estremità frontale è disposto un doppietto (due vetri ottici, o lenti, opportunamente lavorati spaziati in aria) chiamato obiettivo, che ha la funzione di raccogliere e di focalizzare la luce.
L'obiettivo svolge sostanzialmente la funzione di prisma: scompone e ricompone la radiazione luminosa in un determinato punto dato dalla lunghezza focale strumentale.
Il tubo ottico, oltre ad assolvere alla funzione di sostegno dell'obiettivo e dell'oculare (o del dispositivo che esamina la radiazione luminosa) evita, dal momento che è chiuso ai due lati, che si verifichi il degrado dell'immagine dovuto ai moti interni dell'aria.
L'oculare è un altro insieme di lenti che serve a rendere accessibile all'occhio tutti i particolari contenuti nell'immagine formata dall'obiettivo.
I rifrattori possono essere di tipo acromatico, semi-apocromatico o apocromatico in funzione della capacità di focalizzare nello stesso punto la luce di diversi colori.
I rifrattori dal costo maggiore sono senza dubbio i cosiddetti apocromatici, strumenti studiati in modo da ridurre praticamente in modo completo l'aberrazione cromatica, tipica degli strumenti a lenti. Per raggiungere questa eccellenza nelle immagini si utilizzano sistemi ottici a bassa dispersione come lenti alla fluorite minerale o sintetica. 
Questo tipo di aberrazione si manifesta con la comparsa di aloni colorati sui bordi dei soggetti osservati, ed è più evidente nei rifrattori dal basso costo o non perfettamente corretti.
Un tipo particolare di rifrattore è il coronografo, per osservare la corona solare.
Rifrattori che presentano una buona qualità delle ottiche forniscono immagini luminose e molto contrastate. D'altro canto però questi strumenti presentano generalmente piccole aperture rispetto al costo di costruzione e questo li limita "pesantemente" nell'osservazione di tutti quelli che sono gli oggetti deboli (come nebulose o galassie).
All'aumentare delle dimensioni del telescopio il peso e il costo delle lenti rendono impraticabile la costruzione di grandi telescopi di questo tipo.
Nell'uso amatoriale, il telescopio a lenti trova le migliori applicazioni nella osservazione dei pianeti.
Nei rifrattori le immagini astronomiche posseggono una grandissima nitidezza, per questa ragione essi sono generalmente preferiti dagli astrofili che osservano gli sfuggenti particolari dei pianeti, spesso poco contrastati. Tuttavia a causa degli alti costi di produzione, per un astrofilo è possibile comprare un riflettore significativamente più grande a parità di costo e quindi dotato nel complesso di maggiore potere risolutivo. Proprio a causa delle difficoltà costruttive, i telescopi ad uso amatoriale in commercio difficilmente superano il diametro di 10–15 cm.
In caso di osservazioni terrestri si antepone spesso -prima dell'oculare- un prisma raddrizzatore ad angolo retto.
</text>
</doc>
<doc id="4441" url="https://it.wikipedia.org/wiki?curid=4441">
<title>Velocità</title>
<text>
In fisica, in primo luogo in cinematica, la velocità (dal latino "vēlōcitās", a sua volta derivato da "vēlōx", cioè "veloce") è una grandezza vettoriale definita come la variazione della posizione di un corpo in funzione del tempo, ossia, in termini matematici, come la derivata del vettore posizione rispetto al tempo.
Nel Sistema Internazionale la velocità si misura in m·s (metri al secondo).
Quando non specificato, per "velocità" si intende la "velocità traslazionale", sottintendendo che lo spostamento a cui si fa riferimento è una traslazione nello spazio. Il termine, "velocità", infatti, può essere utilizzato con un significato più generale per indicare la variazione di una coordinata spaziale in funzione del tempo. Ad esempio, nella descrizione del moto rotatorio, per definire la "velocità di rotazione" si usano la velocità angolare e la velocità areolare.
Talvolta si usa il termine "rapidità" per indicare il modulo della velocità. Ciò viene fatto in analogia con la lingua inglese, nella quale si indica con "speed" la rapidità e con "velocity" la velocità in senso vettoriale.
La variazione della velocità, sia in aumento che in diminuzione, è l'accelerazione, anche se nel linguaggio comune a volte si parla di "decelerazione" quando la velocità diminuisce.
La velocità è un vettore che indica la rapidità del moto, la direzione e il verso di un punto materiale in movimento. Essa si riduce a una grandezza scalare soltanto in casi particolari, ad esempio, nel moto rettilineo uniforme, in cui il vettore ha una sola componente diversa da zero.
Si definisce "velocità media" formula_1 il rapporto tra lo spostamento, inteso come la variazione dello posizione, formula_2 e l'intervallo di tempo formula_3 impiegato a percorrerlo:
dove formula_5 e formula_6 sono i vettori posizione agli istanti iniziale formula_7 e finale formula_8. La velocità media può essere vista come il coefficiente angolare della retta in un grafico spazio-tempo. In particolare si parla di velocità positiva , se l'angolo che la retta forma con l'asse delle ascisse è acuto e di velocità negativa , se l'angolo che la retta forma con l'asse delle ascisse è ottuso. 
Si definisce "velocità istantanea" formula_9 il limite della velocità media per intervalli di tempo molto brevi, ovvero la derivata della posizione rispetto al tempo:. In parole povere la velocità istantanea è il valore limite della velocità media nell'intorno di un determinato istante quando la variazione di tempo formula_10 considerata tende al valore 0.
Si noti che la velocità media è proprio la media della velocità istantanea in un tempo finito formula_12:
avendo usato il teorema fondamentale del calcolo integrale.
In un contesto più formale, sia formula_14 la lunghezza di un arco della curva percorsa dall'oggetto in moto, ovvero lo spostamento dell'oggetto al tempo formula_15. La norma della velocità istantanea nel punto formula_16 è la derivata dello spostamento rispetto al tempo:
ed il vettore velocità ha la direzione del moto:
con formula_19 il vettore unitario tangente alla curva.
Utilizzando uno spazio bidimensionale, la velocità media e quella istantanea si possono scomporre nel seguente modo:
dove formula_21 e formula_22 sono due versori in direzione degli assi formula_23 e formula_24. Il modulo del vettore velocità è a sua volta scomponibile nei suoi componenti:
dove formula_26 è il momento statico e formula_27 la quantità di moto totale del sistema.
In caso di caduta di un oggetto immerso in un campo gravitazionale, la velocità finale dell'oggetto può essere determinata utilizzando la conservazione dell'energia, ottenendo così una semplice espressione:
dove formula_29 è la differenza di quota tra il punto di caduta e quello in cui l'oggetto si ferma.
In quest'ultimo caso si parla di velocità di impatto.
Per velocità terminale di caduta, o "velocità limite", si intende la velocità massima che raggiunge un corpo in caduta. Cadendo attraverso un fluido infatti il corpo incontra una crescente resistenza all'aumentare della velocità e quando l'attrito eguaglia la forza di attrazione gravitazionale la velocità si stabilizza.
La velocità della luce, o di qualsiasi altra onda elettromagnetica, è identica nel vuoto per tutti i sistemi di riferimento. Questa invarianza, implicita nelle simmetrie delle equazioni di Maxwell per la propagazione delle onde elettromagnetiche e verificata sperimentalmente alla fine del 1800 con l'esperimento di Michelson-Morley, ha portato alla necessità di modificare le equazioni del moto e della dinamica. Una delle conseguenze della teoria della relatività ristretta di Albert Einstein è che la velocità massima raggiungibile al limite da un qualunque oggetto fisico è quella della luce nel vuoto.
</text>
</doc>
<doc id="14776" url="https://it.wikipedia.org/wiki?curid=14776">
<title>Potenziale elettrico</title>
<text>
Il potenziale elettrico (o potenziale coulombiano), in fisica e in particolare in elettromagnetismo, è il potenziale scalare associato al campo elettrostatico:
Il potenziale elettrico costituisce la componente temporale del quadripotenziale: insieme al potenziale magnetico, che ha natura vettoriale, forma il potenziale elettromagnetico.
Data una regione di spazio in cui è presente un campo elettrico conservativo, si definisce potenziale elettrico in un punto, il valore dato dal rapporto dell'energia potenziale elettrica rilevato da una carica elettrica di prova, posta in quel punto, e il valore della carica di prova. Il potenziale elettrico è dunque il rapporto tra l'energia potenziale elettrica, ossia il lavoro che deve compiere la forza dovuta al campo elettrico per spostare una o più cariche da quel punto fino all'infinito (ove si assume potenziale nullo), e la carica di prova.
Non potendo spostare una carica elettrica "fino all'infinito" si pone allora l'attenzione sulla energia "potenziale" liberabile da questa durante l'ipotetico movimento.
L'energia potenziale elettrica della carica è il livello di energia che la carica possiede a causa della sua posizione all'interno del campo elettrico, e pertanto il potenziale elettrico formula_2 della carica di prova è definito operativamente come il rapporto tra l'energia potenziale formula_3 e il valore della carica stessa, cioè:
Il potenziale è dunque una quantità scalare e non dipende dal valore della carica di prova. La sua unità di misura è inoltre il volt: il punto A è al potenziale di 1 volt quando la forza elettrica compirebbe il lavoro di un Joule per portare una carica di un Coulomb libera di muoversi da A a all'infinito. Per estensione, si dice che tra due punti A e B esiste una differenza di potenziale di un volt se una forza elettrica compisse pari lavoro, sulla stessa carica, nello spostamento tra i due punti. Il lavoro formula_5 svolto dal campo elettrico formula_6 per un percorso infinitesimo formula_7 su una carica formula_8 è dato da:
e per calcolare il lavoro lungo una linea formula_10 da un punto A ad un punto B:
Si definiscono superfici equipotenziali per il potenziale elettrico le superfici in ogni punto delle quali il potenziale elettrico assume lo stesso valore. Questo implica che il lavoro del campo elettrico lungo una superficie equipotenziale è nullo ovunque perché è nulla la componente del campo elettrico parallela alla superficie, cioè il campo elettrico è ortogonale alla superficie equipotenziale.
Si consideri il lavoro fatto dal campo elettrico creato da una carica puntiforme formula_12 nel portare una carica di prova formula_8 da un punto A ad un punto B:
dove formula_15 è la costante dielettrica nel vuoto, e:
con formula_17 l'angolo compreso fra i vettori formula_18 e formula_19. Si ha:
e tale formula mostra che il campo elettrostatico è conservativo, poiché il lavoro dipende solo dal valore della funzione formula_21 calcolata nei punti A e B e non dal particolare percorso seguito dalla carica formula_8. Si noti che la conservatività del campo elettrico viene tuttavia a mancare in condizioni non stazionarie.
Dal momento che per il teorema di Helmholtz si può sempre definire una funzione scalare formula_21 il cui gradiente, cambiato di segno, coincida con il campo formula_6:
il potenziale elettrico nel vuoto per una carica puntiforme è dato da:
Il potenziale elettrico è quindi definito a meno di una costante arbitraria, essendo il gradiente di una costante nullo. Questo non rappresenta un problema pratico, poiché normalmente interessa conoscere la differenza di potenziale formula_27, più che il valore del potenziale elettrico in un punto. Convenzionalmente, la costante viene determinata considerando nullo il potenziale che una carica puntiforme produce all'infinito.
In coordinate cartesiane, si ha:
Dalla definizione di potenziale elettrico in termini di lavoro, si ha che:
e quindi, le dimensioni del potenziale corrispondono a:
Una volta introdotta la definizione di potenziale per una carica puntiforme, per il principio di sovrapposizione lineare è possibile generalizzare la definizione del potenziale nel vuoto generato da una distribuzione di cariche puntiformi formula_31, disposte nello spazio nelle posizioni formula_32:
ovvero:
Poiché la carica elettrica è quantizzata, a rigore non esistono distribuzioni continue di carica elettrica. Tuttavia, in un corpo esteso le cariche elementari sono in numero talmente elevato che è conveniente utilizzare il formalismo infinitesimale ed introdurre la densità di carica volumetrica formula_35, superficiale formula_36 e lineare formula_37. In questo modo il potenziale elettrico in un punto dello spazio formula_38 generato da una sorgente estesa con carica totale
è dato dall'integrale:
in cui formula_41 è la distanza dall'origine del punto P e formula_42 è la distanza dall'origine del volume infinitesimale formula_43.
</text>
</doc>
<doc id="14789" url="https://it.wikipedia.org/wiki?curid=14789">
<title>Campo elettrico</title>
<text>
In fisica, il campo elettrico è un campo di forze generato nello spazio dalla presenza di una o più cariche elettriche o di un campo magnetico variabile nel tempo. Insieme al campo magnetico esso costituisce il campo elettromagnetico, responsabile dell'interazione elettromagnetica.
Introdotto da Michael Faraday, il campo elettrico si propaga alla velocità della luce ed esercita una forza su ogni oggetto elettricamente carico. Nel sistema internazionale di unità di misura si misura in newton su coulomb (N/C), o in volt su metro (V/m). Se è generato dalla sola distribuzione stazionaria di carica spaziale, il campo elettrico è detto elettrostatico ed è conservativo.
Sperimentalmente si verifica l'attrazione o la repulsione tra corpi dotati di carica elettrica, corrispondente a due stati di elettrizzazione della materia. La carica si definisce positiva quando vi è una carenza di elettroni nell'oggetto, negativa in presenza di un eccesso. Corpi elettrizzati entrambi positivamente o entrambi negativamente si respingono, mentre corpi elettrizzati in modo opposto si attraggono.
Per misurare l'elettrizzazione di un corpo si usa uno strumento chiamato elettroscopio a foglie, costituito da un'ampolla di vetro nella quale è inserita un'asta metallica la quale, all'interno dell'ampolla, ha due linguette metalliche molto sottili, dette "foglie", mentre all'esterno essa può essere messa a contatto con un corpo carico. Mettendo a contatto con l'asta un corpo carico, le linguette si allontanano l'una dall'altra in proporzione all'elettrizzazione del corpo che è stato messo a contatto.
A partire da tali evidenze sperimentali, nella seconda metà del diciottesimo secolo Charles Augustin de Coulomb formulò la legge di Coulomb, che quantifica la forza elettrica attrattiva o repulsiva che due corpi puntiformi carichi elettricamente si scambiano a distanza. A partire da tale legge si può affermare che un corpo carico elettricamente produce nello spazio circostante un campo tale per cui, se si introduce una carica elettrica, questa risente dell'effetto di una forza, detta forza di Coulomb, direttamente proporzionale al prodotto delle due cariche e inversamente proporzionale al quadrato della loro distanza.
Nel vuoto, il campo elettrico formula_1 in un punto dello spazio è definito come la forza per unità di carica elettrica positiva alla quale è soggetta una carica puntiforme formula_2, detta carica "di prova", se posta nel punto:
Il vettore campo elettrico formula_1 in un punto è quindi definito come il rapporto tra la forza elettrica agente sulla carica di prova ed il valore della carica stessa, purché la carica di prova sia sufficientemente piccola da provocare una perturbazione trascurabile sull'eventuale distribuzione di carica che genera il campo. Il campo è dunque indipendente dal valore della carica di prova usata, essendone indipendente il rapporto tra la forza e la carica stessa, e questo mostra che il campo elettrico è una proprietà caratteristica dello spazio. Dalla definizione si ricava che l'unità di misura del campo elettrico è formula_5, che equivale a formula_6.
Dalla legge di Coulomb segue che una carica formula_7 posta in formula_8 genera un campo elettrico che in un punto qualsiasi formula_9 è definito dalla seguente espressione:
dove formula_11 è la costante dielettrica del vuoto.
Per un numero "n" di cariche puntiformi formula_12 distribuite nello spazio il campo elettrostatico nella posizione formula_9 è dato da:
In generale, per una distribuzione continua di carica si ha:
dove formula_16 rappresenta la densità di carica nello spazio:
e formula_18 rappresenta la regione di spazio occupata dalla distribuzione di carica. Il campo elettrico si può esprimere come gradiente di un potenziale scalare, il potenziale elettrico:
Essendo il potenziale elettrico un campo scalare, il campo elettrico è conservativo.
Il campo elettrico è un campo vettoriale rappresentato attraverso linee di campo: una carica puntiforme positiva produce le linee di campo radiali uscenti da essa, ed è definita sorgente delle linee di forza, mentre per una carica puntiforme negativa le linee di campo sono radiali ed entranti verso la carica, che è così definita pozzo di linee di forza. Le linee di livello a potenziale elettrico costante sono dette superfici equipotenziali, e sono perpendicolari alle linee di flusso del campo elettrico.
Il fatto che una superficie chiusa che racchiuda la sorgente del campo sia attraversata da tutte le linee di forza generate dalla sorgente, si formalizza attraverso il teorema del flusso, anche detto teorema di Gauss, che definisce una proprietà matematica generale per il campo vettoriale elettrico. Nel vuoto il teorema afferma che il flusso del campo elettrico attraverso una superficie chiusa contenente una distribuzione di carica caratterizzata dalla densità di carica volumetrica formula_20 è pari alla carica totale contenuta nel volume racchiuso dalla superficie diviso per la costante dielettrica del vuoto:
Applicando il teorema della divergenza alla prima relazione ed uguagliando gli integrandi si ottiene:
Tale equazione è la prima delle equazioni di Maxwell, e costituisce la forma locale del teorema di Gauss per il campo elettrico.
Il campo elettrostatico viene generato da una distribuzione di carica indipendente dal tempo. Condizione necessaria e sufficiente perché un campo vettoriale sia conservativo in un insieme semplicemente connesso, ad esempio, un insieme stellato o convesso, è che la circuitazione del campo, cioè l'integrale del campo lungo una linea chiusa, sia nulla:
Questo avviene solamente in condizioni stazionarie.
In maniera equivalente, il campo elettrostatico è conservativo dal momento che esiste una funzione scalare, il potenziale elettrico, tale che l'integrale per andare da un punto A ad un punto B non dipenda dal cammino percorso ma solo dal valore della funzione agli estremi:
Dal teorema della divergenza e dal teorema del flusso si ricava la prima equazione di Maxwell nel vuoto:
Per la conservatività del campo elettrostatico è possibile enunciare la terza equazione di Maxwell nel vuoto nelle forme:
Combinando la prima con la seconda si ottiene l'equazione di Poisson:
dove con formula_28 si indica l'operatore differenziale laplaciano. La soluzione dell'equazione di Poisson è unica se sono date le condizioni al contorno. In particolare, un potenziale che soddisfi l'equazione di Poisson e che sia nullo a distanza infinita dalle sorgenti del campo coincide necessariamente con il potenziale elettrico, dato dall'espressione:
In assenza di cariche sorgenti del campo l'equazione diventa omogenea, e prende il nome di equazione di Laplace:
dalla quale risulta che in assenza di cariche il potenziale è una funzione armonica.
Risolvere l'equazione di Poisson in regioni di spazio limitate significa risolvere il problema generale dell'elettrostatica per opportune condizioni al contorno, come l'assenza o la presenza di conduttori e cariche elettriche localizzate. In particolare se ne distinguono tre tipi:
In questo caso non sono presenti cariche localizzate, ed il campo elettrostatico è generato da un sistema di conduttori di geometria nota e potenziale noto. In questo caso vale l'equazione di Laplace, dove le condizioni al contorno sono che il potenziale sia nullo all'infinito e valga formula_31 sulla superficie dei conduttori. Una volta ricavati i potenziali per ogni punto nello spazio risolvendo l'equazione di Laplace, si ricava il campo elettrostatico, ed è possibile determinare la densità di carica superficiali formula_32 sui conduttori mediante il teorema di Coulomb. Infine, si può trovare la carica netta totale su tutti i conduttori e i coefficienti di capacità su questi tramite il seguente sistema, che consente di ricavare i coefficienti.
In questo caso il campo elettrostatico è dato da un sistema di conduttori di geometria nota di cui sono note le cariche su ognuno. Si danno quindi dei potenziali arbitrari sui conduttori formula_34 e si risolve il "problema di Dirichlet" come sopra. Dal momento che le cariche sono note ed i coefficienti di capacità sono indipendenti dalle cariche e dai potenziali, essendo dipendenti solo dal loro rapporto, dal sistema del caso precedente si ricavano i reali potenziali formula_35.
Un esempio può essere quello di avere una distribuzione di carica formula_20 nota nello spazio ed un sistema di conduttori di cui si conoscono solo le cariche su ognuno. Il problema è quello di risolvere l'equazione di Poisson, e dal momento che non si conoscono i potenziali il problema diventa un sistema di equazioni del tipo:
dove i numeri formula_38 sono i coefficienti della matrice di potenziale. Per calcolare i potenziali si utilizza poi il metodo dei potenziali di prova.
L'elettrostatica e la magnetostatica rappresentano due casi particolari di una teoria più generale, l'elettrodinamica, dal momento che trattano i casi in cui i campi elettrico e magnetico non variano nel tempo. In condizioni stazionarie il campo elettrico ed il campo magnetico possono essere infatti trattati indipendentemente l'uno dall'altro, tuttavia in condizioni non stazionarie i due campi appaiono come le manifestazioni di una stessa entità fisica: il campo elettromagnetico.
Le stesse cariche che sono sorgente del campo elettrico, infatti, quando sono in moto generano un campo magnetico. Questo fatto è descritto dalle due leggi fisiche che correlano i fenomeni elettrici con quelli magnetici: la legge di Ampere-Maxwell e la sua simmetrica legge di Faraday, descritte nel seguito.
La legge di Faraday afferma che la forza elettromotrice indotta in un circuito chiuso da un campo magnetico è pari all'opposto della variazione del flusso magnetico del campo attraverso l'area abbracciata dal circuito nell'unità di tempo:
dove formula_40 è il flusso del campo magnetico formula_41. Dalla definizione di forza elettromotrice la precedente relazione può essere scritta come:
applicando il teorema del rotore al primo membro:
si giunge a:
Uguagliando gli integrandi segue la forma locale della legge di Faraday, che rappresenta la terza equazione di Maxwell:
Ovvero il campo elettrico può essere generato da un campo magnetico variabile nel tempo. Una conseguenza fondamentale della legge di Faraday è che il campo elettrico in condizioni non stazionarie non è più conservativo, dal momento che la sua circuitazione non è più nulla. Inoltre, avendo definito:
dove formula_47 è il potenziale vettore magnetico, dalla legge di Faraday segue che:
Dal momento che il rotore è definito a meno di un gradiente, si ha:
Il campo elettrico è così scritto in funzione dei potenziali associati al campo elettromagnetico.
L'estensione della legge di Ampère al caso non stazionario mostra come un campo elettrico variabile nel tempo sia sorgente di un campo magnetico. Ponendo di essere nel vuoto, la forma locale della legge di Ampère costituisce la quarta equazione di Maxwell nel caso stazionario:
Tale relazione vale solamente nel caso stazionario poiché implica che la divergenza della densità di corrente sia nulla, contraddicendo in questo modo l'equazione di continuità per la corrente elettrica:
Per estendere la legge di Ampère al caso non stazionario è necessario inserire la prima legge di Maxwell nell'equazione di continuità:
Il termine
è detto corrente di spostamento, e deve essere aggiunto alla densità di corrente nel caso non stazionario.
Inserendo la densità di corrente generalizzata così ottenuta nella legge di Ampère:
si ottiene la quarta equazione di Maxwell nel vuoto. Tale espressione mostra come la variazione temporale di un campo elettrico sia sorgente di un campo magnetico.
La presenza di materiale dielettrico nello spazio ove esista un campo elettrico formula_1 modifica il campo stesso. Questo è dovuto al fatto che gli atomi e le molecole che compongono il materiale si comportano come dipoli microscopici e si polarizzano in seguito all'applicazione di un campo elettrico esterno. L'effetto della polarizzazione elettrica può essere descritto riconducendo la polarizzazione dei dipoli microscopici ad una grandezza vettoriale macroscopica, che descriva il comportamento globale del materiale soggetto alla presenza di un campo elettrico esterno. Il vettore "intensità di polarizzazione", anche detto vettore di "polarizzazione elettrica" e indicato con formula_56, è il dipolo elettrico per unità di volume posseduto dal materiale.
La polarizzazione del dielettrico crea entro il materiale una certa quantità di carica elettrica indotta, detta carica di polarizzazione formula_57. Introducendo tale distribuzione di carica nella prima delle equazioni di Maxwell, che esprime la forma locale del teorema del flusso per il campo elettrico, si ha:
dove formula_59 è la densità di cariche libere e nel secondo passaggio si è utilizzata la relazione tra la densità volumica di carica di polarizzazione ed il vettore di polarizzazione. Si ha quindi:
L'argomento dell'operatore differenziale è il vettore induzione elettrica, definito come:
E la prima equazione di Maxwell assume la forma:
La maggior parte dei materiali isolanti può essere trattata come un dielettrico lineare omogeneo ed isotropo, questo significa che tra il dipolo indotto nel materiale ed il campo elettrico esterno sussista una relazione lineare. Si tratta di un'approssimazione di largo utilizzo, ed in tal caso i campi formula_63 e formula_64 sono equivalenti a meno di un fattore di scala:
e di conseguenza:
La grandezza formula_67 è la costante dielettrica relativa, e dipende dalle caratteristiche microscopiche del materiale. Se il materiale non è omogeneo, lineare ed isotropo, allora formula_68 dipende da fattori come la posizione all'interno del mezzo, la temperatura o la frequenza del campo applicato.
Nel dominio delle frequenze, per un mezzo lineare e indipendente dal tempo sussiste la relazione:
dove formula_70 è la frequenza del campo.
Inserendo il vettore di induzione elettrica nelle equazioni di Maxwell nei materiali, considerando il caso in cui il dielettrico sia perfetto e isotropo e ponendo che anche per il campo magnetico nei materiali sussista una relazione di linearità, si ha:
dove formula_72 è il campo magnetico nei materiali, e costituisce l'analogo del vettore induzione elettrica per la polarizzazione magnetica.
Considerando dielettrici perfetti ed isotropi, è possibile definire le condizioni di raccordo del campo elettrostatico quando attraversa due dielettrici di costante dielettrica relativa formula_73 e formula_74. Sulla superficie di separazione si consideri una superficie cilindrica di basi formula_75 e altezza formula_76 infinitesima, di ordine di grandezza superiore alla base. Applicando il flusso di Gauss uscente dalle basi si evince che il flusso infinitesimo è nullo poiché non vi sono cariche libere localizzate al suo interno:
dove formula_78 sono le componenti normali del campo di spostamento elettrico. In termini di campo elettrico si ha quindi:
Per la componente tangenziale del campo elettrico vale il teorema di Coulomb, ovvero la direzione del campo elettrico è normale alla superficie del conduttore, e pertanto la componente tangenziale si conserva:
In termini di campo di spostamento elettrico:
Attraversando la superficie di separazione tra due dielettrici perfetti ed isotropi, quindi, la componente normale del campo elettrico subisce una discontinuità mentre quella tangenziale non si modifica, viceversa per il campo di spostamento elettrico. Unendo le due relazioni si ottiene la legge di rifrazione delle linee di forza del campo elettrico:
e dunque:
dove
è l'angolo di rifrazione.
Il campo elettromagnetico è dato dalla combinazione del campo elettrico formula_1 e del campo magnetico formula_41, solitamente descritti con vettori in uno spazio a tre dimensioni. Il campo elettromagnetico interagisce nello spazio con cariche elettriche e può manifestarsi anche in assenza di esse, trattandosi di un'entità fisica che può essere definita indipendentemente dalle sorgenti che l'hanno generata. In assenza di sorgenti il campo elettromagnetico è detto onda elettromagnetica, essendo un fenomeno ondulatorio che non richiede di alcun supporto materiale per diffondersi nello spazio e che nel vuoto viaggia alla velocità della luce. Secondo il modello standard, il quanto della radiazione elettromagnetica è il fotone, mediatore dell'interazione elettromagnetica.
La variazione temporale di uno dei due campi determina il manifestarsi dell'altro: campo elettrico e campo magnetico sono caratterizzati da una stretta connessione, stabilita dalle quattro equazioni di Maxwell. Le equazioni di Maxwell, insieme alla forza di Lorentz, definiscono formalmente il campo elettromagnetico e ne caratterizzano l'interazione con oggetti carichi. Le prime due equazioni di Maxwell sono omogenee e valgono sia nel vuoto che nei mezzi materiali, e rappresentano in forma differenziale la Legge di Faraday e la legge di Gauss per il campo magnetico. Le altre due equazioni descrivono il modo con cui il materiale in cui avviene la propagazione interagisce, polarizzandosi, con il campo elettrico e magnetico, che nella materia sono denotati con formula_87 e formula_88. Esse mostrano in forma locale la Legge di Gauss elettrica e la Legge di Ampère-Maxwell.
La forza di Lorentz è la forza formula_89 che il campo elettromagnetico genera su una carica formula_2 puntiforme:
dove formula_92 è la velocità della carica.
Le equazioni di Maxwell sono formulate anche in elettrodinamica quantistica, dove il campo elettromagnetico viene quantizzato. Nell'ambito della meccanica relativistica, i campi sono descritti dalla teoria dell'elettrodinamica classica in forma covariante, cioè invariante sotto trasformazione di Lorentz. Nell'ambito della teoria della Relatività il campo elettromagnetico è rappresentato dal tensore elettromagnetico, un tensore a due indici di cui i vettori campo elettrico e magnetico sono particolari componenti.
</text>
</doc>
<doc id="15637" url="https://it.wikipedia.org/wiki?curid=15637">
<title>Energia potenziale elettrica</title>
<text>
L'energia potenziale elettrica, anche detta energia potenziale elettrostatica, in fisica ed in elettrotecnica, è l'energia potenziale del campo elettrostatico. Si tratta dell'energia posseduta da una distribuzione di carica elettrica, ed è legata alla forza esercitata dal campo generato dalla distribuzione stessa. Insieme all'energia magnetica, l'energia potenziale elettrica costituisce l'energia del campo elettromagnetico.
L'energia potenziale elettrostatica può essere definita come il lavoro svolto per creare una distribuzione di carica partendo da una configurazione iniziale in cui ogni componente della distribuzione non interagisce con gli altri. Ad esempio, per un sistema discreto di cariche essa coincide con il lavoro svolto per portare le singole cariche da una posizione in cui esse hanno potenziale elettrico nullo alla loro disposizione finale. L'energia potenziale elettrostatica può anche essere definita a partire dal campo elettrostatico generato dalla distribuzione stessa, ed in tale caso la sua espressione è indipendente dalla sorgente del campo.
Si tratta di una quantità che può essere sia negativa che positiva, a seconda che il lavoro svolto per portarle nella configurazione assunta sia positivo o negativo. Due cariche interagenti dello stesso segno hanno energia positiva, poiché il lavoro svolto per avvicinarle deve vincere la loro repulsione, mentre per lo stesso motivo due cariche di segno opposto hanno energia negativa.
L'energia potenziale elettrica formula_1 posseduta da una carica elettrica puntiforme formula_2 nella posizione formula_3 in presenza di un campo elettrico formula_4 è l'opposto del lavoro formula_5 compiuto dalla forza elettrostatica formula_6 per portare formula_2 da una posizione di riferimento formula_8, in cui la carica ha un'energia nota, alla posizione formula_3.
L'energia elettrostatica è definita come il lavoro necessario per portare un sistema di cariche elettriche, o più in generale una distribuzione di carica, in una data configurazione spaziale.
Si consideri dunque un sistema di cariche puntiformi. Per disporre nello spazio la prima carica elettrica formula_10 non si compie lavoro, e quindi formula_11. Per portare la seconda carica, tenendo conto della prima, il lavoro è:
dove formula_13 è la distanza tra le posizioni formula_14 e formula_15 di formula_10 e formula_17. Per la terza si ha, analogamente:
Considerando un sistema di cariche puntiformi si ha in definitiva:
con formula_20. In una forma più simmetrica:
dove il termine formula_22 è introdotto in quanto in tale sommatoria il lavoro per formula_23, che è lo stesso per formula_24, è contato due volte. Separando le due sommatorie si riconosce il potenziale elettrico:
e l'energia potenziale elettrostatica è data da:
L'estensione al caso continuo mostra che, data una distribuzione continua di cariche descritta da una densità di carica formula_27 contenuta nel volume formula_28, l'energia elettrostatica associata alla distribuzione è data dall'integrale:
dove formula_30 è il potenziale elettrico nel punto formula_31.
L'energia di sistemi elettricamente interagenti, così come le altre proprietà meccaniche, può essere descritta in modo analogo in termini del campo elettrico. Tale approccio, equivalente al precedente, permette di descrivere l'energia del sistema attraverso il campo che esso genera, indipendentemente dalle sue sorgenti.
Considerando un volume formula_28, l'energia del campo elettrostatico contenuta in tale regione è:
dove:
è la densità di energia elettrica nel vuoto.
Nel caso ci si trovi in presenza di un dielettrico, tramite gli stessi passaggi si ottiene:
dove formula_36 è il vettore di spostamento elettrico, e:
è la densità di energia elettrica nella materia.
Nel caso di distribuzioni continue di carica si ha:
con formula_27 densità di carica e formula_40 volume infinitesimo. Sfruttando la prima equazione di Maxwell formula_41 si ha:
applicando al contrario l'identità vettoriale formula_43 si ottiene:
Dalla definizione di potenziale tale espressione è pari a:
ed applicando il teorema della divergenza:
A questo punto, si può estendere il dominio di integrazione su tutta la regione dello spazio nel quale il campo elettrico sia apprezzabilmente diverso da zero, e quindi trascurare il primo dei due integrali. Dal punto di vista fisico, l'integrale di flusso che si è trascurato rappresenta il termine energetico aggiuntivo che si deve considerare nel caso la superficie di integrazione non sia sufficientemente estesa da contenere tutto lo spazio in cui il campo non è nullo.
L'utilizzo dell'energia elettrica è diffusissimo nella società moderna e attuale attraverso l'allaccio alla rete elettrica oppure tramite batterie o accumulatori: basta pensare all'uso nell'illuminazione di edifici (pubblici e privati) e strade, nell'alimentazione elettrica degli elettrodomestici e dei computer nonché nei processi produttivi-industriali ovvero nelle macchine elettriche quali i motori elettrici. 
La sua scoperta ha rappresentato dunque una vera e propria rivoluzione tecnologica, economica e sociale innescando una forte e irreversibile dipendenza/pervasività grazie ai suoi vantaggi rispetto all'energia meccanica prodotta dai motori endotermici. Tra questi si ricorda il fatto di poter essere trasportata a distanza, il basso rumore di esercizio delle apparecchiature elettriche, l'assenza di fumi di scarico nei luoghi di utilizzazione e il minor ingombro di una macchina elettrica. 
Tra gli svantaggi si annovera invece proprio il fatto di non essere una fonte primaria e quindi la necessità di una infrastruttura di conversione che inevitabilmente introduce una perdita di efficienza nel processo di conversione a monte e nel trasporto lungo le linee elettriche.
L'energia elettrica, se si eccettua l'elettricità atmosferica dei fulmini e il potenziale debolmente negativo della Terra, non è una fonte di energia primaria sulla Terra per cui deve essere prodotta per trasformazione a partire da una fonte di energia primaria, risultando così una fonte di energia secondaria. Il processo di trasformazione, a rendimento sempre inferiore al 100%, avviene all'interno di centrali elettriche. In queste, escludendo il fotovoltaico, qualunque altra sia la fonte da cui si intende generare energia, tre sono le macchine indispensabili allo scopo che si vuole ottenere:
Altro elemento del quale non si poteva fare a meno per produrre energia elettrica è l'acqua, in forma liquida (come nelle centrali idroelettriche) o di vapore (nelle centrali termoelettriche, geotermoelettriche, a fissione nucleare ed a solare termodinamico), ma sempre ad alta pressione, allo scopo di far girare le turbine ad un numero di giri tale da produrre in maniera il più possibile costante la "corrente alternata" per mezzo dell'alternatore.
L'utilizzo di acqua che, in quasi tutti i casi, deve essere riscaldata fino a divenire vapore presenta due ordini di problemi:
Una volta raggiunta la produzione di energia elettrica, il trasporto su vasta scala e la distribuzione dell'energia elettrica prodotta dalle centrali fino agli utenti finali avviene attraverso la rete di trasmissione e la rete di distribuzione.
</text>
</doc>
<doc id="15818" url="https://it.wikipedia.org/wiki?curid=15818">
<title>Forza di Coulomb</title>
<text>
In fisica, la forza di Coulomb, descritta dalla legge di Coulomb, è la forza esercitata da un campo elettrico su una carica elettrica. Si tratta della forza che agisce tra oggetti elettricamente carichi, ed è operativamente definita dal valore dell'interazione tra due cariche elettriche puntiformi e ferme nel vuoto.
Dalla legge di Coulomb si rende visibile come all'interazione elettromagnetica sia associata una forza particolarmente intensa se confrontata con l'interazione gravitazionale: la forza elettrica tra un elettrone e un protone in un atomo d'idrogeno è 10 volte superiore rispetto alla forza gravitazionale tra le due.
Per facilitare la generazione di cariche elettrostatiche si utilizza solitamente un generatore elettrostatico; tra i più famosi si hanno l'elettroforo perpetuo ed il generatore di Van de Graaff. Lo sfruttamento pratico della forza esercitata tra le cariche elettriche avviene ad esempio con il propulsore ionico e il propulsore ionico elettrostatico, mentre il manifestarsi naturale o indotto di tale forza elettrica è visibile con l'effetto corona o il potere disperdente delle punte (tra cui i fuochi di Sant'Elmo).
Le prime indagini su tale forza si hanno in Grecia nel 600 a.C. con Talete di Mileto e Teofrasto, e riguardano esperimenti di elettrostatica con l'ambra e la seta (o in alternativa la lana). Successivamente, studi più approfonditi presero luogo dal XVI al XIX secolo: in particolare, fino alla metà del XVIII secolo erano noti solo gli aspetti qualitativi della forza elettrica: si iniziò allora a studiarne anche le proprietà quantitative, così che si fece strada l'idea di una somiglianza con la forza di gravità. Aspetti come:
Tra il 1777 e il 1785 fu Charles Augustin de Coulomb a provare sperimentalmente che effettivamente la forza elettrica era proporzionale all'inverso del quadrato della distanza; ma non fu il primo, dato che gli stessi esperimenti di Coulomb furono precedentemente condotti dall'inglese Henry Cavendish, il quale per la sua bizzarra personalità non pubblicò la maggior parte dei suoi lavori. Questo è stato il primo tentativo di capire il funzionamento della "forza elettrica".
Si considerino due cariche puntiformi interagenti, il cui valore (positivo o negativo) è indicato con formula_1 e formula_2, nelle posizioni formula_3 e formula_4. La forza di Coulomb è la forza esercitata da formula_2 su formula_1 (o, in modo opposto, da formula_1 su formula_2), e ha l'espressione:
dove formula_10 è la costante di Coulomb, che è pari a:
con formula_12 la costante dielettrica del vuoto, il cui valore è:
Se formula_14 è la distanza tra le cariche, il modulo formula_15 della forza è:
La forza tra due cariche è proporzionale al prodotto dei loro valori formula_1 e formula_2, inversamente proporzionale al quadrato della loro distanza, ed è diretta come la congiungente formula_19 delle due cariche. Si tratta di una forza repulsiva nel caso le cariche abbiano segno uguale, attrattiva altrimenti. La forma vettoriale si ottiene sapendo che la direzione della forza è uguale alla direzione della differenza dei vettori posizione delle due cariche.
La formula può essere estesa considerando cariche in presenza di altri materiali (non nello spazio vuoto) e non puntiformi. In generale, nella formula di Coulomb deve essere inserita la permittività elettrica del mezzo che separa le due cariche.
In presenza di un dielettrico la forza di Coulomb è diminuita in rapporto alla costante dielettrica relativa formula_20, dove formula_21 è la costante specifica del mezzo e formula_12 la permittività elettrica nel vuoto:
Si noti che formula_24, e quindi il dielettrico riduce la forza di interazione fra due cariche elettriche. La riduzione del campo elettrico è indipendente dalla massa dei dielettrici coinvolti, mentre varia con la loro disposizione spaziale. In secondo luogo, la formula di Coulomb può essere estesa a cariche non puntiformi. Le cariche non puntiformi possono essere considerate ricorrendo agli integrali.
Sperimentalmente si è verificato che la deviazione dell'esponente dal valore teorico 2 è minore di circa 10.
Dato un numero "n" di cariche puntiformi formula_25 distribuite nello spazio, per il principio di sovrapposizione il campo elettrostatico nella posizione formula_26 è dato dalla somma dei singoli contributi:
dove formula_25 e formula_29 sono il valore e le posizioni della "i"-esima carica.
In generale, per una distribuzione continua di carica si ha:
dove formula_31 rappresenta la densità di carica nello spazio:
e formula_33 rappresenta la regione di spazio occupata dalla distribuzione di carica.
Nel vuoto, il campo elettrico formula_34 in un punto dello spazio è definito come la forza per unità di carica elettrica positiva alla quale è soggetta una carica puntiforme, detta carica "di prova", se posta nel punto. Il vettore è quindi dato dal rapporto tra la forza elettrica agente sulla carica di prova ed il valore della carica stessa, purché la carica di prova sia sufficientemente piccola da provocare una perturbazione trascurabile sull'eventuale distribuzione di carica che genera il campo:
Il campo è indipendente dal valore della carica di prova usata, essendone indipendente il rapporto tra la forza e la carica stessa, e questo mostra che il campo elettrico è una proprietà caratteristica dello spazio. Dalla definizione si ricava che l'unità di misura del campo elettrico è formula_36, che equivale a formula_37.
Dalla legge di Coulomb segue che una carica formula_38 posta in formula_39 genera un campo elettrostatico che in un punto qualsiasi formula_26 è definito dalla seguente espressione:
dove formula_42 è la costante dielettrica nel vuoto.
</text>
</doc>
<doc id="33549" url="https://it.wikipedia.org/wiki?curid=33549">
<title>Bussola</title>
<text>
La bussola è uno strumento per l'individuazione dei punti cardinali (nord, sud, est e ovest) sulla superficie terrestre e in atmosfera, a fini di orientamento e navigazione. È provvista di un ago magnetizzato che, libero di girare su un perno, ha la proprietà di allinearsi lungo le linee di forza del campo magnetico terrestre indicando così la direzione nord-sud (entro i limiti d'errore dovuti alla declinazione magnetica).
Il suo uso è fondamentale in mare aperto, in vasti spazi, dove non ci siano punti di riferimento, così come in presenza di riferimenti per localizzarsi goniometricamente rispetto a essi (es. in orienteering). Utilizzata insieme ad un orologio e un sestante dà luogo ad un accuratissimo sistema di navigazione. Questo strumento ha migliorato la navigazione facilitando i commerci marittimi e i viaggi per mare rendendoli più sicuri ed efficienti. Allo strumento può essere associata una meridiana che permette di conoscere l'ora solare durante il giorno, semplicemente osservando l'ombra prodotta dalla barra, perpendicolare all'ago, dopo che quest'ultimo si è posizionato verso Nord.
Deve il suo nome alla scatola in legno di bosso che originariamente conteneva tale strumento. Negli antichi velieri la bussola si custodiva nella chiesuola, armadietto posto a prua del timone.
L'invenzione della bussola si attribuisce ai cinesi (una delle quattro grandi invenzioni). Essi scoprirono il campo magnetico terrestre. Pare che in origine utilizzassero tale scoperta come spettacolo d'attrazione: delle lancette magnetizzate venivano lanciate come si fa coi dadi e queste, per lo stupore degli spettatori presenti, finivano per indicare sempre il Nord. Passò molto tempo prima che questa "attrazione circense" fosse applicata alla navigazione. Una volta conosciuta la posizione del Nord, infatti, era poi possibile identificare il Sud come la direzione opposta, mentre l'est e l'ovest erano rispettivamente alla destra e alla sinistra dell'osservatore rivolto verso il Nord. Fu introdotta in Europa nel XII secolo attraverso gli Arabi e gli amalfitani: il primo riferimento all'uso della bussola nella navigazione nell'Europa occidentale è il "De nominibus utensilium" di Alexander Neckam (1180-1187).
La leggenda che vuole che la bussola sia stata inventata da Flavio Gioia di Amalfi nasce da un errore di interpretazione di un testo latino, che riferiva soltanto che l'invenzione della bussola era attribuita dallo storico Flavio Biondo agli Amalfitani: il filologo Giambattista Pio capì invece che la bussola fosse stata inventata dall'amalfitano Flavio Gioia. Nel testo in questione ("Amalphi in Campania veteri magnetis usus inventus a Flavio traditur"), tuttavia, "Flavio" non si riferisce all'inventore della bussola, ma a colui che ha riportato la notizia, Flavio Biondo.
Le bussole moderne, specialmente quelle nautiche, sono costituite da corone basculanti su un perno immerse in liquidi a bassa densità (uno dei primi utilizzati è stato l'alcool) e racchiuse in cupole sferiche trasparenti. Sulla parte esterna o superiore della corona sono riportati i gradi bussola (0º = Nord; 90º = Est; 180º = Sud; 270º = Ovest), con le gradazioni intermedie (usualmente si arriva a cinque gradi, gradazioni inferiori renderebbero illeggibile lo strumento). Il dispositivo fornisce in questo modo una lettura affidabile nonostante i movimenti dell'imbarcazione sui tre assi (rollio, beccheggio e imbardata). Esse sono costituite da un'asta magnetica all'interno di una capsula riempita di un fluido; il fluido permette all'asta di fermarsi velocemente senza oscillare in direzione del polo nord magnetico.
Altri elementi frequenti delle bussole portatili sono:
Molte bussole moderne permettono anche un aggiustamento per la declinazione magnetica, la differenza tra il polo nord reale e quello magnetico, con un semplice sfasamento della scala goniometrica.
Un'ulteriore evoluzione della bussola moderna è la "bussola digitale", un dispositivo elettronico digitale dotato di un sensore di campo magnetico o magnetometro interfacciato con un programma che elabora opportunamente le informazioni sotto forma di segnali forniti dal sensore visualizzandole all'utente finale su un display con interfaccia grafica. Le bussole digitali sono ad esempio comuni applicazioni presenti o scaricabili (di solito già presenti) sui moderni smartphone.
La "bussola di inclinazione" è una bussola che misura l'angolo di inclinazione tra le linee di campo magnetico e la superficie terrestre, pertanto non distingue il polo nord dal polo sud, ma solo il polo dall'equatore. Viene chiamata anche "bussola aviaria" in quanto è utilizzata da alcune specie di uccelli per orientarsi nelle migrazioni.
La "magnetorecezione" è una sorta di bussola biologica interna ad alcune specie di animali tra i quali molti uccelli, aragoste, balenottere, delfini, squali, mante, api, oltre a microorganismi ed in alcune piante, che funziona secondo principi di meccanica quantistica.
Per la realizzazione di una bussola semplice occorre un'asta magnetica, la quale può essere costruita allineando un'asta d'acciaio o di ferro con il campo magnetico terrestre prima di temprarla o colpirla ripetutamente (questo metodo, però, genera un magnete molto debole e quindi è consigliabile usarne un altro più efficace). Ottenuta l'asta magnetica occorre posizionarla su una superficie con un basso attrito che le permetta di muoversi per allinearsi con il campo magnetico terrestre. Infine indicando i punti cardinali si ottiene una semplice, ma funzionante bussola.
Il sistema più semplice per costruire una bussola è quello di magnetizzare uno spillo o un ago con lo strofinio, poi appoggiarlo con delicatezza su un piccolo corpo galleggiante (ad esempio un sottile disco di sughero ottenuto da un tappo di bottiglia), dentro un bicchiere d'acqua. L'attrito viscoso offerto dall'acqua consente allo spillo di girare, fino ad allinearsi col campo magnetico terrestre, indicando i poli magnetici.
Fin dall'antichità l'uomo ha dovuto risolvere il problema dell'orientamento. Durante le giornate di sole bastava osservare quest'ultimo. Infatti gli antichi notarono che il sole sorgeva verso "Est" e tramontava verso "Ovest". Nell'emisfero nord il "Sud" corrispondeva alla posizione del sole a mezzogiorno e il "Nord" alla posizione opposta.
Considerando questo fatto e con l'utilizzo di un orologio analogico, a lancette, sincronizzato sull'ora solare, si può ugualmente trovare il nord proiettando l'ombra di una pagliuzza posta al centro del quadrante, mantenuto orizzontale, sulla lancetta delle ore. La direzione nord/sud sarà data dalla bisettrice dell'angolo tra l'ombra e la linea che passa dal centro dell'orologio per le ore dodici.
Utilizzarono, di notte, anche le stelle: la Stella Polare si trova per esempio sempre al di sopra del Nord geografico: guardandola terremo il braccio destro a est, quello sinistro a ovest e dietro avremo il sud.
Nell'orienteering in assenza di bussola è frequente far riferimento nei boschi alla posizione del muschio sulle piante o sui massi a terra in quanto esso indica il nord (il sud nell'emisfero meridionale) crescendo questo sul lato più umido e meno esposto alle radiazioni solari. Per identici motivi anche i versanti più boscosi delle montagne spesso indicano i luoghi esposti a nord (il sud nell'emisfero meridionale).
Vi sono popolazioni nelle quali culturalmente le relazioni spaziali non si basano su coordinate egocentriche (ad esempio: gira a destra poi a sinistra) ma su coordinate geografiche (ad esempio: gira a est poi a sud). Una di queste popolazioni, gli guugu yimithirr dell'Australia, è stata sottoposta a studi ed esperimenti che hanno verificato che non perdono l'orientamento neanche in una stanza chiusa.
</text>
</doc>
<doc id="2579" url="https://it.wikipedia.org/wiki?curid=2579">
<title>Lunghezza</title>
<text>
Il termine lunghezza, nell'uso comune, indica una delle dimensioni di un oggetto, ovvero una sua estensione nello spazio. Nelle varie discipline tecniche e scientifiche il termine ha un utilizzo definito più rigorosamente o assume un significato leggermente diverso.
In geometria euclidea, la lunghezza di un segmento è la distanza tra gli estremi del segmento. Si veda la voce lunghezza di un arco per l'estensione di questo concetto ad archi di curva.
In matematica e in fisica il termine lunghezza è utilizzato anche come sinonimo di norma o valore assoluto di un vettore. In altri campi, questo termine può essere impiegato come sinonimo di distanza. Per esempio, in cartografia, la lunghezza di una strada è la "distanza" che bisogna percorrere per spostarsi da un'estremità di essa all'altra.
La lunghezza è assunta come una delle grandezze fisiche fondamentali, nel senso che non può essere definita in termini di altre grandezze. La lunghezza come grandezza fisica non è una proprietà intrinseca ad alcun oggetto. Infatti due osservatori possono misurare lo stesso oggetto e ottenere risultati differenti. Questa strana proprietà dello spazio viene spiegata nella teoria della relatività speciale di Albert Einstein.
L'unità di misura attuale nel SI della lunghezza è il metro ma questo fu definito solo nel 1791 e prima di allora si sono susseguite nel tempo innumerevoli unità di misura della lunghezza.
Le prime unità di misura di essa erano associate al corpo umano. Gli egiziani avevano stabilito il cubito che equivaleva alla distanza fra il gomito e l'estremità del dito medio. In modo simile il piede rappresentò la misurazione dopo la prima metà del Seicento ed equivaleva infatti alla lunghezza del piede reale di Luigi XIV.
Altre misure che si possono ricordare sono il piede e il miglio anche se quest'ultimo è un multiplo. Nel 1791 però si cercò una misura più oggettiva e si scelse quella del metro che, nella definizione attuale, è pari alla distanza coperta dalla luce, nel vuoto, in un intervallo di tempo pari a di secondo.
</text>
</doc>
<doc id="9550" url="https://it.wikipedia.org/wiki?curid=9550">
<title>Forza</title>
<text>
Una forza è una grandezza fisica vettoriale che si manifesta nell'interazione reciproca di due o più corpi, sia a livello macroscopico, sia a livello delle particelle elementari. Quantifica il fenomeno di induzione di una variazione dello stato di quiete o di moto dei corpi stessi; in presenza di più forze, è la risultante della loro composizione vettoriale a determinare la variazione del moto. La forza è descritta classicamente dalla seconda legge di Newton come derivata temporale della quantità di moto di un corpo rispetto al tempo.
In formule:
che, nel caso la massa del corpo sia costante, si riduce a:
La legge evidenzia immediatamente il carattere vettoriale della forza, in quanto la derivata di un vettore è ancora un vettore.
Tenendo conto del secondo principio della dinamica, possiamo quindi affermare che una forza di 1 N imprime ad un corpo con la massa di 1 kg l'accelerazione di 1 m/s².
Nel sistema CGS la forza si misura in dyne.
Le forze sono quindi leettere in moto un corpo che si trovava precedentemente in stato di quiete, modificare il movimento di un corpo già precedentemente in moto, o riportare il corpo in stato di quiete.
A livello pratico, le forze applicate ad un dato corpo possono avere due diversi tipi di effetti:
Si dice "ambiente" di un corpo proprio l'insieme delle forze che altri corpi esercitano su di esso.
Da un punto di vista operativo, è possibile affermare che se un corpo è deformato rispetto al suo stato di riposo, allora è sottoposto all'azione di una forza.
Una definizione statica di forza è possibile misurando la deformazione di un corpo che segua la legge di Hooke, cioè tali che la deformazione sia direttamente proporzionale alla forza applicata. Ciò vuol dire che se si sospende ad una molla ideale un peso campione si ottiene un certo allungamento "x", mentre se alla stessa molla si sospendono due pesi campione, uguali al precedente, l'allungamento risulta uguale a 2"x". Utilizzando questa proprietà lineare delle molle è possibile costruire degli strumenti di misura delle forze, detti dinamometri. Ogni volta che un dinamometro si allunga, significa che ad esso è applicata una forza.
Utilizzando un dinamometro si ottiene una misura indiretta della forza, in quanto la grandezza che viene misurata non è direttamente la forza, ma la deformazione della molla contenuta nel dinamometro; osserviamo tuttavia che la stessa situazione sperimentale ricorre nella misura della temperatura (ciò che si misura in realtà è la dilatazione del mercurio) o della pressione (viene misurata l'altezza di una colonna di liquido).
La forza generalizzata associata ad un grado di libertà del sistema "i" è:
Dove W è il lavoro della risultante attiva F agente sul sistema. Si tratta quindi in termini newtoniani per variabili lunghezza e angolo rispettivamente delle grandezze forza e momento meccanico prese lungo la variabile, nel caso più generale di una combinazione delle due.
Nel caso di vincoli bilaterali permettono di ignorare nell'analisi del sistema le reazioni vincolari (di risultante R), anche per sistemi scleronomi: dato uno spostamento virtuale formula_4, ottenuto considerando solo gli spostamenti ammissibili con i vincoli considerati come "fissi" all'istante di riferimento, il lavoro virtuale agente sull'n-esima particella del sistema vale:
Se i vincoli del sistema sono bilaterali, per il principio delle reazioni vincolari i lavori virtuali vincolari sono nulli, e cioè le reazioni sono ortogonali agli spostamenti virtuali:
Esprimendo formula_7 in funzione delle coordinate generalizzate formula_8, e ricordando che formula_9 per definizione di spostamento virtuale:
Il lavoro virtuale sulla particella sottoposta a vincoli bilaterali è cioè interamente calcolabile tramite le forze generalizzate agenti su di essa. 
A livello ingegneristico dove è necessario risalire allo sforzo che dovrebbe essere fatto da tutte le forze non vincolari se il sistema subisse uno spostamento virtuale formula_11, oppure alle sollecitazioni esterne imposte realmente dai vincoli, l'approccio Lagrangiano risulta quindi particolarmente utile.
In base alle equazioni di Lagrange del I tipo e in forma di Nielsen si può legare la forza generalizzata all'energia cinetica del sistema:
formula_12,
Si noti quindi che le forze generalizzate differiscono quindi per il secondo termine formula_13 dalle formula_14 cui si arriverebbe generalizzando la definizione newtoniana di forza come derivata totale temporale della quantità di moto, cioè il secondo principio della dinamica.
La forza è una grandezza vettoriale, ovvero è descritta da un punto di vista matematico da un vettore (vedi immagine a fianco). Ciò significa che la misura di una forza, ovvero la sua intensità misurata in newton, rappresenta solo il modulo della forza, che per essere definita necessita anche della specificazione di un punto di applicazione (il punto del corpo dove la forza agisce), di una direzione (fascio di rette parallele) e di un verso (indicato dall'orientamento del vettore).
Il carattere vettoriale della forza si manifesta anche nel modo in cui è possibile sommare le forze. Come è possibile verificare sperimentalmente, due forze formula_15 e formula_15 con lo stesso punto di applicazione, ma direzioni diverse si sommano con la regola del parallelogramma (vedi figura a fianco). Ciò significa che se ad un corpo vengono contemporaneamente applicate le forze formula_15 e formula_15, esso si muoverà lungo la direzione della diagonale del parallelogramma, come se ad esso fosse applicata solo la forza formula_19, detta, appunto somma o risultante.
Una forza può esercitarsi non necessariamente a contatto di un corpo, ma può anche esserci un campo di forze, cioè una regione in cui un corpo esercita la propria forza, esistente indipendentemente da un secondo corpo che ne subisca gli effetti.
Alcune forze possiedono una struttura tale che il lavoro compiuto su un corpo sia sempre esprimibile tramite una funzione scalare, chiamata potenziale, dipendente unicamente dagli estremi dello spostamento e non dalla traiettoria. Tali forze sono dette forze conservative e ammettono un'energia potenziale. L'energia potenziale (solitamente indicata con il simbolo "U") rappresenta un campo scalare pari alla funzione potenziale cambiata di segno, di cui la forza è il gradiente:
Esempi classici di forze conservative sono la forza di gravità e la forza elettrica o di Coulomb. Esse sono caratterizzate dalle relazioni:
Il significato geometrico del gradiente di una funzione si può facilmente interpretare, almeno nel caso di funzioni in due variabili come nell'esempio in figura, come il vettore che indica, punto per punto, la massima pendenza, nella direzione crescente; il fatto che una forza conservativa sia pari a meno il gradiente dell'energia significa che è indirizzata in modo da minimizzare la stessa.
La definizione operativa sopra presentata è quella più comune nei libri di testo, ma notiamo che non è soddisfacente, in quanto richiede l'introduzione della legge di Hooke e della forza peso, incappando in una definizione circolare (la definizione della legge di Hooke è dipendente dalla forza). In altre esposizioni della dinamica la forza è proprio definita dalla seconda legge della dinamica, oppure col rischio di incappare in un vero e proprio truismo. In altri trattati la forza viene introdotta come un concetto intuitivo, legato alle impressioni connesse allo sforzo muscolare: per evidenziare l'inadeguatezza di tale impostazione è sufficiente notare la pericolosità insita nella confusione tra concetti fisici appartenenti ad una teoria e concetti provenienti dall'esperienza ingenua: ad esempio mantenere sollevato un peso fermo comporta sforzo muscolare, ma non lavoro (nel senso fisico del termine).
In molte esposizioni recenti della meccanica, la forza è normalmente definita implicitamente in termini di equazioni che lavorano con essa: questa impostazione, qualora si vada a considerare la necessità di specificare il riferimento in cui tali equazioni valgono, non è completamente soddisfacente nel risolvere le problematiche associate alla definizione "classica". Alcuni fisici, filosofi e matematici, come Ernst Mach, Clifford Truesdell e Walter Noll, hanno trovato problematico questo fatto e hanno cercato una definizione più esplicita di forza, evidenziando peraltro la non essenzialità di questo concetto per la comprensione della meccanica. Ernst Mach criticò peraltro anche l'idea, a suo avviso metafisica, che le forze siano le cause del moto: solo i corpi possono influire sullo stato di moto di altri corpi e difficilmente si può pensare che la forza, un concetto astratto, possa essere la causa di alcunché.
Nella relazione
derivata dalla seconda legge del moto di Newton, formula_24 è la forza (il cui modulo è espresso usualmente in Newton), "m" la massa in chilogrammi e formula_25 è l'accelerazione (il cui modulo è espresso in metri su secondo al quadrato). Per un fisico, il chilogrammo è l'unita di massa, ma nell'uso comune "chilogrammo" è un'abbreviazione per "il peso di un corpo di un chilogrammo di massa a livello del mare sulla terra". A livello del mare, l'accelerazione dovuta alla gravità ("a" nell'equazione di cui sopra) è 9,807 metri per secondo quadrato, quindi il peso di un chilogrammo è 1 kg × 9,807 m/s² = 9,807 N.
A volte, soprattutto in contesti di tipo ingegneristico, si distingue il "chilogrammo massa" (indicato con "kgm") per indicare il kg e il "chilogrammo forza" (o "chilogrammo peso", indicato con "kgf") per indicare il valore di 9,807 N. Il chilogrammo peso non è tuttavia riconosciuto come unità di misura del Sistema Internazionale e sarebbe opportuno non usarlo per non creare confusione tra i concetti di massa e di peso.
Infatti la massa è una proprietà intrinseca dell'oggetto mentre il peso dipende dalla gravità.
Secondo le teorie scientifiche attualmente più accreditate, in natura esistono quattro forze, o meglio interazioni fondamentali che operano sui corpi: la gravità, l'interazione elettromagnetica, l'interazione nucleare forte e l'interazione debole. La prima, secondo la teoria della relatività generale è un effetto della geometria dello spazio-tempo, mentre le altre tre interazioni, che sono delle teorie di gauge, sono dovute a scambi di particelle, dette bosoni di gauge, secondo la seguente tabella:
Il modello standard fornisce un riquadro coerente nel quale sono inserite le tre teorie di gauge, mentre ad oggi è stato impossibile ricondurre ad esso una versione quantistica della gravità, sebbene sia stata teorizzata una particella mediatrice (il gravitone) della quale non si hanno evidenze empiriche.
</text>
</doc>
<doc id="9553" url="https://it.wikipedia.org/wiki?curid=9553">
<title>Frequenza</title>
<text>
La frequenza è una grandezza che riguarda fenomeni periodici o processi ripetitivi. 
In fisica la frequenza di un fenomeno che presenta un andamento costituito da eventi che nel tempo si ripetono identici o quasi identici, viene data dal numero degli eventi che vengono ripetuti in una data unità di tempo; inoltre la velocità angolare rientra nel concetto generale di velocità (variazione di una grandezza), in questo caso esprime la variazione di un angolo nel tempo. Un modo per calcolare una tale frequenza consiste nel fissare un intervallo di tempo, nel contare il numero di occorrenze dell'evento che si ripete in tale intervallo di tempo e nel dividere quindi il risultato di questo conteggio per l'ampiezza dell'intervallo di tempo. In alternativa, si può misurare l'intervallo di tempo tra gli istanti iniziali di due eventi successivi (il periodo) e quindi calcolare la frequenza come grandezza inversa di questa durata.
dove "T" esprime il periodo. Il risultato è dato nell'unità di misura chiamata hertz (Hz), dal fisico tedesco Heinrich Rudolf Hertz, dove 1 Hz caratterizza un evento che occorre una volta in un secondo. Quindi formula_2
Gli strumenti usati per la misura sono l'oscilloscopio e il frequenzimetro: il primo permette una misurazione complementare ad altre, in circostanze non impegnative; per misure accurate e precise occorre usare il secondo, il quale è uno strumento elettronico specializzato per misure di frequenza e di tempo e talvolta in grado di generare fenomeni periodici a frequenze impostate.
Nel misurare la frequenza del suono, di onde elettromagnetiche (come le onde radio o la luce), di segnali elettrici oscillatori o di altre onde simili, la frequenza in hertz è il numero di cicli della forma d'onda ripetitiva per secondo.
Poiché il prodotto tra la frequenza di un'onda elettromagnetica e la sua lunghezza d'onda è uguale al valore della velocità della luce (formula_3 ), si ha che
In altre parole, in un'onda elettromagnetica tra lunghezza d'onda e frequenza esiste un rapporto di proporzionalità inversa.
La frequenza di campionamento è uno dei parametri fondamentali che caratterizzano il processo di conversione analogico-digitale nei sistemi elettronici di elaborazione dell'informazione.
In un sistema elettronico, l'informazione è usualmente codificata in un segnale di tensione variabile nel tempo. Si parla di segnale analogico se esso varia in modo continuo in ciascun istante di tempo, di segnale numerico se esso può assumere solo un numero finito di valori discreti oppure di segnale digitale se i valori che può assumere sono solo 0 ed 1, in precisi istanti di tempo. Il processo di campionamento consente di convertire un segnale analogico in un segnale digitale e consiste nel misurare e registrare, in precisi istanti di tempo (istanti di campionamento) il valore istantaneo del segnale analogico in esame. La sequenza di tali valori, detti campioni, costituisce il segnale digitale. Il dispositivo che realizza la conversione da segnale analogico a segnale digitale viene detto convertitore A/D. La frequenza di campionamento indica il numero di campioni registrati in un secondo: il Teorema di Nyquist-Shannon stabilisce che, affinché sia possibile ricostruire il segnale analogico a partire dai suoi campioni, è necessario che la frequenza di campionamento sia almeno il doppio della frequenza più alta contenuta nello spettro del segnale di partenza: formula_5.
Ad esempio, un segnale audio ha uno spettro compreso fra 20 Hz e 20 kHz: per poter registrare il segnale su un supporto digitale (come, un CD audio) la frequenza di campionamento deve essere almeno di 40 kHz. Usualmente, tale campionamento viene effettuato a 44.1 kHz, valore che soddisfa appieno il teorema di Nyquist-Shannon e che consente di ricostruire fedelmente il segnale analogico di partenza.
</text>
</doc>
<doc id="9556" url="https://it.wikipedia.org/wiki?curid=9556">
<title>Lunghezza d'onda</title>
<text>
In fisica, la lunghezza d'onda di un'onda periodica è la distanza tra due "creste" o fra due "ventri" della sua forma d'onda, e viene comunemente indicata dalla lettera greca λ.
La lunghezza d'onda λ è definita come:
dove formula_2 è la velocità di propagazione e f la frequenza dell'onda.La lunghezza d'onda è correlata al numero d'onda formula_3 dalla relazione:
Quando le onde, solitamente quelle elettromagnetiche, passano attraverso un materiale, la loro velocità di propagazione viene ridotta di un fattore pari all'indice di rifrazione "n" del materiale, mentre la frequenza non cambia. Detta λ la lunghezza d'onda nel vuoto, la lunghezza d'onda λ' in un materiale con indice di rifrazione "n" è data da:
Le lunghezze d'onda della radiazione elettromagnetica sono normalmente riferite al vuoto come mezzo di propagazione (anche se questo non è sempre dichiarato esplicitamente).
La velocità formula_2 delle onde elettromagnetiche è pari alla velocità della luce, cioè circa 3×10m/s. Quindi, per esempio, la lunghezza d'onda formula_7 di un segnale a 100 MHz (un'onda radio), è di circa 3×10 m/s diviso 100×10 Hz = 3 metri.
Tutte le particelle con una certa quantità di moto hanno una lunghezza d'onda, chiamata lunghezza d'onda di de Broglie, che si sviluppa in base alla relatività ristretta:
dove "h" è la costante di Planck, "p" è la quantità di moto della particella, "m" è la massa della particella, "v" è la velocità della particella, e "c" è la velocità della luce.
</text>
</doc>
<doc id="9640" url="https://it.wikipedia.org/wiki?curid=9640">
<title>Coulomb</title>
<text>
Il coulomb (simbolo C) è l'unità di misura derivata SI della carica elettrica ed è definita in termini di ampere: 1 coulomb è la quantità di carica elettrica trasportata in 1 secondo dal flusso di corrente di 1 ampere:
Dal 2019, dalla ridefinizione dell'ampere, risulta che il coulomb può essere definito a partire dalla carica elementare, poiché essa ha valore esatto ("e =" 1,602176634 ⋅ 10 C):
L'unità di misura prende il nome da Charles Augustin de Coulomb (1736 – 1806), il primo scienziato a studiare quantitativamente le cariche e le forze che ne regolano il moto.
Quest'ultimo valore può essere definito Numero di Coulomb, in analogia con il numero di Avogadro: corrisponde al numero di cariche elementari contenute in un Coulomb.
formula_3
Se si utilizza questa grandezza per calcolare la forza elettrostatica esercitata tra una carica positiva e una negativa poste a una distanza di (il raggio di Bohr) si ottiene che tale forza ha una intensità pari a F""=.
Sembra una forza debole, ma se viene confrontata con la forza gravitazionale si scopre che:
ovvero la forza gravitazionale, alle scale atomiche, è trascurabile.
</text>
</doc>
<doc id="38616" url="https://it.wikipedia.org/wiki?curid=38616">
<title>Radiazione elettromagnetica</title>
<text>
In fisica la radiazione elettromagnetica (o radiazione γ, gamma) è la radiazione dell'energia nel campo elettromagnetico.
Si tratta di un fenomeno sia ondulatorio, sia corpuscolare:
La radiazione elettromagnetica può propagarsi nel vuoto, come ad esempio lo spazio interplanetario, in mezzi poco densi come l'atmosfera, oppure in strutture guidanti come le guide d'onda. Le applicazioni tecnologiche che sfruttano la radiazione elettromagnetica sono svariate. In generale si possono distinguere due macrofamiglie applicative: nella prima figurano le onde elettromagnetiche utilizzate per trasportare informazioni (radiocomunicazioni come radio, televisione, telefoni cellulari, satelliti artificiali, radar, radiografie), nella seconda quelle per trasportare energia, come il forno a microonde.
Le onde elettromagnetiche furono predette teoricamente prima di essere rilevate sperimentalmente: le equazioni di Maxwell, che riassumono l'elettromagnetismo classico, ammettono una soluzione ondulatoria propagantesi nel vuoto alla velocità della luce. Furono poi le esperienze di Hertz a confermare l'esistenza delle cosiddette "onde hertziane", ed a misurarne la velocità. L'esperimento di Michelson-Morley provò l'indipendenza della velocità della luce dalla direzione di propagazione e, grazie ad altre esperienze che attualmente si considerano sufficienti a falsificare le cosiddette teorie balistiche della luce, viene oggi considerata l'esperienza cruciale che mise in crisi la meccanica classica richiedendo la formulazione della relatività ristretta. È sulla base di tale teoria, una delle teorie meglio controllate empiricamente, che è possibile enunciare le proprietà della radiazione elettromagnetica nel vuoto.
Gli studi sull'effetto fotoelettrico, tra i quali spicca il contributo del 1905 di Albert Einstein (che gli valse il premio Nobel), evidenziarono l'esistenza di una frequenza di soglia sotto la quale tale effetto non ha luogo, indipendentemente dall'intensità (ampiezza) della radiazione incidente. Esperienze correlate, quali la misura dello spettro di corpo nero, ed i relativi tentativi di giustificazione teorica, indussero i fisici dell'inizio del secolo scorso a riaprire il secolare dibattito sulla natura della luce, di cui le equazioni di Maxwell sembravano costituire la soluzione definitiva, introducendo la nozione di quanto di energia. Il quanto di radiazione elettromagnetica prende il nome di fotone ed è una particella (nel senso della meccanica quantistica) che segue la statistica di Bose-Einstein, ovvero un bosone.
L'equazione che descrive la propagazione di un'onda elettromagnetica è l'equazione delle onde, che può essere scritta a partire dai campi elettrico e magnetico ed è un'equazione omogenea. In modo equivalente, l'equazione delle onde può essere espressa in termini delle sorgenti del campo: in questo caso si ricorre all'utilizzo dei potenziali, e si tratta di un'equazione non omogenea.
Si supponga di trovarsi in un dielettrico omogeneo ed isotropo, elettricamente neutro e perfetto e privo di cariche libere localizzate, sorgenti del campo elettromagnetico. Le equazioni che descrivono la propagazione del campo sono le equazioni delle onde per il campo elettrico e magnetico, due equazioni differenziali alle derivate parziali vettoriali:
cioè formula_2 è il versore della propagazione dell'onda.
Un generico campo elettromagnetico con frequenza formula_3 è una somma di soluzioni di tali equazioni, che si possono esprimere utilizzando l'espansione in armoniche sferiche con coefficienti proporzionali alle funzioni di Bessel sferiche. Per ottenere soluzioni a divergenza nulla il termine che si sviluppa in armoniche è formula_4 o formula_5, ottenendo:
dove formula_8 e formula_9 sono i campi di multipolo dell'ordine formula_10, formula_11 e formula_12 sono i corrispondenti campi magnetici, mentre formula_13 e formula_14 sono i coefficienti dell'espansione. I campi sono dati da:
dove formula_17 sono le funzioni di Hankel sferiche, formula_18 e formula_19 sono le condizioni al contorno e:
sono le armoniche sferiche vettoriali, che sono normalizzate in modo tale che:
Si consideri il piano definito dal versore perpendicolare ad esso:
Le soluzioni planari dell'equazione d'onda sono:
dove formula_24 è la posizione. Entrambe le espressioni soddisfano l'Equazione di Helmholtz:
Soluzioni di questo tipo rappresentano onde piane che si propagano nella direzione del versore normale al piano. Se si pone "z" la direzione del versore e "x" la direzione del campo elettrico, allora il campo magnetico ha direzione "y" e si ha che formula_26. Inoltre, essendo nulla la divergenza del campo magnetico non vi sono campi nella direzione di propagazione.
Le equazioni di Maxwell forniscono diverse informazioni riguardanti la propagazione delle onde elettromagnetiche. Si consideri un generico campo:
dove formula_28 è l'ampiezza costante, formula_29 è una funzione differenziabile al secondo ordine, formula_30 è il versore della direzione di propagazione e formula_31 la posizione. Si osserva che formula_32 è una generica soluzione dell'equazione delle onde, cioè:
per un'onda generica che si propaga nella direzione formula_34. Tale funzione deve inoltre soddisfare le equazioni di Maxwell:
La prima equazione implica quindi che il campo elettrico è ortogonale alla direzione di propagazione, mentre la seconda definisce il campo magnetico, ortogonale sia al campo elettrico che alla direzione di propagazione.
Dalle equazioni di Maxwell si evince dunque che in un'onda elettromagnetica i campi sono ortogonali fra loro e ortogonali alla direzione di propagazione, che le loro ampiezze sono proporzionali, e che la costante di tale proporzionalità è la velocità di propagazione, che dipende dalle caratteristiche del mezzo in cui si propaga.
Ogni onda elettromagnetica è in grado di trasferire energia tra due punti dello spazio. Si consideri il caso di un'onda piana, e si prenda un volume arbitrario τ contenente un campo elettromagnetico. Al suo interno la densità di energia elettrica vale:
mentre la densità di energia magnetica vale:
L'energia totale all'interno del volume sarà quindi:
Derivando quest'equazione e sfruttando le relazioni tra gli operatori rotore e divergenza si ottiene:
Il termine:
è il vettore di Poynting, mentre il secondo integrale al secondo membro rappresenta il contributo dell'energia del campo elettrico per la presenza della carica contenuta nel volume formula_42. Dal punto di vista fisico la precedente espressione esprime il fatto che la variazione nel tempo dell'energia contenuta nel volume formula_42 delimitato dalla superficie formula_44 è pari al flusso del vettore di Poynting attraverso la superficie, più l'energia dissipata per effetto Joule nella materia contenuta all'interno. In generale, dunque, secondo l'interpretazione classica ondulatoria l'energia posseduta del campo è riconducibile all'ampiezza (precisamente al quadrato dell'ampiezza) dell'onda che ne descrive la propagazione.
Nel caso di un'onda piana, sapendo che i campi elettrico e magnetico sono ortogonali tra loro:
e che oscillano ortogonali alla direzione di propagazione dell'onda, ponendo che non vi siano effetti dissipativi si ha:
dove formula_47 è la velocità di propagazione dell'onda. Oppure, in termini di campo elettrico:
dove formula_49 è il versore che identifica la direzione di propagazione dell'onda e:
è l'impedenza caratteristica del materiale entro cui si propaga l'onda.
Il modulo del vettore di Poynting è l'intensità dell'onda, cioè l'energia che attraversa la superficie ortogonale alla velocità di propagazione, nell'unità di tempo:
Se l'onda piana è approssimabile con un'onda monocromatica, essa è caratterizzata da un andamento sinusoidale del tipo:
e lo stesso vale per il campo magnetico. Segue che l'intensità dell'onda è anch'essa una funzione sinusoidale negli stessi argomenti, e deve essere mediata su un periodo:
dove formula_54 è il valore medio dell'intensità d'onda calcolato su un periodo.
Nel caso di un'onda sferica il fronte d'onda è una superficie sferica e la velocità è radiale. Per cui l'intensità d'onda dipende da formula_55:
dunque essa diminuisce come l'inverso del quadrato della distanza.
Un'onda elettromagnetica che incide o si propaga in un materiale trasferisce ad esso una certa quantità di energia, e la sua forma cambia a seconda delle caratteristiche del mezzo considerato.
Si consideri un'onda elettromagnetica incidente su un certo materiale, la forza esercitata dal campo elettromagnetico per unità di volume è data dalla forza di Lorentz generalizzata:
dove formula_58 è il numero di cariche formula_59 contenute nel volume formula_60, e formula_61 la loro velocità di deriva media.
La potenza trasferita dall'onda elettromagnetica per unità di volume al materiale è dovuta solamente al campo elettrico, in quanto la forza relativa al campo magnetico non compie lavoro. Moltiplicando scalarmente la precedente espressione per la velocità, che è ortogonale al vettore formula_62, si ottiene infatti l'espressione della densità di potenza:
dove formula_64 è la densità di corrente, che è proporzionale al campo:
La costante di proporzionalità, detta conducibilità elettrica, è un numero complesso. Si ha quindi in generale:
Nel caso considerevole in cui l'onda ha una rappresentazione sinusoidale, anche la densità di corrente ha una dipendenza sinusoidale, per cui la densità di potenza deve essere mediata su un periodo:
dove si è sviluppato il prodotto scalare, e "α" è l'angolo tra il campo elettrico e il vettore densità di corrente.
Oltre all'energia, un'onda trasferisce una certa quantità di moto formula_68, il cui modulo è pari all'energia trasferita all'unità di volume del materiale e per unità di tempo divisa per la velocità di propagazione. La quantità di moto è data dalla media temporale della forza subita dall'unità di volume definita in precedenza:
diretta lungo la direzione di propagazione dell'onda. Nel vuoto si ha:
dove formula_71 è la velocità della luce.
Avendo definito la quantità di moto di un'onda elettromagnetica, è possibile ricavare il relativo momento angolare:
Inoltre, l'onda possiede anche un momento angolare intrinseco quando essa è polarizzata circolarmente, dato da:
dove il segno dipende dal verso della rotazione e la direzione è longitudinale alla direzione di propagazione dell'onda.
Lo studio della propagazione delle radiazione in un materiale cambia a seconda ci si trovi in presenza di un conduttore o di un dielettrico.
Un'onda elettromagnetica che incide su un conduttore elettrico ha come effetto di accelerare gli elettroni di conduzione, che effettuano un moto oscillatorio dipendente dalla forma dell'onda. L'onda non penetra oltre gli strati superficiali del conduttore, e viene per la maggior parte riflessa o dissipata per effetto Joule. Lo studio del comportamento dei campi nel conduttore si basa sull'estensione delle equazioni di Maxwell al caso in cui la radiazione si propaghi in un conduttore elettrico, le quali permettono di ricavare l'equazione delle onde per il campo elettrico ed il campo magnetico all'interno di un conduttore.
Si consideri un conduttore ohmico omogeneo e isotropo, l'equazione delle onde elettromagnetiche ha la forma:
dove formula_76 è la conducibilità elettrica. L'equazione delle onde si può ricavare introducendo nelle equazioni di Maxwell la legge di Ohm generalizzata:
dove formula_78 è la densità di corrente. La precedente relazione locale vale anche nel caso non stazionario, sebbene la conducibilità elettrica dipenda in generale dal campo.
La soluzione generale nel caso di onda piana che si propaga nella direzione x è:
dove formula_80 è l'unità immaginaria e la funzione complessa formula_81 ha soluzione del tipo:
dove:
con parte reale e immaginaria data da:
In definitiva l'onda piana assume una soluzione del tipo:
A questo punto l'onda trasferisce un'oscillazione smorzata per formula_87 con coefficiente di attenuazione formula_88.
Nelle misure reali dei campi elettromagnetici, tipicamente ad alta frequenza, si utilizza la relazione tra il campo magnetico ed il campo elettrico espressa attraverso l'impedenza caratteristica del mezzo nel quale si propaga la radiazione. L'impedenza d'onda formula_89 è espressa attraverso i parametri dell'onda elettromagnetica e del mezzo in cui essa si propaga:
dove formula_91 è la permeabilità magnetica, formula_92 la permittività elettrica e formula_76 la conducibilità elettrica del materiale in cui l'onda si propaga. In questa equazione, formula_80 è l'unità immaginaria, e formula_3 la frequenza angolare dell'onda.
Nel caso di un dielettrico, in cui la conducibilità è trascurabile, l'equazione si riduce nella seguente:
Nel vuoto, e quindi approssimativamente anche in aria, tale rapporto vale circa 377 ohm:
La relazione tra i campi in tale caso diventa:
Questa formula può essere utilizzata solo in campo lontano dalla sorgente, e viene utilizzata in particolare per la valutazione dell'esposizione umana ai campi elettromagnetici.
La velocità di propagazione di un'onda elettromagnetica è indipendente dalla velocità della sorgente, dalla direzione di propagazione, e dalla velocità dell'osservatore. La velocità dipende soltanto dal mezzo in cui si propaga la radiazione, e nel vuoto è pari alla velocità della luce, la quale è l'esempio più noto di onda elettromagnetica.
La velocità della luce nel vuoto si indica in genere con la lettera formula_71 ed il suo valore numerico, misurato con grande precisione, in unità del sistema internazionale è 299 792 458 m/s. È importante notare che tale valore è stato assunto come esatto: ciò vuol dire che la velocità della luce è posta per definizione uguale a formula_71, e per questo motivo essa non è affetta da alcuna incertezza, al contrario di ciò che avviene per i valori che derivano da un processo di misura. Quest'assunzione ha comportato anche la modifica della definizione del metro.
Nei mezzi materiali e nelle guide d'onda la propagazione della radiazione elettromagnetica diviene un fenomeno più complesso. Innanzitutto la sua velocità è diversa rispetto a quella nel vuoto secondo un fattore che dipende dalle proprietà del mezzo o della guida d'onda. Può dipendere inoltre dalla frequenza della radiazione, secondo una relazione di dispersione. Restano definite due velocità, dette velocità di gruppo e velocità di fase.
L'astronomo danese Ole Rømer fu il primo a determinare empiricamente la velocità della luce per mezzo dell'osservazione del satellite di Giove di nome "Io". Annunciò la sua scoperta nel 1675.
Romer misurò il tempo che il satellite impiegava ad attraversare il cono d'ombra provocato da Giove notando che il tempo impiegato era diverso ad ogni misurazione. Questo perché quando "Io" entrava nel cono d'ombra di Giove la distanza di questi dalla terra era una, mentre, quando "Io" usciva dal cono d'ombra, la distanza dalla terra era diversa. Così ogni volta che la misura viene ripetuta il tempo impiegato appare diverso (a seconda che la terra si stia avvicinando a Giove, tempo più breve del reale, o che si stia allontanando, tempo più lungo). Attraverso l'osservazione di questo fenomeno riuscì infine a calcolare la velocità della luce ottenendo un valore () molto simile al valore reale (299 792 458 m/s).
Oggi la velocità della luce viene misurata direttamente, calcolando il tempo che impiega un impulso luminoso emesso da un laser a percorrere un determinato spazio. Dal momento che questa procedura è molto precisa e la velocità della luce è costante nel vuoto, si è pensato di definire il metro in termini di velocità della luce (vedere in proposito metro).
Gli effetti della radiazione elettromagnetica sugli esseri viventi dipendono principalmente da due fattori: 
Per quanto riguarda la frequenza della radiazione si usa distinguere tra:
Si ritiene comunemente, vedere in proposito la voce elettrosmog, che le radiazioni non ionizzanti possano avere effetti sui viventi non solo per i loro effetti termici, ma possedendo effetti interferenti con i sistemi biologici, presentino quindi il potenziale mutageno e cancerogeno delle radiazioni ionizzanti, anche se in termini diversi e sicuramente in minor misura.
Le radiazioni non ionizzanti come le onde radio, in diversi intervalli di frequenza sono considerate possibilmente cancerogene e classificate come tali dalla IARC, che nel 2011 ha indicato i campi elettromagnetici a radiofrequenza, tipici dei telefoni cellulari come possibili cause di alcuni tipi di cancro. La conseguenza è stata l'inserimento delle radiofrequenze nella classe 2B, che include gli agenti con possibili effetti carcinogeni. Nel 2012 nel "volume 102 Radiofrequency electromagnetic fields", si è poi esposto compiutamente lo stato dell'arte delle indagini relative. Altri agenti non ionizzanti, come campi elettrici e magnetici a estremamente bassa frequenza erano già stati esaminati e pubblicati nel volume 80, classificandosi rispettivamente di classe 3 (impossibilità con gli studi finora svolti di classificazione degli agenti come cancerogeno o non cancerogeno), e di classe 2B.
Le radiazioni non ionizzanti, dette NIR dall'acronimo inglese Non Ionizing Radiation, comprendono tutte le radiazioni elettromagnetiche non ionizzanti, dalle ELF fino all'ultravioletto vicino.
Per quanto riguarda gli effetti biologici e sanitari, una certezza è data dal fatto che un'onda e.m. trasferisce calore e quindi un effetto dell'interazione di un'onda e.m. con un sistema vivente è che parte dell'energia viene rilasciata, con un aumento della temperatura locale o di tutto il sistema. Per quanto riguarda gli effetti termici, occorre verificare quanto l'organismo umano è in grado di sostenere un rialzo termico. Poiché il principale "scambiatore" di calore presente nel corpo umano è costituito dal sangue, si può pensare che gli organismi meno vascolarizzati costituiscano organi critici per quanto riguarda l'esposizione alle radiazioni e.m., in quanto, se riscaldati dall'esterno non hanno più modo di ridistribuire il calore ricevuto tramite un'idonea circolazione sanguigna. Da questo punto di vista gli organi critici per eccellenza sono il cristallino e le gonadi maschili.
L'Istituto Ramazzini nel 2018 ha terminato uno studio pluriennale condotto su topi di laboratorio sottoposti a radio frequenze riscontrando evidenze significative della comparsa di tumori rari al cuore e al cervello.
Negli ultimi anni sono andati crescendo gli interrogativi sui possibili effetti sulla salute legati all'esposizione a campi elettromagnetici a radiofrequenze (RF) e microonde (MW). Nuove tecnologie si vanno diffondendo a tutti i livelli nella società, con una varietà di applicazioni mai viste prima. In molti laboratori si sta lavorando intorno a interrogativi quali l'effetto della applicazione di un campo elettromagnetico sulla permeabilità delle membrane cellulari a determinate specie ioniche e su quali basi biofisiche sia ipotizzabile un'influenza diretta del campo elettromagnetico sull'integrazione e l'elaborazione dei segnali nervosi.
Un'onda elettromagnetica che si propaga nello spazio trasporta energia che viene in parte assorbita e in parte riflessa dagli oggetti che tale onda incontra sul suo percorso. L'assorbimento avviene con modalità ed in misura diversa a seconda delle caratteristiche del mezzo. L'effetto sugli organismi viventi di tale assorbimento di energia da un campo elettromagnetico a radiofrequenza e microonde è oggetto di numerose indagini scientifiche.
A livello microscopico, manca ancora uno schema interpretativo soddisfacente dell'azione di un campo elettromagnetico sulle cellule degli organismi viventi. Questo dipende innanzitutto dall'incompleta conoscenza dei fenomeni a livello di membrana cellulare legati allo scambio di materiali ed informazioni tra cellule e ambiente esterno. In secondo luogo, la complessità strutturale dei tessuti biologici e la loro disomogeneità, rende assai problematico un calcolo dettagliato della deposizione locale di energia nei tessuti da parte dell'onda elettromagnetica incidente.
La controversia sulla possibilità di manifestazione di effetti non termici, cioè dovuti ad esposizioni a livelli di campo elettromagnetico a radiofrequenze e microonde non abbastanza elevati da produrre riscaldamento dei tessuti, si riflette nella scelta degli standard ammissibili di esposizione per lavoratori e popolazione civile da parte di Stati ed Organizzazioni internazionali diversi.
Le ricerche compiute nei Paesi occidentali hanno condotto alla conclusione (ANSI, 1982) che l'esposizione protratta per un periodo inferiore ad 1 ora, e comportante un tasso di assorbimento specifico medio al corpo intero inferiore a 4 W/kg non è in grado di produrre effetti sulla salute. Per cautelarsi dai possibili effetti cumulativi dovuti ad esposizioni prolungate (giorni oppure settimane) si è considerato per l'uomo un valore limite di SAR 10 volte inferiore, pari quindi a 0.4 W/kg
</text>
</doc>
<doc id="38875" url="https://it.wikipedia.org/wiki?curid=38875">
<title>Effetto Doppler</title>
<text>
L'effetto Doppler è un fenomeno fisico che consiste nel cambiamento apparente, rispetto al valore originario, della frequenza o della lunghezza d'onda "percepita" da un osservatore raggiunto da un'onda emessa da una sorgente che si trovi in movimento rispetto all'osservatore stesso.
Se la sorgente e l'osservatore si muovono entrambi rispetto al mezzo di propagazione delle onde, l'effetto Doppler totale è derivato dalla combinazione dei due movimenti. Perciò ognuno di essi è analizzato separatamente.
L'effetto fu analizzato per la prima volta da Christian Andreas Doppler nel 1845. Procedette quindi a verificare la sua analisi in un famoso esperimento: si piazzò accanto ai binari della ferrovia, e ascoltò il suono emesso da un vagone pieno di musicisti, assoldati per l'occasione, mentre si avvicinava e poi mentre si allontanava. Confermò che l'altezza del suono era più alta quando l'origine del suono si stava avvicinando, e più bassa quando si stava allontanando, dell'ammontare predetto. Hippolyte Fizeau scoprì indipendentemente lo stesso effetto nelle onde elettromagnetiche nel 1848 (in Francia, l'effetto è a volte chiamato "effetto Doppler-Fizeau").
L'effetto Doppler si può constatare ascoltando la differenza nel suono emesso dalla sirena di un mezzo di soccorso quando si avvicina e quando si allontana, oppure quella nel fischio di un treno in avvicinamento prima e in allontanamento poi. L'effetto è tanto più evidente quanto più il mezzo è veloce. L'effetto è anche più evidente quando l'oggetto o la fonte che emette il suono si trova vicino ad un osservatore.
È importante notare che la frequenza del suono emesso dalla sorgente non cambia "nel sistema di riferimento solidale alla sorgente". Per comprendere il fenomeno, consideriamo la seguente analogia: se siamo fermi sulla spiaggia, vediamo arrivare le onde supponiamo ogni cinque secondi, quindi ad una determinata frequenza; se ora entriamo in acqua e navighiamo verso il mare aperto, andiamo incontro alle onde, quindi le incontriamo più frequentemente (la frequenza aumenta), mentre se navighiamo verso riva, nella stessa direzione delle onde, la frequenza con cui le incontriamo diminuisce. Per fare un altro esempio: qualcuno lancia una palla ogni secondo nella nostra direzione. Assumiamo che le palle viaggino con velocità costante. Se colui che le lancia è fermo e ogni palla è alla stessa velocità media della prima, riceveremo una palla ogni secondo. Ma, se si sta invece muovendo nella nostra direzione, ne riceveremo un numero maggiore nel medesimo lasso di tempo (ovvero, a una frequenza maggiore), perché esse saranno meno spaziate. Al contrario, se si sta allontanando ne riceveremo di meno nell'unità di tempo. Ciò che cambia è quindi la frequenza "nel sistema di riferimento del rilevatore"; come conseguenza, l'altezza del suono percepito cambia.
Se una sorgente che si sta allontanando sta emettendo onde con una frequenza "f", allora un osservatore stazionario (rispetto al mezzo di trasmissione) percepirà le onde con una frequenza "f"' data da:
mentre se si sta avvicinando:
formula_2
dove "v" è la velocità delle onde nel mezzo e "v" è la velocità della sorgente rispetto al mezzo (considerando solo la componente nella direzione che unisce sorgente ed osservatore). In termini relativi si può scrivere anche:
Questa formula è equivalente a quella più comunemente usata in spettroscopia nella misure astronomiche, per ricavare la velocità "v" di allontanamento di una sorgente di luce, sulla base dell'aumento della sua lunghezza d'onda, cioè dello spostamento del suo colore verso il rosso (vedi figura in basso e "voci correlate", spostamento verso il rosso, spostamento verso il blu):
dove c è la velocità della luce nel vuoto, "Δλ' = λ' - λ"
e "λ" è la lunghezza d'onda a riposo di una determinata riga di un dato elemento chimico, misurata precedentemente in laboratorio.
Consideriamo un'onda sferica emessa da una sorgente puntiforme e la sua lunghezza d'onda "λ" che è la distanza tra due fronti d'onda successivi che si trovano nella stessa fase, ad esempio due massimi.
La relazione che lega la frequenza "f" , il periodo "T = 1 / f" e la velocità "v" di propagazione dell'onda vale:
Se la sorgente è in moto con velocità "v" rispetto all'osservatore (fermo),
detto θ l'angolo tra la velocità e la direzione verso l'operatore, e detta "v=vcosθ" la componente della velocità in direzione dell'osservatore,
nel tempo T, che passa tra un fronte d'onda e il successivo, la sorgente si avvicina all'osservatore di un tratto pari a "vT". La distanza tra i due fronti, in direzione dell'osservatore, si accorcia di questa quantità e quindi la lunghezza d'onda percepita diventa minore e vale:
Sostituendo al periodo "T" la formula equivalente "λ /v" , si ricava:
ed anche:
Analogamente, passando alle frequenze, mettendo formula_9 al posto di formula_10 e formula_11 al posto di formula_12 , si ricava:
ed anche:
Un'analisi simile per un osservatore in movimento e una sorgente stazionaria fornisce la frequenza osservata (la velocità dell'osservatore è indicata come "v"):
In questo caso l'osservatore in moto verso la sorgente riceve un numero maggiore di fronti d'onda nello stesso intervallo di tempo e percepisce una frequenza maggiore: più precisamente, in un tempo pari a un secondo, l'osservatore in moto riceve, oltre al numero "f"="v/λ" di treni d'onda emessi dalla sorgente, anche un numero di treni d'onda pari a "v/λ"="v• f/v".
In generale, la frequenza osservata è data da:
dove "v" è la velocità dell'osservatore, "v" è la velocità della sorgente, "v" è la velocità del suono nel mezzo, e tutte le velocità sono positive se nello stesso verso lungo cui si propaga l'onda, o negative se nel verso opposto.
Il primo tentativo di estendere l'analisi di Doppler alle onde luminose fu fatto poco dopo da Fizeau. Ma le onde luminose non richiedono un mezzo per propagarsi, e un corretto trattamento dell'effetto Doppler per la luce richiede l'uso della relatività speciale (vedi effetto Doppler relativistico). Nel caso di onde meccaniche, come quelle sonore, il mezzo in cui le onde si propagano individua un sistema di riferimento privilegiato. C'è perciò una differenza fisica tra il caso in cui l'osservatore è fermo e la sorgente in moto, e viceversa quello in cui la sorgente è ferma e l'osservatore in moto. Per la luce, però, tutti i sistemi di riferimento sono fisicamente equivalenti. Nell'espressione relativistica lo spostamento Doppler deve dipendere soltanto dalla velocità relativa della sorgente e dell'osservatore.
La sirena di un'ambulanza inizierà ad essere percepita più alta del tono che ha da ferma, si abbasserà mentre passa accanto all'osservatore, e continuerà più bassa del suo tono da ferma mentre si allontana dall'osservatore. L'astronomo amatoriale John Dobson ha descritto l'effetto in questo modo:
In altre parole, se la sirena si stesse avvicinando direttamente verso l'osservatore, il tono rimarrebbe costante (anche se più alto dell'originale) fino a raggiungere l'osservatore, e salterebbe immediatamente ad un tono inferiore una volta che lo avesse oltrepassato (sempre che l'osservatore fosse ancora in grado di sentirla). Poiché, normalmente, la sirena passa ad una certa distanza dall'osservatore, la sua velocità radiale cambia continuamente, in funzione dell'angolo tra la linea di vista dell'osservatore e la velocità vettoriale della sirena:
dove "v" è la velocità della sirena rispetto al mezzo di trasmissione, e formula_19 è l'angolo tra la direzione di moto della sirena e la linea di vista tra la sirena e l'osservatore.
L'effetto Doppler, applicato alle onde luminose, è fondamentale nella astronomia radar. Interpretandolo come dovuto ad un effettivo moto della sorgente (esistono anche interpretazioni alternative, ma meno diffuse), è stato usato per misurare la velocità con cui stelle e galassie si stanno avvicinando o allontanando da noi, per scoprire che una stella apparentemente singola è, in realtà, una stella binaria con componenti molto vicine tra loro, e anche per misurare la velocità di rotazione di stelle e galassie.
L'uso dell'effetto Doppler in astronomia si basa sul fatto che lo spettro elettromagnetico emesso dagli oggetti celesti non è continuo, ma mostra delle linee spettrali a frequenze ben definite, correlate con le energie necessarie ad eccitare gli elettroni di vari elementi chimici.
L'effetto Doppler è riconoscibile quando le linee spettrali non si trovano alle frequenze ottenute in laboratorio, utilizzando una sorgente stazionaria. La differenza in frequenza può essere tradotta direttamente in velocità utilizzando apposite formule.
Poiché i colori posti ai due estremi dello spettro visibile sono il blu (per lunghezze d'onda più corte) e il rosso (per lunghezze d'onda più lunghe), l'effetto Doppler è spesso chiamato in astronomia spostamento verso il rosso se diminuisce la frequenza della luce, e spostamento verso il blu se l'aumenta.
L'effetto Doppler ha condotto allo sviluppo delle teorie sulla nascita ed evoluzione dell'Universo come il Big Bang, basandosi sul sistematico spostamento verso il rosso mostrato da quasi tutte le galassie esterne. Tale effetto è stato codificato nella legge di Hubble.
L'effetto Doppler è una prova inoltre della continua espansione dell'universo. Consideriamo infatti una stella: controllando la sua lunghezza d'onda noteremo che si sposta sempre di più verso il rosso. Ciò significa che la sua lunghezza d'onda è aumentata e conseguentemente la stella è sempre più lontana da noi. Questo indica che l'universo è in continua espansione e ogni elemento tende ad allontanarsi da tutto, allungando sempre di più la sua lunghezza d'onda.
L'effetto Doppler è anche usato in alcune forme di radar per misurare la velocità degli oggetti rilevati. Un fascio radar è lanciato contro un oggetto in movimento, per esempio un'automobile, nel caso dei radar in dotazione alle forze di polizia di molti Paesi del mondo. Se l'oggetto si sta allontanando dall'apparecchio radar, ogni onda di ritorno ha dovuto percorrere uno spazio maggiore della precedente per raggiungere l'oggetto e tornare indietro, quindi lo spazio tra due onde successive si allunga, e la frequenza delle onde radio cambia in modo misurabile. Usando le formule dell'effetto Doppler si può risalire alla velocità dell'oggetto. Questa tipologia di radar è molto utilizzata per le previsioni meteorologiche perché permettono di individuare con precisione distanza, velocità e direzione dei fronti nuvolosi.
L'effetto Doppler è anche usato in medicina per la rilevazione della velocità del flusso sanguigno. Tale principio infatti è sfruttato dai Flussimetri Eco-Doppler (ADV, ovvero Acoustic Doppler Velocimeter), nei quali una sorgente di onde sonore, generalmente ultrasuoni, viene orientata opportunamente. Queste onde acustiche vengono poi riflesse con una nuova frequenza, a seconda della velocità vettoriale delle particelle sanguigne, rilevata e rielaborata in modo da ottenere tale misura di velocità.
Un'altra applicazione è il laser Doppler imager, utilizzato in particolare per studi sull'angiogenesi, sulla disfunzione endoteliale, sulle ulcere cutanee, per la valutazione di prodotti farmaceutici o cosmetologici ad applicazione locale, per lo studio delle ustioni.
Esistono strumenti musicali che sfruttano l'effetto Doppler per rendere particolari effetti onomatopeici; come ad esempio il tamburo a frizione rotante che in Romagna è chiamato “Raganella”;. Per questo tipo di strumenti a Fabio Lombardi si devono le osservazioni sull'accentuazione della resa sonora per l'effetto Doppler: Quando il piccolo tamburo rotea, l'ascoltatore percepisce due picchi di frequenza modulati progressivamente ed alternativamente verso l'alto e verso il basso, per l'effetto sopra citato, e questo porta ad un suono simile al gracidare di rana da cui il nome dello strumento giocattolo.
</text>
</doc>
<doc id="38911" url="https://it.wikipedia.org/wiki?curid=38911">
<title>Onda</title>
<text>
In fisica con il termine onda si indica una perturbazione che nasce da una sorgente e si propaga nel tempo e nello spazio, trasportando energia o quantità di moto senza comportare un associato spostamento della materia.
Le onde possono propagarsi sia attraverso la materia, sia nel vuoto. Ad esempio la radiazione elettromagnetica e la radiazione gravitazionale possono esistere e propagarsi anche in assenza di materia, mentre altri fenomeni ondulatori esistono unicamente in un mezzo fisico, che deformandosi produce le forze elastiche di ritorno in grado di permettere all'onda di propagarsi.
Dal punto di vista matematico un'onda è una soluzione dell'equazione delle onde (o di altre equazioni più complicate), la cui espressione varia a seconda del tipo di perturbazione.
Non è semplice dare una definizione autonoma e precisa del termine "onda", sebbene questo termine sia comunemente molto usato in contesti molto differenti fra loro. La definizione delle caratteristiche necessarie e sufficienti che identificano il fenomeno ondulatorio è flessibile. Intuitivamente il concetto di onda è qualificato come il trasporto di una perturbazione nello spazio senza comportare un trasporto netto della materia del mezzo, qualora presente, che occupa lo spazio stesso. I fisici Albert Einstein e Leopold Infeld hanno cercato di rispondere alla domanda "Cos'è un'onda?" unendo questo fatto all'esperienza comune:
Una vibrazione può essere definita come il moto avanti e indietro intorno a un punto definito "x", tuttavia una vibrazione non è necessariamente un'onda. Infatti in un'onda sulla superficie dell'acqua, oppure lungo una stringa, l'energia vibrazionale si muove dalla sorgente sotto forma di perturbazione senza un moto collettivo delle particelle dell'acqua o della corda in cui si propaga. Questa rappresentazione diventa però problematica quando si ha a che fare con le onde stazionarie (per esempio le onde sulle corde di una chitarra), dove l'energia in tutte le direzioni è identica e non viene trasportata lungo lo spazio, perciò talvolta nella definizione di onda si cita solamente la propagazione di un disturbo senza richiedere il trasporto di energia o quantità di moto. Per le onde elettromagnetiche (ad esempio la luce) bisogna considerare ulteriormente che il concetto di mezzo non può essere applicato, in quanto queste si propagano anche nello spazio vuoto.
Per queste ragioni la teoria delle onde rappresenta una particolare branca della fisica teorica riguardante lo studio delle onde indipendentemente dalla loro origine fisica. Questa peculiarità deriva dal fatto che la teoria matematica delle onde si può applicare per descrivere fenomeni ondulatori in contesti anche molto differenti. Per esempio l'acustica si distingue dall'ottica per il fatto che la prima si occupa del trasporto vibrazionale di energia meccanica, mentre la seconda di perturbazioni del campo elettrico e magnetico. Concetti come massa, inerzia, quantità di moto, elasticità diventano quindi cruciali per descrivere i processi ondulatori acustici, al contrario dell'ottica. La struttura particolare del mezzo introduce inoltre alcuni fattori di cui bisogna tenere conto, come ad esempio i fenomeni vorticosi per l'aria e l'acqua o la complessa struttura cristallina nel caso di alcuni solidi.
Altre proprietà tuttavia possono essere usate per descrivere indifferentemente tutti i tipi di onde. Per esempio, basandosi sull'origine meccanica delle onde acustiche, ci può essere un movimento nello spazio e nel tempo di una perturbazione se e solo se il mezzo non è né infinitamente flessibile né infinitamente rigido. Se tutte le parti che compongono il mezzo si dispongono in modo rigido l'una rispetto all'altra, non sarà possibile alcun movimento infinitesimo e quindi non ci sarà alcuna onda (ad esempio l'idealizzazione del corpo rigido). Al contrario, se tutte le parti sono indipendenti l'una dall'altra senza alcun tipo di interazione reciproca, non vi sarà alcuna onda in quanto non ci sarà trasmissione di energia fra le varie parti componenti del corpo. Nonostante queste considerazioni non si possano applicare alle onde che non si propagano in alcun senso, si possono comunque trovare caratteristiche comuni a tutte le onde: ad esempio, in un'onda la fase è differente per punti adiacenti nello spazio, perché la vibrazione raggiunge questi punti in tempi differenti.
Similmente, alcuni fenomeni che si sono scoperti in determinati contesti, sono poi stati generalizzati ad altri fenomeni ondulatori. L'interferenza è stata studiata da Young nel caso particolare delle onde luminose, tuttavia è stata recentemente analizzata in alcuni problemi riguardanti le proprietà atomiche quantistiche dell'elettrone.
Un'onda può essere caratterizzata da una singola oscillazione oppure da un "treno" o successione di onde aventi caratteristiche simili, come ad esempio la periodicità intrinseca. In generale le onde sono caratterizzate da una "cresta" (punto alto), da un "ventre" (punto più basso) e da fronti d'onda di propagazione nel caso di treni di onde e sono in prima istanza classificabili come longitudinali o trasversali. Nelle onde trasversali la vibrazione è perpendicolare alla direzione di propagazione (ad esempio le onde su una corda, le parti infinitesime si muovono in alto e in basso in verticale, mentre l'onda si propaga orizzontalmente).
Le onde longitudinali sono invece caratterizzate da una vibrazione concorde con la direzione di propagazione dell'onda (ad esempio le onde sonore, le particelle dell'aria si muovono infinitesimamente nella stessa direzione di propagazione del suono). Esistono onde che sono sia longitudinali che trasversali e sono dette "onde miste" (ad esempio le onde sulla superficie del mare). Parametri di riferimento di un'onda sono l'ampiezza, la lunghezza d'onda, il periodo, la frequenza, la fase, la velocità di propagazione, l'energia e la potenza ad essa associata. Per quanto riguarda la velocità di un'onda sono definibili la velocità di fase e la velocità di gruppo.
Il mezzo in cui le onde viaggiano può essere classificato a seconda delle seguenti proprietà:
Durante la propagazione nel mezzo l'onda è soggetta ad attenuazione da parte del mezzo fino all'esaurimento dell'energia trasportata.
Tutte le onde hanno un comportamento comune in situazioni standard e possono subire i seguenti effetti o fenomeni:
Un'onda è polarizzata se può oscillare solo in una direzione. La polarizzazione di un'onda trasversale descrive la direzione di oscillazione, nel piano perpendicolare alla direzione di moto. Onde longitudinali come quelle sonore non hanno polarizzazione, in quanto per queste onde la direzione di oscillazione è lungo la direzione di moto. Un'onda può essere polarizzata con un filtro polarizzatore.
A seconda delle caratteristiche, le onde si possono classificare in molti modi.
Riguardo al tipo di mezzo:
Riguardo alle dimensioni del mezzo in cui si propagano:
Riguardo alla direzione del moto di oscillazione rispetto a quella di propagazione:
Riguardo alla propagazione:
A seconda del mezzo in cui si propagano e della caratteristica fisica che usiamo per rappresentarle:
Alcune onde caratteristiche sono:
I fenomeni ondulatori possono essere matematicamente descritti dall'equazione delle onde, almeno in prima approssimazione. Questa semplice equazione fornisce utili strumenti di analisi di tutte le onde e spesso, come nel caso di una stringa vibrante, le sue soluzioni rappresentano una prima approssimazione valida per piccole perturbazioni.
L'equazione delle onde per una funzione scalare formula_1 è un'equazione differenziale alle derivate parziali iperbolica della forma:
In una dimensione questa equazione si riduce a:
la cui soluzione generale si ottiene definendo le variabili:
e riscrivendo l'equazione:
la cui soluzione è dunque:
ovvero:
Tale soluzione di basa sul principio di Duhamel.
Una funzione formula_8 rappresenta quindi un'onda dotata di ampiezza costante che si propaga lungo l'asse formula_9 di un sistema di riferimento cartesiano se la dipendenza dallo spazio formula_9 e dal tempo formula_11 è data dalla sola combinazione formula_12:
dove formula_14 è una costante positiva. A seconda che l'argomento sia formula_15 o formula_16, l'onda si dice rispettivamente "regressiva" o "progressiva".
Un'onda progressiva di velocità formula_14 dipende dall'argomento formula_12 e trasla lungo spazio e nel tempo a velocità costante, senza cambiare la sua forma. Se si considera infatti la stessa perturbazione al tempo formula_19, si ha, per la definizione di onda:
Dato che l'onda è funzione solo di formula_21, allora la traslazione formula_22 può essere vista come una semplice traslazione spaziale di formula_23:
e quindi l'onda ad un istante successivo formula_25 non è altro che la stessa onda dell'istante formula_11, con la medesima forma, ma solo traslata di formula_27.
Si definisce fronte d'onda il luogo dei punti nello spazio in cui formula_21 assume il medesimo valore ad un determinato istante temporale.
Se formula_29 è periodica nel suo argomento allora descrive un'onda periodica. La periodicità dell'onda è identificata dal periodo formula_30, che rappresenta il tempo necessario affinché un ciclo completo di oscillazione venga completato. La frequenza formula_31 dell'onda è inoltre il numero di periodi per unità di tempo; se l'unità di tempo è il secondo la frequenza si misura in hertz.
Periodo e frequenza sono legate dalla relazione:
Ad un periodo temporale corrisponde un periodo spaziale detto lunghezza d'onda formula_33, e vale la relazione:
Nel caso di un'onda periodica, la rappresentazione in serie di Fourier permette di descrivere l'onda come somma di termini sinusoidali del tipo:
Equivalentemente, usando la formula di Eulero, questi termini possono essere rappresentati come la parte reale di una funzione immaginaria:
In queste formule formula_37 è il vettore d'onda, che identifica la direzione di propagazione dell'onda al posto della velocità di propagazione. Il suo modulo è chiamato "pulsazione spaziale", ed è legato alla lunghezza d'onda dalla relazione:
Lo scalare formula_39 è l'ampiezza dell'onda, e rappresenta il massimo valore della grandezza rappresentativa dell'onda in un periodo. Il termine formula_40 rappresenta la fase iniziale dell'onda.
Un'onda può essere descritta per mezzo della sua frequenza angolare, che è correlata alla frequenza formula_31 secondo la relazione:
In certi casi le onde presentano caratteristiche, come la dispersione (la velocità di propagazione dipende dalla frequenza) o la non linearità (il comportamento dell'onda dipende dalla sua ampiezza) che non possono essere descritte dalle soluzioni dell'equazione delle onde. Per questo motivo tali onde devono essere descritte da equazioni più complicate, come l'equazione di Sine-Gordon (che nel caso classico può descrivere la propagazione di un'onda di torsione in una stringa elastica, a cui è agganciato un sistema di pendoli che oscillano nel piano trasverso alla stringa), l'equazione di Schrödinger non lineare, l'equazione di Korteweg-de Vries o quella di Boussinesq. Tali equazioni permettono di descrivere fenomeni non previsti dall'equazione delle onde di D'Alembert, come i solitoni, le onde cnoidali o la turbolenza d'onda. Fenomeni di questo tipo si osservano in un gran numero di branche della fisica, come la fluidodinamica, la fisica dei plasmi, l'ottica non lineare, i condensati di Bose-Einstein o la relatività generale, anche se nei casi più semplici ci si può spesso ricondurre all'approssimazione lineare fornita dall'equazione delle onde di D'Alembert. 
I fenomeni ondulatori rappresentano una classe di fenomeni naturali estremamente importante in fisica; alcuni esempi di onde sono: le onde elastiche, le onde di pressione (onde acustiche e onde d'urto), le onde marine, le onde elettromagnetiche (la luce), le onde gravitazionali, le onde sismiche. In prima approssimazione, secondo il modello concettuale della fisica classica, si può affermare che in natura, al di là delle nozioni di spazio, tempo, energia e carica elettrica, tutto ciò che non è materia (cioè dotato di massa) è un'onda, cioè "energia in propagazione". La differenza sostanziale tra onda e corpuscolo materiale è che mentre il corpuscolo in un certo istante temporale è sempre "localizzato" in un preciso volume dello spazio, l'onda appare invece più "delocalizzata" nello spazio.
È solo con la fisica moderna che si raggiunge un punto di contatto nella realtà fisica tra le due diversissime classi di fenomeni, ondulatori e corpuscolari: agli inizi del XX secolo la meccanica quantistica, attraverso il principio di complementarità, sancisce infatti il cosiddetto dualismo onda-particella nei fenomeni fisici che avvengono a scala atomica e subatomica, secondo il quale le stesse particelle microscopiche dotate di massa propria, oltre alle proprietà classiche quali energia meccanica e quantità di moto, assumono proprietà ondulatorie nell'interpretazione di determinati contesti e fenomeni.
Un caso particolare di onda, descrivibile matematicamente a partire dall'equazione delle onde imponendo opportune condizioni al contorno, è l'onda stazionaria cioè un'onda che rimane in una posizione spaziale costante fissa nel tempo senza propagarsi oscillando tra punti fissi detti nodi. Questo fenomeno può accadere per esempio quando il mezzo si muove in direzione opposta all'onda oppure come risultato di una interferenza fra due onde, di eguale ampiezza e frequenza, che viaggiano in opposte direzioni.
In un'onda stazionaria vi sono alcuni punti, detti nodi, che restano fissi e non oscillano. Questo fatto determina a stretto rigore per questo tipo di perturbazione delle caratteristiche intrinsecamente differenti da una "onda" nel senso stretto del termine. In quanto tale, un'onda stazionaria può permettere per esempio di immagazzinare energia in una regione spaziale ma non rappresenta quindi alcun trasporto energetico netto fra differenti punti dello spazio.
La sovrapposizione di due onde che si muovono in direzione opposte con uguale ampiezza e frequenza, ma fase opposta è un fenomeno tipico indotto dalla riflessione di una singola onda contro un ostacolo fisso, esattamente quanto accade per esempio in una onda elettromagnetica che incide contro una lastra di materiale conduttore. Questo meccanismo è usato per generare onde stazionarie ed è alla base del funzionamento di alcuni strumenti musicali a corda, a fiato
e delle cavità risonanti.
Le onde che possono svilupparsi lungo una stringa sono di tipo trasversale e soddisfano l'equazione delle onde di d'Alembert solamente se l'ampiezza della perturbazione che genera il fenomeno ondulatorio è piccola. In questo limite si ricava che la velocità di propagazione è pari a:
dove formula_30 è la tensione a cui è sottoposta la stringa mentre formula_45 è la sua densità lineare o massa lineica, cioè la massa per unità di lunghezza. Un'onda su una stringa può essere riflessa in seguito all'urto contro un estremo fisso oppure essere parzialmente trasmessa e parzialmente riflessa in seguito all'incontro di una giunzione fra due stringhe di differente densità lineare formula_45. Questo tipo di onde, insieme al fenomeno delle onde stazionarie, sono alla base del funzionamento di molti strumenti a corda.
Un'onda elettromagnetica è un fenomeno ondulatorio dato dalla propagazione in fase del campo elettrico e del campo magnetico, oscillanti in piani tra loro ortogonali e ortogonali alla direzione di propagazione. Tale fenomeno è descritto matematicamente come soluzione dell'equazione delle onde, a sua volta ottenuta a partire dalle equazioni di Maxwell secondo la teoria dell'elettrodinamica classica. Questo tipo di radiazione viaggia nella direzione sempre perpendicolare alle direzioni di oscillazione dei campi, ed è quindi un'onda trasversale. Nel diciannovesimo secolo James Clerk Maxwell ha scoperto infatti che i campi elettrici e magnetici soddisfano l'equazione delle onde, con una velocità di propagazione vuoto pari alla velocità della luce, come determinato sperimentalmente da Heinrich Hertz. Le onde elettromagnetiche, come ad esempio la luce visibile, hanno caratteristiche di propagazione nei mezzi o in presenza di ostacoli dipendenti dalla frequenza (e quindi dalla lunghezza d'onda), alcuni materiali sono trasparenti al passaggio della radiazione elettromagnetica sulla lunghezza d'onda del visibile (come alcuni tipi di vetro), mentre le onde radio sono difficilmente ostacolate nella propagazione da oggetti di piccola dimensione, come anche piccoli edifici, infine la radiazione elettromagnetica a lunghezza d'onda inferiore a quella degli ultravioletti può essere dannosa per la salute dell'uomo. Un caso particolare di onda elettromagnetica è l'onda monocromatica.
Le onde gravitazionali sono distorsioni della curvatura dello spazio tempo che viaggiano come un'onda, propagandosi da una sorgente, come ad esempio un corpo massivo. La loro esistenza è stata prevista da Albert Einstein nel 1916 in base alla relatività generale e dovrebbero teoricamente trasportare energia sotto forma di radiazione gravitazionale. Le principali possibili sorgenti dovrebbero essere i sistemi binari composti da pulsar o buchi neri. L'11 febbraio 2016 il team del rivelatore Advanced LIGO ha annunciato di aver rilevato il 14 settembre 2015 onde gravitazionali causate dalla collisione di due buchi neri.
Le onde gravitazionali sono in generale non lineari e interagiscono con la materia e l'energia. Tuttavia questo fenomeno può essere lineare quando le onde sono molto lontane dalle sorgenti e per piccole perturbazioni dello spazio tempo.
Le onde hanno applicazioni diffusissime nella vita comune e in molti campi di studio tecnico-scientifico: dallo studio delle proprietà delle onde elettromagnetiche emesse da un corpo è possibile risalire alle caratteristiche chimico-fisiche del corpo (spettroscopia); le stesse onde elettromagnetiche (ad es. le onde portanti modulate oppure onda quadra) sono anche il mezzo utilizzato per veicolare informazione all'interno dei sistemi di telecomunicazioni attraverso segnali (es. onda radio nelle radiocomunicazioni); inoltre molte tecniche di diagnostica medica, prospezione geofisica, telerilevamento, applicazioni radar ecc., utilizzano particolari onde elettromagnetiche o acustiche.
</text>
</doc>
<doc id="38959" url="https://it.wikipedia.org/wiki?curid=38959">
<title>Ohm</title>
<text>
L'ohm (simbolo Ω) è l'unità di misura della resistenza elettrica nel Sistema Internazionale.
Notevolmente, è pari a circa 1/30 dell'unità naturale di resistenza.
Un resistore ha resistenza pari ad 1 ohm quando una differenza di potenziale ai suoi capi pari ad un volt genera una corrente di intensità pari ad un ampere.
Dimensionalmente si ha:
L'ohm è l'unità di misura anche dell'impedenza e della reattanza.
Il suo nome deriva da quello del fisico tedesco Georg Simon Ohm scopritore dell'omonima legge di Ohm.
Per esprimere la resistenza si usano anche multipli e sottomultipli. Gamme di valori comunemente usate sono:
- mΩ (milliohm), molto usati per la resistenza di conduttori e di resistori per la misura di correnti elettriche;
- Ω, kΩ (chiloohm) e MΩ (megaohm), di cui molto usati in elettronica generale soprattutto i kΩ;
- GΩ (gigaohm) usati, ad esempio, per la polarizzazione dei microfoni a condensatore e, insieme ai TΩ (teraohm), per la resistività di materiali definiti isolanti (mai perfetti).
Il modo corretto di scrivere ohm è con la lettera iniziale minuscola, tranne quando secondo le regole grammaticali si debba scrivere una parola con la maiuscola iniziale; allo stesso modo è da ritenersi errato scrivere o dire ohms al plurale (in quanto questa è una regola grammaticale anglofona), rimanendo invece la parola invariata.
In Unicode esiste un simbolo apposta per l'ohm: Ω (U+2126), diverso dalla lettera greca omega maiuscola: Ω (U+03A9).
</text>
</doc>
<doc id="16911" url="https://it.wikipedia.org/wiki?curid=16911">
<title>Elettrostatica</title>
<text>
In fisica classica l'elettrostatica è una branca dell'elettromagnetismo che studia le cariche elettriche stazionarie nel tempo, generatrici del campo elettrostatico. Fin dai tempi di Talete, nel V secolo a.C., si era notato che una bacchetta di ambra strofinata con un panno era in grado di attirare piume, pagliuzze, fili, definendo il fenomeno "elettrizzazione dei corpi".
Il termine "elettricità" deriva infatti proprio dalla parola greca ""elektron"", che significa ambra.
Esistono due tipi di stati elettrici (o cariche):
+ positivo, come quello che assume il vetro elettrizzato;
- negativo, come quello che assume l'ambra.
Cariche dello stesso segno si respingono, mentre cariche di segno opposto si attraggono, in entrambi i casi nel rispetto della legge di Coulomb.
I primi esperimenti e ricerche documentati sull'elettrostatica risalgono alla Grecia antica del 600 a.C. con Talete di Mileto e Teofrasto, scoprirono che strofinando dell'ambra con un panno di lana, permetteva di attirare verso se delle pagliuzze, piume, fili e similari. Le diverse forme di questo espediente vennero successivamente nel 1600 chiamate "fenomeni elettrici" grazie ad un secondo contributo scientifico riguardo allo studio di tali fenomeni, da parte di William Gilbert, che distinse tali fenomeni da quelli magnetici, in quanto aveva osservato che i fenomeni elettrici avevano un'energia finita (che venne chiamata fluido elettrico) e la forza di attrazione durava fin quanto c'era sufficiente energia, coniando anche il termine forza elettrica.
Il terzo contributo avvenne da parte di Otto Von Guericke verso la metà del XVII secolo, sia per la realizzazione del primo generatore elettrostatico (macchina elettrostatica a strofinio di Guericke), con la quale osservò le scariche elettriche generate durante il suo caricamento e la relativa luminescenza e crepitio (tale fenomeno venne chiamato "fuoco elettrico"), ma che dimostrò anche come la forza elettrica generata da un corpo carico potesse essere trasportata, applicando a tale corpo carico un filo, che ha le stesse proprietà del corpo carico. Guericke fece poi un'altra importante scoperta: studiando l'ambra notò infatti come gli oggetti che venivano inizialmente attirati da essa, una volta a contatto con l'ambra caricata, venivano respinti dalla stessa, dimostrando così che la forza elettrica può essere sia attrattiva che repulsiva.
Successivamente Charles François de Cisternay du Fay, determinò l'esistenza di una carica elettrica positiva e di una negativa, che venivano generate da sostanze differenti, che vennero chiamati "resinosi" (ambra, gomma dura, ceralacca, sostanze resinose) e "vetrosi" (vetro e simili), scoprendo anche che i corpi caricati allo stesso modo e quindi con le stesse cariche elettriche si respingevano (ambra-ambra o vetro-vetro), mentre i corpi caricati con cariche differenti si attraevano (ambra-vetro), allo stesso tempo suppose che i corpi neutri contenessero uguale quantità dei due fluidi elettrici, mentre i corpi carichi avessero un eccesso di un fluido rispetto all'altro.
Successivamente venne definito con più precisione come l'elettrizzazione avviene per cessione di elettroni (quindi caricandosi positivamente) o per acquisizione di elettroni (caricandosi negativamente), in quanto lo spostamento degli elettroni richiede energie molto esigue rispetto ai protoni.
Sappiamo già che un atomo è elettricamente neutro perché il numero dei protoni, particelle con carica positiva, è uguale al numero di elettroni, particelle con carica negativa; quindi una bacchetta di vetro, o una di ambra, costituita da atomi elettricamente neutri, è neutra. Quando si strofina la bacchetta di vetro con il panno di lana, alcuni elettroni, che sono le particelle libere di muoversi, lasciano la bacchetta e si trasferiscono sul panno.
La bacchetta di vetro perde dunque elettroni e, quindi, ha un numero di protoni maggiore di quello degli elettroni: si dice allora che si carica positivamente; il panno, invece, acquista elettroni e quindi ha un numero di elettroni maggiori di quello dei protoni: ecco perché si dice che si carica negativamente.
Se invece di una bacchetta di vetro si utilizza una bacchetta di ambra, gli elettroni passano dal panno alla bacchetta.
In questo caso la bacchetta di ambra, che acquista elettroni, si carica negativamente e il panno, che perde elettroni, si carica positivamente.
Dunque quando si strofinano due corpi, uno si elettrizza positivamente e uno negativamente.
L'elettrizzazione di un corpo si può ottenere anche con tre metodi differenti:
L'unità di misura della carica elettrica è il Coulomb (C), dal nome del fisico francese Charles Augustin de Coulomb (1736-1806), che scoprì la legge fondamentale dell'elettrostatica, nota appunto come Legge di Coulomb.
Rispetto alle cariche elettriche i materiali non si comportano tutti allo stesso modo e si dividono in "conduttori" e "isolanti".
Materiali come i metalli, l'acqua (non deionizzata) e lo stesso corpo umano, in cui le cariche prodotte si muovono liberamente, si dicono conduttori, cioè che si lascino attraversare dalle scariche elettriche.
La plastica, la gomma, il vetro, il legno, la porcellana si dicono, invece, isolanti, perché le cariche elettriche non li riescono ad attraversare. Si può dire che l'isolante è impermeabile alle scariche elettriche e il conduttore è permeabile a esse.
</text>
</doc>
<doc id="8536" url="https://it.wikipedia.org/wiki?curid=8536">
<title>Luce</title>
<text>
Il termine luce (dal latino "lux") si riferisce alla porzione dello spettro elettromagnetico visibile dall'occhio umano, approssimativamente compresa tra 400 e 700 nanometri di lunghezza d'onda, ossia tra 790 e di frequenza. Questo intervallo coincide con il centro della regione spettrale della luce emessa dal Sole che riesce ad arrivare al suolo attraverso l'atmosfera. I limiti dello spettro visibile all'occhio umano non sono uguali per tutte le persone, ma variano soggettivamente e possono raggiungere i 720 nanometri, avvicinandosi agli infrarossi, e i 380 nanometri avvicinandosi agli ultravioletti. La presenza contemporanea di tutte le lunghezze d'onda visibili, in quantità proporzionali a quelle della luce solare, forma la "luce bianca".
La luce, come tutte le onde elettromagnetiche, interagisce con la materia. I fenomeni che più comunemente influenzano o impediscono la trasmissione della luce attraverso la materia sono: l'assorbimento, la diffusione ("scattering"), la riflessione speculare o diffusa, la rifrazione e la diffrazione. La riflessione diffusa da parte delle superfici, da sola o combinata con l'assorbimento, è il principale meccanismo attraverso il quale gli oggetti si rivelano ai nostri occhi, mentre la diffusione da parte dell'atmosfera è responsabile della luminosità del cielo.
Sebbene nell'elettromagnetismo classico la luce sia descritta come un'onda, l'avvento della meccanica quantistica agli inizi del XX secolo ha permesso di capire che questa possiede anche proprietà tipiche delle particelle e di spiegare fenomeni come l'effetto fotoelettrico. Nella fisica moderna la luce (e tutta la radiazione elettromagnetica) viene composta da quanti, unità fondamentali di campo elettromagnetico chiamate anche fotoni.
Formulata da Isaac Newton nel XVII secolo. La luce veniva vista come composta da piccole particelle di materia (corpuscoli) emesse in tutte le direzioni. Oltre che essere matematicamente molto più semplice della teoria ondulatoria, questa teoria spiegava molto facilmente alcune caratteristiche della propagazione della luce che erano ben note all'epoca di Newton.
Innanzitutto la meccanica galileiana prevede, correttamente, che le particelle (inclusi i corpuscoli di luce) si propaghino in linea retta ed il fatto che questi fossero previsti essere molto leggeri era coerente con una velocità della luce alta ma non infinita. Anche il fenomeno della riflessione poteva essere spiegato in maniera semplice tramite l'urto elastico della particella di luce sulla superficie riflettente.
La spiegazione della rifrazione era leggermente più complicata ma tutt'altro che impossibile: bastava infatti pensare che le particelle incidenti sul materiale rifrangente subissero, ad opera di questo, delle forze perpendicolari alla superficie che ne aumentassero la velocità, cambiandone la traiettoria e avvicinandola alla direzione normale alla superficie.
I colori dell'arcobaleno venivano spiegati tramite l'introduzione di un gran numero di corpuscoli di luce diversi (uno per ogni colore) ed il bianco era pensato come formato da tante di queste particelle. La separazione dei colori ad opera, ad esempio, di un prisma poneva qualche problema teorico in più perché le particelle di luce dovrebbero avere proprietà identiche nel vuoto ma diverse all'interno della materia.
Formulata da Christiaan Huygens nel 1678, ma pubblicata solo nel 1690 nel "Traité de la Lumière", la luce veniva vista come un'onda che si propaga in un mezzo, chiamato etere, in maniera del tutto simile alle onde del mare o a quelle acustiche. Si supponeva che l'etere pervadesse tutto l'universo e fosse formato da microscopiche particelle elastiche. La teoria ondulatoria della luce permetteva di spiegare numerosi fenomeni: oltre alla riflessione ed alla rifrazione, Huygens riuscì infatti a spiegare anche il fenomeno della birifrangenza nei cristalli di calcite.
Nel 1801 Thomas Young dimostrò come i fenomeni della diffrazione (osservata per la prima volta da Francesco Maria Grimaldi nel 1665) e dell'interferenza fossero interamente spiegabili dalla teoria ondulatoria e non lo fossero dalla teoria corpuscolare. Agli stessi risultati arrivò Augustin-Jean Fresnel nel 1815. Nel 1814 Joseph von Fraunhofer fu il primo ad investigare seriamente sulle righe di assorbimento nello spettro del Sole, che vennero esaurientemente spiegate da Kirchhoff e da Bunsen nel 1859, con l'invenzione dello spettroscopio. Le righe sono ancora oggi chiamate linee di Fraunhofer in suo onore.
Il fatto che le onde siano capaci di aggirare gli ostacoli mentre la luce si propaga in linea retta (questa proprietà era già stata notata da Euclide nel suo "Optica") può essere facilmente spiegato assumendo che la luce abbia una lunghezza d'onda microscopica.
Al contrario della teoria corpuscolare, quella ondulatoria prevede che la luce si propaghi più lentamente all'interno di un mezzo che nel vuoto.
Per la grandissima maggioranza delle applicazioni questa teoria è ancora utilizzata al giorno d'oggi.
Proposta da James Clerk Maxwell alla fine del XIX secolo, sostiene che le onde luminose sono elettromagnetiche. La luce visibile è solo una piccola parte dello spettro elettromagnetico. Con la formulazione delle equazioni di Maxwell vennero completamente unificati i fenomeni elettrici, magnetici ed ottici. Per Maxwell, tuttavia, era ancora necessario un mezzo di diffusione dell'onda elettromagnetica, ossia l'etere. Solo più tardi si negò l'etere e si scoprì che la luce può propagarsi anche nel vuoto.
Per risolvere alcuni problemi sulla trattazione della radiazione emessa da corpo nero, Max Planck ideò nel 1900 un artificio matematico: pensò che l'energia associata ad un'onda elettromagnetica non fosse proporzionale al quadrato della sua ampiezza (come nel caso delle onde elastiche in meccanica classica), bensì inversamente proporzionale alla sua lunghezza d'onda, e che la sua costante di proporzionalità fosse discreta e non continua. 
L'interpretazione successiva che Albert Einstein diede dell'effetto fotoelettrico, indirizzò verso una nuova strada. Si cominciò a pensare che quello di Planck non fosse un mero artificio matematico, ma l'interpretazione di una nuova struttura fisica; cioè che la natura della luce potesse avere un qualche rapporto con una forma discreta di alcune sue proprietà. Si cominciò a parlare di pacchetti discreti d'energia, oggi chiamati fotoni.
La luce si propaga a una velocità finita. Anche gli osservatori in movimento misurano sempre lo stesso valore di "c", la velocità della luce nel vuoto, dove "c" = 299 792 458 m/s che viene approssimato in "c" = 300 000 000 m/s, mentre viaggia nell'acqua a circa 225 407 863 m/s e nel vetro a 185 057 072 m/s.
La velocità della luce è stata misurata molte volte da numerosi fisici. Il primo tentativo di misura venne compiuto da Galileo Galilei con l'ausilio di lampade oscurabili ma la rudimentalità dei mezzi disponibili non permise di ottenere alcun valore. La migliore tra le prime misurazioni venne eseguita da Olaus Roemer (un fisico danese), nel 1675. Egli sviluppò un metodo di misurazione, osservando Giove e una delle sue lune con un telescopio. Grazie al fatto che la luna veniva eclissata da Giove a intervalli regolari, calcolò il periodo di rivoluzione della luna in 42,5 ore, quando la Terra era vicina a Giove. Il fatto che il periodo di rivoluzione si allungasse quando la distanza tra Giove e Terra aumentava, poteva essere spiegato assumendo che la luce impiegava più tempo a coprire la distanza Giove-Terra, ipotizzando quindi, una velocità finita per essa. La velocità della luce venne calcolata analizzando la distanza tra i due pianeti in tempi differenti. Roemer calcolò una velocità di 227 326 km/s.
Albert A. Michelson migliorò il lavoro di Roemer nel 1926. Usando uno specchio rotante, misurò il tempo impiegato dalla luce per percorrere il viaggio di andata e ritorno dal monte Wilson al monte Sant'Antonio in California. La misura precisa portò a una velocità di 299 702 km/s.
Questo esperimento in realtà misurò la velocità della luce nell'aria. Infatti, quando la luce passa attraverso una sostanza trasparente, come l'aria, l'acqua o il vetro, la sua velocità "c" si riduce a "v = c/n" (dove "n" è il valore dell'indice di rifrazione del mezzo) ed è sottoposta a rifrazione. In altre parole, "n" = 1 nel vuoto e "n" &gt; 1 nella materia. L'indice di rifrazione dell'aria di fatto è molto vicino a 1, e in effetti la misura di Michelson è un'ottima approssimazione di "c."
L'ottica è lo studio della luce e dell'interazione tra la luce e materia. 
L'osservazione e lo studio dei fenomeni ottici offre molti indizi sulla natura stessa della luce; tra i primi si ricordano gli esperimenti di rifrazione della luce con prisma eseguiti da Newton tra il 1665 e il 1666.
Le conclusioni di Newton, secondo cui la luce era un fenomeno composto, furono contestate ai primi dell'Ottocento da Goethe, il quale nella sua "Teoria dei colori" osservò che non è la luce a scaturire dai colori, bensì il contrario: la luce è per Goethe un fenomeno «primario», di natura quasi spirituale, che interagendo con l'oscurità genera la varietà dei colori per effetto del suo maggiore o minore offuscamento.
Le differenti lunghezze d'onda vengono interpretate dal cervello come colori, che vanno dal rosso delle lunghezze d'onda maggiori (frequenze più basse) al violetto delle lunghezze d'onda minori (frequenze più alte).
Non a tutti i colori possiamo associare una lunghezza d'onda precisa. Non c'è, cioè, una relazione biunivoca tra i colori che noi percepiamo e le lunghezze d'onda. Quasi tutte le radiazioni luminose che il nostro occhio percepisce dall'ambiente circostante non sono del tutto "pure", ma sono in realtà una "sovrapposizione" di luci di diverse lunghezze d'onda. Se ad ogni lunghezza d'onda è associabile un colore, non è vero il contrario.
Quei colori a cui non sono associate lunghezze d'onda, sono invece generati dal meccanismo di funzionamento del nostro apparato visivo (cervello + occhio). In particolare i coni, cellule della retina responsabili della visione del colore, si differenziano in tre tipi perché sensibili a tre diverse regioni spettrali della luce, Coni M (sensibili alle lunghezze d'onda medie), coni S (sensibili alle lunghezze d'onda corte) e coni L (sensibili alle lunghezze d'onda lunghe). Quando al nostro occhio arriva luce composta da più onde monocromatiche, appartenenti a regioni diverse dello spettro, il nostro cervello interpreta i segnali provenienti dai tre tipi di sensori come un nuovo colore, "somma" di quelli originari. Il che è molto simile al procedimento inverso di quello che si fa con la riproduzione artificiale dei colori, per esempio con il metodo RGB.
Le frequenze immediatamente al di fuori dello spettro percettibile dall'occhio umano vengono chiamate "ultravioletto" (UV), per le alte frequenze, e "infrarosso" (IR) per le basse. Anche se noi non possiamo vedere l'infrarosso, esso viene percepito dai recettori della pelle come calore. Tutte le lunghezze d'onda dello spettro elettromagnetico, a partire dalla luce visibile - escludendo le parti minoritarie dei raggi x, delle onde radio e solo una porzione degli ultravioletti -, sono fonte di calore. Telecamere in grado di captare i raggi infrarossi e convertirli in luce visibile vengono chiamati visori notturni. Alcuni animali, come le api, riescono a vedere gli ultravioletti; altri invece riescono a vedere gli infrarossi. Gli ultravioletti di tipo b sono "i responsabili" delle scottature se l'esposizione solare è avvenuta in modo inadeguato.
La luce visibile è una porzione dello spettro elettromagnetico compresa approssimativamente tra i 400 e i 700 nanometri (nm) (nell'aria). Ogni singola radiazione elettromagnetica dello spettro è caratterizzata da una frequenza " f " e relativa lunghezza d'onda "λ", che obbediscono alla relazione:
dove v è la velocità della radiazione o fotone nel mezzo considerato.
Nel vuoto, essendo v = c (velocità della luce nel vuoto), la relazione diventa:
Di seguito sono riportate quantità o unità di misura legate a fenomeni luminosi:
La luce può essere prodotta a partire dalle seguenti sorgenti:
</text>
</doc>
<doc id="8775" url="https://it.wikipedia.org/wiki?curid=8775">
<title>Chilogrammo</title>
<text>
Il chilogrammo o kilogrammo, nell'uso parlato comunemente chilo (simbolo: kg), è l'unità di misura di base della massa nel Sistema internazionale di unità di misura (SI) corrispondente alla massa di ≈ mP, (relazione con la massa unitaria di Planck).
La 26ª Conferenza Generale dei Pesi e delle Misure tenuta a Versailles dal 13 al 16 novembre 2018 ha introdotto questa definizione: come per le altre unità di base del SI, dal 20 maggio 2019 è riferito ad una costante fisica, in particolare alla costante di Planck tramite una misurazione per mezzo della bilancia di Watt.
Storicamente, è nato come la massa di un litro (decimetro cubo) di acqua distillata alla temperatura di circa . 
Fino al 20 maggio 2019, era definito come la massa di un particolare cilindro di altezza e diametro di circa di una lega di platino-iridio depositato presso l'Ufficio internazionale dei pesi e delle misure a Sèvres, in Francia, chiamato anche prototipo internazionale.
Il chilogrammo è pari a mille grammi (simbolo: g).
Il kilogrammo è l'unica tra le unità di misura SI che, fino al 20 maggio 2019, era rimasta definita in relazione a un manufatto, e non da una proprietà fisica. Seguiva cioè ancora l'approccio tipico di un sistema tecnico.
Il grammo entrò a far parte del sistema metrico francese il 1º agosto 1793, definito come la massa di un centimetro cubo (10 m³) di acqua alla temperatura di 3,98 °C a pressione atmosferica standard. Questa particolare temperatura venne scelta poiché per questa, l'acqua, possiede la sua massima densità. Il 7 aprile 1795 fa la sua comparsa il "chilogrammo", come suo multiplo (1 kg = 1 000 g).
Tale definizione era difficile da realizzare accuratamente, anche perché la densità dell'acqua è legata in parte alla pressione, e l'unità di pressione include la massa tra i fattori, introducendo una dipendenza circolare nella definizione del chilogrammo.
Per evitare questo problema, il chilogrammo venne ridefinito come la massa "precisa" di una particolare massa standard, creata per approssimare la definizione originale e realizzata nel 1875. Il prototipo internazionale del chilogrammo «Le Grand Kilo», un cilindro retto a base circolare che misura 39 mm in altezza e diametro, composto da una lega di platino e iridio, è conservato al "Bureau International des Poids et Mesures" (Ufficio internazionale dei pesi e delle misure) presso il Pavillon de Breteuil a Sèvres, Francia.
Dal 1889, il SI definisce l'unità di misura come pari alla massa di questo prototipo. Copie ufficiali del prototipo sono rese disponibili come prototipi nazionali, che sono stati confrontati periodicamente con il prototipo di Parigi, negli anni 1889, 1946-1948, e 1988-1992; ulteriore comparazione straordinaria fu effettuata nel 2014.
Quando, nel 1889, si confrontò il campione del chilogrammo costruito nel 1875 con il precedente campione definito tramite la massa d'acqua, ci si accorse che essi non concordavano, essendo il manufatto equivalente a 1,000 027 dm³ di acqua. Alla scelta di mutare il campione del chilogrammo con un manufatto più preciso, i tecnici preferirono mantenere il campione già in uso da quattordici anni, abbandonando la definizione legata alla massa d'acqua.
Inoltre, ritenuto importante il volume legato alla definizione mantenuta, si introdusse il litro come volume di acqua alla temperatura di 3,98 °C a pressione atmosferica standard di massa uguale a un chilogrammo (una definizione soppiantata nel 1964 quando si decise di rendere il litro esattamente pari a un decimetro cubo).
Per definizione, l'errore nella replicabilità del prototipo era pari a zero; in realtà, era valutato nell'ordine dei 2 microgrammi. Questo risulta dalla comparazione del prototipo con le sue copie ufficiali, che sono, per quanto possibile, composte dallo stesso materiale e conservate nelle stesse condizioni. Non ci sono motivi per ritenere che il prototipo sia più o meno stabile delle copie ufficiali. Sembra che il chilogrammo originale abbia perso circa 50 microgrammi negli ultimi 100 anni per ragioni ancora sconosciute (probabilmente per sublimazione del metallo, infatti anch'essi col tempo "evaporano"), per questo motivo si è deciso di definire il chilogrammo attraverso delle leggi fisiche, e non più attraverso un campione di riferimento.
La conservazione del vecchio prototipo internazionale è stata realizzata con i più rigorosi criteri: viene utilizzato un sotterraneo blindato, per la cui apertura occorre l'uso contemporaneo di tre diverse chiavi, custodite da tre personalità del Bureau international des poids et mesures. L'apertura avviene previa autorizzazione del "Comitato Internazionale dei Pesi e delle Misure".
Le condizioni di temperatura, pressione e umidità sono costanti e si evita il contatto con la polvere tenendo il prototipo sotto tre campane di vetro. Tale prototipo veniva usato per le comparazioni (ogni 40 anni circa). Altri campioni dell'unità di massa, realizzati per gli stati aderenti alla "Convenzione del Metro" differiscono dal capostipite per ±0,3 mg. Sei di tali campioni servivano a ricostruire il prototipo internazionale nel malaugurato caso questo dovesse divenire inservibile.
La scomodità legata all'accessibilità dei campioni unita alla loro dubbia accuratezza hanno convinto i tecnici e gli scienziati a cercare una definizione di chilogrammo maggiormente soddisfacente.
In Italia il campione del chilogrammo è conservato a Roma presso il ministero dello Sviluppo Economico.
Per evitare conflitti con una vecchia unità di misura del peso, chiamata anch'essa kilogrammo, e con l'uso del prefisso "kilo" nel nome, nel 1954 la Commissione italiana di metrologia propose il nome "bes" (simbolo "b") per l'unità di misura di base della massa; ma la proposta non ricevette il sufficiente consenso internazionale e fu quindi ritirata. Nel 1988 fu la volta della Commissione svedese di metrologia a proporre di sostituire il termine "kilogrammo" con "GIO", per onorare il metrologo italiano Giovanni Giorgi, ma anche questa proposta non ebbe successo.
Il Decreto del Presidente della Repubblica n. 802 del 12 agosto 1982 ha recepito la Direttiva 80/181/CEE relativa alle unità di misura stabilendo "le unità di misura legali da utilizzare per esprimere grandezze"; nell'allegato, per la massa viene indicato il kilogrammo, definito come "pari alla massa del prototipo internazionale del kilogrammo (3ª CGPM, 1901, pag. 70 del resoconto)".
Dal 2010 al 2018 si è lavorato per introdurre una nuova definizione di chilogrammo, per mezzo di costanti fondamentali o atomiche. Ciò allo scopo di renderla maggiormente precisa e realizzabile in ogni laboratorio specializzato.
Le proposte avanzate sono state:
Con votazione avvenuta il 16 novembre 2018, il Bureau international des poids et mesures ha stabilito che, dopo il 20 maggio 2019, il chilogrammo viene definito tramite una proprietà fisica correlata ad una costante fondamentale, ossia come la quantità di massa per compensare una forza di 6,626 070 15 × 10 J s. Se per la sua misura si utilizza una Bilancia di Watt, è necessario che quest'ultima sia percorsa da una data quantità di corrente e, utilizzando le definizioni di volt e di ohm, tale misurazione è correlata alla Costante di Planck (6,626 070 15 × 10 J s).
I prefissi SI vengono usati per i multipli e i sottomultipli del chilogrammo. È da notare che, contrariamente alle altre unità di misura, il kilogrammo è l'unica unità di base che contiene già un prefisso ("kilo", inteso come 10 volte l'unità di misura), infatti multipli e sottomultipli si applicano al grammo.
Nell'uso comune inoltre, si segnalano alcune particolarità: il megagrammo (10 grammi) viene chiamato tonnellata, 100 chili (o 10 g), che non prevedono un prefisso SI, sono detti quintale. Per indicare 10 kg si usa il miriagrammo, ma ora è in disuso. Tali unità di misura, anche se non appartenenti al SI, sono da esso riconosciute in virtù del loro largo uso. Sempre nell'uso comune, ossia non in ambito propriamente scientifico o tecnico, il chilogrammo e l'ettogrammo vengono talvolta indicati semplicemente come "chilo" ed "etto", dando per scontata l'unità di misura.
Il microgrammo (simbolo: µg, a volte indicato con mcg) viene usato, insieme al milligrammo (mg), per indicare il dosaggio terapeutico di farmaci o microelementi. Ad esempio, il fabbisogno giornaliero di acido folico (vitamina B9) in una donna è di 200 µg (0,2 mg), ma in gravidanza è il doppio (400 µg = 0,4 mg).
La massa di una cellula uovo umana è circa 1 µg.
In alcuni contesti, e.g. in medicina, al posto del microgrammo, come unità di misura viene utilizzato il gamma (γ): 1 µg = 1 γ.
Questa piccolissima unità di misura, posta in rapporto con un volume (ad esempio ng/ml), viene usata per indicare concentrazioni bassissime di determinate sostanze, ad esempio: dosaggio terapeutico di alcuni farmaci, concentrazioni di enzimi, concentrazioni di sostanze inquinanti, impurità, doping, ecc.
</text>
</doc>
<doc id="22998" url="https://it.wikipedia.org/wiki?curid=22998">
<title>Elettroscopio</title>
<text>
L'elettroscopio è un apparecchio che permette di stabilire se un corpo è carico elettricamente. A differenza dell'elettrometro, non può quantificare la carica elettrica. Si tratta quindi di un rivelatore di carica.
L'apparecchio fu messo a punto da Alessandro Volta (anche se è invece stato recentemente riconosciuto il diritto d'invenzione a Tiberio Cavallo) verso il 1780.
L'elettroscopio è costituito da un pomello conduttore collegato a due sottili lamine metalliche chiamate "foglioline" (solitamente d'oro) che sono racchiuse in un recipiente di vetro per isolarlo. Avvicinando al pomello metallico un corpo caricato elettricamente, ad esempio una bacchetta di plastica strofinata con un panno di lana, si vedranno le due lamine divergere. La vicinanza del corpo elettricamente carico produce un fenomeno fisico detto induzione elettrostatica. Le due lamine diventeranno pertanto cariche dello stesso segno e si respingeranno. Allontanando il corpo carico, detto anche corpo induttore, le due lamine tornano ad avvicinarsi. Questo fenomeno può essere ripetuto nel tempo e se si tocca con il dito il pomello le foglioline rimangono cariche e restano separate, se poi si tocca nuovamente si scaricano e ritornano neutre.
</text>
</doc>
<doc id="18888" url="https://it.wikipedia.org/wiki?curid=18888">
<title>Effetto fotoelettrico</title>
<text>
Nella fisica dello stato solido l'effetto fotoelettrico è il fenomeno fisico di interazione radiazione-materia caratterizzato dall'emissione di elettroni da una superficie, solitamente metallica, quando questa viene colpita da una radiazione elettromagnetica, ossia da fotoni aventi una certa lunghezza d'onda.
La scoperta dell'effetto fotoelettrico va fatta risalire alla seconda metà del XIX secolo e ai tentativi di spiegare la conduzione nei liquidi e nei gas.
Nel 1887 Hertz, riprendendo e sviluppando gli studi di Schuster sulla scarica dei conduttori elettrizzati stimolata da una scintilla elettrica nelle vicinanze, si accorse che tale fenomeno è più intenso se gli elettrodi vengono illuminati con luce ultravioletta.
Nello stesso anno Wiedemann e Ebert stabilirono che la sede dell'azione di scarica è l'elettrodo negativo e Hallwachs trovò che la dispersione delle cariche elettriche negative è accelerata se i conduttori vengono illuminati con luce ultravioletta.
Nei primi mesi del 1888 il fisico italiano Augusto Righi, nel tentativo di capire i fenomeni osservati, scoprì un fatto nuovo: una lastra metallica conduttrice investita da una radiazione UV si carica positivamente. Righi introdusse, per primo, il termine "fotoelettrico" per descrivere il fenomeno.
Hallwachs, che aveva sospettato ma non accertato il fenomeno qualche mese prima di Righi, dopo qualche mese dimostrava, indipendentemente dall'italiano, che non si trattava di trasporto, ma di vera e propria produzione di elettricità.
Sulla priorità della scoperta tra i due scienziati si accese una disputa, riportata sulle pagine del Nuovo Cimento. La comunità scientifica tagliò corto e risolse la controversia chiamando il fenomeno "effetto Hertz-Hallwachs".
Fu poi Einstein nel 1905 a darne l'interpretazione corretta, intuendo che l'estrazione degli elettroni dal metallo si spiegava molto più coerentemente ipotizzando che la radiazione elettromagnetica fosse costituita da pacchetti di energia o quanti, poi denominati fotoni.
L'ipotesi quantistica di Einstein non fu accettata per diversi anni da una parte importante della comunità scientifica, tra cui Hendrik Lorentz, Max Planck e Robert Millikan (vincitori del Premio Nobel per la fisica, rispettivamente, nel 1902, 1918 e 1923), secondo i quali la reale esistenza dei fotoni era un'ipotesi inaccettabile, considerato che nei fenomeni di interferenza le radiazioni elettromagnetiche si comportano come onde. L'iniziale scetticismo di questi grandi scienziati dell'epoca non deve sorprendere dato che perfino Max Planck, che per primo ipotizzò l'esistenza dei quanti (anche se con riferimento agli atomi, che emettono e assorbono "pacchetti di energia"), ritenne, per diversi anni, che i quanti fossero un semplice artificio matematico e non un reale fenomeno fisico. Ma successivamente lo stesso Robert Millikan dimostrò sperimentalmente l'ipotesi di Einstein sull'energia del fotone, e quindi dell'elettrone emesso, che dipende soltanto dalla frequenza della radiazione, e nel 1916 effettuò uno studio sugli elettroni emessi dal sodio che contraddiceva la classica teoria ondulatoria di Maxwell.
L'aspetto corpuscolare della luce fu confermato definitivamente dagli studi sperimentali di Arthur Holly Compton. Infatti il fisico statunitense nel 1921 osservò che, negli urti con gli elettroni, i fotoni si comportano come particelle materiali aventi energia e quantità di moto che si conservano; nel 1923 pubblicò i risultati dei suoi esperimenti (effetto Compton) che confermavano in modo indiscutibile l'ipotesi di Einstein: la radiazione elettromagnetica è costituita da quanti (fotoni) che interagendo con gli elettroni si comportano come singole particelle. Per la scoperta dell'effetto omonimo Arthur Compton ricevette il premio Nobel nel 1927.
Per i suoi studi sull'effetto fotoelettrico e la conseguente scoperta dei quanti di luce Einstein ricevette il Premio Nobel per la fisica nel 1921.
L'effetto fotoelettrico fu rivelato da Hertz nel 1887 nell'esperimento ideato per generare e rivelare le onde elettromagnetiche; in quell'esperimento, Hertz usò uno spinterometro in un circuito accordato per generare onde e un altro circuito simile per rivelarle. Nel 1900 Lenard studiò tale effetto, trovando che la luce incidente su una superficie metallica provoca l'emissione di elettroni, la cui energia non dipende dall'intensità della luce, ma dal suo colore, cioè dalla frequenza.
Quando la luce colpisce una superficie metallica pulita (il catodo "C") vengono emessi elettroni. Se alcuni di questi colpiscono l'anodo "A", si misura una corrente nel circuito esterno. Il numero di elettroni emessi che raggiungono l'anodo può essere aumentato o diminuito rendendo l'anodo positivo o negativo rispetto al catodo.
Detta "V" la differenza di potenziale tra "A" e "C", si può vedere che solo da un certo potenziale in poi (detto "potenziale d'arresto") la corrente inizia a circolare, aumentando fino a raggiungere un valore massimo, che rimane costante. Questo massimo valore è, come scoprì Lenard, direttamente proporzionale all'intensità della luce incidente. Il potenziale d'arresto è legato all'energia cinetica massima degli elettroni emessi dalla relazione
dove "m" è la massa dell'elettrone, "v" la sua velocità, "e" la sua carica.
Ora, la relazione che lega le due grandezze è proprio quella indicata perché se "V" è negativo, gli elettroni vengono respinti dall'anodo, tranne se l'energia cinetica consente loro, comunque, di arrivare su quest'ultimo. D'altra parte si notò che il potenziale d'arresto non dipendeva dall'intensità della luce incidente, sorprendendo lo sperimentatore, che si aspettava il contrario. Infatti, classicamente, il campo elettrico portato dalla radiazione avrebbe dovuto mettere in vibrazione gli elettroni dello strato superficiale fino a strapparli al metallo. Usciti, la loro energia cinetica sarebbe dovuta essere proporzionale all'intensità della luce incidente e non alla sua frequenza, come invece sembrava risultare sperimentalmente.
Come comprese Einstein, riprendendo la teoria di Planck, l'effetto fotoelettrico evidenzia la natura quantistica della luce. Nella radiazione elettromagnetica, l'energia non è distribuita in modo uniforme sull'intero fronte dell'onda ma è concentrata in singoli quanti (pacchetti discreti) di energia, i fotoni. Un solo fotone per volta, e non l'intera onda nel suo complesso, interagisce singolarmente con un elettrone, al quale cede la sua energia. Affinché ciò si verifichi è necessario che il singolo fotone abbia un'energia sufficiente a rompere il legame elettrico che tiene legato l'elettrone all'atomo. Questa "soglia minima" di energia del fotone si determina in base alla relazione di Planck
dove h è la costante di Planck, f e λ sono rispettivamente la frequenza e la lunghezza d'onda del fotone, c è la velocità della luce (ricordando la relazione formula_3).
In altri termini, l'elettrone può uscire dal metallo solo se l'energia del fotone è almeno uguale al “lavoro di estrazione” (formula_4). Esiste, pertanto, una “soglia minima” di estrazione per ogni metallo, che fa riferimento o alla lunghezza d'onda o alla frequenza del fotone incidente e, quindi, alla sua energia “hf”, la quale coincide con il “lavoro di estrazione” (Wₑ).
Il valore di soglia varia in base al tipo di materiale considerato (in genere metalli) e dipende, pertanto, dalle sue caratteristiche atomiche; anche il grado di purezza del metallo influisce sul valore di soglia (per tale motivo i testi o i siti specializzati riportano spesso valori di soglia differenti per lo stesso metallo).
Nella tabella che segue sono riportati i valori di soglia di alcuni metalli. Il dato iniziale noto è quello del lavoro di estrazione in eV (col. 2), che equivale al valore di soglia del fotone (in eV) incidente sul metallo considerato; i valori di soglia riportati nelle colonne 3, 4 e 5 sono stati ricavati dalle rispettive formule.
VALORI DI SOGLIA PER L'EMISSIONE DI ELETTRONI DA UN METALLO
Si precisa che:
Spesso il parametro di soglia iniziale noto è:
Con l'aumentare dell'energia dei fotoni incidenti (ossia quando aumenta “ f ” oppure quando diminuisce “ λ ”) aumenta anche l'energia cinetica degli elettroni estratti.
Va in proposito sottolineato che aumentando l'intensità della radiazione elettromagnetica (ossia il numero di fotoni al secondo, di pari energia, che colpiscono l'unità di superficie) aumenta il numero degli elettroni estratti ma non la loro energia cinetica, la quale dipende esclusivamente dall'energia dei fotoni incidenti. Questa è una conseguenza della teoria quantistica di Einstein, in base alla quale ogni fotone incidente interagisce soltanto con un singolo elettrone. Infatti secondo la teoria ondulatoria classica di Maxwell l'estrazione di elettroni dal metallo dipende dall'intensità dell'irradiamento per unità di superficie (che deve raggiungere un valore sufficiente) e prescinde, quindi, dalla frequenza della radiazione incidente (ipotesi, questa, smentita dalle evidenze sperimentali).
L'effetto fotoelettrico, oggetto di studi da parte di molti fisici, è stato fondamentale per comprendere la natura quantistica della luce.
Un caso particolare di effetto fotoelettrico è l'effetto fotovoltaico.
Einstein, nel lavoro del 1905 che gli fruttò il Premio Nobel per la fisica nel 1921, fornisce una spiegazione dei fatti sperimentali partendo dal principio che la radiazione incidente possiede energia quantizzata. Infatti i fotoni che arrivano sul metallo cedono energia agli elettroni dello strato superficiale del solido; gli elettroni acquisiscono così l'energia necessaria per rompere il legame: in questo senso l'ipotesi più semplice è che il fotone ceda all'elettrone tutta l'energia in suo possesso. A questo punto l'elettrone spenderà parte dell'energia per rompere il legame e parte incrementerà la sua energia cinetica che gli permetterà di arrivare in superficie e abbandonare il solido: da qui si può capire che saranno gli elettroni eccitati più vicini alla superficie ad avere la massima velocità normale alla stessa. Per questi, posto "P" il lavoro (che varia da sostanza a sostanza) utile all'elettrone per uscire, si avrà che l'energia cinetica è pari a:
A questo punto detta formula_6 la carica dell'elettrone e "Π" il potenziale positivo del corpo e tale da impedire perdita di elettricità allo stesso (il potenziale di arresto), si può scrivere:
oppure, con i simboli consueti
che diventa
dove "E" è la carica di un grammo-equivalente di uno ione monovalente e P il potenziale di questa quantità.
Ponendo, poi, "E" = 9,6 · 10, "Π" · 10 rappresenterà il potenziale in volt del corpo in caso di irradiazione nel vuoto.
Ora, ponendo "P"' = 0, "ν" = 1,03·10 (limite dello spettro solare dalla parte ultravioletta), "β" = 4,866·10, si ottiene "Π"·10 = 4,3V: il risultato trovato è così in accordo, per quanto riguarda gli ordini di grandezza, con quanto trovato da Lenard.
Si può concludere che:
I risultati matematici cambiano se si rifiuta l'ipotesi di partenza (energia trasmessa totalmente)
che diventa
per la fotoluminescenza, che è il processo inverso.
Se poi la formula è corretta, "Π"("ν") riportata sugli assi cartesiani risulterà una retta con pendenza indipendente dalla sostanza. Nel 1916 Millikan eseguì la verifica sperimentale di tale fatto, misurando il potenziale d'arresto e trovando che questo è una retta di "ν" con pendenza "h"/"e", come previsto.
Le normali cellule fotelettriche dei cancelli automatici funzionano basandosi sull'effetto fotoelettrico: una sorgente elettromagnetica di una cellula sorgente irradia elettromagneticamente a distanza una cellula ricevente che funge da ricevitore trasformandosi in interruttore per il sistema elettromeccanico.
</text>
</doc>
<doc id="8893" url="https://it.wikipedia.org/wiki?curid=8893">
<title>Hertz</title>
<text>
L'hertz (simbolo Hz) è l'unità di misura del Sistema Internazionale della frequenza. Prende il nome dal fisico tedesco Heinrich Rudolf Hertz che portò importanti contributi alla scienza, nel campo dell'elettromagnetismo.
oppure:
Tale unità di misura può essere applicata a qualsiasi evento periodico. Un hertz equivale ad un impulso al secondo, per cui, per esempio, si può dire che il ticchettio di un orologio ha una frequenza di 1 Hz.
Nelle vecchie pubblicazioni si trova l'unità equivalente "cicli per secondo" (cps).
</text>
</doc>
<doc id="8916" url="https://it.wikipedia.org/wiki?curid=8916">
<title>Pianeta</title>
<text>
Un pianeta è un corpo celeste che orbita attorno a una stella e che, a differenza di questa, non produce energia tramite fusione nucleare, la cui massa è sufficiente a conferirgli una forma sferoidale, laddove la propria dominanza gravitazionale gli permette di mantenere libera la sua fascia orbitale da altri corpi di dimensioni comparabili o superiori.
Questa definizione è entrata ufficialmente nella nomenclatura astronomica il 24 agosto 2006, con la sua promulgazione ufficiale da parte dell'Unione Astronomica Internazionale. In precedenza non esisteva una definizione precisa, ma un'atavica indicazione derivante dall'antica astronomia greca, per cui si considerava pianeta qualunque corpo celeste dotato di massa significativa che si muovesse su orbite fisse.
Nell'antichità, come rivela l'etimologia del termine "pianeta" (in greco antico "plànētes astéres", stelle vagabonde), venivano considerati tali tutti gli astri che si spostavano nel cielo notturno rispetto allo sfondo delle stelle fisse, ovvero la Luna, il Sole, Mercurio, Venere, Marte, Giove e Saturno, escluse le comete, che venivano considerate fenomeni atmosferici.
Nel XVI secolo, con l'affermarsi del sistema eliocentrico, divenne chiaro che Luna e Sole non condividevano in realtà la natura fisica e le caratteristiche orbitali proprie degli altri pianeti e che anche la Terra doveva essere inclusa nel novero dei pianeti.
Nel 1781 venne scoperto Urano, il primo pianeta che non era noto agli astronomi greci. Nei successivi 150 anni sarebbero stati individuati, in successione, altri due pianeti, Nettuno e Plutone; quest'ultimo è stato annoverato tra i pianeti dalla scoperta nel 1930 fino al 2006, anno in cui venne promulgata la nuova definizione di pianeta.
Inoltre a partire dal 1801 vennero progressivamente scoperti oltre centomila corpi di dimensioni subplanetarie, orbitanti attorno al Sole principalmente nella regione di spazio compresa fra l'orbita marziana e quella gioviana, la cosiddetta fascia principale. Sebbene in un primo tempo questi corpi fossero designati come pianeti, in virtù del loro numero sempre crescente vennero presto definiti come una classe di oggetti a sé: gli asteroidi. Fra di essi, solo poche decine sono caratterizzati da una forma approssimativamente sferica.
Lo schema dei nove pianeti classici rimase inalterato fino agli anni novanta del XX secolo; tuttavia alla fine del 2002 le moderne tecniche osservative avevano già permesso l'individuazione di oltre cento corpi di questo tipo, fra pianeti extrasolari e planetoidi ghiacciati orbitanti nelle regioni periferiche del sistema solare esterno. In particolare nel caso di questi ultimi la scoperta di corpi dalle dimensioni confrontabili o addirittura maggiori di quelle di Plutone, il più piccolo dei nove pianeti, riaccese un forte dibattito sulla necessità di promulgare una definizione precisa di "pianeta". Il problema nasceva dal fatto che la classificazione dei corpi celesti derivava in parte dall'astronomia dell'antica Grecia, che si limitava a chiarire che un pianeta era un qualsiasi corpo celeste che si muovesse lungo orbite fisse (o "schemi"). Questa descrizione era stata limata col tempo fino a quella corrente, che tuttavia peccava in vaghezza e in genericità.
Nel 2005 l'Unione Astronomica Internazionale (UAI) istituì il "Comitato per la definizione di pianeta" (PDC), composto da sette esperti riconosciuti a livello mondiale, cui assegnò il compito di fornire una definizione precisa del termine. Nel corso della ventiseiesima Assemblea generale dell'UAI, avvenuta dal 14 al 25 agosto 2006, la risoluzione proposta dal comitato fu discussa e modificata e il 24 agosto 2006 venne ufficializzata. Precedentemente considerato un pianeta, da questa data Plutone fu ridefinito, assieme ad altri corpi di recente scoperta, come pianeta nano.
I nomi dei pianeti nella cultura occidentale sono derivati dalle consuetudini dei Romani, che in ultima analisi derivano da quelle dei Greci e dei Babilonesi. Nell'antica Grecia, il Sole e la Luna erano chiamati "Elio" e "Selēnē"; il pianeta più lontano era chiamato "Phàinōn", il "più luminoso"; il penultimo pianeta era "Phaéthon", il "brillante"; il pianeta rosso era indicato come "Pyróeis", l'"ardente"; il più luminoso era conosciuto come "" "Phōsphóros", il "portatore di luce", mentre il fugace pianeta più interno era chiamato "Stílbōn", "lo splendido". Inoltre i Greci associarono ogni pianeta a una divinità del loro pantheon, gli Olimpi: Elio e Selene erano i nomi sia dei pianeti, sia degli dèi; "Phainon" era sacro a Crono, il Titano che generò gli Olimpi; "Phaethon" era sacro a Zeus, figlio di Crono; "Pyroeis" ad Ares, figlio di Zeus e dio della guerra; "Phosphoros" era retto da Afrodite, la dea dell'amore; mentre Hermes, messaggero degli dei e dio dell'apprendimento e dell'ingegno, dominava "Stilbon".
L'abitudine greca di dare i nomi dei propri dei ai pianeti derivò quasi certamente da quella dei Babilonesi, che indicavano Phosphoros con il nome della propria dea dell'amore, Ishtar; Pyroeis era identificato dal dio della guerra, Nergal; Stilbon dal dio della saggezza, Nabu, e Phaethon dal capo degli dei, Marduk. Le concordanze tra i due sistemi di nomenclatura sono troppe, perché essi possano essere stati sviluppati in modo indipendente. La corrispondenza tra le divinità non era perfetta. Per esempio, Nergal fu identificato con Ares; tuttavia Nergal era per i Babilonesi, oltre che il dio della guerra, anche la divinità delle pestilenze e dell'oltretomba.
Oggi i nomi utilizzati per designare i pianeti nella maggior parte delle culture occidentali derivano da quelli delle divinità olimpiche, spesso in una versione mutuata dalla mitologia romana. Infatti l'influenza dell'Impero romano prima e della Chiesa cattolica poi ha portato all'adozione dei nomi in latino. Inoltre il "pantheon" romano, in conseguenza della comune origine indoeuropea, aveva numerose similitudini con quello greco, sebbene mancasse di una ricca tradizione narrativa. Durante l'ultimo periodo della Repubblica romana, gli scrittori romani attinsero ai miti greci e li estesero alle proprie divinità, al punto che i due pantheon divennero quasi indistinguibili. In seguito, quando i Romani studiarono i testi di astronomia dei Greci, assegnarono ai pianeti i nomi delle proprie divinità: Mercurio (per Hermes), Venere (per Afrodite), Marte (per Ares), Giove (per Zeus) e Saturno (per Crono). Quando nei secoli XVIII e XIX furono scoperti nuovi pianeti, la comunità internazionale scelse di proseguire nella tradizione e furono nominati Urano e Nettuno.
Secondo una credenza originatasi in Mesopotamia, sviluppatasi nell'Egitto ellenistico e in seguito diffusasi anche tra i Romani, le sette divinità da cui i pianeti erano nominati si prendevano cura degli affari della Terra con turni orari, stabiliti in base alla distanza dal nostro pianeta nell'ordine seguente: Saturno, Giove, Marte, il Sole, Venere, Mercurio e la Luna. Il giorno era intitolato al dio che ne reggeva la prima ora, così al giorno dedicato a Saturno, che reggeva la prima ora del primo giorno e della settimana, seguiva quello dedicato al Sole, che reggeva la venticinquesima ora della settimana e la prima del secondo giorno, a cui seguivano i giorni dedicati alla Luna, a Marte, Mercurio, Giove e Venere. Quest'ordine è stato quindi ripreso dall'ordine dei giorni della settimana nel calendario romano che sostituì il ciclo nundinale e che ancora oggi è preservato in numerose lingue e culture. Nella maggior parte delle lingue romanze, i nomi dei primi cinque giorni della settimana sono traduzione diretta delle originarie espressioni latine: ad esempio da "lunae dies" derivano lunedì, in italiano; "lundi" in francese, "lunes" in spagnolo. Differentemente è accaduto per il sabato e la domenica, i cui nomi hanno subito l'influsso della tradizione della Chiesa. Invece nelle lingue germaniche è stato preservato il significato originario dei nomi di questi due giorni. A titolo di esempio, le parole inglesi "Sunday" e "Saturday" tradotte letteralmente significano: "giorno del Sole" e "giorno di Saturno"; analogamente è accaduto per il lunedì. Invece i nomi dei restanti giorni della settimana sono stati riassegnati a dèi considerati simili o equivalenti alle corrispondenti divinità romane.
Poiché la Terra fu classificata tra i pianeti solo nel XVII secolo, a essa non è generalmente associato il nome di una divinità. Nelle lingue romanze il suo nome deriva dalla parola latina "terra"; mentre nelle lingue germaniche dalla parola "*erþā", da cui derivano le forme "Earth" in inglese, "Erda" e, la più recente, "Erde" in tedesco, "Aarde" in olandese e "Jorden" (forma determinata di "jord") nelle lingue scandinave; tutte col significato di "suolo". In greco si è preservato il nome originario: "Ghê" (Gea o Gaia).
Le culture non europee adottano altri sistemi di nomenclatura planetaria. In India essa è basata sul Navagraha, che include i sette pianeti tradizionali (Sūrya per il Sole, Chandra per la Luna e Budha, Shukra, Mangala, Bṛhaspati e Shani per i pianeti Mercurio, Venere, Marte, Giove e Saturno) e i nodi ascendente e discendente dell'orbita della Luna come Rahu e Ketu. La Cina e i Paesi dell'Estremo Oriente influenzati dalla sua cultura (come Giappone, Corea e Vietnam) usano una nomenclatura basata sul Wu Xing (la teoria dei cinque elementi): Mercurio è identificato con l'acqua, Venere con il metallo, Marte con il fuoco, Giove con il legno e Saturno con la terra.
Tutti i pianeti, a eccezione dei pianeti interstellari, orbitano attorno a stelle o comunque oggetti sub-stellari. L'orbita percorsa da un pianeta attorno alla propria stella è descritta dalle leggi di Keplero: «i pianeti orbitano su orbite ellittiche, di cui la stella occupa uno dei fuochi.» Nel sistema solare tutti i pianeti orbitano intorno al Sole nella stessa direzione di rotazione del Sole, quindi in senso anti-orario, se visto dal polo nord della nostra stella. Tuttavia si è visto che almeno un pianeta extrasolare, WASP-17b, si muove in direzione opposta a quella in cui ruota la stella.
Il periodo che un pianeta impiega per compiere una rivoluzione completa intorno alla stella è conosciuto come periodo siderale o anno. La massima distanza tra il pianeta e il centro dell'orbita è detta semiasse maggiore. L'anno di un pianeta dipende dal valore del semiasse maggiore dell'orbita che esso percorre: più è grande, maggiore è la distanza che deve percorrere il pianeta lungo la propria orbita e con minor velocità, perché meno attratto dalla gravità della stella. La distanza tra il pianeta e la stella varia nel corso del periodo siderale. Il punto in cui il pianeta è più vicino alla stella viene chiamato periastro (perielio nel sistema solare), mentre il punto più lontano è chiamato afastro o apoastro (afelio nel sistema solare). Al periastro la velocità del pianeta è massima, convertendo l'energia gravitazionale in energia cinetica; all'apoastro la velocità assume il suo valore minimo.
L'orbita di ogni pianeta è descritta attraverso sei parametri orbitali: il semiasse maggiore; l'eccentricità, l'inclinazione orbitale, l'ascensione retta del nodo ascendente, l'argomento del perielio o pericentro e l'anomalia vera. L'eccentricità descrive la forma dell'orbita: le orbite caratterizzate da una piccola eccentricità sono più circolari, mentre quelle con eccentricità maggiori ellissi più schiacciate. I pianeti del sistema solare percorrono orbite con basse eccentricità e pertanto quasi circolari. Invece le comete e gli oggetti della fascia di Kuiper, così come alcuni pianeti extrasolari, hanno orbite molto eccentriche e quindi particolarmente allungate.
L'inclinazione e l'ascensione retta del nodo ascendente sono due parametri angolari che individuano la disposizione del piano orbitale nello spazio. L'inclinazione è misurata rispetto al piano dell'orbita della Terra (piano dell'eclittica) per i pianeti del sistema solare, mentre per i pianeti extrasolari si usa il piano di vista dell'osservatore da terra. Gli otto pianeti del sistema solare giacciono molto vicini al piano dell'eclittica; le comete e gli oggetti della fascia di Kuiper invece possono discostarsene molto.
I punti in cui il pianeta attraversa il piano dell'eclittica sono detti nodi, ascendente o discendente in base alla direzione del moto. L'ascensione retta del nodo ascendente è misurata rispetto a una direzione di riferimento, individuata nel sistema solare dal punto d'Ariete. "L'argomento del pericentro" specifica l'orientazione dell'orbita all'interno del piano orbitale, mentre "l'anomalia vera" la posizione dell'oggetto sull'orbita in funzione del tempo. A questi parametri possono essere affiancati o sostituiti degli altri che sono una loro rielaborazione, come il tempo di passaggio al perielio, equivalente nella meccanica kepleriana all'indicazione dell'argomento del pericentro, o il periodo orbitale, equivalente all'asse maggiore per la terza legge di Keplero.
Diversi pianeti e pianeti nani del sistema solare, come Nettuno e Plutone e alcuni pianeti extrasolari, hanno periodi orbitali che sono in risonanza l'un con l'altro o con corpi più piccoli. Questo fenomeno è comune anche nei sistemi dei satelliti.
I pianeti ruotano attorno ad assi invisibili che passano per il loro centro. Il periodo di rotazione di un pianeta è conosciuto come il suo giorno. La maggior parte dei pianeti del sistema solare ruota nello stesso verso in cui orbitano attorno al Sole, ovvero in verso antiorario se guardati dal polo nord celeste; le uniche eccezioni sono Venere e Urano che ruotano in verso orario. A causa dell'estrema inclinazione dell'asse di Urano esistono due convenzioni che si differenziano nel polo che scelgono come nord e, di conseguenza, nell'indicare come oraria o antioraria la rotazione attorno a questo polo; la rotazione di Urano è retrograda rispetto alla sua orbita, indipendentemente dalla convenzione adottata. Grande è la variabilità della durata del giorno tra i pianeti, con Venere che completa una rotazione in 243 giorni terrestri e i giganti gassosi che la completano in poche ore. Non sono noti i periodi di rotazione dei pianeti extrasolari finora scoperti. Tuttavia, per quanto riguarda i pianeti gioviani caldi, la loro prossimità alle stelle attorno a cui orbitano suggerisce che siano in rotazione sincrona, ovvero che il loro periodo di rotazione sia uguale al periodo di rivoluzione; di conseguenza essi mostrano sempre la stessa faccia alla stella intorno a cui orbitano e mentre su un emisfero è perpetuamente giorno, sull'altro è perpetuamente notte.
L'asse intorno a cui ruota il pianeta può essere, e in genere è, inclinato rispetto al piano orbitale. Ciò determina che nel corso dell'anno il quantitativo di luce che ogni emisfero riceve dalla stella vari: quando l'emisfero settentrionale è diretto verso essa e riceve maggiore illuminazione, quello meridionale si trova nella condizione opposta, e viceversa. È l'inclinazione dell'asse di rotazione a comportare l'esistenza delle stagioni e i cambiamenti climatici annuali a esse associate.
I momenti in cui la stella illumina la superficie massima o minima di un emisfero sono detti solstizi. Ve ne sono due nel corso dell'orbita e a essi corrisponde la durata massima (solstizio d'estate) e minima (solstizio d'inverno) del giorno. I punti dell'orbita in cui il piano equatoriale e il piano orbitale del pianeta vengono a giacere sullo stesso piano sono detti equinozi. Agli equinozi la durata del giorno eguaglia la durata della notte e la superficie illuminata si divide equamente tra i due emisferi geografici.
Tra i pianeti del sistema solare la Terra, Marte, Saturno e Nettuno possiedono valori dell'inclinazione dell'asse di rotazione prossimi ai 25°. Mercurio, Venere e Giove ruotano attorno ad assi inclinati di pochi gradi rispetto ai rispettivi piani orbitali e le variazioni stagionali sono minime. Urano possiede l'inclinazione assiale maggiore, pari a circa 98° e ruota praticamente su un fianco. I suoi emisferi in prossimità dei solstizi sono quasi perennemente illuminati o perennemente in ombra. La durata delle stagioni è determinata dalla dimensione dell'orbita: su Venere durano circa 55-58 giorni, sulla Terra 90-93 giorni, su Marte sei mesi, su Nettuno quarant'anni.
Le inclinazioni assiali dei pianeti extrasolari non sono state determinate con certezza. Gli studiosi ritengono che la maggior parte dei pianeti gioviani caldi possegga inclinazioni assiali nulle o quasi, in conseguenza della prossimità alla loro stella.
La caratteristica dinamica che definisce un pianeta è la dominanza orbitale. Un pianeta è gravitazionalmente dominante, o "avrà ripulito le proprie vicinanze orbitali" (riportando le parole utilizzate nella definizione di pianeta approvata dall'Unione Astronomica Internazionale) se nella propria zona orbitale non orbiteranno altri corpi di dimensioni comparabili a quelle del pianeta che non siano o suoi satelliti o comunque a esso gravitazionalmente legati. Questa caratteristica è la discriminante tra pianeti e pianeti nani. Sebbene questo criterio a oggi sia applicato soltanto al sistema solare, sono stati scoperti diversi sistemi planetari extrasolari in formazione in cui si osserva in atto il processo che condurrà alla formazione di pianeti gravitazionalmente dominanti.
La principale caratteristica fisica che consente di identificare un pianeta è la sua massa. Un pianeta deve possedere una massa sufficientemente elevata affinché la sua gravità domini sulle forze elettromagnetiche, presentandosi in uno stato di equilibrio idrostatico; più semplicemente, ciò significa che tutti i pianeti possiedono una forma sferica o sferoidale. Infatti un corpo celeste può assumere una forma irregolare se possiede una massa inferiore a un valore limite, che è funzione della propria composizione chimica; superato questo valore si innesca un processo di collasso gravitazionale che lo conduce, con tempi più o meno lunghi, ad assumere una forma sferica.
La massa è anche il principale attributo che consente di distinguere un pianeta da una nana bruna. Il limite superiore per la massa di un corpo planetario equivale a circa 13 volte la massa di Giove, valore oltre il quale nel nucleo del corpo celeste si raggiungono le condizioni adatte per la fusione del deuterio, il che rende l'oggetto una nana bruna. A parte il Sole, nel sistema solare non esiste alcun altro oggetto con una massa superiore a questo valore; tuttavia sono stati scoperti numerosi oggetti extra-solari con masse che si avvicinano a questo valore limite e che possono essere definiti pertanto pianeti. L'"Extrasolar Planets Encyclopedia" (Enciclopedia dei pianeti extrasolari) ne riporta una lista, che comprende HD 38529 c, AB Pictoris b, HD 162020 b, e HD 13189 b.
Il più piccolo pianeta conosciuto, escludendo pianeti nani e satelliti, è PSR B1257+12A, uno dei primi pianeti extrasolari scoperti, individuato nel 1992 in orbita intorno a una pulsar; la sua massa è circa la metà di quella del pianeta Mercurio.
 
Ogni pianeta ha iniziato la sua esistenza in uno stato fluido; nelle fasi iniziali della sua formazione i materiali più densi e più pesanti sono affondati verso il centro del corpo, lasciando i materiali più leggeri in prossimità della superficie. Ogni pianeta ha quindi un interno differenziato, costituito da un nucleo denso circondato da un mantello, che può presentarsi allo stato fluido.
I pianeti terrestri sono sigillati all'interno di una crosta dura, mentre nei giganti gassosi il mantello si dissolve semplicemente negli strati nuvolosi superiori.
I pianeti terrestri posseggono nuclei di elementi ferromagnetici, quali ferro e nichel, e mantelli di silicati. Si ritiene che Giove e Saturno posseggano nuclei composti da rocce e metalli, circondati da idrogeno metallico. Urano e Nettuno, più piccoli, posseggono nuclei rocciosi, circondati da mantelli composti da ghiacci d'acqua, ammoniaca, metano e di altre sostanze volatili. I moti dei fluidi in prossimità dei nuclei planetari determina l'esistenza di un campo magnetico.
Tutti i pianeti del sistema solare hanno un'atmosfera, dal momento che la gravità associata alle loro grandi masse è abbastanza forte da intrappolare le particelle gassose. I giganti gassosi sono sufficientemente massicci da trattenere grandi quantitativi di gas leggeri come idrogeno ed elio, mentre i pianeti più piccoli li perdono nello spazio. L'atmosfera terrestre è diversa rispetto a quelle degli altri pianeti. Infatti i processi vitali che hanno luogo sul pianeta ne hanno alterato la composizione, arricchendola di ossigeno molecolare (O). Mercurio è l'unico pianeta del sistema solare che possiede un'atmosfera estremamente tenue, che è stata soffiata via per la maggior parte, sebbene non totalmente, dal vento solare.
Le atmosfere planetarie ricevono energia in vario grado dal Sole e dagli strati planetari più interni; ciò determina il verificarsi di fenomeni meteorologici quali cicloni sulla Terra, tempeste di sabbia che interessano l'intero Marte, tempeste cicloniche e anticicloniche, come ad esempio la celebre Grande Macchia Rossa su Giove, e forti venti sui giganti gassosi. Anche sui pianeti extrasolari sono state identificate tracce di attività meteorologica: su HD 189733 b è stata individuata una tempesta simile alla Grande Macchia Rossa, ma due volte più ampia.
Si è visto che alcuni pianeti gioviani caldi perdono la loro atmosfera nello spazio a causa delle radiazioni e del vento stellare in modo molto simile a quanto accade alle code delle comete: è quanto accade ad esempio per HD 209458 b. È stato ipotizzato che su questi pianeti si verifichi una grande escursione termica diurna e che possono pertanto svilupparsi venti supersonici tra l'emisfero illuminato e quello in ombra, con velocità che nel caso di HD 209458 b sono comprese tra 5000 e 10 000 km/h. Osservazioni eseguite su HD 189733 b sembrano indicare che l'emisfero buio e l'emisfero illuminato abbiano temperature molto simili, a indicazione del fatto che l'atmosfera del pianeta ridistribuisce globalmente e con elevata efficienza l'energia ricevuta dalla stella.
Una caratteristica importante dei pianeti è l'esistenza di un momento magnetico intrinseco, che indica che il pianeta è ancora geologicamente attivo o, in altre parole, che al suo interno esistono ancora moti convettivi di materiali elettricamente conduttivi che generano il campo. La presenza di un campo magnetico planetario modifica significativamente l'interazione tra il pianeta e il vento stellare; infatti attorno al pianeta si crea una "cavità" (una zona dello spazio in cui il vento solare non riesce a entrare) chiamata magnetosfera, che può raggiungere dimensioni molto più grandi rispetto al pianeta stesso. Al contrario, pianeti che non posseggono un campo magnetico intrinseco sono circondati da piccole magnetosfere indotte dall'interazione della ionosfera con il vento solare, che non sono in grado di proteggere efficacemente il pianeta.
Degli otto pianeti del sistema solare, solo Venere e Marte mancano di un campo magnetico intrinseco, mentre ne possiede uno la più grande luna di Giove, Ganimede. Il campo magnetico intrinseco di Ganimede è diverse volte più forte di quello di Mercurio, il più debole tra quelli posseduti dai pianeti e appena sufficiente a deflettere il vento solare. Il campo magnetico planetario più forte all'interno del sistema solare è quello di Giove. Le intensità dei campi magnetici degli altri giganti gassosi sono pressappoco simili a quella del campo terrestre, sebbene i loro momenti magnetici siano significativamente più grandi. I campi magnetici di Urano e Nettuno sono fortemente inclinati rispetto ai rispettivi assi di rotazione e scostati rispetto al centro del pianeta.
Nel 2004 un gruppo di astronomi delle Hawaii ha osservato un pianeta extrasolare creare una macchia sulla superficie della stella attorno a cui era in orbita, HD 179949. I ricercatori hanno ipotizzato che la magnetosfera del pianeta stesse interagendo con la magnetosfera stellare, trasferendo energia alla fotosfera stellare e incrementando localmente la già alta temperatura di 14 000 K di ulteriori 750 K.
Tutti i pianeti, a esclusione di Mercurio e Venere, hanno satelliti naturali, chiamati comunemente "lune". La Terra ne ha una, Marte due, mentre i giganti gassosi ne hanno un elevato numero, organizzate in sistemi complessi simili a sistemi planetari. Alcune lune dei giganti gassosi hanno caratteristiche simile a quelle dei pianeti terrestri e dei pianeti nani e alcune di esse sono state studiate come possibili dimore di forme di vita (specialmente Europa, uno dei satelliti di Giove).
Attorno ai quattro giganti gassosi orbitano degli anelli planetari di dimensione e complessità variabili. Gli anelli sono composti principalmente da polveri ghiacciate o silicati e possono ospitare minuscoli satelliti pastore la cui gravità ne delinea la forma e ne conserva la struttura. Sebbene l'origine degli anelli planetari non sia nota con certezza, si crede che derivino da un satellite naturale che ha sofferto un grosso impatto oppure siano il risultato piuttosto recente della disgregazione di un satellite naturale, distrutto dalla gravità del pianeta dopo aver oltrepassato il limite di Roche.
Nessuna caratteristica secondaria è stata osservata attorno agli esopianeti fino scoperti, anche se si ipotizza che alcuni di questi, in particolare i giganti più massicci, potrebbero ospitare uno stuolo di esosatelliti simili a quelli che orbitano attorno a Giove. Tuttavia si crede che la sub-nana bruna Cha 110913-773444, classificata come un pianeta interstellare, sia circondata da un disco da cui in futuro potrebbero avere origine dei piccoli pianeti o satelliti.
Il modello maggiormente accettato dalla comunità scientifica per spiegare la formazione dei sistemi planetari è il modello della nebulosa solare, formulato originariamente, come arguibile dal nome, per spiegare la formazione del sistema solare.
In accordo con il modello standard della formazione stellare, la nascita di una stella avviene attraverso il collasso di una nube molecolare, il cui prodotto è la protostella. Non appena la stella nascente conclude la fase protostellare e fa ingresso nella pre-sequenza principale (fase di T Tauri), il disco che ne ha mediato l'accrescimento diviene protoplanetario; la sua temperatura diminuisce, permettendo la formazione di piccoli grani di polvere costituiti da roccia (in prevalenza silicati) e ghiacci di varia natura, che a loro volta possono fondersi tra loro per dar luogo a blocchi di diversi chilometri detti planetesimi. Se la massa residua del disco è sufficientemente grande, in un lasso di tempo astronomicamente breve (100 000–300 000 anni) i planetesimi possono fondersi tra loro per dar luogo a embrioni planetari, detti protopianeti, i quali, in un arco temporale compreso tra 100 milioni e un miliardo di anni, vanno incontro a una fase di violente collisioni e fusioni con altri corpi simili; il risultato sarà la formazione, alla fine del processo, di alcuni pianeti terrestri.
La formazione dei giganti gassosi è invece un processo più complicato, che avverrebbe al di là della cosiddetta "frost line" (chiamata in letteratura anche "limite della neve"). I protopianeti ghiacciati posti oltre questo limite possiedono una massa superiore e sono in maggior numero rispetto ai protopianeti esclusivamente rocciosi. Non è completamente chiaro cosa succeda in seguito alla formazione dei protopianeti ghiacciati; sembra tuttavia che alcuni di questi, in forza delle collisioni, crescano fino a raggiungere una massa superiore alle 10 masse terrestri – M – (secondo recenti simulazioni si stima 14-18), necessaria per poter innescare un fenomeno di accrescimento, simile a quello cui è andata incontro la stella ma su scala ridotta, a partire dall'idrogeno e dall'elio che sono stati spinti nelle regioni esterne del disco dalla pressione di radiazione e dal vento della stella neonata. L'accumulo di gas da parte del nucleo protopianetario è un processo inizialmente lento, che prosegue per alcuni milioni di anni fino al raggiungimento di circa 30 M, dopo di che subisce un'imponente accelerazione che lo porta in breve tempo (poche migliaia di anni) ad accumulare il 90% di quella che sarà la sua massa definitiva: si stima che pianeti come Giove e Saturno abbiano accumulato la gran parte della loro massa in appena 10 000 anni. L'accrescimento si conclude all'esaurimento dei gas disponibili; successivamente il pianeta subisce, a causa della perdita di momento angolare dovuta all'attrito con i residui del disco, un decadimento dell'orbita che risulta in un processo di migrazione planetaria, più o meno accentuato a seconda dell'entità dell'attrito; questo spiega come mai in alcuni sistemi extrasolari siano stati individuati dei giganti gassosi a brevissima distanza dalla stella madre, i cosiddetti pianeti gioviani caldi ("Hot Jupiters"). Si ritiene che i giganti ghiacciati, come Urano e Nettuno, costituiscano dei "nuclei falliti", formatisi quando oramai gran parte dei gas erano stati esauriti.
I protopianeti che non sono stati inglobati dai pianeti son potuti diventare loro satelliti, in seguito a un processo di cattura gravitazionale, o hanno mantenuto un'orbita eliosincrona raggruppati in fasce con altri oggetti simili, diventando pianeti nani o altri corpi minori.
Gli impatti con i planetesimi, così come il decadimento radioattivo dei loro costituenti, hanno riscaldato i pianeti in formazione, causandone una parziale fusione. Ciò ha permesso che il loro interno si sia differenziato conducendo alla formazione di un nucleo più denso, di un mantello e di una crosta (si veda anche il paragrafo Differenziazione interna). Nel processo, i pianeti terrestri, più piccoli, hanno perduto la maggior parte della loro atmosfera; i gas perduti sono stati in parte reintegrati da quelli eruttati dal mantello e dagli impatti di corpi cometari. I pianeti più piccoli in seguito hanno continuato a perdere la propria atmosfera attraverso vari meccanismi di fuga.
È importante notare che esistono dei sistemi planetari estremamente diversi dal sistema solare: sono stati scoperti, ad esempio, sistemi planetari intorno a pulsar; in merito a questi ultimi non vi sono ancora teorie certe sulla loro formazione, ma si pensa che possano originarsi a partire da un disco circumstellare costituitosi dai materiali espulsi dalla stella morente durante l'esplosione in supernova.
Si è scoperto inoltre che la metallicità, ovvero l'abbondanza di elementi più pesanti dell'elio, è un parametro importante nel determinare se una stella possegga o meno pianeti: si ritiene che sia meno probabile che una stella povera di metalli, appartenente alla popolazione stellare II, possa essere circondata da un sistema planetario articolato, mentre le probabilità aumentano per le stelle ricche di metalli, appartenenti alla popolazione stellare I.
Ogni pianeta, pur nella propria unicità, condivide con gli altri delle caratteristiche comuni; alcune di queste, come la presenza di anelli o satelliti naturali, sono state osservate solo nel sistema solare; altre invece, quali l'atmosfera, sono comuni anche ai pianeti extrasolari.
Gli otto pianeti che, in base alla definizione ufficiale del 24 agosto 2006, compongono il sistema solare, in ordine di distanza crescente dal Sole, sono:
Dal 1930 al 2006 era considerato pianeta anche Plutone (), che possiede cinque satelliti naturali: Caronte, Notte, Idra, Cerbero; il quinto satellite, Stige, è stato scoperto dal telescopio spaziale Hubble l'11 luglio 2012.
Nel 2006 Plutone è stato riclassificato come pianeta nano.
Tutti i pianeti del sistema solare (eccetto la Terra) possiedono nomi derivati dalla mitologia romana; al contrario, i nomi dei principali satelliti naturali sono derivati da quelli di divinità o personaggi della mitologia greca (a eccezione di quelli di Urano, che portano nomi di personaggi delle opere di Shakespeare e Pope).
Gli asteroidi, al contrario, possono essere battezzati, a discrezione del loro scopritore e con l'approvazione dell'UAI, con un nome qualunque.
Non sono ancora chiare le convenzioni di nomenclatura che verranno adottate per la categoria dei pianeti nani.
I pianeti del sistema solare, secondo la loro composizione, possono essere divisi in pianeti terrestri e pianeti gioviani.
I pianeti di tipo terrestre si trovano nel sistema solare interno e sono costituiti principalmente da roccia (da cui il nome alternativo di "pianeti rocciosi"). Il termine deriva direttamente dal nome del nostro pianeta, per indicare quei pianeti simili alla Terra. Essi sono caratterizzati da una temperatura superficiale relativamente alta, dovuta alla vicinanza del Sole, assenza o basso numero di satelliti naturali, con un'atmosfera molto sottile se confrontata a quella dei giganti gassosi. Raggiungono dimensioni relativamente piccole (meno di 15 000 chilometri di diametro).
Nel sistema solare essi sono quattro:
I pianeti di tipo gioviano sono composti principalmente da gas, donde il nome di "giganti gassosi". Prototipo di tali pianeti è Giove. Essi sono caratterizzati da un elevato valore della massa, che consente loro di trattenere un'estesa atmosfera ricca di idrogeno ed elio, e da dimensioni notevoli. Sono accompagnati da un elevato numero di satelliti naturali e da elaborati sistemi di anelli.
Nel sistema solare sono presenti quattro giganti gassosi:
I pianeti nani sono oggetti celesti orbitanti attorno a una stella e caratterizzati da una massa sufficiente a conferire loro una forma sferoidale (avendo raggiunto la condizione di equilibrio idrostatico), ma che non sono stati in grado di "ripulire" la propria fascia orbitale da altri oggetti di dimensioni confrontabili; da ciò deriva il fatto che i pianeti nani si trovano all'interno di cinture asteroidali. Nonostante il nome, un pianeta nano non è necessariamente più piccolo di un pianeta. Si osservi inoltre che la classe dei pianeti è distinta da quella dei pianeti nani, e non comprende quest'ultima. Inoltre, i pianeti nani posti oltre l'orbita di Nettuno sono detti plutoidi.
L'UAI riconosce cinque pianeti nani:
Il termine "pianetino" e la locuzione "pianeta minore" sono solitamente utilizzate per designare gli asteroidi. Ciò deriva dal fatto che i primi quattro asteroidi scoperti (Cerere – oggi classificato come pianeta nano, Pallade, Giunone e Vesta), furono in effetti considerati dei pianeti veri e propri per circa quarant'anni. Il primo a suggerire di distinguerli dai pianeti fu William Herschel, che propose il termine "asteroide", ovvero "di aspetto stellare", riferendosi al fatto che sono oggetti troppo piccoli perché possa essere risolto il loro disco e, di conseguenza, osservati con un telescopio appaiono come le stelle.
La maggior parte degli astronomi, comunque, preferì continuare a utilizzare il termine pianeta almeno fino alla seconda metà dell'Ottocento, quando il numero degli asteroidi conosciuti superò le cento unità. Allora, diversi osservatori in Europa e negli Stati Uniti iniziarono a riferirsi loro collettivamente come a "pianeti minori", espressione ancora in uso.
La prima scoperta confermata di un pianeta extrasolare è avvenuta il 6 ottobre 1995, quando Michel Mayor e Didier Queloz dell'Università di Ginevra hanno annunciato l'individuazione di un pianeta attorno a 51 Pegasi, nella costellazione di Pegaso. La maggior parte degli oltre 600 pianeti extrasolari scoperti fino a ottobre 2011 hanno masse pari o superiori a quella di Giove. Il motivo di questa apparente difformità nella distribuzione di masse osservata nel sistema solare è dato da un classico effetto di selezione, in virtù del quale i nostri strumenti sono capaci di vedere solo pianeti molto grandi e prossimi alla rispettiva stella madre, perché i loro effetti gravitazionali sono maggiori e più agevoli da individuare.
Tra le eccezioni più rilevanti ci sono tre pianeti orbitanti la pulsar PSR B1257+12, il resto di un'esplosione di supernova. Sono stati individuati, inoltre, circa una dozzina di esopianeti con masse comprese tra le 10 e le 20 masse terrestri (confrontabili dunque con la massa di Nettuno, pari a 17 masse terrestri), come quelli che orbitano intorno alle stelle , 55 Cancri e GJ 436, a cui a volte ci si riferisce chiamandoli appunto "pianeti nettuniani".
Al maggio del 2011 il numero dei pianeti rocciosi individuati supera il centinaio. Essi appartengono, per lo più, alla categoria delle "Super Terre", caratterizzate da una massa superiore a quella della Terra, ma inferiore a quella di Urano e Nettuno. Gliese 876 d, con una massa pari a circa 6 masse terrestri, è stato il primo a essere scoperto, nel 2005. OGLE-2005-BLG-390Lb e MOA-2007-BLG-192Lb, mondi glaciali, sono stati scoperti attraverso l'effetto delle microlenti gravitazionali, COROT-Exo-7b, un pianeta con un diametro stimato in circa 1,7 volte quello della Terra (la cui scoperta fu annunciata con grande enfasi nel 2009), ma che orbita attorno alla sua stella alla distanza di 0,02 UA e ciò determina che sulla sua superficie si raggiungano temperature di 1 500 °C e due pianeti in orbita attorno a una vicina nana rossa, Gliese 581.
Di particolare interesse è il sistema planetario in orbita attorno alla nana rossa Gliese 581, composto da sei pianeti, due dei quali non confermati. Gliese 581 d ha una massa pari a circa 7,7 volte quella della Terra, mentre Gliese 581 c è cinque volte la Terra e al momento della sua scoperta si pensò che fosse il primo pianeta terrestre extrasolare scoperto in prossimità della zona abitabile di una stella. Tuttavia, studi più approfonditi hanno rivelato che il pianeta è leggermente troppo vicino alla sua stella per essere abitabile, mentre Gliese 581 d, sebbene sia molto più freddo della Terra, potrebbe esserlo, se la sua atmosfera contenesse una quantità sufficiente di gas serra. Gliese 581 g, se confermato, sarebbe il primo pianeta scoperto nella zona abitabile della propria stella.
Il 2 febbraio 2011 la NASA ha diffuso una lista di 1235 probabili pianeti extrasolari individuati attraverso il telescopio spaziale Kepler. Essa comprende 68 possibili pianeti di dimensioni simili alla Terra (R &lt; 1,25 R) e altre 288 possibili super Terre (1,25 R &lt; R &lt; 2 R). Inoltre, 54 probabili pianeti sono stati individuati nella zona abitabile del loro sistema, sei dei quali hanno dimensioni inferiori al doppio di quelle terrestri.
È probabile che alcuni pianeti fin qui scoperti non siano molto simili ai giganti gassosi del Sistema solare, perché ricevono un quantitativo di radiazione stellare molto superiore rispetto a essi, dal momento che presentano orbite circolari ed estremamente vicine alle proprie stelle. Corpi di questo tipo sono noti con l'appellativo di pianeti gioviani caldi ("Hot Jupiters"). Potrebbero esistere, inoltre, dei pianeti gioviani caldi (indicati come pianeta ctonii) che orbitano tanto vicini alla propria stella da aver perduto la propria atmosfera, soffiata via dalla radiazione stellare. Sebbene siano stati individuati dei processi di dissoluzione dell'atmosfera su numerosi pianeti gioviani caldi, al 2009 non è stato individuato alcun pianeta che possa essere qualificato come ctonio.
L'individuazione di un numero maggiore di pianeti extrasolari e una loro migliore conoscenza richiede la costruzione di una nuova generazione di strumenti. Il programma COROT, del CNES, in collaborazione con l'Agenzia Spaziale Europea, e Kepler della NASA sono le principali missioni spaziali attualmente operative. È prevista per la primavera del 2011 l'entrata in funzione del telescopio "Automated Planet Finder", che farà parte dell'Osservatorio Lick. Le principali agenzie spaziali hanno allo studio diversi progetti che prevedono la creazione di una rete di telescopi spaziali per l'individuazione di pianeti delle dimensioni della Terra, anche se il loro finanziamento rimane ancora incerto.
La probabilità dell'occorrenza dei pianeti terrestri è una delle variabili dell'equazione di Drake, che cerca di stimare il numero di civiltà extraterrestri evolute presenti nella nostra Galassia.
Un pianeta interstellare è un corpo celeste avente una massa equivalente a quella di un pianeta ("planemo"), ma non legato gravitazionalmente a nessuna stella: questi corpi celesti si muovono dunque nello spazio interstellare come oggetti indipendenti da qualsiasi sistema planetario, il che giustifica l'appellativo di "pianeta orfano" attribuito a volte, in maniera alternativa, a questo tipo di oggetti.
Sebbene siano state annunciate diverse scoperte di questi oggetti, nessuna di esse è stata finora confermata. La comunità scientifica, inoltre, dibatte sull'opportunità di considerarli o meno pianeti; alcuni astronomi hanno suggerito infatti di chiamarli sub-nane brune. La differenza principale tra i due oggetti starebbe nel processo che ha condotto alla loro formazione: una sub-nana bruna si forma dalla contrazione di una nube di gas e polveri, in maniera simile a quanto avviene per una stella o una nana bruna; un pianeta, invece, dall'accrescimento di gas e polveri intorno a un embrione planetario orbitante all'interno di un disco circumstellare, con un processo analogo a quello descritto precedentemente (si veda a tal proposito il paragrafo Formazione dei pianeti e dei sistemi planetari). Successivamente, il pianeta verrebbe espulso nello spazio interstellare in seguito a instabilità dinamiche proprie dei sistemi planetari neoformati, come è stato suggerito da diverse simulazioni computerizzate.
L'Unione Astronomica Internazionale non è entrata nel merito della diatriba, salvo indicare, in una dichiarazione del 2003, che "gli oggetti vaganti in giovani ammassi stellari con valori della massa inferiori al valore della massa limite per la fusione termonucleare del deuterio non sono "pianeti", ma sono "sub-nane brune" (o qualunque altro nome sarà ritenuto appropriato)." Va notato come la definizione data si riferisca espressamente a oggetti vaganti in giovani ammassi stellari.
Un "pianeta ipotetico" è un pianeta o corpo planetario la cui esistenza è ritenuta possibile ma non è stata confermata da dati empirici.
Diversi corpi planetari rientrano in questo novero; non di meno, vi sono state nel passato o vi sono tutt'oggi credenze occasionali pseudoscientifiche, teorie complottiste o gruppi religiosi volti ad accettare tali ipotesi come scientifiche e fondate. Si distinguono dai pianeti immaginari della fantascienza per il fatto che questi gruppi credono nella loro reale esistenza. Esempi di questi pianeti ipotetici sono Antiterra, Lilith, Kolob e il Pianeta X.
In altri casi, l'esistenza di pianeti ipotetici è stata postulata come possibile spiegazione di fenomeni astronomici osservati nel sistema solare, al momento della loro scoperta. Successivamente, il miglioramento delle conoscenze astronomiche ha condotto alla smentita della loro esistenza.
Infine, lo studio dei meccanismi di formazione dei sistemi planetari e l'osservazione dei pianeti extrasolari finora scoperti ha portato a ipotizzare l'esistenza di nuove classi di pianeti quali: i pianeti oceano, la cui superficie sarebbe ricoperta da un oceano profondo centinaia di chilometri; pianeti di carbonio, che potrebbero essersi formati a partire da dischi protoplanetari ricchi dell'elemento e poveri di ossigeno; pianeti ctoni, l'ultimo stadio di un pianeta gioviano caldo tanto prossimo alla propria stella da essere privato della caratteristica atmosfera.
Per pianeti immaginari si intendono tutti i luoghi genericamente abitabili di carattere astronomico, completamente inventati o ridescritti immaginariamente a partire da quelli realmente esistenti che si trovano in opere letterarie, cinematografiche e d'animazione. Non costituiscono quindi un pianeta ipotetico, perché i lettori non credono nella loro reale esistenza.
L'esplorazione di altri pianeti è un tema costante della fantascienza, specie in relazione al contatto con forme di vita aliene. Durante le prime fasi dello sviluppo della fantascienza, Marte rappresentò il pianeta più frequentemente utilizzato e romanzato del nostro sistema solare; le sue condizioni in superficie erano ritenute le più favorevoli alla vita.
Gli scrittori nelle loro opere hanno creato migliaia di pianeti immaginari. Molti di questi sono quasi indistinguibili dalla Terra. In questi mondi, le differenze rispetto alla Terra sono prevalentemente di tipo sociale; altri tipici esempi sono i pianeti prigione, le culture primitive, gli estremismi politici e religiosi, e così via.
Pianeti più insoliti e descrizioni più accurate dal punto di vista fisico si possono trovare soprattutto nelle opere di fantascienza hard o classica; tipici esempi sono quelli che presentano su gran parte della loro superficie un unico ambiente climatico, ad esempio i pianeti desertici, i mondi acquatici, artici o interamente ricoperti da foreste.
Alcuni scrittori, scienziati e artisti hanno poi speculato riguardo a mondi artificiali o pianeti-equivalenti.
Alcune delle più celebri serie televisive fantascientifiche, come "Star Trek" e "Stargate SG-1", sono basate sulla scoperta e sull'esplorazione di nuovi pianeti e di civiltà aliene.
</text>
</doc>
<doc id="8925" url="https://it.wikipedia.org/wiki?curid=8925">
<title>Massa (fisica)</title>
<text>
La massa (dal greco: , "máza", torta d'orzo, grumo di pasta) è una grandezza fisica propria dei corpi materiali che ne determina il comportamento dinamico quando sono soggetti all'influenza di forze esterne.
Nel corso della storia della fisica, in particolare della fisica classica, la massa è stata considerata una proprietà intrinseca della materia, rappresentabile con un valore scalare e che si conserva nel tempo e nello spazio, rimanendo costante in ogni sistema isolato. Inoltre, il termine massa è stato utilizzato per indicare due grandezze potenzialmente distinte: l'interazione della materia con il campo gravitazionale e la relazione che lega la forza applicata a un corpo con l'accelerazione su di esso indotta. Tuttavia, è stata verificata l'equivalenza delle due masse in numerosi esperimenti (messi in atto già da Galileo Galilei per primo).
Nel quadro più ampio della relatività ristretta, specialmente in una prospettiva storica, la massa relativistica non è più una proprietà intrinseca della materia, ma dipende anche dallo stato della materia stessa e dal sistema di riferimento in cui viene osservata. Il concetto di massa relativistica non è centrale alla teoria, al punto che alcuni autori la ritengono un concetto fuorviante. Nella relatività ristretta un corpo ha una massa relativistica direttamente proporzionale alla sua energia, tramite la famosa formula "E" = "mc"². È possibile invece definire un invariante relativistico, detto massa a riposo o massa invariante, al quale la massa relativistica si riconduce nel caso in cui la particella sia ferma. La massa a riposo è definita in termini dell'energia e dell'impulso della particella ed è la stessa in ogni sistema di riferimento, risultando una grandezza fisica molto più utile della massa relativistica, al posto della quale può essere usata l'energia della particella.
A differenza di spazio e tempo, per cui si possono dare definizioni operative in termini di fenomeni naturali, per definire il concetto di massa occorre fare esplicito riferimento alla teoria fisica che ne descrive significato e proprietà. I concetti intuitivi pre-fisici di "quantità di materia" (da non confondere con "quantità di sostanza", misurata in moli) sono troppo vaghi per una definizione operativa, e fanno riferimento a proprietà comuni, l'inerzia e il peso, che vengono considerati ben distinti dalla prima teoria che introduce la massa in termini quantitativi, la dinamica newtoniana.
Il concetto di massa diventa più complesso al livello della fisica subatomica dove la presenza di particelle elementari con massa (elettroni, quark, ...) e prive di massa (fotoni, gluoni) non ha ancora una spiegazione in termini fondamentali. In altre parole, non è chiaro il perché alcune particelle siano dotate di massa e altre no. Le principali teorie che cercano di dare una interpretazione alla massa sono: il meccanismo di Higgs, la teoria delle stringhe e la gravità quantistica a loop; di queste, a partire dal 4 luglio 2012 grazie all'acceleratore di particelle LHC, soltanto la Teoria di Higgs ha avuto i primi riscontri sperimentali.
Nell'attuale Sistema internazionale di unità di misura (SI) la massa è stata scelta come grandezza fisica fondamentale, cioè non esprimibile in termini di altre grandezze. La sua unità di misura è il chilogrammo, indicato col simbolo kg.
Nel sistema CGS l'unità di massa è il grammo. Nel Regno Unito e negli Stati Uniti viene comunemente usata la libbra (circa 454 g) e la "stone" (letteralmente "pietra", 14 libbre). Altre unità di misura vengono comunemente utilizzate in specifici campi della fisica.
In fisica atomica e fisica della materia vengono comunemente utilizzate le unità di misura di Hartree, basate sulla massa dell'elettrone o l'unità di massa atomica, equivalente grossomodo alla massa di un protone. In chimica si usa frequentemente la mole che, pur non essendo una unità di massa, vi è legata da un semplice fattore di proporzionalità.
In fisica nucleare e sub-nucleare è comune l'utilizzo dell'unità di massa atomica. Tuttavia, soprattutto nel campo delle alte energie, si usa esprimere la massa (a riposo o invariante) tramite la sua energia equivalente E = mc². L'energia viene a sua volta espressa in eV. Per esempio un elettrone ha una massa di circa
L'elettrone ha quindi una massa a riposo equivalente a 0,511 MeV. Negli esperimenti di fisica sub-nucleare l'energia cinetica delle particelle studiate è spesso dello stesso ordine di grandezza, il che rende questa scelta di unità di misura particolarmente conveniente.
Le unità di misura della massa, in particolare il chilogrammo e la libbra, vengono talvolta usate anche per misurare una forza. Quest'uso pur essendo tecnicamente scorretto è molto diffuso nell'uso comune e giustificato dal fatto che l'accelerazione di gravità sulla terra ("g") è grossomodo costante. Una forza può quindi essere espressa come massa equivalente tramite la costante di proporzionalità "g". In altre parole, affermare che una forza ha l'intensità di un chilogrammo è equivalente ad affermare che un corpo del peso di un chilogrammo, al livello del mare, sarebbe soggetto a una forza gravitazionale di entità equivalente. Quest'uso non è tuttavia conforme al Sistema Internazionale. Massa e forza sono due grandezze concettualmente distinte, con unità di misura SI diverse, rispettivamente il chilogrammo per la massa e il newton per la forza; ed è bene sottolineare che il peso di un oggetto è una forza, non una proprietà fisica intrinseca dell'oggetto (quale invece è la massa).
In meccanica classica il termine massa si può riferire a tre diverse grandezze fisiche scalari, distinte tra loro:
La massa inerziale e quella gravitazionale sono state sperimentalmente provate come equivalenti, anche se concettualmente sono distinte. I primi esperimenti mirati a stabilire questa equivalenza sono stati quelli di Galileo Galilei.
La massa inerziale "m" di un corpo viene definita nei Principia come quantità di materia legandola al principio di proporzionalità come costante di proporzionalità tra la forza applicata formula_2 e l'accelerazione subita formula_3:
La massa inerziale si può in effetti ottenere "operativamente" misurando l'accelerazione del corpo sottoposto a una forza nota, essendo l'indice della tendenza di un corpo ad accelerare quando è sottoposto a una forza, cioè dell'inerzia del corpo.
Il problema di utilizzare questa proprietà come definizione è che necessita del concetto pregresso di forza; per evitare il circolo vizioso generato da Newton che non specificava lo strumento per misurarla; spesso la forza viene allora definita legandola all'allungamento di una molla che segua la legge di Hooke, definizione chiaramente insoddisfacente in quanto particolare e non generale.
Inoltre questa definizione ha dato origine a diverse problematiche, legate in particolare al sistema di riferimento nel quale si effettua la misura: il concetto di inerzia, come quello di forza, fu infatti storicamente criticato da molti pensatori, tra i quali Berkeley, Ernst Mach, Percy Williams Bridgman e Max Jammer.
Il capitolo più importante per la storia del concetto venne dal tentativo di Mach di eliminare gli elementi metafisici che persistevano nell'edificio della Meccanica classica, riformulando la massa in una definizione chiara divenuta classica anche in quanto da questa prese poi le mosse la Relatività generale, anche se non risolutiva tant'è che Einstein stesso disperò nell'includere il principio di Mach all'interno della Teoria.
Questa si basa invece sul principio di azione-reazione, lasciando che il principio di proporzionalità definisca successivamente la forza. Consideriamo un sistema isolato formato da due corpi (puntiformi) interagenti tra loro. Qualunque sia la forza che agisce fra i due corpi, si osserva sperimentalmente che le accelerazioni subite dai due corpi sono sempre proporzionali e in rapporto costante fra loro:
Ciò che è particolarmente rilevante è che il rapporto formula_6 fra le due accelerazioni istantanee non solo è costante nel tempo, ma non dipende dallo stato iniziale del sistema: è quindi associato a una proprietà fisica intrinseca dei due corpi in esame. Cambiando uno dei due corpi, varia anche la costante di proporzionalità. Supponiamo quindi di utilizzare tre corpi, ed effettuare separatamente tre esperimenti con le tre possibili coppie (assumiamo sempre l'assenza di forze esterne). In questo modo potremo misurare le costanti formula_7 Si noti che per definizione
Confrontando i valori delle costanti osservate, si troverà invariabilmente che questi soddisfano la relazione formula_9 Quindi il prodotto formula_10 non dipende dalla natura del corpo 1, poiché uguale all'inverso di formula_11, vale a dire formula_12, che ne risulta indipendente per via della indipendenza di formula_11. Da questo si ricava che ogni coefficiente formula_14 deve poter essere espresso come prodotto di due costanti, ognuna dipendente solo da uno dei due corpi. Chiamiamo formula_15; ma deve valere identicamente
quindi
in ogni istante, per qualunque coppia di corpi. La quantità "m" che risulta così definita (a meno di un fattore costante, che corrisponde alla scelta dell'unità di misura) è chiamata massa inerziale del corpo: è quindi possibile misurare la massa di un corpo misurando le accelerazioni dovute alle interazioni tra questo e un altro corpo di massa nota, senza bisogno di conoscere quali siano le forze agenti fra i due punti (purché il sistema formato dai due corpi si possa considerare isolato, ossia non soggetto a forze esterne). Il legame tra le masse è dato da:
Consideriamo un corpo, per esempio una palla da tennis. Notiamo che se la palla è lasciata libera in aria, essa è attratta verso il basso da una forza, in prima approssimazione costante, chiamata forza peso. Tramite una bilancia a piatti si può notare che corpi diversi, in generale, sono attratti diversamente dalla forza peso, cioè "pesano" diversamente. La bilancia a piatti si può usare per dare una definizione operativa della massa gravitazionale: si assegna massa unitaria a un oggetto campione e gli altri oggetti hanno una massa pari al numero di campioni necessari a bilanciare i piatti.
La massa gravitazionale passiva è una grandezza fisica proporzionale all'interazione di ciascun corpo con il campo gravitazionale. All'interno dello stesso campo gravitazionale, un corpo con massa gravitazionale piccola sperimenta una forza minore di quella di un corpo con massa gravitazionale grande: la massa gravitazionale è proporzionale al peso, ma mentre quest'ultimo varia a seconda del campo gravitazionale, la massa resta costante. Per definizione, possiamo esprimere la forza peso P come il prodotto della massa gravitazionale "m" per un vettore g, chiamato "accelerazione di gravità", dipendente dal luogo nel quale si effettua la misurazione e le cui unità di misura dipendono da quella della massa gravitazionale. La direzione del vettore g è chiamata "verticale".
Come detto precedentemente, la massa gravitazionale attiva di un corpo è proporzionale all'intensità del campo gravitazionale da esso generata. Maggiore è la massa gravitazionale attiva di un corpo, più intenso è il campo gravitazionale da esso generato, e quindi la forza esercitata dal campo su un altro corpo; per fare un esempio, il campo gravitazionale generato dalla Luna è minore (a parità di distanza dal centro dei due corpi celesti) di quello generato dalla Terra perché la sua massa è minore. Misure di masse gravitazionali attive si possono eseguire, per esempio, con bilance di torsione come quella usata da Henry Cavendish nella determinazione della costante di gravitazione universale.
L'equivalenza tra la massa gravitazionale attiva e quella passiva è una diretta conseguenza del terzo principio della dinamica di Newton: chiamiamo "F" il modulo della forza che il corpo 1 esercita sul corpo 2, "F" il modulo della forza che il corpo 2 esercita sul corpo 1 e "m", "m", "m" e "m" le masse gravitazionali, attive e passive, dei due corpi. Abbiamo:
da cui:
cioè
Data l'arbitrarietà dei corpi, le leggi della meccanica classica stabiliscono la sostanziale equivalenza tra le masse gravitazionali attive e passive; molte verifiche sperimentali si sono aggiunte nel tempo, come per esempio quella di D. F. Bartlett e D. Van Buren del 1986 compiuta sfruttando la diversa composizione della crosta e del mantello lunari, con una precisione sull'uguaglianza del rapporto "massa gravitazionale attiva"/"massa gravitazionale passiva" pari a 4×10.
Da qui in poi le masse gravitazionali attiva e passiva saranno identificate dall'unico termine "massa gravitazionale".
La massa gravitazionale è a tutti gli effetti la carica del campo gravitazionale, esattamente nello stesso senso in cui la carica elettrica è la carica del campo elettrico: essa contemporaneamente "genera" e "subisce gli effetti" del campo gravitazionale. Notiamo che eventuali oggetti con massa gravitazionale nulla ("es. fotoni") non subirebbero gli effetti del campo: in realtà un risultato della relatività generale è che qualunque corpo segue una traiettoria dovuta al campo gravitazionale. Per ulteriori informazioni, vedi il paragrafo riguardante la massa nella relatività generale.
Gli esperimenti hanno dimostrato che la massa inerziale e quella gravitazionale sono sempre proporzionali con la stessa costante di proporzionalità, entro la precisione delle misure effettuate sinora. I primi esperimenti furono condotti da Galileo; si dice comunemente che Galileo ottenne i suoi risultati lasciando cadere oggetti dalla torre di Pisa, ma ciò è probabilmente apocrifo: più verosimilmente studiò il moto di biglie tramite l'uso di piani inclinati. La biografia scritta da Vincenzo Viviani asserisce che Galileo abbia lasciato cadere sfere dello stesso volume ma di materiale diverso, cioè di diversa massa, dalla torre di Pisa, ma fu probabilmente un esperimento mentale che non fu mai eseguito realmente; Galileo usò invece piani inclinati per rallentare la caduta dei corpi.
Supponiamo di avere un oggetto di massa inerziale e gravitazionale rispettivamente "m" ed "m". Se la forza peso è la sola forza agente sugli oggetti la seconda legge di Newton ci fornisce:
da cui:
Un esperimento di verifica dell'equivalenza tra le due definizioni di massa, una volta fissato il luogo (altrimenti potrebbe variare g) potrebbe consistere, per esempio, nel misurare a per diversi corpi cercando eventuali variazioni; in parole povere, verificare se due corpi qualsiasi, cadendo, accelerano nello stesso modo ("universalità della caduta libera", oppure "UFF" dall'inglese "universality of free fall"). Come detto sopra, sperimentalmente non si riscontrano violazioni dell'equivalenza, quindi scegliendo la stessa unità di misura per le due masse il rapporto vale esattamente 1: per ogni corpo non solo massa gravitazionale e massa inerziale hanno le stesse unità di misura, ma sono anche espresse dallo stesso numero. Di conseguenza g è un'accelerazione, e viene chiamata infatti "accelerazione di gravità".
Le verifiche sperimentali dell'equivalenza tra massa inerziale e gravitazionale e dell'UFF sono state effettuate mediante l'uso di piani inclinati (Galileo), pendoli (Newton), fino ad arrivare alle bilance di torsione (Loránd Eötvös).
Attualmente la precisione raggiunta dagli esperimenti è nell'ordine di una parte su 10, precisione ottenuta dalla misurazione della distanza lunare tramite laser. Sono previsti, o comunque in pianificazione, i lanci di diversi satelliti artificiali come STEP ("Satellite Test of the Equivalence Principle"), MICROSCOPE ("Micro-Satellite à traînée Compensée pour l'Observation du Principe d'Equivalence") e Galileo Galilei, che dovrebbero testare l'equivalenza a meno di una parte su 10.
Un pendolo è formato da un lungo filo leggero (di massa trascurabile), vincolato al soffitto, alla cui estremità inferiore sia agganciato un corpo, per esempio una sfera metallica. Una misura del periodo del pendolo fornisce una misura del rapporto tra la massa gravitazionale e la massa inerziale del corpo: ripetendo la misura con corpi di vari materiali, densità e dimensioni è possibile verificare se questo rapporto rimanga costante o no. La misura è tanto più accurata quanto è piccolo l'angolo di oscillazione massimo "θ".
L'equazione del moto del pendolo è data da:
Se "θ" è sufficientemente piccolo abbiamo:
dove ω è la pulsazione del pendolo. Il periodo d'oscillazione è dato da:
da cui:
Sperimentalmente, si osserva che "T" è costante per ogni massa usata, perciò per ogni corpo il rapporto "m" / "m" deve essere costante.
Un esperimento decisamente più accurato fu compiuto da Loránd Eötvös a partire dal 1895
sfruttando la bilancia di torsione la cui invenzione è accreditata a Charles-Augustin de Coulomb nel 1777 (sebbene anche John Michell in maniera del tutto indipendente ne costruì una in periodo antecedente al 1783) e che fu successivamente perfezionata da Henry Cavendish. Una bilancia di torsione è formata da un braccio con due masse uguali alle estremità, vincolato al soffitto tramite un filo di un materiale opportuno (es. quarzo). Applicando una forza alle masse si applica un momento torcente al manubrio: grazie al fatto che la forza peso agente sulle masse ha anche una componente dovuta alla forza centrifuga causata dalla rotazione della terra sul suo asse, è possibile correlare massa inerziale e gravitazionale, che risultano sperimentalmente essere di diretta proporzionalità.
Sia il manubrio inizialmente diretto verso la direzione est-ovest. Definiamo un sistema di riferimento con l'asse x da sud a nord, l'asse y da ovest a est e l'asse z dal basso verso l'alto; α è la latitudine alla quale si svolge l'esperimento. Proiettando le forze gravitazionale e centrifuga sull'asse z abbiamo all'equilibrio:
che si può anche scrivere come:
Se il rapporto tra le masse gravitazionali e le masse inerziali fosse diverso, ciò implicherebbe la diversità delle masse inerziali dei due corpi: ma ciò causerebbe una rotazione sul piano "xy", dovuta alla componente orizzontale della forza centrifuga. I momenti delle forze, proiettati sull'asse orizzontale danno:
Se questa relazione non fosse verificata si avrebbe un momento torcente agente sulla bilancia e di conseguenza una rotazione dell'apparato sperimentale; invertendo le masse si otterrebbe ovviamente una rotazione nel senso opposto. Eötvös non notò nessuna torsione del filo entro gli errori sperimentali, e quindi stabilì l'equivalenza delle masse gravitazionali e inerziali a meno di un fattore nell'ordine di 10 (una parte su un miliardo)
Nella meccanica classica vige la fondamentale legge della conservazione della massa, in varie formulazioni. In generale, dato un volume di controllo "V", fissato, la variazione della massa contenuta in esso è pari al flusso uscente della massa attraverso la frontiera formula_31, cioè attraverso la superficie chiusa che delimita il volume "V", cambiato di segno: in parole povere, la variazione di massa di un sistema è uguale alla massa entrante meno la massa uscente; ciò implica, per esempio, che la massa non può venire né creata né distrutta, ma solo spostata da un luogo a un altro. In chimica, Antoine Lavoisier stabilì nel XVIII secolo che in una reazione chimica la massa dei reagenti è uguale alla massa dei prodotti.
Il principio di conservazione della massa vale con ottima approssimazione nell'esperienza quotidiana, ma cessa di valere nelle reazioni nucleari e, in generale, nei fenomeni che coinvolgono energie relativistiche: in questo caso esso viene incorporato nel principio di conservazione dell'energia (vedi oltre).
Oggetti carichi possiedono una inerzia maggiore rispetto agli stessi corpi scarichi. Ciò si spiega con una interazione delle cariche elettriche in moto con il campo da esse stesse generato, detta "reazione di campo"; l'effetto è interpretabile come un aumento della massa inerziale del corpo ed è ricavabile dalle equazioni di Maxwell.
L'interazione delle cariche elettriche con il campo dipende dalla geometria del sistema: l'inerzia di un corpo carico assume un carattere tensoriale, in contraddizione con la meccanica classica, e bisogna perciò distinguere tra una componente parallela al moto e due componenti trasversali. Si dimostra che si può dividere la massa inerziale di un corpo carico in due componenti, la massa elettromagnetica e la massa "non-elettromagnetica". Mentre la massa elettromagnetica dipende dalla geometria del sistema, la massa non-elettromagnetica ha le stesse caratteristiche "standard" di invarianza della massa inerziale, e a essa si riconduce la massa inerziale se il corpo è scarico.
Il concetto di massa elettromagnetica esiste anche nella teoria della relatività ristretta e nella teoria quantistica dei campi. La massa elettromagnetica ebbe una grande importanza nella storia della fisica a cavallo tra i secoli XIX e XX a causa del tentativo, portato avanti principalmente da Max Abraham e Wilhelm Wien, inizialmente supportato dai lavori sperimentali di Walter Kaufmann, di ricavare la massa inerziale unicamente dall'inerzia elettromagnetica; questa interpretazione dell'inerzia fu però in seguito abbandonata con l'accettazione della teoria della relatività; esperimenti più precisi, eseguiti per la prima volta da A.H. Bucherer nel 1908, mostrarono che le relazioni corrette per la massa longitudinale e la massa trasversa non erano quelle fornite da Abraham, ma quelle di Hendrik Antoon Lorentz ("vedi il paragrafo successivo").
Nella relatività ristretta, il termine "massa" (o massa propria, o massa a riposo) si riferisce solitamente alla massa inerziale di un corpo così come viene misurata nel sistema di riferimento nel quale è in quiete. Anche in questo caso la massa è una proprietà intrinseca di un corpo e l'unità di misura è la stessa, il kilogrammo. Si può ancora determinare la massa di un oggetto come rapporto tra forza e accelerazione, a patto che si faccia in modo che la velocità del corpo sia molto più piccola di quella della luce. Infatti, ad alte velocità, il rapporto tra la forza impressa F e l'accelerazione a del corpo dipende in maniera sostanziale dalla sua velocità nel sistema di riferimento scelto, o meglio dal fattore di Lorentz relativo alla velocità alla quale si trova il corpo: in particolare se facciamo tendere la velocità all'infinito, il rapporto diverge. Il legame tra forza F e accelerazione A per un corpo con massa a riposo non nulla formula_32, con velocità "v" lungo l'asse "x" in un sistema di riferimento inerziale ("del laboratorio"), si ricava esprimendo le componenti spaziali della quadriaccelerazione e della quadriforza nel sistema di riferimento del laboratorio:
Sostituendo formula_35, con semplici passaggi si ottengono le seguenti relazioni, dovute a Lorentz:
Se la velocità del corpo è molto minore della velocità della luce "c", i fattori di Lorentz "γ" tendono a 1, perciò la massa a riposo del corpo è proprio equivalente alla massa inerziale.
Storicamente, nell'ambito della relatività ristretta si hanno altre definizioni di massa oltre a quella di "massa a riposo". Definendo massa il rapporto tra quantità di moto e la velocità otteniamo quella che viene indicata con massa relativistica formula_37. Se invece cerchiamo di identificare la massa come rapporto tra forza e accelerazione dobbiamo distinguere tra massa longitudinale formula_38 e massa trasversa formula_39, introdotte dal fisico tedesco Max Abraham; notiamo che questa distinzione tra le componenti della massa è analoga al caso della massa elettromagnetica. Sia la massa relativistica che le masse longitudinale/trasversa non sono considerate buone definizioni di massa in quanto dipendono dal sistema di riferimento nel quale la massa è misurata, e sono oggi in disuso. Utilizzando questi concetti, il sistema di equazioni precedente diventa:
L'energia "E" è definita in relatività ristretta come il prodotto tra la velocità della luce "c" e la componente temporale "P" del quadrimpulso (o quadrivettore quantità di moto). In formule:
dove γ è il fattore di Lorentz relativo alla velocità del corpo. Se misuriamo l'energia di un corpo fermo, chiamata "energia a riposo" "E", otteniamo:
Questa equazione stabilisce una corrispondenza tra massa a riposo di un corpo ed energia: in altri termini, ogni corpo con massa a riposo diversa da zero possiede una energia a risposo "E" dovuta unicamente al fatto di avere massa.
Questa equazione permette inoltre di incorporare il principio di conservazione della massa nel principio di conservazione dell'energia: per esempio l'energia del Sole è dovuta a reazioni termonucleari nelle quali la massa a riposo degli atomi che intervengono nella reazione è maggiore della massa dei prodotti, ma si conserva l'energia totale in quanto il "difetto di massa" viene convertito in energia (cinetica) e liberato successivamente dai prodotti sotto forma di fotoni e neutrini oppure negli urti con altri atomi.
L'equazione implica di fatto che la massa inerziale totale di un sistema isolato, in generale, non si conserva.
La conservazione della massa in meccanica classica può essere interpretata come parte della conservazione dell'energia quando non si verificano reazioni nucleari o subnucleari, che implicano variazioni significative della somma delle masse a riposo del sistema; al contrario, data la piccolezza del difetto di massa nei legami chimici, la massa è praticamente conservata nelle reazioni chimiche.
Nella meccanica relativistica abbiamo una relazione notevole che lega massa a riposo di un corpo, la sua energia e la sua quantità di moto. Dalla definizione di energia abbiamo:
dove γ è il fattore di Lorentz. Le componenti spaziali "P" del quadrimpulso sono invece:
D'altra parte il vettore è uno scalare "m" per una quadrivelocità: la norma quadra di un tale quadrivettore vale sempre -m²c², perciò, chiamando "p" la norma euclidea del vettore tridimensionale quantità di moto (cioè l'intensità dell'usuale quantità di moto moltiplicata per il fattore γ):
Sostituendo nell'ultima equazione quelle precedenti otteniamo l'equazione cercata:
Da questa equazione si nota come anche particelle con massa nulla possano avere energia/quantità di moto diverse da zero. Nella meccanica classica invece una forza piccola a piacere produrrebbe un'accelerazione infinita su una ipotetica particella di massa nulla ma la sua energia cinetica e quantità di moto resterebbero pari a zero. Invece all'interno della relatività ristretta quando "m = 0", la relazione si semplifica in:
Per esempio, per un fotone si ha formula_48, dove ν è la frequenza del fotone: la quantità di moto del fotone è quindi pari a:
La meccanica classica si limita a prendere atto della proporzionalità tra massa inerziale e massa gravitazionale come fenomeno empirico ma tenendo queste due grandezze ben distinte e separate. Solo con la teoria della relatività generale si ha una unificazione dei due concetti, risultato che, secondo Albert Einstein, dà ""alla teoria generale della relatività una tale superiorità rispetto alla meccanica classica che tutte le difficoltà che si incontrano nel suo sviluppo vanno considerate ben poca cosa"".
Uno dei principi sui quali si basa la relatività generale è il principio di equivalenza. Nella sua versione "forte", esso afferma che in un campo gravitazionale è sempre possibile scegliere un sistema di riferimento che sia localmente inerziale, cioè che in un intorno sufficientemente piccolo del punto le leggi del moto assumono la stessa forma che avrebbero in assenza di gravità. È facile verificare che questo principio implica il principio di equivalenza debole, che sancisce proprio l'equivalenza tra massa inerziale e gravitazionale: infatti supponiamo di avere due corpi sottoposti unicamente alla forza di gravità (supponiamo che siano abbastanza vicini da poter trascurare eventuali variazioni del campo gravitazionale).
Se la massa inerziale e quella gravitazionale dei due corpi fossero diverse, esse subirebbero accelerazioni diverse, ma allora sarebbe impossibile trovare un sistema di riferimento nel quale viaggino entrambe di moto rettilineo uniforme, cioè in condizione di assenza di forze.
Un celebre esperimento mentale che si basa sull'equivalenza tra la massa inerziale e quella gravitazionale è quello dell'ascensore di Einstein. In una delle versioni di questo esperimento, una persona si trova all'interno di una cabina chiusa, senza la possibilità di osservare l'esterno; lasciando cadere una palla, osserva che cade con una accelerazione "g" = 9,81 m/s². Schematizzando, ciò può essere dovuto a due motivi:
Einstein diede molta importanza al fatto che l'osservatore non possa decidere, dal suo punto di vista, quale delle due situazioni si verifichi realmente: ciò determina una sostanziale equivalenza tra i sistemi di riferimento accelerati e quelli sottoposti alla forza di gravità. Questo esperimento mentale è una delle linee-guida che hanno portato Albert Einstein alla formulazione della teoria della relatività generale, tramite una rivisitazione del principio d'inerzia: infatti i corpi liberi non percorrono sempre delle rette, ma delle geodetiche nello spazio-tempo, curvato dalla presenza di masse. Si noti che in uno spazio-tempo piatto, cioè nel quale vige la metrica di Minkowski, in assenza di forze gravitazionali, le geodetiche sono proprio rette e ci si riconduce quindi al principio d'inerzia newtoniano.
Sul finire degli anni trenta si è capito che l'unione della meccanica quantistica con la relatività ristretta doveva portare allo sviluppo di teorie fisiche delle interazioni elementari in termini di campi quantizzati. In questa rappresentazione le particelle elementari sono descritte come eccitazioni quantizzate dello stato di vuoto, che può contenere un numero intero di particelle e/o antiparticelle di ogni tipo, create e distrutte nelle interazioni fra i campi. Il formalismo necessario a questo salto concettuale è contenuto nella procedura della seconda quantizzazione.
In prima quantizzazione, l'evoluzione dei campi relativistici è governata da varie equazioni, analoghe dell'equazione di Schrödinger, la cui forma dipende dai gradi di libertà e dal tipo di particelle che sono descritte. Ad esempio un campo scalare soddisfa l'equazione di Klein-Gordon:
e descrive i bosoni di spin nullo; l'equazione di Dirac:
descrive invece i fermioni di spin 1/2. Le soluzioni di queste equazioni soddisfano esattamente la relazione di dispersione fra energia e momento richiesta dalla relatività ristretta:
Nonostante questo, la probabilità per una particella di spin nullo di propagarsi al di fuori del cono luce è non nulla, sebbene esponenzialmente decrescente. Per risolvere questa e altre inconsistenze si rese necessario lo sviluppo della teoria di campo quantistica.
Nell'ambito delle teorie di campo, e quindi della seconda quantizzazione, la situazione è più complicata a causa del fatto che le particelle fisiche sono descritte in termini di campi e interagiscono tra di loro attraverso lo scambio di particelle virtuali. Per esempio, nell'elettrodinamica quantistica, un elettrone ha una probabilità non nulla di emettere e riassorbire un fotone, oppure un fotone può creare una coppia elettrone-positrone che a loro volta, annichilendosi, formano un fotone identico all'originale. Questi processi sono inosservabili direttamente, ma producono effetti sulla misura delle "costanti" delle teorie fisiche che dipendono dalla scala di energie a cui queste stesse costanti vengono misurate. Ad esempio, in una teoria asintoticamente libera, come la cromodinamica quantistica per le interazioni nucleari forti, la massa dei quark tende a decrescere logaritmicamente con l'aumentare dell'energia. Questa dipendenza dalla scala delle masse e delle costanti di accoppiamento è il principale risultato ottenuto dalla teoria della rinormalizzazione.
La predizione teorica del bosone di Higgs nasce dal fatto che alcune particelle mediatrici di forza sono massive e per descriverle consistentemente con le procedure della rinormalizzazione, la relativa teoria deve essere invariante rispetto alle simmetrie interne di gauge. È facile mostrare che le lagrangiane contenenti termini espliciti di massa (come quelli con la "m" nelle equazioni del moto del paragrafo precedente) rompono la simmetria di gauge. Per ovviare a questo problema si introduce un campo, detto campo di Higgs, accoppiato agli altri campi (fermioni e campi di gauge) in modo da fornire, sotto determinate ipotesi, un termine di massa che mantenga la simmetria del sistema sotto trasformazioni interne. Il meccanismo di Higgs è il metodo più semplice di dare massa alle particelle in modo completamente covariante, e il bosone di Higgs è stato a lungo considerato il "tassello mancante" del modello standard. Una particella consistente con il bosone di Higgs è stata infine scoperta nel 2012 dagli esperimenti ATLAS e CMS presso l'acceleratore LHC presso il CERN. A rigor di termini, il meccanismo di Higgs è l'accoppiamento necessario a dare massa ai bosoni vettori W e Z, mentre la massa dei leptoni (elettroni, muoni, tauoni) e dei quark, ovverosia dei fermioni, è regolata dalla interazione di Yukawa; si noti che gli accoppiamenti del bosone di Higgs con i fermioni non sono calcolabili da principi primi, ma sono anch'essi numeri introdotti "ad hoc" nelle equazioni.
Articoli:
</text>
</doc>
<doc id="8953" url="https://it.wikipedia.org/wiki?curid=8953">
<title>Temperatura</title>
<text>
La temperatura di un corpo può essere definita come una misura dello stato di agitazione delle entità molecolari delle quali è costituito. In altre parole, la temperatura è una proprietà fisica intensiva, definibile per mezzo di una grandezza fisica scalare (ovvero non dotata di direzione e verso), che indica lo "stato termico" di un sistema.
Essa inoltre può essere utilizzata per prevedere la direzione verso la quale avviene lo scambio termico tra due corpi. Infatti la differenza di temperatura tra due sistemi, che sono "in contatto termico", determina un flusso di calore in direzione del sistema meno caldo (o più freddo), che continua finché non si sia raggiunto l'equilibrio termico, in corrispondenza del quale i due sistemi hanno la stessa temperatura.
Il concetto di temperatura nasce come tentativo di quantificare le nozioni comuni di "caldo" e "freddo".
In seguito la comprensione via via maggiore dei fenomeni termici estende il concetto di temperatura e mette in luce il fatto che le percezioni termiche al tatto sono il risultato di una complessa serie di fattori (calore specifico, conducibilità termica, eccetera) che include la temperatura. Tuttavia la corrispondenza tra le impressioni sensoriali e la temperatura è approssimativa, infatti in genere al tatto il materiale a temperatura più alta appare più caldo, però ci sono numerose eccezioni. Un oggetto d'argento, per esempio, appare più freddo (o più caldo) di un oggetto di plastica alla stessa temperatura, se tale temperatura è minore (o maggiore) della temperatura del corpo umano per la diversa conduttività termica che fa si che siano diverse le velocità con cui viene sottratto calore dalla superficie della cute. Ovviamente il materiale come l'argento che è più conduttore del calore sembra più freddo perché raffredda più rapidamente la cute.
Ciò è dovuto al fatto che il nostro cervello percepisce la temperatura in corrispondenza delle terminazioni nervose, il che implica che ad innescare la percezione di caldo/freddo è la variazione di temperatura della parte del nostro corpo in contatto con il materiale, non la temperatura del materiale. Per tale motivo l'argento è recepito come "più caldo" rispetto ad un oggetto di plastica alla stessa temperatura se tale temperatura è maggiore della temperatura del corpo umano poiché l'argento, essendo un ottimo conduttore termico, scambia calore più velocemente rispetto alla plastica, scaldando la pelle più velocemente. Per lo stesso motivo, l'argento è recepito come "più freddo" rispetto ad un oggetto di plastica alla stessa temperatura se tale temperatura è inferiore della temperatura del corpo umano.
I primi tentativi di dare un numero alla sensazione di caldo o di freddo risalgono ai tempi di Galileo e dell'Accademia del Cimento. Il primo termometro ad alcool, di tipo moderno, viene attribuito tradizionalmente all'inventiva del granduca di Toscana Ferdinando II de' Medici. Ma si va affermando la convinzione che il termometro a liquido in capillare chiuso sia stato inventato da altri, molto prima. Il termometro a mercurio viene attribuito a Gabriel Fahrenheit, che nel 1714 introdusse una scala di temperature in uso ancor'oggi; un'altra, detta all'epoca della definizione scala centigrada, si deve a Anders Celsius nel 1742.
La relativa precocità delle misure di temperatura non implica che il concetto di temperatura fosse ben chiaro a quei tempi. La distinzione chiara fra calore e temperatura è stata posta solo dopo la metà del 1700, da Joseph Black. In ogni caso, il termometro consente di definire il concetto di equilibrio termico.
La temperatura è la proprietà fisica che registra il trasferimento di energia termica da un sistema ad un altro.
Quando due sistemi si trovano in equilibrio termico non avviene nessun trasferimento di energia e si dice che sono alla stessa temperatura. Quando esiste una differenza di temperatura, il calore tende a muoversi dal sistema che viene detto a temperatura più alta verso il sistema che diremo a temperatura più bassa, fino al raggiungimento dell'equilibrio termico.
Il trasferimento di calore può avvenire per conduzione, convezione o irraggiamento.
Le proprietà formali della temperatura vengono studiate dalla termodinamica. La temperatura svolge un ruolo importante in quasi tutti i campi della scienza, in particolare in fisica, chimica, biologia.
La temperatura "non è" una misura della quantità di energia termica o calore di un sistema, però è ad essa correlata. Pur con notevoli eccezioni, se ad un sistema viene fornito calore la sua temperatura aumenta, mentre se gli viene sottratto calore la sua temperatura diminuisce; in altre parole un aumento di temperatura del sistema corrisponde a un assorbimento di calore da parte del sistema, mentre un abbassamento di temperatura del sistema corrisponde a una cessione di calore da parte del sistema.
Su scala microscopica, nei casi più semplici, la temperatura di un sistema è legata in modo diretto al movimento casuale dei suoi atomi e delle sue molecole, cioè un incremento di temperatura corrisponde a un incremento del movimento degli atomi. Per questo, la temperatura viene anche definita come l'indice dello "stato di agitazione molecolare del sistema" (inoltre l'entropia viene definita come "lo stato di disordine molecolare"). Ci sono casi in cui è possibile fornire o sottrarre calore senza variazione della temperatura, poiché il calore fornito o sottratto può essere causa della variazione di qualche altra proprietà termodinamica del sistema (pressione, volume, etc.) oppure può essere implicata in fenomeni di transizione di fase (come i passaggi di stato), descritti termodinamicamente in termini di calore latente. Analogamente è possibile aumentare o diminuire la temperatura di un sistema senza fornire o sottrarre calore.
La temperatura è una grandezza fisica scalare ed è "intrinsecamente" una proprietà intensiva di un sistema. Essa infatti non dipende dalle dimensioni del sistema o dalla sua quantità di materia, ma "non" corrisponde alla "densità" di nessuna proprietà estensiva.
Sono stati sviluppati molti metodi per la misurazione della temperatura. La maggior parte di questi si basano sulla misurazione di una delle proprietà fisiche di un dato materiale, che varia in funzione della temperatura.
Uno degli strumenti di misura più comunemente utilizzati per la misurazione della temperatura è il termometro a liquido. Esso consiste di un tubicino capillare di vetro riempito con mercurio o altro liquido. L'incremento di temperatura fa espandere il liquido e la temperatura viene determinata misurando il volume del fluido all'equilibrio. Questi termometri possono essere calibrati in modo che sia possibile leggere le temperature su una scala graduata, osservando il livello del fluido nel termometro.
Un altro tipo di termometro è il termometro a gas.
Altri strumenti importanti per la misurazione della temperatura sono:
I termometri che acquisiscono immagini nella banda dell'infrarosso sfruttano tecniche di termografia, basate sul fatto che ogni corpo emette radiazioni elettromagnetiche la cui intensità dipende dalla temperatura.
Nel misurare la temperatura di un materiale occorre accertarsi che lo strumento di misura sia alla stessa temperatura del materiale.
In certe condizioni il calore dello strumento può introdurre una variazione della temperatura, la misura rilevata risulta quindi differente dalla temperatura del sistema. In questi casi la temperatura misurata varia non solo con la temperatura del sistema, ma anche con le proprietà di trasferimento di calore del sistema. Per esempio, in presenza di un forte vento, a parità di temperatura esterna, si ha un abbassamento della temperatura corporea, dovuto al fatto che l'aria accelera i processi evaporativi dell'epidermide. La temperatura dell'aria misurata con un termometro avvolto in una garza umida prende il nome di temperatura di bulbo umido. Essa è influenzata dall'umidità relativa del flusso: con diminuire di questo valore una quota crescente di calore dell'acqua all'interno della garza viene assorbito dalla porzione di acqua che evapora. Ciò causa l'abbassamento di temperatura dell'acqua rimanente. Succede di conseguenza che la temperatura di bulbo umido in generale risulti inferiore alla corrispondente temperatura misurata a bulbo secco (o asciutto). In questo modo è possibile determinare con buona approssimazione l'umidità relativa di una massa d'aria conoscendo le due temperature.
Nello specifico: il sudore si porta sulla superficie corporea da cui tenderà ad evaporare assorbendo calore latente di vaporizzazione questo assorbimento di calore dovuto al passaggio di stato dell'acqua (sudore che evapora) comporta un abbassamento della temperatura corporea quale conseguenza del fatto che il calore viene prelevato dall'organismo; ora: essendo l'evaporazione un processo diffusivo, esso viene accelerato in rapporto al gradiente di concentrazione del vapore in aria.
Se ci troviamo in presenza di vento il gradiente di concentrazione del vapore in prossimità dell'interfaccia pelle/aria verrà mantenuto basso grazie alla continua diluizione del fluido (aria).
Praticamente: l'aria contiene una certa quantità di vapore detta umidità relativa che è frazione della quantità massima di vapore contenibile (si veda Pressione di vapore a saturazione) che è a sua volta funzione esclusiva della temperatura, il sudore evaporando tenderà ad aumentare la concentrazione di vapore nell'aria attigua alla superficie da cui sta evaporando (interfaccia pelle/aria). 
Se non vi fosse movimento d'aria il vapore tenderebbe a diffondersi pian piano dalla zona a più alta concentrazione (prossimità del corpo) alla zona a concentrazione più bassa (resto dell'ambiente circostante) con una velocità che diminuirebbe man mano che la concentrazione di vapore nell'aria aumenta (in concomitanza con la diminuzione progressiva del gradiente di concentrazione) il tutto seguendo una legge di diffusione Leggi di Fick. 
In questo caso il processo avverrebbe con una velocità contenuta. 
Nel caso invece, in cui fosse presente del vento, questo andrebbe a diluire l'aria carica di vapore in prossimità della superficie cutanea con dell'aria a tenore di vapore più basso (quella dell'ambiente circostante), andando così a ristabilire il precedente gradiente di concentrazione accelerando il tal modo l'evaporazione.
Un'evaporazione accelerata aumenta la velocità di cessione di calore latente di evaporazione con conseguente abbassamento della temperatura. Ecco perché un corpo bagnato cede calore più velocemente di uno asciutto da cui il concetto di temperatura di bulbo umido.
Due corpi A e B si dicono in equilibrio termico quando hanno la medesima temperatura, misurata con l'aiuto di un terzo corpo, il termometro C. Quando formula_1 e formula_2 si afferma che formula_3 e quindi A e B sono in equilibrio.
Si tratta dell'applicazione alla fisica di uno dei principi fondamentali della logica, il principio della transitività dell'uguaglianza, per questo alcuni chiamano l'affermazione sopraddetta "principio zero della termodinamica".
La temperatura non costituisce una vera e propria grandezza fisica. La proprietà fisica che il concetto di temperatura intende quantificare può essere ricondotta essenzialmente a una relazione d'ordine fra i sistemi termodinamici rispetto al verso in cui fluirebbe il calore se fossero messi a contatto. Per questo, alla scelta, necessariamente arbitraria, di un'unità di misura per una grandezza fisica, corrisponde, nel caso della temperatura, la scelta, anch'essa necessariamente arbitraria, di una "scala di misurazione".
L'arbitrarietà in questo caso è maggiore rispetto a quello dell'unità di misura per grandezza fisica: in quest'ultimo la relazione di trasformazione fra un'unità di misura e un'altra può essere solo proporzionale (il rapporto fra le due unità di misura considerate). Nel caso della temperatura, invece, una qualsiasi trasformazione monotòna di una particolare scala termometrica scelta preserverebbe comunque la relazione d'ordine e dunque quella così ottenuta costituirebbe un'alternativa del tutto legittima al problema di quantificare la temperatura. Ecco perché, per esempio, le scale termometriche di Celsius, di Kelvin e di Fahrenheit hanno fra di loro relazioni che includono costanti additive (dunque non sono proporzionali).
Nonostante la temperatura non sia in senso stretto una grandezza fisica, si fa riferimento alle scale termometriche usando espressioni mutuate da quelle delle altre grandezze fisiche, quindi anche per la temperatura si parla di "unità di misura".
Le prime unità di temperatura, dell'inizio del '700, sono di derivazione completamente empirica poiché si riferiscono tutte alla transizione di stato di una sostanza in condizioni ambiente. Sono anteriori anche al pieno sviluppo della termodinamica classica. Per citarne alcune appartengono a questa categoria le scale Rømer (1701), Newton (attorno al 1700), Réaumur (1731), Fahrenheit (1724), Delisle o de Lisle (1738), Celsius (1742). Tutte le unità di misura di queste scale venivano e sono tuttora chiamate gradi (cui corrisponde sempre il prefisso ° al simbolo dell'unità: °C è il simbolo del grado Celsius, mentre C è il simbolo del Coulomb).
In Europa nelle applicazioni di tutti i giorni è ancora comunemente usata e tollerata la scala Celsius (chiamata in passato "scala centigrada"), nella quale si assume il valore di 0 °C corrisponde al punto di fusione del ghiaccio e il valore di 100 °C corrisponde al punto di ebollizione dell'acqua a livello del mare.
Il simbolo °C si legge «grado Celsius» perché la dizione «grado centigrado» non è più accettata dall'SI in quanto può confondersi con l'unità assoluta Kelvin.
Nel Sistema Internazionale il grado Celsius è tollerato.
Un'altra scala relativa, usata spesso nei paesi anglosassoni, è la scala Fahrenheit. Su questa scala il punto di fusione dell'acqua corrisponde a 32 °F(attenzione a non confondere temperatura di fusione 0 °C,cioè 32 °F, con temperatura di congelamento che comincia a 4 °C, cioè 39,2 °F); e quello di ebollizione a 212 °F (temperatura che rimane invariata per tutto il tempo di ebollizione, cioè cambiamento di fase).
La seguente equazione converte i gradi Fahrenheit in gradi Celsius:
Le unità assolute nascono nella seconda metà dell'800 e tengono conto del traguardo raggiunto dalla termodinamica classica rappresentato della definizione della temperatura assoluta . In ordine alcune delle più importanti sono: il Rankine (1859), il kelvin (1862) e il Leiden (circa 1894?).
Il kelvin è tuttora l'unità di misura adottata dal Sistema Internazionale (simbolo: K). Il sistema internazionale considera sbagliati sia la dicitura "grado kelvin" che l'uso del simbolo °K. Fino al 2019 un kelvin (1 K) viene formalmente definito come la frazione 1/273,16 della temperatura del punto triplo dell'acqua (il punto in cui acqua, ghiaccio e vapore acqueo coesistono in equilibrio).
Una differenza di temperatura in kelvin quindi è equivalente in Celsius, ma le scale sono fra loro diverse in quanto hanno punto zero diverso: c'è uno scostamento tra le due pari alla temperatura assoluta della fusione dell'acqua a pressione atmosferica: (273,15 K).:
formula_5
Dal 2019, la scala termometrica assoluta viene definita a partire dalla costante di Boltzmann, il cui valore è definito esatto.
La seguente tabella mette a confronto varie scale di misurazione della temperatura; i valori riportati, quando necessario, sono arrotondati per difetto.
Con l'avvento a fine '800 della meccanica statistica, la temperatura assoluta è stata definitivamente fatta coincidere con la energia di agitazione termica delle molecole del materiale considerato. Perciò la temperatura può essere misurata in unità di misura energetiche (per esempio nel Sistema Internazionale, il joule), introducendo un fattore di conversione:
questo fattore di conversione (o costante dimensionale) viene chiamato costante di Boltzmann e ha le dimensioni di unità di energia/unità assoluta. Per esempio, per convertire un valore di temperatura da kelvin a joule la costante di Boltzmann deve essere espressa in joule/kelvin, e in questo caso ha valore numerico esatto:
Invece se si vuole convertire un valore di temperatura da kelvin a elettronvolt il valore è il precedente diviso per il valore della carica fondamentale, quindi:
Quindi per esempio 27,0 °C equivalgono a 27,0+273,15= 300,15 kelvin che equivalgono a qualche zeptojoule ovvero a qualche centielettronvolt:
Come Planck ha scritto nella sua "Nobel lecture" nel 1920:
In effetti Boltzmann fu il primo a mettere in relazione entropia e probabilità nel 1877, ma sembra che tale relazione non sia mai stata espressa con una specifica costante finché Planck, nel 1900 circa introdusse per primo "k", calcolandone il valore preciso, e dandole il nome in onore di Boltzmann. Prima del 1900, le equazioni in cui ora è presente la costante di Boltzmann non erano scritte utilizzando l'energia delle singole molecole, ma nella costante universale dei gas e nell'energia interna del sistema.
Se poniamo in contatto termico due sistemi inizialmente chiusi e di volume fissato (per esempio due vani di un recipiente a pareti rigide, separati da una parete non adiabatica, anch'essa rigida), avverranno cambiamenti nelle proprietà di entrambi i sistemi, dovuti al trasferimento di calore tra loro. Il raggiungimento dell'equilibrio termico si ha dopo un certo intervallo di tempo: si raggiunge uno stato termodinamico di equilibrio in cui non avvengono più cambiamenti.
Una definizione formale della temperatura si può ottenere dal principio zero della termodinamica, che afferma che se due sistemi ("A" e "B") sono in equilibrio termico tra loro e un terzo sistema ("C") è in equilibrio termico con "A", allora anche i sistemi "B" e "C" sono in equilibrio termico. Il principio zero della termodinamica è una legge empirica, cioè è basata sull'osservazione dei fenomeni fisici. Siccome "A", "B" e "C" sono in equilibrio termico tra loro, è ragionevole asserire che questi sistemi condividono un valore comune di qualche loro proprietà. Meglio ancora, possiamo dire che ciascuno di questi sistemi si trova in uno stato termico equivalente ("allo stesso livello") rispetto a un ordinamento basato sulla direzione del flusso di calore eventualmente scambiato. Il concetto di temperatura esprime proprio questa "scala di ordinamento".
Per quanto detto, il valore assoluto della temperatura non è misurabile direttamente, perché rappresenta solo un livello ("grado") su una scala. È possibile scegliere delle "temperature di riferimento", o "punti fissi", basandoci su fenomeni che avvengono a temperatura costante, come la fusione o l'ebollizione (cambiamenti di stato) dell'acqua, ed esprimere la temperatura di un sistema come compresa fra due delle temperature scelte come riferimento. È chiaro che così facendo avremmo bisogno di un numero via via maggiore di temperature di riferimento per poter distinguere il livello termico di due sistemi molto vicini fra loro sulla scala termometrica.
In alternativa, si può considerare un sistema fisico e una sua proprietà che sperimentalmente varia con la temperatura. Per esempio, certi metalli come il mercurio variano il proprio volume in corrispondenza di variazioni di temperatura. Finché non viene stabilita una scala termometrica, non è possibile stabilire in maniera quantitativa la dipendenza del volume dalla temperatura. Non ha senso chiedersi se l'aumento è lineare, quadratico o esponenziale, perché per il momento la temperatura è solo una "proprietà di ordinamento". Possiamo invece usare le misure della grandezza termoscopica scelta, la dilatazione del metallo, per assegnare un valore numerico alla temperatura. Basterà prendere una sola temperatura di riferimento (per esempio quella di fusione dell'acqua) e misurare la lunghezza di una barra di metallo termoscopico a quella temperatura.
Qualsiasi altro sistema che, in equilibrio termico con quella barra, risulterà in una lunghezza maggiore (minore), sarà a temperatura maggiore (minore) della temperatura di fusione dell'acqua. Inoltre basterà confrontare la lunghezza della barra in equilibrio con due sistemi diversi per poter stabilire, senza bisogno di metterli a contatto, quale dei due è a temperatura più alta. Quindi è possibile sfruttare la lunghezza della barra come valore numerico per indicare la temperatura di sistema. L'andamento lineare fra le differenze di temperatura di due sistemi e le differenze di lunghezza nella barra termoscopica non è una proprietà fisica del metallo bensì una conseguenza della definizione di grado termometrico.
Le scelte del sistema fisico e della grandezza termoscopica, che cioè varia con la temperatura, da impiegare come riferimento sono arbitrarie.
Si può considerare per esempio come sistema termodinamico di riferimento una certa quantità di gas. La legge di Boyle indica che la pressione p di un gas è direttamente proporzionale alla temperatura, mentre la legge di Gay-Lussac indica che la pressione è direttamente proporzionale alla densità di numero. Questo può essere espresso dalla legge dei gas ideali come:
dove "T" è la temperatura assoluta, "n" è la densità numerica del gas, (misurabile per esempio in unità fisiche tipo molecole/nanometro cubo, o in unità tecniche come moli/litro introducendo il fattore di conversione corrispondente alla costante dei gas). Si può quindi definire una scala di temperature basata sulle corrispondenti pressioni e volumi del gas. Il "termometro a gas" presenta una elevata precisione, per cui è utilizzato per calibrare altri strumenti di misura della temperatura.
L'equazione dei gas ideali indica che per un volume fissato di gas, la pressione aumenta all'aumentare della temperatura. La pressione è una misura della forza applicata dal gas sull'unità di area delle pareti del contenitore ed è correlata all'energia interna del sistema, in particolare ad un aumento di temperatura corrisponde un aumento di energia termica del sistema.
Quando due sistemi con temperature differenti vengono posti a contatto termico tra di loro, la temperatura del sistema più caldo diminuisce, indicando in generale che il calore "lascia" il sistema, mentre il sistema più freddo incamera energia e aumenta la sua temperatura. Quindi il calore "si muove" sempre da una regione a temperatura maggiore verso una a temperatura minore, questa differenza di temperatura, detto anche gradiente di temperatura, influenza il trasferimento di calore tra i due sistemi.
È possibile definire la temperatura anche in termini del secondo principio della termodinamica, che stabilisce che ogni processo risulta in un'assenza di cambiamento (per un processo reversibile, ovvero un processo che è possibile far evolvere all'inverso) o in un aumento netto (per un processo irreversibile) dell'entropia dell'universo.
La seconda legge della termodinamica può essere vista in termini di probabilità: si consideri una serie di lanci di una moneta; in un sistema perfettamente ordinato il risultato di tutti i lanci sarà sempre testa o sempre croce. Per ogni numero di lanci esiste solo una combinazione in cui il risultato corrisponde a questa situazione. D'altra parte, esistono numerose combinazioni risultanti in un sistema disordinato, dove una parte dei risultati è testa e un'altra croce. All'aumentare del numero di lanci, aumenta il numero di combinazioni corrispondenti a sistemi non perfettamente ordinati. Per un numero abbastanza elevato di lanci, è preponderante il numero di combinazioni corrispondenti a circa 50% di teste e circa 50% di croci e ottenere un risultato significativamente differente da 50-50 diventa improbabile. Allo stesso modo i sistemi termodinamici progrediscono naturalmente verso uno stato di massimo "disordine", ovvero massima entropia.
Abbiamo stabilito precedentemente che la temperatura di due sistemi controlla il flusso di calore tra di loro e abbiamo appena mostrato che l'universo - e ci aspetteremmo qualsiasi sistema naturale - tende ad avanzare verso lo stato di massima entropia. Quindi ci aspetteremmo che esista un qualche tipo di relazione tra temperatura ed entropia. Allo scopo di trovare questa relazione, consideriamo innanzitutto la relazione tra calore, lavoro e temperatura.
Un motore termico è un congegno che converte una parte del calore in lavoro meccanico; l'analisi della macchina di Carnot ci fornisce la relazione cercata. Il lavoro prodotto da un motore termico corrisponde alla differenza tra il calore immesso nel sistema ad alta temperatura, "q" e il calore emesso a bassa temperatura, "q". L'efficienza "η" è pari al lavoro diviso il calore immesso, ovvero:
dove "w" è il lavoro svolto ad ogni ciclo. Si vede che l'efficienza dipende solo da "q/q". Poiché "q" e "q" corrispondono rispettivamente al trasferimento di calore alle temperature "T" e "T", "q/q" è funzione di queste temperature, cioè:
Il teorema di Carnot stabilisce che i motori reversibili operanti alle due stesse temperature assolute sono ugualmente efficienti. Quindi qualsiasi motore termico operante tra "T" e "T" deve avere la stessa efficienza di un motore consistente di due cicli, uno tra "T" e "T", l'altro tra "T" e "T". Questo è vero solo se:
per cui:
Siccome la prima funzione è indipendente da "T", "f(T,T)" è della forma "g(T)/g(T)", ovvero:
dove g è una funzione di una singola temperatura. Possiamo scegliere una scala di temperature per cui:
Sostituendo quest'ultima equazione nell'equazione in quella dell'efficienza, otteniamo una relazione per l'efficienza in termini di temperatura:
Per "T"=0 K l'efficienza è del 100% e diventa superiore al 100% per ipotetiche temperature minori di 0 K. Poiché un'efficienza superiore al 100% vìola il primo principio della termodinamica, 0 K è la temperatura asintoticamente raggiungibile. In effetti, la temperatura più bassa mai ottenuta in un sistema macroscopico reale è stata di 450 picokelvin, o 4,5×10 K, conseguita da Wolfgang Ketterle e colleghi al Massachusetts Institute of Technology nel 2003. Sottraendo il termine di destra dell'equazione (5) dalla porzione intermedia e riarrangiando l'espressione, si ottiene:
dove il segno − indica che il calore è ceduto dal sistema. Questa relazione suggerisce l'esistenza di una funzione di stato, chiamata entropia "S", definita come:
dove il pedice "rev" indica che il processo è reversibile. La variazione dell'entropia in un ciclo è zero, per cui l'entropia è una funzione di stato. L'equazione precedente può essere riarrangiata al fine di ottenere una nuova definizione della temperatura in termini di entropia e calore:
Siccome l'entropia "S" di un dato sistema può essere espressa come una funzione della sua energia E, la temperatura "T" è data da:
Il reciproco della temperatura è il tasso di crescita dell'entropia con l'energia.
La temperatura è legata alla quantità di energia termica posseduta dal sistema. Tranne che nei passaggi di stato, quando a un sistema viene fornito calore la sua temperatura aumenta proporzionalmente a quella quantità di calore. La costante di proporzionalità viene detta capacità termica e corrisponde alla 'capacità' del materiale di immagazzinare calore.
Il calore è conservato in diversi modi, corrispondenti ai vari stati quantici accessibili dal sistema. Con l'aumento della temperatura più stati quantici diventano accessibili, risultando in un incremento della capacità calorica. Per un gas monoatomico a bassa temperatura, gli unici modi accessibili corrispondono al movimento traslazionale degli atomi, così tutta l'energia è dovuta al movimento degli atomi.
Ad alte temperature diventa possibile la transizione degli elettroni, che incrementa la capacità calorica. Per molti materiali queste transizioni non sono importanti sotto i 10 K, invece per alcune molecole comuni le transizioni sono importanti anche a temperatura ambiente. A temperature estremamente alte (&gt;10 K) possono intervenire fenomeni di transizione nucleare. In aggiunta alle modalità traslazionali, elettroniche e nucleari, le molecole poliatomiche possiedono modalità associate con la rotazione e le vibrazioni lungo i legami chimici molecolari, che sono accessibili anche a basse temperature. Nei solidi la maggior parte del calore immagazzinato corrisponde alla vibrazione atomica.
Come detto sopra, per un gas monoatomico ideale la temperatura è legata al moto traslazionale o alla velocità media degli atomi. La teoria cinetica dei gas fa uso della meccanica statistica per correlare questo movimento all'energia cinetica media degli atomi e delle molecole del sistema.
In particolare per un gas monoatomico ideale l'energia interna è pari ai 3/2 della temperatura (assoluta, in unità energetiche):
Quindi un gas ha un'energia interna di circa 1 eV a una temperatura di circa 666 meV cioè a circa 7736 K, mentre a temperatura ambiente (circa 298 K) l'energia media delle molecole d'aria è pari a circa 38,5 meV. Questa energia media è indipendente dalla massa delle particelle. Benché la temperatura sia legata all'energia cinetica "media" delle particelle di un gas, ogni particella ha la sua energia, che potrebbe non corrispondere alla media. In un gas in equilibrio termodinamico la distribuzione dell'energia (e quindi delle velocità) delle particelle corrisponde alla distribuzione di Maxwell.
La temperatura 0 K viene detta zero assoluto e corrisponde al punto in cui le molecole e gli atomi hanno la minore energia termica possibile, cioè zero. Nessun sistema macroscopico può dunque avere temperatura inferiore od uguale allo zero assoluto.
Non esiste un limite superiore per i valori di temperatura. In termini di meccanica statistica, l'aumento di temperatura corrisponde ad un aumento dell'occupazione degli stati microscopici a energie via via più alte rispetto allo stato fondamentale. Formalmente la temperatura infinita corrisponde a uno stato del sistema macroscopico in cui tutti gli stati microscopici possibili sono ugualmente probabili (o, in altri termini, sono occupati con uguale frequenza).
La temperatura di Planck costituisce l'unità di misura di Planck (o unità di misura naturale) per la temperatura. Come molti "valori di Planck", essa rappresenta l'ordine di grandezza in cui effetti quantistici e gravitazionali ("general-relativistici") non possono più essere trascurati gli uni rispetto agli altri, dunque individua la regione al limite della nostra capacità di descrizione attuale (visto che non abbiamo ancora una teoria coerente della gravità quantistica). Il fatto che corrisponda a un valore straordinariamente alto (1.415 × 10 K) e che quindi probabilmente è stata raggiunta solo dall'universo in una precocissima fase immediatamente successiva (circa 10 secondi) al Big Bang, non costituisce un vincolo teorico sui valori fisicamente ammissibili per la temperatura.
L'esistenza di un limite superiore per la velocità degli oggetti non pone in ogni caso un limite superiore per la temperatura, per il semplice motivo che l'energia cinetica di un corpo relativistico non è data dalla formula newtoniana che cresce quadraticamente, ma da una formula più complessa che dà valore infinito quando la velocità si avvicina a quella della luce nel vuoto. Dato che la temperatura è proporzionale all'energia cinetica (per i sistemi di particelle libere) la temperatura comunque diverge all'avvicinarsi della velocità media a c.
A basse temperature, le particelle tendono a muoversi verso gli stati a più bassa energia. Incrementando la temperatura, le particelle si spostano in stati di energia sempre più alti.
Come detto, a temperatura infinita, il numero di particelle negli stati di energia bassi e negli stati di energia alti diventa uguale. In alcune situazioni è possibile creare un sistema in cui ci sono più particelle negli stati alti che in quelli bassi. Questa situazione può essere descritta con una "temperatura negativa".
Una temperatura negativa non è inferiore allo zero assoluto, ma invece è superiore a una temperatura infinita.
Precedentemente abbiamo visto come il calore viene conservato nei vari stati traslazionali, vibrazionali, rotazionali, elettronici e nucleari di un sistema. La temperatura macroscopica di un sistema è correlata al calore totale conservato in tutti questi modi, e in un normale sistema termico l'energia viene costantemente scambiata tra i vari modi. In alcuni casi, però, è possibile isolare uno o più di questi modi.
In pratica i modi isolati continuano a scambiare energia con gli altri, ma la scala temporale di questi scambi è molto più lenta di quella degli scambi all'interno del modo isolato. Un esempio è il caso dello spin nucleare in un forte campo magnetico esterno. In questo caso l'energia scorre abbastanza rapidamente tra gli stati di spin degli atomi interagenti, ma il trasferimento di energia verso gli altri modi è relativamente lento. Siccome il trasferimento di energia è predominante all'interno del sistema di spin, in genere si considera una temperatura di spin distinta dalla temperatura dovuta alle altre modalità.
Basandoci sull'equazione (7), possiamo dire che una temperatura positiva corrisponde alla condizione in cui l'entropia incrementa mentre l'energia termica viene introdotta nel sistema. Questa è la condizione normale del mondo macroscopico, ed è sempre il caso per le modalità traslazionale, vibrazionale, rotazionale, e per quelle elettroniche e nucleari non legate allo spin. La ragione di questo è che esiste un infinito numero di queste modalità e aggiungere calore al sistema incrementa le modalità energeticamente accessibili, e di conseguenza l'entropia. Ma nel caso dei sistemi di spin elettronico e nucleare ci sono solo un numero finito di modalità disponibili (spesso solo 2, corrispondenti allo "spin-up" e allo "spin-down"). In assenza di un campo magnetico, questi stati di spin sono "degeneri", ovvero corrispondono alla stessa energia. Quando un campo magnetico esterno viene applicato, i livelli di energia vengono separati, in quanto gli stati di spin che sono allineati al campo magnetico hanno un'energia differente da quelli anti-paralleli a esso.
In assenza di campo magnetico, ci si aspetterebbe che questi sistemi con doppio spin abbiano circa metà degli atomi con "spin-up" e metà con "spin-down", perché così si massimizzerebbe l'entropia. In seguito all'applicazione di un campo magnetico, alcuni degli atomi tenderanno ad allinearsi in modo da minimizzare l'energia del sistema, portando a una distribuzione con un po' più di atomi negli stati a bassa energia (in questo esempio assumeremo lo "spin-down" come quello a minore energia). È possibile aggiungere energia al sistema di spin usando delle tecniche a radio frequenza. Questo fa sì che gli atomi saltino da "spin-down" a "spin-up". Siccome abbiamo iniziato con più di metà degli atomi in "spin-down", questo porta il sistema verso una miscela 50/50, così che l'entropia aumenta e corrisponde a una temperatura positiva. Ma a un certo punto più di metà degli spin passerà in "spin-up" e in questo caso aggiungere altra energia abbassa l'entropia, perché allontana il sistema dalla miscela 50/50. Questa riduzione di entropia a seguito di un'aggiunta di energia corrisponde a una temperatura negativa.
Molte proprietà dei materiali, tra cui gli stati (solido, liquido, gassoso o plasma), la densità, la solubilità, la pressione di vapore, e la conducibilità elettrica dipendono dalla temperatura. La temperatura gioca anche un ruolo importante nel determinare la velocità con cui avvengono le reazioni chimiche. 
Questa è una delle ragioni per cui il corpo umano ha vari e complessi meccanismi per mantenere la temperatura attorno ai 37 °C, dal momento che pochi gradi in più possono provocare reazioni dannose, con serie conseguenze.
La temperatura controlla anche il tipo e la quantità di radiazione termica emessa da una superficie. Un'applicazione di questo effetto è la lampada a incandescenza, dove un filamento di tungsteno è scaldato elettricamente, fino a raggiungere una temperatura alla quale sono emesse quantità significative di radiazione visibile.
"Impatto della temperatura sulla velocità del suono, la densità dell'aria e l'impedenza acustica":
</text>
</doc>
<doc id="8990" url="https://it.wikipedia.org/wiki?curid=8990">
<title>Joule</title>
<text>
Il joule (simbolo J, IPA:; pronuncia inglese , pronuncia italiana o ) è un'unità di misura del Sistema internazionale (SI). Il joule è l'unità di misura dell'energia, del lavoro e del calore (per quest'ultimo è spesso usata anche la caloria), e dimensionalmente è kg·m²/s² = 1 N·m = 1 W·s. Prende il nome da James Prescott Joule.
Un joule è il lavoro svolto esercitando la forza di un newton per una distanza di un metro, perciò la stessa quantità può essere riferita come newton metro. Comunque, per evitare confusione, il newton metro è tipicamente usato come la misura del momento meccanico e non dell'energia. Ci si può fare un'idea di quanto sia un joule considerando che è circa pari al lavoro richiesto per sollevare una massa di 102 g (una piccola mela) per un metro, opponendosi alla forza di gravità terrestre.
Un joule è anche il lavoro svolto per erogare la potenza di un watt per un secondo.
1 joule equivale a:
L'ultima relazione, in cui compare la costante di Boltzmann, serve solo per ricordare il fatto che le due grandezze sono omogenee. Non è impiegata nella pratica tecnica neanche per convertire temperature molto alte, dell'ordine dei milioni di gradi, che si manifestano ad esempio nelle stelle o in fisica delle particelle. Si vede infatti che una temperatura di un miliardesimo di joule corrisponde infatti già a più di 72mila miliardi di gradi. Risulta quindi più comodo in questi casi estremi utilizzare l'elettronvolt.
L'unità di misura "joule" è correlabile anche, in secondo luogo, alle unità derivate del Sistema Internazionale:
Fra i multipli del joule (si vedano anche i prefissi del Sistema internazionale di unità di misura) troviamo:
</text>
</doc>
<doc id="8997" url="https://it.wikipedia.org/wiki?curid=8997">
<title>Lavoro (fisica)</title>
<text>
In fisica, il lavoro è l'energia scambiata tra due sistemi quando avviene uno spostamento attraverso l'azione di una forza, o una risultante di forze, che ha una componente non nulla nella direzione dello spostamento. Pertanto, ha le dimensioni di una forza applicata lungo una determinata distanza.
Dunque, il lavoro complessivo esercitato su un corpo è pari alla variazione della sua energia cinetica. In presenza, invece, di un campo di forza conservativo, cioè in assenza di effetti dissipativi, il lavoro svolto è pari alla variazione di energia potenziale tra gli estremi del percorso. Il lavoro compiuto da una forza è nullo se lo spostamento è nullo o se questa non ha componenti lungo la direzione dello spostamento.
Il termine "lavoro" è stato introdotto nel 1826 dal matematico francese Gaspard Gustave de Coriolis. Il simbolo più utilizzato per indicare il lavoro è la lettera formula_1, dall'inglese "work", anche se, spesso, nella letteratura italofona lo si indica con la lettera formula_2.
Nel Sistema Internazionale l'unità di misura del lavoro è il joule.
Il lavoro lineare elementare di un campo vettoriale, come una forza formula_3, associato allo spostamento elementare formula_4 è definito come:
che in termini di coordinate cartesiane, si può esprimere come:
Di conseguenza, il lavoro totale lungo una curva formula_7 è definito come l'integrale di linea di seconda specie dell'1-forma differenziale formula_8:
ovvero l'integrale di linea del campo vettoriale formula_3 lungo la curva formula_7.
Nel caso di un corpo che ruota, il lavoro elementare può essere espresso in funzione del momento meccanico, calcolato attorno a un polo formula_12, con formula_13 come braccio:
Pertanto, il lavoro totale del momento corrispondente ai due spostamenti angolari formula_15 e formula_16 è formalmente uguale all'integrale:
Dalla definizione di integrale curvilineo, si hanno le seguenti conseguenze immediate:
Per la proprietà di linearità dell'operatore integrale si ha che:
Per il teorema dell'energia cinetica, il lavoro compiuto dalla risultante delle forze agenti su un corpo è uguale alla variazione della sua energia cinetica: formula_27.
In generale, a causa della generalità del campo formula_28, che varia da punto a punto, il lavoro dipende dalla traiettoria per andare da formula_29 a formula_30. Vi sono però casi di notevole rilevanza fisica nei quali è possibile limitarsi a forze per le quali il lavoro non dipende dalla traiettoria seguita ma solo dalle posizioni iniziale e finale della traiettoria.
Nel caso di un campo di forza conservativo il lavoro è la variazione di energia potenziale tra gli estremi del percorso. In questo caso il lavoro non dipende dal particolare cammino seguito, ma solo dalla posizione iniziale formula_31 e dalla posizione finale formula_32.
A partire dal lavoro è possibile definire la conservatività di un campo di forze: il campo è conservativo "se e solo se" il lavoro della forza lungo una traiettoria chiusa qualsiasi è zero. Infatti:
Nel caso in cui la forza sia costante e il percorso rettilineo, il lavoro diventa pari al prodotto scalare del vettore forza per il vettore spostamento formula_45:
dove formula_47 l'angolo tra la direzione della forza e la direzione dello spostamento.
Il lavoro può essere sia positivo sia negativo, il segno dipende dall'angolo "formula_47" compreso tra il vettore forza formula_49 ed il vettore spostamento formula_45.
Il lavoro svolto dalla forza è positivo seformula_51ovvero se formula_52. Un lavoro positivo è causato da una forza detta "motrice", uno negativo (formula_53), invece, da una forza "resistente".
Il termine utilizzato in fisica differisce dalla definizione usuale di lavoro, che è decisamente legata all'esperienza quotidiana e si può ricondurre, ad esempio, alla fatica muscolare. Infatti si compie un lavoro se si ha uno spostamento: se per esempio si spinge contro un muro naturalmente esso rimarrà fermo e non si avrà lavoro.
Quando la forza ha la stessa direzione dello spostamento, il prodotto scalare equivale al prodotto aritmetico dei moduli dei due vettori:
Anche nel caso di forza parallela ma opposta allo spostamento, l'espressione del lavoro si riduce al prodotto aritmetico dei moduli, ma con segno opposto:
Quando forza e spostamento sono perpendicolari, il lavoro è nullo:
Per i campi conservativi è possibile definire una funzione scalare, detta energia potenziale, la cui variazione tra i punti formula_31 e formula_32 rappresenta il lavoro compiuto dalle forze per andare da formula_29 a formula_30, per quanto detto prima lungo un qualunque percorso.
si indica formula_62 perché, per convenzione, si considera solitamente la variazione di qualcosa dal punto finale a quello iniziale, cioè formula_63.
Il concetto continua a valere se formula_64 non dipende dalla "posizione" ma da uno "stato", ovvero da una posizione nello spazio delle fasi del sistema: ovviamente sostituendo formula_65 con l'equivalente nel caso in questione. Un esempio è il diagramma pressione/volume usato per le macchine termiche.
Considerando campi conservativi, dal teorema dell'energia cinetica (formula_66), si ha che la variazione di energia potenziale è contraria alla variazione di energia cinetica:
e quindi la somma dell'energia cinetica e dell'energia potenziale, detta energia meccanica, è costante in virtù del teorema dell'energia meccanica.
ovvero
L'esempio classico di campi non conservativi si ha considerando le forze d'attrito: l'attrito si oppone sempre al moto, quindi lungo qualsiasi traiettoria avremo l'integrale di una funzione costantemente negativa. E il risultato sarà un lavoro costantemente negativo anche lungo traiettorie chiuse.
Avremmo un lavoro pari a zero, e quindi un campo conservativo solo se l'attrito fosse zero lungo tutto il percorso, solo se, cioè, non avessimo attriti.
Scomponendo, nel teorema dell'energia cinetica, il lavoro in due addendi: formula_70 quello derivante da forze conservative, pari alla variazione di energia potenziale, e quello derivante da forze non conservative abbiamo:
e quindi:
cioè la variazione dell'energia meccanica, cioè la somma di energia cinetica e potenziale, è uguale al lavoro compiuto dalla forze non conservative.
Nel Sistema Internazionale l'unità di misura per il lavoro è il joule che corrisponde allo spostamento di un metro di una forza di un newton:
Tra le altre unità di misura del lavoro ricordiamo:
In termodinamica, il "lavoro" viene scomposto per comodità in due contributi: un contributo relativo alla variazione di volume, detto lavoro di volume, e un contributo indipendente dalla variazione di volume, che prende il nome di lavoro isocoro.
In termodinamica un gas esercita una pressione formula_78 interna sulle pareti del recipiente in cui è contenuto. Se una di queste pareti di superficie formula_79 è mobile e si sposta di una quantità infinitesima formula_80 sotto l'azione di questa pressione, allora il lavoro infinitesimo compiuto dal gas è dato da:
dove formula_82 è la variazione del volume corrispondente. Questo è vero se la trasformazione è reversibile, infatti solo se il sistema è in equilibrio termodinamico è possibile conoscere il valore della pressione formula_78 interna al contenitore. La notazione formula_84 è usata per indicare che il lavoro in fisica non è una funzione di stato, ed invece dipende dalla particolare trasformazione eseguita sul sistema: in termini matematici si dice che il lavoro non è, in generale, esprimibile come un differenziale esatto.
Infine, se il sistema termodinamico subisce una trasformazione generica, quindi per lo più irreversibile, allora possiamo ancora quantificare il lavoro fatto dal gas o dal sistema così:
lavoro fatto contro la pressione esterna formula_86.
Sotto il termine di lavoro isocoro si annoverano tutti i tipi di lavoro che non si riflettono in una variazione di volume, ad esempio: il lavoro elettrico, il lavoro di un campo magnetico, oppure il lavoro svolto da una girante.
In un circuito elettrico il lavoro infinitesimo compiuto dalla batteria che genera la differenza di potenziale formula_87 per far circolare una corrente elettrica formula_88 per un tempo infinitesimo formula_89 è data da formula_90, il segno di tale lavoro sarà positivo o negativo a seconda che rispettivamente la pila eroghi o assorba corrente.
Il valore del lavoro elettrico scambiato tra il tempo formula_91 e il tempo formula_92 si può ottenere integrando l'equazione precedente, dalla quale si ottiene:
nel caso in cui la differenza di potenziale "formula_87" rimanga costante durante l'intervallo di tempo considerato, si può scrivere:
essendo:
La forza che agisce su una particella carica in movimento, immersa in un campo magnetico, è detta forza di Lorentz, che in assenza di campi elettrici esterni è data da:
dove formula_100 è la carica della particella, formula_103 è la sua velocità, formula_104 è il vettore di campo magnetico, legati dal prodotto vettoriale.
Se il moto della carica avviene in senso parallelo alle linee di forza del campo magnetico, significa che la forza è ortogonale allo spostamento, dunque il lavoro è nullo.
</text>
</doc>
<doc id="25642" url="https://it.wikipedia.org/wiki?curid=25642">
<title>Condensatore (elettrotecnica)</title>
<text>
Il capacitore (dall'inglese "capacitor", è noto solo nell'italiano gergale come "condensatore") è un componente elettrico che ha la capacità di immagazzinare l'energia elettrostatica associata ad un campo elettrostatico.
Nella teoria dei circuiti il condensatore è un componente "ideale" che può mantenere la carica e l'energia accumulata all'infinito. Nei circuiti in regime sinusoidale permanente la corrente che attraversa un condensatore ideale risulta in anticipo di un quarto di periodo rispetto alla tensione che è applicata ai suoi morsetti.
Alessandro Volta intorno al 1780 compì numerose esperienze sull'elettricità. In una di queste, notò che lo scudo carico di un elettroforo perpetuo, appoggiato sulla superficie di alcuni materiali scarsamente conduttori, anziché dissipare la propria elettricità la conservava meglio che isolato in aria. 
Si convinse allora che l'afflusso di carica sulla superficie prossima a quella dello scudo richiama carica sulla superficie affacciata di quest'ultimo. Due dischi metallici, delle stesse dimensioni, così che uno può essere sovrapposto all'altro in modo da combaciare perfettamente, compongono quello che Volta stesso chiama "condensatore di elettricità".
Un condensatore (indicato abitualmente con "C") è generalmente costituito da una coppia di conduttori (armature o piastre) separati da un isolante (dielettrico). La carica è immagazzinata sulla superficie delle piastre, sul bordo a contatto con il dielettrico. Quindi all'esterno si avrà un campo elettrico pari a zero a causa dei due campi, uno positivo e uno negativo, che hanno per l'appunto stesso modulo ma segno (verso) opposto, mentre all'interno del dispositivo due volte il campo elettrico perché entrambi i campi, sia quello positivo sia quello negativo, hanno stesso modulo e stesso verso. L'energia elettrostatica che il condensatore accumula si localizza nel materiale dielettrico che è interposto fra le armature.
Se si applica una tensione elettrica alle armature, le cariche elettriche si separano e si genera un campo elettrico all'interno del dielettrico. L'armatura collegata al potenziale più alto si carica positivamente, negativamente l'altra. Le cariche positive e negative sono uguali e il loro valore assoluto costituisce la carica "Q" del condensatore. La carica è proporzionale alla tensione applicata e la costante di proporzionalità è una caratteristica di quel particolare condensatore che si chiama "capacità elettrica" e si misura in farad:
Ossia la capacità è uguale al rapporto tra la carica elettrica fornita Q e la tensione elettrica applicata ΔV.
La capacità di un condensatore piano (armature piane e parallele) è proporzionale al rapporto tra la superficie "S" di una delle armature e la loro distanza "d". La costante di proporzionalità è una caratteristica dell'isolante interposto e si chiama permittività elettrica assoluta e si misura in farad/m.
La capacità di un condensatore piano a facce parallele è quindi:
In figura non sono rappresentati i cosiddetti "effetti di bordo" ai confini delle facce parallele dove le linee di forza del campo elettrico da una faccia all'altra non sono più rettilinee ma via via più curve.
L'energia immagazzinata in un condensatore è pari al lavoro fatto per caricarlo. Si consideri, ora, un condensatore con capacità "C", con carica "+q" su una piastra e -"q" sull'altra. Per muovere un piccolo elemento di carica "dq" da una piastra all'altra sotto l'azione della differenza di potenziale "V"="q/C", il lavoro necessario è "dW":
Integrando questa equazione, infine, si può determinare l'energia potenziale "U" immagazzinata dal condensatore. Gli estremi dell'integrazione saranno 0, ovvero un condensatore scarico, e "Q", ovvero la carica immessa sui piatti del condensatore:
Le due piastre del condensatore sono caricate con cariche di segno opposto, quindi esiste un campo elettrico E fra le piastre. Tale campo E istante per istante è direttamente proporzionale all'energia "U" che si trova nel condensatore e inversamente proporzionale alla distanza "d" fra le piastre. 
Questo risultato vale sia nel caso in cui il condensatore sia collegato a un circuito esterno che mantenga costante la tensione fra le piastre, che nel caso in cui il condensatore sia isolato e sia costante la carica sulle piastre.
La capacità di un condensatore aumenta se fra le piastre viene inserito un dielettrico con una buona
costante dielettrica. In tal caso, se il condensatore è isolato e la carica rimane costante, l'energia
immagazzinata nel condensatore scende e questa energia fornisce il lavoro necessario per "risucchiare" il dielettrico dentro il condensatore. Una lastra di dielettrico che si inserisce esattamente nello spazio tra le piastre viene risucchiata con una forza "F" non costante che dipende dalla lunghezza "x" della porzione di lastra già entrata fra le piastre. È facile dimostrare che tale forza è:
Dove formula_7 è la costante dielettrica relativa della lastra, formula_8 è l'energia nel condensatore all'inizio dell'inserzione (formula_9) e formula_10 è la corsa del dielettrico (ovvero per formula_11 il dielettrico è completamente inserito).
La situazione cambia se il dielettrico è inserito mentre il condensatore è collegato a un circuito che mantenga costante la tensione tra le piastre. In tal caso la forza di risucchio rimane costante e non dipende da "x" e vale:
Il condensatore è un componente di grande importanza e utilizzo all'interno dei circuiti elettrici. Nel seguito si espone il suo comportamento sia in corrente continua sia in corrente alternata.
Dal momento che gli elettroni non possono passare direttamente da una piastra all'altra attraverso il dielettrico che le separa, il condensatore costituisce una discontinuità elettrica nel circuito: quando viene applicata una differenza di potenziale a un condensatore utilizzando un generatore, le due armature si caricano di una quantità "Q" uguale in modulo, ma di segno opposto indotta da un'armatura all'altra. Se la differenza di potenziale è variabile nel tempo si produce inoltre una corrente virtuale indotta, detta corrente di spostamento. Nel dielettrico si assiste al fenomeno della polarizzazione: le cariche si dispongono a formare un dipolo elettrico.
Sapendo che la differenza di potenziale tra le armature è direttamente proporzionale alla carica accumulata su di esse e inversamente proporzionale alla capacità del dispositivo, si ottiene che l'espressione per la tensione è:
prendendo la derivata e moltiplicando per la capacità "C" si ottiene l'espressione per la corrente:
Questa formula equivale alla definizione fisica di corrente di spostamento scritta in termini di potenziale variabile nel tempo anziché in termini di campo elettrico variabile nel tempo. Le due precedenti espressioni costituiscono le relazioni costituitive del condensatore in un circuito elettrico.
Se scriviamo formula_15 come formula_16, valida per un condensatore piano, si nota che il campo formula_17 indotto sulle facce del condensatore diminuisce all'aumentare della distanza formula_18 tra le armature, e quindi è inversamente proporzionale alla capacità elettrica formula_19: la capacità indica quindi un accumulo di energia elettrica nel condensatore stesso.
Quando si montano "n" condensatori in parallelo su ognuno di essi si misurerà la medesima caduta di potenziale. La capacità equivalente formula_20 sarà, quindi, data dalla formula:
Quando si montano "n" condensatori in serie, attraverso ognuno di essi passerà la stessa carica istantanea (in regime dinamico, la stessa corrente), mentre la caduta di potenziale sarà differente da condensatore a condensatore; in particolare, essendo formula_22, a parità di Q la tensione maggiore sarà localizzata ai morsetti della capacità minore. La capacità equivalente totale formula_20 sarà pertanto definita dalla seguente relazione:
In regime di tensione costante (o corrente costante, indicato con la sigla "DC"), il condensatore si carica nel transitorio e a regime raggiunge una situazione di equilibrio dove la carica sulle armature corrisponde esattamente alla caduta di potenziale V applicata moltiplicata per la capacità C secondo la relazione "Q"="CV"; in tal caso, a regime, il condensatore si comporta come un 'circuito aperto' ovvero interrompe ogni flusso di corrente all'interno del circuito (se però la tensione applicata supera il valore di rigidità dielettrica del dielettrico, la 'rottura' di quest'ultimo provoca il rilascio impulsivo di corrente elettrica e il condensatore si scarica quasi istantaneamente comportandosi come un semplice resistore). Al cessare dell'eccitazione sul circuito l'energia elettrica accumulata nel condensatore torna a scaricarsi sotto forma di corrente elettrica rilasciata nel circuito.
Un circuito RC composto da un resistore e un condensatore in serie a un generatore che fornisce una differenza di potenziale "V" è detto "circuito di carica".
Posto il condensatore inizialmente scarico, segue dalle leggi di Kirchhoff:
derivando e moltiplicando per "C" si ottiene l'equazione differenziale ordinaria del primo ordine:
A "t" = 0, la tensione ai capi del condensatore è nulla e la tensione ai capi della resistenza è "V". La corrente iniziale è dunque "i" (0) ="V" /"R", ossia la corrente nel resistore, pertanto:
e sostituendo nella relazione Vo = Vr(t)+ Vc(t)= R i(t)+ Vc(t), si ottiene per Vc(t):
dove "τ" = "RC" è la costante di tempo del sistema. La precedente relazione rappresenta la legge di carica di un condensatore, che ha dunque un andamento esponenziale, e con lo stesso ragionamento si ottengono le equazioni di scarica di un condensatore.
In regime di tensione a corrente alternata (AC) questa induce invece variazioni di potenziale in corrispondenza delle quali le armature si caricano e si scaricano in continuazione per induzione elettrostatica generando ai suoi capi una corrente variabile (alla stessa frequenza dell'eccitazione) che circola poi nel circuito.
A partire dalla relazione:
e posto:
segue che:
ottenendo
Il rapporto tra la tensione e la corrente ai capi del condensatore vale formula_33, e si osserva che la tensione alternata sfasa la corrente di -90 gradi. Esprimendo il rapporto in forma polare, si ottiene l'espressione dell'impedenza caratteristica del dispositivo:
che applicando la formula di Eulero diventa:
dove "j" è l'unità immaginaria, "f" è la frequenza della AC misurata in hertz e "C" la capacità, misurata in farad.
A meno di fenomeni parassiti di dissipazione comunque presenti nei casi reali, il condensatore ideale ha dunque impedenza puramente immaginaria pari alla sua reattanza, indicando con essa la sua capacità di immagazzinare energia elettrica.
Nella legge di Ohm in forma simbolica si considera inoltre l'operatore fasoriale:
dove "X" è la reattanza capacitiva, misurata in ohm, che può essere considerata come analoga a una sorta di resistenza che il condensatore oppone alla corrente e dipende dalla frequenza della AC. Si osserva inoltre che:
La reattanza è così chiamata poiché il condensatore non dissipa potenza, ma semplicemente accumula energia per poi rilasciarla nel transitorio finale. Nei circuiti elettrici, come in meccanica, il condensatore costituisce un carico reattivo, dal momento che immagazzina l'energia e la rilascia alla fine, "reagendo" così alle variazioni di tensione nel circuito. È anche significativo che l'impedenza sia inversamente proporzionale alla capacità, a differenza dei resistori e degli induttori per cui le impedenze sono linearmente proporzionali a resistenza e induttanza rispettivamente.
In un circuito sintonizzato, come un radio ricevitore, la frequenza selezionata è una funzione della serie tra l'induttanza "L" e la capacità "C":
Questa è la frequenza alla quale si trova la risonanza in un circuito RLC.
Come descritto sopra, la reattanza del condensatore fa sì che la corrente sia sfasata in anticipo di 90° rispetto alla tensione. Tuttavia, vari fattori di perdita fanno sì che questo angolo sia leggermente inferiore al caso ideale di 90°. Viene definito di conseguenza l'angolo δ dato dalla differenza tra i 90° ideali e il reale angolo di sfasamento φ. Nelle specifiche tecniche di alcuni condensatori possono esservi due parametri: cos φ e/o tan δ.
Entrambi tendono a 0 per φ che tende al valore ideale di 90°, quindi quanto più sono piccoli, tanto migliore è la qualità del condensatore;
tan δ è anche detto fattore di dissipazione DF e rappresenta il rapporto tra i moduli delle correnti resistiva e reattiva a una certa frequenza (tipicamente 1 kHz).
Il condensatore ha molte applicazioni, quasi tutte nei campi dell'elettronica e dell'elettrotecnica.
I condensatori di rifasamento hanno lo scopo di ridurre la reattanza di un bipolo elettrico e abbassare quindi lo sfasamento fra corrente e tensione alternate (vedi potenza reattiva). A tal fine vengono collegati in parallelo allo stesso, formando un circuito LC accordato sulla frequenza della tensione di alimentazione. Essi possono essere impiegati per bilanciare la reattanza induttiva dei grandi motori elettrici (rifasamento industriale) o per compensare la potenza reattiva circolante sulle reti di trasmissione e di distribuzione (rifasamento di rete). Per tali impieghi vengono installati banchi trifase di condensatori, dove ogni fase è formata da più unità capacitive.
Vengono, inoltre usati come condensatori di avviamento e condensatori di fase per permettere la partenza dei motori asincroni bifase alimentati da reti monofase, che presenterebbero, senza di essi, una coppia di spunto uguale a zero. In tal caso il condensatore, sfasando la corrente di 90 gradi rispetto alla tensione, alimenta un avvolgimento ausiliario: si forma un campo magnetico rotante con coppia motrice diversa da zero, permettendo quindi l'avviamento del motore. Una volta avviato si può rimuovere l'alimentazione a quella fase (avvolgimento della seconda fase e condensatore) del motore, nei sistemi automatizzati viene utilizzato un interruttore/disgiuntore centrifugo.
Nei circuiti elettronici, il condensatore è utilizzato per la sua peculiarità di lasciar passare le correnti variabili nel tempo, ma di bloccare quelle costanti: tramite un condensatore si può fare in modo di unire o separare a volontà i segnali elettrici e le tensioni di polarizzazione dei circuiti, usando i condensatori come bypass o come disaccoppiamento. Un caso particolare di condensatore di bypass è il condensatore di livellamento, usato nei piccoli alimentatori.
Nei condensatori reali, oltre alle caratteristiche ideali si deve tenere conto di fattori come la tensione massima di funzionamento, determinata dalla rigidità dielettrica del materiale isolante, della resistenza e induttanza parassite, della risposta in frequenza e delle condizioni ambientali di funzionamento ("deriva"). La perdita dielettrica inoltre è la quantità di energia persa sotto forma di calore nel dielettrico non ideale. La corrente di perdita è invece la corrente che fluisce attraverso il dielettrico, che in un condensatore ideale è invece nulla.
Sono disponibili in commercio molti tipi di condensatori, con capacità che spaziano da pochi picofarad a diversi farad e tensioni di funzionamento da pochi volt fino a molti kilovolt. In generale, maggiore è la tensione e la capacità, maggiori sono le dimensioni, il peso e il costo del componente.
Il valore nominale della capacità è soggetto a una tolleranza, ovvero un margine di scostamento possibile dal valore dichiarato. La tolleranza spazia dall'1% fino al 50% dei condensatori elettrolitici.
I condensatori sono classificati in base al materiale con cui è costituito il dielettrico, con due categorie: a dielettrico solido e a ossido metallico (detti condensatori elettrolitici).
A seconda delle caratteristiche di capacità e tensione desiderate, e dell'uso che ne deve essere fatto, esistono diverse categorie di condensatori: in mylar, al tantalio, condensatori elettrolitici, ceramici, variabili in aria, diodi varicap, ecc.
In alcuni condensatori d'epoca, la capacità è indicata in centimetri anziché in farad. Questo è dovuto all'utilizzo del Sistema CGS, che prevede appunto la capacità elettrica in cm. In questo caso, la capacità di 1 cm equivale a 1,113 picofarad.
È prassi comune nell'industria riempire zone di circuito stampato non utilizzate con aree di uno strato collegate a massa e di un altro strato collegato all'alimentazione: si realizza così un condensatore distribuito e nel contempo si aumenta la superficie utile alle piste di alimentazione.
Nei condensatori elettrolitici non è presente un materiale dielettrico, ma l'isolamento è dovuto alla formazione e mantenimento di un sottilissimo strato di ossido metallico sulla superficie di una armatura a contatto con una soluzione chimica umida
Vista la esiguità del dielettrico, non possono sopportare tensioni molto alte.
A differenza dei condensatori comuni, la sottigliezza dello strato di ossido consente di ottenere, a parità di dimensioni, capacità molto più elevate. Per contro, occorre adottare particolari accorgimenti per conservare l'ossido stesso.
I condensatori elettrolitici più comuni si basano sulla passivazione dell'alluminio, cioè sulla comparsa di una pellicola isolante di ossido, estremamente sottile, che fa da dielettrico fra il metallo e una soluzione elettrolitica acquosa: per questo, essi hanno una polarità ben precisa che deve essere rispettata, pena il cedimento dell'isolamento e la possibilità di esplosione del condensatore.
Causa di guasto di tali dispositivi è spesso anche il disseccamento della soluzione chimica.
Per consentire l'utilizzo dei condensatori elettrolitici in corrente alternata, si usa connettere due condensatori identici in antiserie, ovvero connessi in serie con la stessa polarità in comune (positivo con positivo o negativo con negativo), lasciando disponibili per la connessione al circuito due terminali della stessa polarità.
La capacità di un condensatore elettrolitico non è definita con precisione come avviene nei condensatori a isolante solido. Specialmente nei modelli in alluminio è frequente avere la specifica "valore minimo garantito", senza un limite massimo alla capacità. Questo non rappresenta un limite per la maggior parte delle applicazioni, come il filtraggio dell'alimentazione dopo il raddrizzamento o l'accoppiamento di segnale.
Esistono diversi tipi di condensatori elettrolitici, sempre in base al tipo di dielettrico:
Un compensatore è un condensatore la cui capacità può essere variata intenzionalmente e ripetutamente entro un intervallo caratteristico. L'applicazione tipica si ha nei circuiti di sintonia delle radio, per variare la frequenza di risonanza di un circuito RLC.
Esistono due categorie di condensatori variabili:
La variazione di capacità è sfruttata anche in alcune applicazioni per convertire un dato fisico in un segnale elettrico:
Solitamente può essere regolato da al dato di targa; quelli più diffusi in mercato raggiungono capacità molto basse, solitamente tra i e gli , molto più raramente si trovano quelli con capacità intorno ai .
Condensatori a film:
Se il codice del tipo di componente è preceduto da una 'M' , si tratta di un film/foglio metallizzato e il condensatore è molto stabile; la sua assenza (oppure una 'F' se il componente è della WIMA Tedesca) indica un foglio metallico d'interconnessione e che il componente è destinato alle alte correnti.
I condensatori per gli impieghi in alta tensione (oltre 1 kV) sono costituiti da "unità capacitive", che vengono collegate in serie e in parallelo in modo da ottenere la reattanza capacitiva richiesta.
Le unità capacitive sono formate da "elementi capacitivi", a loro volta collegati in parallelo e in serie tra loro. L'elemento capacitivo è un pacco di sottili strati alternati di materiale conduttore (solitamente alluminio) e di isolante (solitamente polipropilene), immersi in un liquido isolante (olio minerale). Ogni pacco è dotato di un fusibile, sottile filo conduttore che interrompe il passaggio di corrente in caso di scarica tra diversi strati conduttori del pacco. L'unità capacitiva è dotata internamente di una resistenza di scarica posta tra i suoi terminali.
</text>
</doc>
<doc id="5786552" url="https://it.wikipedia.org/wiki?curid=5786552">
<title>Specchio curvo</title>
<text>
Uno specchio curvo è uno specchio con una superficie riflettente curva. La superficie può essere concava o convessa.
In uno specchio curvo concavo la superficie riflettente è posta nella parte interna della concavità. Negli specchi sferici concavi i raggi di luce incidenti parallelamente all'asse di simmetria dello specchio convergono verso un punto detto fuoco. Per tale caratteristica sono utilizzati in alcune centrali termodinamiche allo scopo di produrre calore tramite la concentrazione dei raggi solari.
Gli specchi concavi possono essere utilizzati per ingrandire delle immagini. Sono quindi ad esempio utilizzati dai dentisti e nella cosmesi.
Negli specchi curvi convessi la superficie riflettente è posta nella parte esterna della concavità. Negli specchi convessi i raggi incidenti parallelamente all'asse dello specchio divergono in diverse direzioni. I prolungamenti dei raggi si incontrano dietro allo specchio in punto detto fuoco virtuale.
Gli specchi convessi vengono utilizzati ad esempio come specchi di sorveglianza.
</text>
</doc>
<doc id="2766335" url="https://it.wikipedia.org/wiki?curid=2766335">
<title>Diffusione ottica</title>
<text>
In fisica, per diffusione ottica (o dispersione; comune anche il termine inglese scattering, che significa letteralmente "sparpagliamento") si intende un'ampia classe di fenomeni di interazione radiazione-materia in cui onde o particelle vengono deflesse (ovvero cambiano traiettoria) a causa della collisione con altre particelle o onde (dal punto di vista quantistico). La deflessione avviene in maniera disordinata e in buona misura casuale e per questo la diffusione si distingue dalla riflessione e dalla rifrazione, che invece cambiano le traiettorie in maniera regolare e determinata. Sono considerati processi di scattering solo le interazioni elastiche o quasi elastiche, che cioè non comportino rilevanti cessioni o guadagni di energia (la diffusione o dispersione di cui si parla qui non hanno nulla a che fare con la diffusione termica (moto casuale di particelle microscopiche) o con la dispersione cromatica (separazione della luce nei suoi vari colori).
In ottica la diffusione ottica rientra nei fenomeni di interazione radiazione-materia ed è di solito riferito alla dispersione della luce da parte di oggetti più o meno microscopici come le particelle colloidali in liquidi o i solidi polverizzati o il pulviscolo o le molecole dell'atmosfera.
Un esempio molto comune di diffusione della luce (scattering di Rayleigh) è dato dal colore blu del cielo: la luce (bianca) del sole incide sull'atmosfera terrestre, le cui molecole diffondono con più facilità le frequenze più alte (ovvero i colori più vicini al blu e al violetto); di conseguenza, mentre il grosso della luce ci arriva direttamente dal sole, la luce blu diffusa ci proviene da tutte le direzioni. E il sole che, quasi per definizione, dovrebbe essere perfettamente bianco, ci appare giallastro, perché gli è stata sottratta un po' di luce blu.
Un altro esempio tipico è il colore bianco del latte o della farina o delle nuvole: in questo caso le particelle del latte o della farina, o le goccioline d'acqua delle nuvole, diffondono uniformemente tutte le frequenze e, siccome il processo si ripete moltissime volte all'interno del mezzo, la direzione di provenienza della luce non è più riconoscibile e il mezzo assume un colore bianco.
Ma la diffusione che ci è di gran lunga più familiare è la "riflessione diffusa" che viene dalla superficie dei solidi che influenza quasi tutto ciò che noi vediamo quotidianamente. Tranne gli oggetti riflettenti e quelli trasparenti (o comunque "limpidi", anche se colorati), come vetri, specchi, liquidi limpidi, metalli lucidati, tutte le altre cose "opache" mandano al nostro occhio quasi solo luce diffusa, bianca, grigia o colorata a seconda se sulla loro superficie la luce incidente è stata solo dispersa o anche assorbita più o meno selettivamente. Possiamo anzi dire che se non ci fosse la diffusione l'aspetto del mondo sarebbe completamente diverso, e ci sembrerebbe di vivere in un gigantesco magazzino di cristallerie, sia pure con parecchi oggetti neri e qualche vetro colorato.
La teoria che sta alla base degli esperimenti con una diffusione finale si basa sul calcolo della sezione d'urto, una misura dell'area coperta dalle particelle presenti nello stato finale (le particelle deflesse o sparpagliate). Una sua semplice definizione è il rapporto tra il numero di particelle che vengono deviate nell'angolo solido (dΩ) in 1 secondo e il numero di particelle che in 1 secondo attraversano l'unità di superficie.
Detto "b" il parametro d'impatto (le dimensioni del bersaglio o il raggio dell'interazione studiata), un buon modo di vedere la sezione d'urto è uguagliare la superficie a disposizione del fascio prima e dopo l'impatto:
dove "Ω" è l'angolo solido, "θ" l'angolo rispetto alla direzione di moto del fascio, "φ" quello sul piano x-y, "σ" la sezione d'urto, funzione degli angoli "θ" e "φ".
Un semplice esempio di diffusione può essere l'urto contro una sfera rigida. In questo caso il parametro d'impatto sarà:
dove "R" è il raggio della sfera.
Ora, poiché la simmetria è sferica, la prima equazione si riduce a:
È semplice, quindi, calcolare la sezione d'urto angolare:
e da questa la sezione d'urto totale:
La sezione d'urto, però, può essere calcolata anche e soprattutto utilizzando la meccanica quantistica. In questo caso ci si dovrà dimenticare del parametro d'impatto, essendo legato al concetto di traiettoria, non sempre definibile in quantistica. Da un punto di vista operazionale, bisogna innanzitutto saper distinguere un caso in cui può essere applicato l'approccio classico fin qui visto da uno in cui è necessario applicare l'approccio quantistico. Il discriminante è, giustamente, l'energia, e più precisamente si distingue tra le basse energie, in cui va bene il regime classico (ottica fisica, ovvero la lunghezza d'onda de Broglie della particella incidente "λ"»"L" dimensioni della targhetta), mentre alle alte energie si applicherà il regime quantistico (ottica geometrica, "λ"«"L").
Per rappresentare i fasci di particelle bisogna, necessariamente, utilizzare le cosiddette funzioni d'onda. Il fascio incidente, ad esempio, può essere caratterizzato da una funzione del tipo "onda piana":
Per il fascio deflesso si utilizzerà un'onda sferica:
La funzione d'onda complessiva risulta quindi:
dove si è scelto di chiamare "z" la direzione privilegiata, ovvero quella lungo il quale si svolge l'urto (la direzione del fascio incidente).
Questa funzione è la "soluzione asintotica" dell'equazione di Schrödinger, ovvero "fotografa" la situazione molto prima e molto dopo l'urto. L'informazione su quest'ultimo sarà contenuta all'interno del fattore f ("θ", "φ").
Innanzitutto è bene sapere che le funzioni d'onda possono essere descritte attraverso alcuni numeri quantici, tra cui il numero quantico azimutale "l", che può assumere solo valori interi positivi. Per scrivere la sezione d'urto, però, è più che sufficiente fermarsi allo sviluppo in "onda S", ovvero con "l" = 0. In questo caso la funzione d'onda totale risulta essere:
dove
Ora, poiché in "onda S" una possibile funzione totale soluzione dell'equazione di Schrödinger libera è l'armonica sferica
si può tranquillamente affermare che mentre la parte entrante (con il segno -) rimane invariata, quella uscente viene alterata di un vettore "S", comunemente detto matrice S, poiché in problemi d'urto complessi diventa una matrice. Tra le proprietà si "S" c'è che il suo quadrato vale l'identità e poi risulta essere unitaria.
Ora, dall'equazione di continuità, posta nulla la variazione di densità di carica nel tempo, si ottiene che il flusso di corrente è pari a:
e poiché la divergenza di quest'ultima è nulla, si ricava proprio la prima proprietà della "S", che può così essere scritta come fattore di fase:
ottenendo come risultato della collisione uno spostamento di fase.
Manipolando, quindi, la "u" si ottiene per il fattore f una semplice espressione dipendente da "δ":
e quindi
La sezione d'urto totale quantistica, integrando sull'angolo solido Ω, risulta essere semplicemente:
Diffusione degli elettroni nell'atmosfera
Prendiamo come esempio un elettrone, e ipotizziamo l'azione di un campo elettrico E non polarizzato, come quello della normale luce solare. Sull'elettrone è presente una forza F dovuta a E, una reazione uguale e opposta dovuta all'attrazione del nucleo, e un certo coefficiente di smorzamento γ. Si ha anche una forza dovuta al campo magnetico, ma la sua intensità, essendo B pari a "eε/c" E, risulta piccola e possiamo trascurarla in prima approssimazione.
Abbiamo dunque il moto di un oscillatore forzato con smorzamento, cioè se poniamo "k = γ m", per la frequenza di risonanza si ha
e abbiamo
F è pari a q(E + ν × B), dove q è la carica dell'elettrone.
Prendiamo "F" come la parte reale di " F e" e "x" come parte reale di " x e".
Sostituendo e dividendo per " e" ottengo la soluzione di questa equazione differenziale
dove ω è la frequenza del campo elettrico e ω la frequenza di risonanza dell'elettrone.
Se F è "Fcos(ωt + Δ)", la x avrà un ulteriore sfasamento θ, la cui tangente è
Possiamo ignorare lo sfasamento in quanto andremo a utilizzare solo la media dello spostamento.
In generale, però, un elettrone o qualsiasi altra particella avrà più di un singolo modo di oscillazione, quindi avremo in realtà una serie di modi di oscillazione. Il modo k-esimo sarà dunque
dove f è una costante di proporzionalità, inferiore a 1, per il modo di oscillazione.
Consideriamo ora l'energia irradiata da un elettrone che oscilla. Il campo elettrico a un angolo φ rispetto all'asse di oscillazione, a distanza "r", dipenderà dal tempo, e dalla posizione "ritardata" della carica, in quanto l'effetto della carica si propaga a velocità c.
Risulta allora
dove ε è la costante dielettrica del vuoto, e "c" la velocità della luce
La potenza irradiata lungo l'angolo φ a distanza "r" è "εcE(r,t)", ossia
Per una variazione dφ su una superficie sferica di raggio "r", il settore di superficie sferica è "dA = 2 π rsenφ dφ", l'energia irradiata su dA è "P(φ)dA", integrando sulla superficie si ha
l'integrale vale 4/3 e quindi
Se deriviamo due volte rispetto al tempo la x ricavata sopra, ottengo
Il valore medio del quadrato del coseno, su un periodo vale 1/2, come si può anche vedere disegnando la funzione "y =" cos"x" e notando che è simmetrica rispetto alle rette "y = 1/2" e "x = π". La potenza media su un ciclo irradiata su una superficie unitaria sarà allora
Vediamo se possiamo trovare un'altra relazione per la potenza. Per la definizione di sezione d'urto,
dove "U" è la densità di energia incidente e σs la sezione d'urto.
Ora, se consideriamo il raggio classico dell'elettrone
e posto
sostituendo nella (1) e sommando su tutti i modi di oscillazione ottengo
Se considero il vettore di Poynting, la densità di energia di un campo elettrico incidente è
Sostituendo questo valore nella (2) ottengo
Questo risultato è valido per i modi di oscillazione per un singolo elettrone. Facendo la media pesata di tutti i tipi di atomi presenti nell'atmosfera, possiamo ottenere la diffusione totale dell'atmosfera.
Le equazioni che descrivono la diffusione sono molto complesse e, specialmente quando questo fenomeno si ripete molte volte, impossibili da risolvere esattamente nel caso generale.
Una soluzione approssimata molto usata è quella detta di Rayleigh:
nel caso in cui le particelle responsabili della diffusione abbiano dimensioni molto minori della lunghezza d'onda della luce incidente la dispersione della luce è isotropa e il coefficiente di diffusione è dato dalla formula:
dove "n" è il numero di centri di diffusione presenti, "d" il loro diametro, "m" il loro indice di rifrazione e formula_35 la lunghezza d'onda della luce incidente.
Nel caso in cui le particelle responsabili della diffusione della luce siano sfere perfette esiste una soluzione matematicamente rigorosa per le equazioni che regolano la diffusione singola detta "soluzione di Mie" dal nome dello scopritore Gustav Mie, che spiegò anche l'effetto Tyndall.
Osservato per la prima volta da Arthur Compton nel 1922, divenne ben presto uno dei risultati sperimentali decisivi in favore della descrizione quantistica della radiazione elettromagnetica. Compton osservò che la radiazione elettromagnetica di alta frequenza (fra gli 0,5 e i 3,5 MeV) che attraversa un bersaglio subisce un aumento di lunghezza d'onda (ossia 'vira' verso il rosso), in misura diversa a seconda dell'angolo di cui viene deflessa la sua direzione di propagazione.
Il cosiddetto effetto Compton può essere spiegato semplicemente se, adottando l'ipotesi dei quanti di luce di Einstein, si pensa alla radiazione elettromagnetica come composta di fotoni che perdono energia nell'urto contro gli elettroni. Questa spiegazione contraddice, apparentemente, la teoria ondulatoria della luce, che sulla base delle equazioni di Maxwell dà conto degli effetti di interferenza. La soluzione del paradosso sta nell'introduzione di una teoria quantistica della radiazione.
La diffusione Thomson non lineare è una generalizzazione della diffusione Thomson, introdotta per studiare lo scattering di impulsi di raggi X ultracorti. Nella diffusione Thomson non lineare, l'intensità della diffusione dell'elettrone da parte di un fotone varia secondo l'ampiezza e la fase a cui l'elettrone vede il campo elettrico del laser impiegato.
La diffusione coulombiana prende il suo nome dal fatto che l'unica forza che si esercita sulle particelle è la forza di Coulomb. Questo tipo di scattering è noto anche come diffusione Rutherford dal celeberrimo esperimento compiuto da Ernest Rutherford nel 1911 allorquando inviò un fascio di particelle alfa (un nucleo di elio) contro una collezione di atomi d'oro (una lamina sottile). L'idea era quella di determinare la struttura dell'atomo e capire se la sua struttura era quella supposta da Thomson (atomo senza nucleo, noto anche come atomo a "panettone") o se c'era qualcosa di diverso.
In particolare, se l'atomo avesse avuto un nucleo al suo interno separato dagli elettroni esterni, allora si sarebbero dovuti osservare anche eventi, ovvero particelle, a grande angolo di deviazione. Ottenuti, effettivamente, questi risultati, il fisico neozelandese concluse allora che l'atomo era costituito da un centro piccolo ma con alta densità di carica circondato da una nuvola elettronica.
Quando la luce propagantesi in un mezzo (aria, acqua, cristalli ecc.) trova una variazione di indice di rifrazione può subire un urto (spesso anelastico) e cambiare la propria direzione di propagazione. Questo tipo di urto è chiamata diffusione di Brillouin.
In particolare le variazioni di indice di rifrazione possono essere dovute, specialmente nei mezzi comprimibili ma anche nelle strutture cristalline, da onde di tipo meccanico che si propagano nel mezzo stesso. Dal punto di vista della meccanica quantistica questo fenomeno viene visto come un'interazione fra i fotoni che compongono la luce con i fononi che compongono l'onda meccanica.
In seguito alla diffusione di Brillouin la luce può subire uno spostamento in frequenza di alcuni GHz (shift di Brillouin).
L'effetto Raman (dal nome del suo scopritore C.V. Raman che nel 1928 lo osservò per primo) è un esempio di diffusione anelastica, ovvero di un urto in cui le particelle che interagiscono si scambiano energia. Nella diffusione Raman un fotone incidente su di una molecola può perdere energia per dare vita a un quanto di oscillazione) o può annichilirne uno, sottraendo energia al materiale, e cambiare così la propria frequenza.
Questo tipo di scattering è ampiamente utilizzato in chimica (spettroscopia Raman) per studiare i modi rotazionali e vibrazionali delle molecole.
Si definiscono fenomeni di diffusione multipla quei casi dove le particelle (o la luce) subiscono, all'interno del mezzo, un numero molto alto di eventi di diffusione. In questi casi gli effetti complessivi sono spesso dominati più da effetti di media che dalle proprietà particolare dei singoli eventi.
Un parametro fondamentale per descrivere la diffusione multipla è il cammino libero medio formula_36, definito come la distanza media fra due eventi di urto successivi.
Data l'estrema complicazione matematica questi fenomeni vengono di solito trattati attraverso delle ipotesi semplificative. Nel novembre 2004, ad esempio, è stato proposto un modello che spiega la polarizzazione della luce diffusa dal cielo sereno tramite un'equazione di quarto grado, ottenuta tramite la teoria delle singolarità.
Quando sia le dimensioni degli scatteratori sia il cammino libero medio sono molto minori della lunghezza d'onda della luce questa non è in grado di risolvere le variazioni microscopiche della polarizzabilità e quindi "vede" un mezzo omogeneo. In questo caso vale l'approssimazione del mezzo effettivo equivalente (EMT, "Effective Medium Theory"), ovvero si può pensare di sostituire al mezzo reale un mezzo omogeneo le cui caratteristiche (prima fra tutte l'indice di rifrazione) dipendono dalla media delle proprietà microscopiche del mezzo reale. Quest'approssimazione è valida in genere per i liquidi e per i solidi amorfi (vetri), che sono di solito omogenei e hanno distanze interatomiche molto minori della lunghezza d'onda della luce, e porta, come soluzione, alle ben note leggi dell'ottica geometrica. La maggior parte dei solidi, invece, sono policristallini o, se organici, sono composti di fibre o di cellule che li rendono disomogenei su una scala che, anche se è microscopica, è superiore o confrontabile con la lunghezza d'onda della luce. Questa approssimazione non è quasi mai verificata per lo scattering multiplo delle particelle perché la lunghezza d'onda di Schrödinger a esse associata è dell'ordine delle distanze interatomiche o minore.
Quando il cammino libero medio formula_36 è molto maggiore della lunghezza d'onda della luce i singoli eventi di scattering possono essere considerati come indipendenti e casuali. A meno che la sezione d'urto non abbia delle divergenze (come accade ad esempio nei cristalli liquidi) il teorema del limite centrale ci dice che la sezione d'urto media vista dalla luce sarà di tipo gaussiano e quindi potremo descrivere la propagazione della luce tramite l'equazione di diffusione.
Nel caso di particelle classiche il processo diffusivo si ha come conseguenza del moto browniano che la particella segue a causa degli urti (statisticamente indipendenti) con le particelle che costituiscono il mezzo in cui si muove.
Quando il cammino libero medio formula_36 è confrontabile con la lunghezza d'onda della luce o delle particelle diffuse le approssimazioni discusse qui sopra non sono più valide. In questi casi gli effetti di interferenza giocano un ruolo cruciale e sorgono molti fenomeni controintuitivi e, a oggi, soggetti a un'intensa attività di ricerca.
Se la lunghezza di coerenza della luce è superiore alle dimensioni caratteristiche coinvolte nel fenomeno, come per esempio la dimensione del campione o la lunghezza del percorso della luce, allora i fenomeni di interferenza si mostrano appieno. Se inoltre le dimensioni più piccole coinvolte sono più lunghe della lunghezza d'onda della luce, allora delle proprietà microscopiche sopravvive solo la media. Quando queste due condizioni sono soddisfatte contemporaneamente, si parla di regime mesoscopico.
È un fenomeno ben noto in ottica sin dai primi studi sui laser. Illuminando con una sorgente di luce coerente, come un laser, una lastra di un materiale fortemente disperdente (in molti casi basta un foglio di carta bianca) si osserva che la luce trasmessa non è distribuita in maniera continua, come ci si aspetterebbe dal modello diffusivo, ma è composta da picchi di intensità molto grande su di uno sfondo quasi nero. Questi sono l'effetto dell'interferenza fra i vari cammini che la luce può seguire all'interno del mezzo e che si sommano costruttivamente solo per alcune direzioni e non per altre.
Quando un'onda incide su di un sistema disordinato e subisce un gran numero di eventi di dispersione c'è una probabilità non nulla che riemerga dalla stessa faccia del mezzo da cui è entrata (in questo caso si dice che l'onda è riflessa). Durante il percorso all'interno del mezzo quest'onda subirà una certa variazione di fase, in parte dovuta ai singoli eventi di scattering, in parte dovuta alla propagazione libera e quindi il fascio incidente e quello riflesso non avranno una relazione di fase ben definita (si dice che i due fasci non sono coerenti). Per via della simmetria per inversione temporale delle leggi fisiche che regolano lo scattering un'onda che percorresse esattamente lo stesso percorso, ma in senso contrario, subirebbe la stessa variazione di fase; questo vuol dire che le due onde che percorrono esattamente lo stesso cammino ma in senso opposto mantengono il proprio accordo di fase e quindi andranno a dare interferenza costruttiva.
Assumendo di prendere in considerazione tre punti (formula_39) i possibili cammini per le onde riflesse saranno: 
Simbolicamente possiamo scrivere l'intensità totale riflessa come:
dove formula_43 rappresenta il complesso coniugato.
I termini misti del tipo formula_44 rappresentano la parte di interferenza casuale dovuta alla particolare realizzazione del disordine nel campione e alla scelta arbitraria dei punti e dà luogo allo "speckle". Questi termini si annullano se facciamo una media su tutte le possibili configurazioni del sistema. Al contrario i termini del tipo formula_45 danno sempre luogo a interferenza costruttiva per ogni configurazione. Ricordandosi che all'interno del mezzo i due fasci subiranno la stessa variazione di fase è facile vedere che se A e B hanno la stessa ampiezza in entrata (ovvero provengono dalla stessa sorgente) si può scrivere
dove formula_47 e formula_48 sono i vettori d'onda iniziali e finali dei due fasci. Ovviamente questo termine sarà massimo quando formula_49 (ovvero formula_50) e andrà a diminuire all'aumentare dell'angolo fra i due fasci. In questo senso si può parlare di un cono di retrodiffusione coerente.
Il profilo angolare del cono può essere calcolato sommando su tutti i possibili percorsi che la luce può compiere nel mezzo e integrando sul tempo. Misure di apertura angolare del cono di retrodiffusione coerente vengono utilizzate per misurare il coefficiente di diffusione di materia di mezzi fortemente disperdenti.
Ci sono situazioni, ad esempio la presenza di un forte campo magnetico, che rendono il sistema non reciproco, ovvero la fase accumulata percorrendo un dato cammino in un senso o in un altro è diversa. In questi casi non si osserva il fenomeno del cono di retrodiffusione coerente.
Proposta per la prima volta da P.W. Anderson nel 1958 (in un articolo che gli valse il Premio Nobel per la fisica nel 1977) la localizzazione di Anderson è un fenomeno dove il normale trasporto diffusivo delle onde (non solo elettromagnetiche ma anche onde di Schrödinger, ovvero elettroni, onde di spin e così via) viene inibito dalla presenza di un forte disordine. Le onde vengono in effetti confinate in una regione limitata del sistema.
Questo ha alcune conseguenze decisamente controintuitive come ad esempio il fatto che il sistema non possa raggiungere l'equilibrio termodinamico e che la resistenza di un mezzo in regime di localizzazione cresca esponenzialmente (invece che linearmente come previsto dalla celeberrima legge di Ohm).
Attualmente la localizzazione di Anderson è oggetto di un acceso dibattito nella comunità scientifica internazionale e molte delle sue proprietà non sono ancora chiare.
</text>
</doc>
<doc id="2766819" url="https://it.wikipedia.org/wiki?curid=2766819">
<title>Resistenza meccanica</title>
<text>
In scienza dei materiali, la resistenza meccanica (o tensione di rottura) è una proprietà meccanica che indica il massimo sforzo che un generico materiale è in grado di sopportare prima che sopraggiunga la sua rottura.
Tale resistenza meccanica ai vari tipi di sollecitazione può essere misurata con prove specifiche di compressione, trazione, flessione, taglio e torsione su un provino, che permettono di caratterizzare rispettivamente di resistenza a compressione, resistenza a trazione, ecc.
Essa dipende da innumerevoli fattori quali:
I materiali ricorrenti nell'ingegneria civile sono di tre specie:
Di norma i materiali duttili vengono provati a trazione; i fragili alla compressione; i plastici a taglio.
I materiali plastici sono largamente studiati per esempio in geotecnica.
La resistenza dei materiali duttili si può calcolare con due metodi:
</text>
</doc>
<doc id="2806922" url="https://it.wikipedia.org/wiki?curid=2806922">
<title>Microscopio ottico</title>
<text>
Il microscopio ottico è un tipo di microscopio che sfrutta la luce con lunghezza d'onda dal vicino infrarosso all'ultravioletto, coprendo tutto lo spettro visibile. I microscopi ottici sono storicamente quelli più vecchi e sono anche tra i più semplici.
Il microscopio ottico a scansione in campo prossimo (SNOM) è un microscopio a scansione di sonda che consente di superare il limite risolutivo legato alla diffrazione (circa 0,2 μm con luce visibile).
Il microscopio ottico (LM) tradizionale (LM acronimo di "light microscope") è il più semplice. Per mezzo di lenti ingrandisce l'immagine del campione, illuminato con luce nell'intervallo spettrale del visibile.
Può essere semplice (un solo sistema di lenti o addirittura una sola lente) o composto (almeno due sistemi, oculare ed obiettivo), e l'illuminazione può raggiungere il campione da dietro, attraversandolo (luce trasmessa), o esserne riflessa (luce riflessa). Il microscopio ottico permette di avere immagini di soggetti dimensionalmente collocati all'incirca tra il millimetro ed il micrometro, anche di esseri viventi. I primi esempi di ingrandimento ottico datano migliaia di anni e risalgono alle civiltà mesopotamiche. Nel 1648 Antoni van Leeuwenhoek osservò e descrisse numerosi microorganismi, utilizzando un microscopio semplice, inizialmente dotato di pochi ingrandimenti e poi perfezionato fino a raggiungerne alcune centinaia (275 accertati, 500 ipotizzati). Nel 1665 Robert Hooke, utilizzando una forma molto rudimentale di microscopio ottico composto, con un limitato potere di ingrandimento ed osservando il sughero vide e descrisse per la prima volta la struttura cellulare propria di tutti i viventi. Nell'Ottocento il microscopio ottico venne anche usato in campo metallurgico, militare e per il controllo dei metalli. Un esempio può essere quello costruito da Andrew Pritchard per la Royal Arsenal.
Si considerano nella classe dei microscopi ottici anche strumenti affini, come i microscopi nell'ultravioletto, che benché sfruttino frequenze non visibili hanno principi e tecnologie simili.
Ogni microscopio a visione diretta può essere fornito di un solo oculare di osservazione oppure di un sistema per la visione binoculare, con entrambi gli occhi. I modelli monoculari permettono uno schema costruttivo più semplice ed economico, riducono le perdite di luminosità, permettono con semplicità di proiettare l'immagine su un sistema di registrazione della stessa. I sistemi binoculari permettono a scapito della semplicità costruttiva e dell'economicità una visione più comoda, specie per lunghi periodi d'osservazione. I sistemi stereoscopici sono ovviamente obbligatoriamente binoculari. Costruttivamente, le teste binoculari, o multioculari, nel caso di ulteriori deviazioni a strumenti di ripresa e misura, vengono realizzate con sistemi ottici che prima suddividono il fascio luminoso, in genere con un sistema di prismi triangolari incollati, in due percorsi ottici che poi riportano sul piano focale dei due oculari mediante un sistema di prismi di Porro o di Porro-Abbe. Il sistema induce ovviamente ulteriori aberrazioni, dovute in massima parte alle proprietà di dispersione ottica degli elementi. Queste aberrazioni devono essere dovutamente valutate calcolate e compensate.
I sistemi binoculari si diffondono sempre più grazie ai fenomeni di sommatoria binoculare, per cui la soglia di rilevamento di uno stimolo è più bassa con due occhi che con uno solo. Ci sono due effetti vantaggiosi nella sommatoria binoculare. In primo luogo, nel rilevare un segnale debole c'è un vantaggio statistico nell'utilizzare due rivelatori: matematicamente il vantaggio è uguale alla radice quadrata di 2, circa 1.41. In secondo luogo, quando alcune cellule della corteccia visiva primaria ricevono segnali da entrambi gli occhi contemporaneamente, le cellule mostrano l'agevolazione binoculare, un maggiore livello di attività rispetto alla somma delle due attività evocate separatamente da ciascun occhio. Il vantaggio d'usare due occhi con attività di rilevazione 1,41 è chiamato sommatoria neurale.
Fenomeni d'interazione binoculare, oltre alla sommatoria bioculare possono inoltre influenzare a vicenda i due occhi in almeno tre modi:
Questi fenomeni spiegano in gran parte la spontanea preferenza nell'utilizzo dei sistemi binoculari, specie nell'uso prolungato.
Come nei primi esemplari di van Leeuwenhoek si tratta di una semplice lente o sistema di lenti (un doppietto frequentemente) con una serie di supporti per il campione ed un sistema elementare di spostamento dell'ottica per la messa a fuoco.
Come il precedente, l'illuminazione in questo caso è frontale o laterale, il caso tipico è la lente contafili in uso in filatelia e nel controllo dei filati dell'industria tessile.
È un microscopio che utilizza la luce trasmessa attraverso il campione da guardare. La luce proviene da una piccola lampadina incorporata o da una sorgente esterna, nel qual caso è indirizzata sul campione tramite uno specchio. Rappresenta il microscopio per antonomasia e viene utilizzato largamente in applicazioni didattiche, scientifiche, tecniche ed industriali in un intervallo di ingrandimenti compreso generalmente tra le decine ed il migliaio. Nei primi esemplari storici si utilizzava luce diurna o luce proveniente da una candela o lampada ad olio. Negli esemplari di uso corrente la sorgente più convenientemente utilizzata è una lampada alogena. Da notare che senza particolari accorgimenti in genere nei microscopi composti l'immagine osservata risulta invertita.
Come il precedente, l'illuminazione in questo caso proviene dall'alto, tramite diversi sistemi. Focalizzazione della sorgente tramite specchi, sistemi a fibre ottiche, LED, epi-illuminazione (che sfrutta lo stesso obiettivo anche per illuminare il campione).
Lo Stereomicroscopio (Microscopio stereoscopico o Stereoscopio) è un microscopio che si avvale in realtà di due diversi e distinti microscopi, in generale composti, ed a basso ingrandimento, formanti tra loro un certo angolo. L'osservazione, particolarmente in luce riflessa, produce un'immagine tridimensionale come la visione diretta, eliminando l'effetto di appiattimento tipico degli altri tipi di microscopi. In genere sono dotati di un sistema di prismi ottici per il raddrizzamento dell'immagine, e quindi l'eventuale manipolazione corretta del campione, senza le consuete inversioni destra-sinistra tipiche dei microscopi composti. Sono per questo utilizzati nell'industria (micro-componentistica), nella dissezione, nella micro-chirurgia ecc.
Il microscopio polarizzatore sfrutta un fascio di luce polarizzata di dato orientamento che attraversa il corpo del preparato, posto su un tavolino rotante, per venire poi analizzato attraverso un secondo filtro polarizzatore, anch'esso orientabile e in genere posto sul piano posteriore dell'obiettivo. Questo tipo di microscopio è molto utilizzato in petrografia, cioè nello studio delle rocce e dei minerali che le compongono. La luce polarizzata è composta da onde che oscillano in un solo piano, detto piano di vibrazione o piano di polarizzazione. Il piano di polarizzazione è perpendicolare alla direzione di propagazione dell'onda stessa. In un raggio di luce normale (non polarizzato) le onde oscillano in tutti i possibili piani.
È un tipo particolare di microscopio che lavora nel campo del visibile come il microscopio a luce trasmessa. Si basa sul fenomeno dell'interferenza luminosa.
Il preparato viene illuminato da un fascio luminoso che viene suddiviso a livello del condensatore in due porzioni di fase differente e con diverso angolo di incidenza. L'ulteriore cambiamento di fase dovuto alla porzione di luce che attraversa il campione, andandosi a ricombinare con la luce non rifratta renderà visibili componenti trasparenti, ma con indice di rifrazione differente da quello del mezzo. In campo biologico la maggior parte dei componenti cellulari è trasparente alla luce visibile, anche a causa dell'elevata presenza di acqua, tuttavia vediamo che le radiazioni luminose una volta oltrepassata una componente o un organello cellulare subiscono dei cambiamenti di fase che dipendono sia dallo spessore, sia dal diverso indice di rifrazione della struttura oltrepassata. Mediante il microscopio a contrasto di fase è possibile andare a determinare tali cambiamenti e convertirli in differenze di densità così da ottenere delle informazioni utili circa la composizione di cellule e tessuti analizzati. Questa tecnica di microscopia è molto utilizzata per osservare le cellule mantenute in vita in apposite colture in vitro; infatti tramite la microscopia a contrasto di fase si evita l'utilizzo di coloranti e fissativi che spesso comportano notevoli alterazioni strutturali, ottenendo così dei dati molto più reali di quella che è l'organizzazione cellulare. La tecnica in questione fu messa a punto dal fisico olandese Frederik Zernike (Frits Zernike) negli anni trenta e gli valse il Premio Nobel per la fisica nel 1953.
Utilizza due treni di onde completamente separate che seguono due diversi percorsi ottici: uno attraversa il preparato che lo sfasa, indi si incontra con il secondo non sfasato, dando luogo a fenomeni di interferenza. Queste forniscono notizie utili, anche quantitative, sui componenti presenti nel campione. E le immagini che ne derivano sono immagini tridimensionali. Danno dei dati quantitativi perché la massa secca è direttamente proporzionale all'indice di rifrazione . 
Analogamente al microscopio a contrasto di fase, questo microscopio è utilizzato per osservare strutture trasparenti non altrimenti visibili in campo chiaro. Combina effetti di interferenza e di polarizzazione e fornisce immagini più contrastate e con un effetto di tipo tridimensionale. Questo metodo di contrasto è conosciuto anche con il nome di "DIC" (differential interference contrast). Una delle due tecnologie più utilizzate è la "Nomarski", dal nome dell'inventore della configurazione ottica che si ritrova in diffusi microscopi odierni a contrasto interferenziale.
L'ultramicroscopio ed il microscopio in campo oscuro sono microscopi ottici con un condensatore paraboloide con pareti a specchio, o comunque munito di uno schermo anulare che ferma i raggi che illuminerebbero direttamente il preparato. Vengono raccolti dall'obiettivo solo i raggi opportunamente deviati (rifratti) oppure diffratti. La diffrazione è il fenomeno fisico per cui, quando un oggetto è illuminato da un'onda luminosa primaria, ogni suo punto diventa a sua volta una sorgente di onde sferica secondarie. In generale questo fenomeno è associato alle tipiche figure di interferenza. Le particelle del preparato che diffrangono sono di dimensioni submicroscopiche, comprese fra 0,1 micrometro e 1 nanometro. Queste particelle, pur avendo dimensioni sotto la soglia di risoluzione del microscopio, appariranno come punti luminosi su sfondo buio, senza dettagli morfologici (effetto Tyndall).
Di questo tipo di microscopio strutturalmente esisterebbero sistemi a luce trasmessa e riflessa, ma per motivi tecnici i primi sono stati relegati ad usi limitati ai campioni opachi. La maggior parte della produzione ed uso prevede sistemi ad epifluorescenza. Questo tipo di microscopio serve per osservare preparati naturalmente fluorescenti o legati con molecole fluorescenti o rese tali da particolari coloranti detti fluorocromi. Questi composti vanno selettivamente a legarsi con strutture cellulari definite.La sorgente luminosa (alogena di alta potenza, lampada di Wood, lampada ad arco e scarica di gas e più recentemente diodi LED ad alta efficienza e laser), che trasmette radiazioni ultraviolette, o comunque di bassa lunghezza d'onda nel visibile, eccita il preparato generalmente dall'alto (sistemi ad "epifluorescenza"). Le componenti del preparato emettono luce di lunghezza d'onda maggiore di quella emessa dalla sorgente luminosa. Questo fenomeno è conosciuto come fluorescenza.Attualmente gli utilizzi più diffusi prevedono l'utilizzo di anticorpi specifici, appositamente prodotti per andare a legarsi con determinate molecole nel campione (che rappresentano l'antigene) utilizzando fluoresceina, rodamina, ed altre simili molecole come fluorocromo legato all'anticorpo per renderlo appunto fluorescente e visibile. Nell'osservazione è fondamentale l'utilizzo corretto dei filtri ottici per selezionare la giusta lunghezza d'onda di eccitazione, la giusta lunghezza d'onda di emissione visibile, e l'arresto della radiazione ultravioletta che danneggerebbe l'occhio dell'osservatore. Gli obiettivi microscopici usati per questo tipo di osservazione non devono contenere lenti che presentino fenomeni di autofluorescenza (come spesso succede per quelli alla fluorite) e devono trasmettere l'ultravioletto (se in epifluorescenza, visto che l'illuminazione passa attraverso l'obiettivo), mentre il grado di correzione cromatico è poco influente sulla qualità dell'immagine, per cui vanno generalmente bene le ottiche acromatiche.
Il Vertico SMI combina infine la microscopia con la modulazione spaziale dell'illuminazione raggiungendo risoluzioni inferiori ai 10 nanometri (1 nanometro = 1 nm = 1 × 10 m)..
Il microscopio confocale si basa su una tecnologia ottica volta ad accrescere sensibilmente la risoluzione spaziale del campione, eliminando gli aloni dovuti alla luce diffusa dai piani fuori fuoco del preparato.
Esistono diverse tecniche per ottenere questo risultato: a disco rotante ("Nipkow disk"), "Programmable Array Microscopes" (PAM), e laser. Quest'ultimo tipo, il più diffuso e denominato CLSM acronimo di "Confocal Laser Scanning Microscope", è un evoluto microscopio a fluorescenza che permette di focalizzare con estrema precisione un laser sul preparato, aumentando notevolmente la risoluzione e la profondità di campo. La sua sorgente luminosa è costituita uno o più laser, generalmente a semiconduttore, per ogni diversa frequenza d'eccitazione. Il meccanismo di direzione del fascio luminoso viene gestito da sistemi computerizzati. Le immagini ottenute sincronizzando col fascio di eccitazione il dispositivo di relazione sono particolarmente definite e spettacolari e possono permettere di evidenziare in colori diversi le differenti molecole presenti nel preparato, permettendone di apprezzarne la tridimensionalità (esempio actina in rosso tubulina del citoscheletro in verde e DNA del nucleo in blu).
Il limite principale della microscopia ottica è la risoluzione massima, strettamente legata al fenomeno della diffrazione. Il criterio di Abbe limita la risoluzione massima a circa 0.5 λ/(n sin θ) per un sistema ottico avente apertura numerica n sin θ con luce di lunghezza d'onda λ. Per luce nello spettro visibile essa si attesta circa a 0,2 µm, dati i limiti teorici imposti dalla massima apertura numerica di una lente. Un sistema per superare il limite è spostarsi su lunghezze d'onda nello spettro dell'ultravioletto. Si riesce così ad incrementare notevolmente la risoluzione, ma lo sviluppo di questo strumento è stato limitato a causa dei problemi tecnici derivanti dalla dannosità della radiazione per gli organismi viventi, dalla non visibilità dell'ultravioletto (occorrono schermi fluorescenti o la fotografia dell'immagine), dalla necessità di usare ottiche in quarzo (trasparenti agli ultravioletti), soprattutto con l'avvento dei microscopi elettronici a basso costo .
</text>
</doc>
<doc id="2749273" url="https://it.wikipedia.org/wiki?curid=2749273">
<title>Spettro di emissione</title>
<text>
Lo spettro di emissione di un elemento chimico o di un composto chimico è l'insieme delle frequenze della radiazione elettromagnetica emesse dagli elettroni dei suoi atomi quando questi compiono una transizione da uno stato ad energia maggiore verso uno a energia minore.
Eccitando (ad esempio per riscaldamento) una sostanza, essa produrrà delle radiazioni, che sono il risultato delle vibrazioni dei suoi atomi, facendo passare tali radiazioni attraverso un prisma, queste verranno deviate a seconda della loro lunghezza d'onda.
Raccogliendo le radiazioni separate su uno schermo o su una lastra fotografica si ottiene lo spettro di emissione della sostanza in questione.Per ogni transizione tra stati, l'energia del fotone emesso è uguale alla differenza di energia dei due stati secondo l'equazione
che mette in correlazione l'energia della transizione con la frequenza del fotone di luce emesso (h è la costante di Planck). Dal momento che in ogni elemento o composto chimico vi sono numerose transizioni possibili, l'insieme dei fotoni di diverse frequenze emessi dall'elemento o dalla molecola ne costituisce lo spettro.
Lo spettro di emissione di ciascun elemento o molecola è unico, per questo la sua analisi, detta spettroscopia, può essere usata per analizzare qualitativamente e quantitativamente una sostanza.
La comparsa di soli determinati colori nello spettro di emissione di un elemento significa che solo certe frequenze di luce sono emesse. Il principio dello spettro di emissione è alla base del funzionamento delle lampade a scarica (le "luci al neon") e del saggio alla fiamma.
Si noti come l'idrogeno - che ha un solo elettrone - abbia meno transizioni possibili e produca uno spettro molto più semplice del ferro (26 elettroni).
</text>
</doc>
<doc id="2044085" url="https://it.wikipedia.org/wiki?curid=2044085">
<title>Metro</title>
<text>
Il metro (simbolo: m, talvolta erroneamente indicato con "mt") è l'unità base SI (Sistema internazionale di unità di misura) della lunghezza.
In origine l'Assemblea nazionale francese approvò il 26 marzo 1791 la proposta di una definizione teorica del metro come 1/ del quarto del meridiano terrestre (compreso fra il polo nord e l'equatore) che passava per Parigi (il cosiddetto meridiano di Parigi). Studi successivi determinarono però che la lunghezza del quarto terrestre era di metri anziché i previsti. Nel 1799 venne creato il primo campione standard in platino iridio.
Col progredire della scienza si ebbero sviluppi successivi finché nel 1983, durante la 17ª "Conférence générale des poids et mesures" (Conferenza generale di pesi e misure) a Parigi, il metro venne ridefinito come la distanza percorsa dalla luce nel vuoto in un intervallo di tempo pari a 1/ di secondo, assumendo che la velocità della luce nel vuoto, per definizione, è pari a "c" = . Questa definizione, ed il valore della costante fisica, sono stati confermati nel 2018 dalla 26ª CGPM.
Il termine "metro" deriva dal greco “metron” che significa misura. Fu ripreso nel 1675 da Tito Livio Burattini, che propose una delle prime definizioni basate sulla lunghezza di un pendolo che batte il secondo, dato che il semi periodo di un pendolo di tale lunghezza è 1,003 s.
La definizione originale del metro basata sulle dimensioni della Terra viene fatta risalire al 1791, stabilita dall'Accademia delle scienze francese come 1/10 000 000 della distanza tra polo nord ed equatore, lungo la superficie terrestre, calcolata sul meridiano di Parigi. Il 7 aprile 1795 la Francia adottò il metro come unità di misura ufficiale, seguita da altri paesi europei. In Italia il metro venne per la prima volta introdotto da parte di Napoleone durante la campagna d'Italia del 1796. Da allora, nonostante svariate resistenze politiche, esacerbatesi durante il Congresso di Vienna, il metro non abbandonò più la penisola italiana, anche se venne adottato dagli stati italiani in tempi e secondo percorsi diversi.
L'incertezza nella definizione del metro portò il Bureau international des poids et mesures (BIPM) a ridefinire nel 1889 il metro come la distanza tra due linee incise su una barra campione di platino-iridio conservata a Sèvres presso Parigi.
Nel 1960, con la disponibilità dei laser, l'undicesima Conferenza generale di pesi e misure cambiò la definizione del metro in: la lunghezza pari a 1 650 763,73 lunghezze d'onda nel vuoto della radiazione corrispondente alla transizione fra i livelli 2p e 5d dell'atomo di kripton-86.
Nel 1983 la XVII Conferenza generale di pesi e misure definì il metro come la distanza percorsa dalla luce nel vuoto in 1/299 792 458 di secondo (ovvero la velocità della luce nel vuoto venne definita essere 299 792 458 metri al secondo). Poiché si ritiene che la velocità della luce nel vuoto sia la stessa ovunque, questa definizione è più universale della definizione basata sulla misurazione della circonferenza della Terra o della lunghezza di una specifica barra di lega metallica e il metro campione può essere riprodotto fedelmente in ogni laboratorio appositamente attrezzato. L'altro vantaggio è che può (in teoria) essere misurato con precisione superiore rispetto alla circonferenza terrestre o alla distanza tra due punti.
Sempre grazie agli esperimenti in laboratorio, dalla fine del 1997 è possibile raggiungere un ordine di accuratezza dell'ordine di 10 m. Questo risultato è ottenibile sfruttando la relazione "λ" = "c"/"ν" ("λ" lunghezza d'onda, "c" velocità della luce, "ν" frequenza della radiazione) utilizzando oscillatori laser stabilizzati a frequenza conosciuta (imprecisione Δ"ν"/"ν" migliore di 10) la cui radiazione viene utilizzata in sistemi di misura interferometrici.
Utilizzando i prefissi SI si ottengono i seguenti multipli e sottomultipli (in "corsivo" i multipli e sottomultipli non ricavati con uso di prefissi o non facenti parte del Sistema internazionale di unità di misura):
Il picometro è comunemente usato nella misura di distanze su scala atomica; il diametro di un atomo è compreso circa tra 30 e 600 pm. È uguale a un milionesimo di micron ed era chiamato micromicron, stigma o bicron. Una volta era utilizzato il simbolo µµ.
Lo yottametro potrebbe essere utilizzato per misurare distanze intergalattiche, ma gli astronomi sono da tempo abituati ad utilizzare anni luce e parsec e continuano a preferirli.
</text>
</doc>
<doc id="2015075" url="https://it.wikipedia.org/wiki?curid=2015075">
<title>Onda trasversale</title>
<text>
Si ha un'onda trasversale quando le particelle del mezzo in cui si propaga l'onda oscillano perpendicolarmente alla direzione di propagazione. Sono onde trasversali quelle che si propagano, per esempio, sulle corde di una chitarra e di altri strumenti a corda.
Un' onda trasversale è un'onda in movimento che è composta da oscillazioni che avvengono perpendicolari alla direzione del trasferimento di energia. Se un'onda trasversale si muove nella direzione positiva "x", le sue oscillazioni sono nelle direzioni sopra e sotto che giacciono nel piano "y-z".
Se si fissa l'estremità di un nastro o corda e si tiene l'altra estremità nella mano, si possono creare onde trasversali muovendo la mano in su e giù. Notate che si possono creare anche "onde di lancio" ("launch waves") muovendo la mano da un lato all'altro. Questo è un punto importante. Ci sono due direzioni indipendenti in cui il moto accade che, in questo caso, sono "y" e "z", menzionate sopra. Inoltre, se accuratamente si muove la mano circolarmente in senso orario, si avranno "onde di lancio" che descrivono un'elica in movimento verso sinistra così come vengono via via propagate. Similarmente, se si muove la mano circolarmente in senso antiorario, si formerà al contrario un'elica in movimento verso destra. Questi fenomeni di moto "simultaneo" in due direzioni vanno oltre i tipi di onde che si possono creare sulla superficie dell'acqua; in generale un'onda su una corda può essere a "due dimensioni". Le onde a due dimensioni trasversali mostrano un fenomeno chiamato polarizzazione. Un'onda prodotta muovendo con la mano una linea, su e giù per esempio, è un'onda polarizzata linearmente, un caso speciale. Un'onda prodotta muovendo la mano circolarmente è un'onda polarizzata circolarmente, un altro caso speciale. Se il moto non è strettamente confinato in una linea o un circolo la mano potrà descrivere un'ellisse e l'onda sarà polarizzata ellitticamente.
Le onde elettromagnetiche funzionano in questo stesso modo, sebbene sia più difficile a vedersi, e sono anch'esse onde trasversali a due dimensioni. Si può pensare a un raggio di luce come un qualcosa funzionante come le onde prodotte su una corda.
Questa natura a due dimensioni non dovrebbe essere confusa con i due componenti di un'onda elettromagnetica, campo elettrico e magnetico, che sono mostrati qui nel diagramma dell'onda elettromagnetica. Il diagramma dell'onda di luce mostra polarizzazione lineare. Ognuno di questi campi, elettrico e magnetico, mostra il comportamento dell'onda in due dimensioni trasversali, esattamente come le onde su una corda.
L'animazione dell'onda sul piano trasversale mostrata è anche un esempio di polarizzazione lineare. L'onda mostrata potrebbe avvenire sulla superficie dell'acqua.
Le onde trasversali sono onde che hanno movimento perpendicolare alla direzione della vibrazione
Esempi di onde trasversali includono le onde sismiche S (secondarie), e il moto dei campi elettrico (E) e magnetico (M) in un'onda elettromagnetica; entrambi oscillano perpendicolarmente l'un l'altro come pure verso la direzione del trasferimento di energia. Quindi un'onda elettromagnetica consiste di due onde trasversali, essendo la luce visibile un esempio di onda elettromagnetica. Vedi lo spettro elettromagnetico per informazioni sui differenti tipi di onde elettromagnetiche.
Una corda oscillante è un altro esempio di onda trasversale; un esempio più quotidiano potrebbe essere un'onda del pubblico (detta anche "onda messicana" o "onda da stadio").
</text>
</doc>
<doc id="2037948" url="https://it.wikipedia.org/wiki?curid=2037948">
<title>Campo gravitazionale terrestre</title>
<text>
Il campo gravitazionale terrestre è un fenomeno naturale presente sulla Terra, per il quale il pianeta esercita un'attrazione sui corpi che si manifesta attraverso il peso. La forza attrattiva del nostro pianeta, rispetto ad un altro corpo, deriva dalla sua massa 
e dalla distanza, secondo la legge universale di Newton. 
Dove G è la costante gravitazionale che vale circa 6,67 × 10  m³kgs. M ed m sono le due masse prese in considerazione e d² è la distanza fra le due masse. formula_2 è il versore che indica la congiungente dei due corpi e punta da un corpo in direzione dell'altro: la forza gravitazionale è, infatti, attrattiva.
Einstein, nella sua relatività generale, era giunto a questa conclusione matematica per definire il campo gravitazionale: 
dove:formula_4 è il tensore di Ricci e indica la curvatura del vettore nello spazio
formula_5 la curvatura scalare
formula_6 è il tensore metrico
formula_7 è il tensore energetico o tensore energia impulso
formula_8 è la costante cosmologica pensata per rendere statico l'universo
formula_9 la velocità della luce e G la costante gravitazionale.
Il vettore del campo, l'accelerazione di gravità, varia a seconda non solo della distanza dal centro della Terra (e quindi dall'altitudine), ma anche dalla latitudine. Questo determina disomogeneità del campo gravitazionale stesso, che risulta essere meno forte all'equatore e più forte ai poli.
Il 17 marzo 2009 è stato lanciato in orbita bassa (circa 250 km dalla superficie terrestre) il satellite GOCE, realizzato dall'ESA in collaborazione con un consorzio di 45 aziende europee, allo scopo di ottenere una mappatura precisa del campo gravitazionale del pianeta.
Il peso di un corpo non è sempre costante sulla superficie della Terra ma cambia in base alla sua posizione, alla latitudine e all'altezza, oltre che al materiale sottostante. Si deve, inoltre, considerare l'influsso che dà al peso dei corpi la forza centrifuga generata dal nostro pianeta nel suo movimento. Il peso infatti è il risultato tra la forza di attrazione e la forza centrifuga che è massima all'equatore e diventa praticamente nulla ai poli; di conseguenza, il peso di un corpo all'equatore sarà minore del peso del medesimo corpo misurato ai poli. 
Il secondo fattore da tener conto è la forma distorta del geoide Terra, essendo la superficie terrestre, all'equatore, più distante dal centro del pianeta che ai poli. L'accelerazione di gravità varia da g = 9,7799 m/s² all'equatore a g = 9,83217 m/s² ai poli. Si è perciò assegnato all'accelerazione di gravità un valore medio convenzionale pari a g = 9,80665 m/s²
G è la costante di gravitazione universale e fra i primi a darne una misura concreta fu Henry Cavendish nel 1798 che pose due masse in equilibrio sospese ad un filo in torsione e di fronte a queste altre due masse più grandi in modo da poter apprezzare la debole attrazione. In realtà fu John Michell che concepì prima del 1783 l'esperimento oggi noto come Esperimento Cavendish e fu questi il primo a misurare la forza di gravità fra due corpi in laboratorio formulando la prima valutazione accurata della massa della Terra e della costante "g".I risultati ci dimostrano la debolezza della forza gravitazionale; il rapporto tra la forza gravitazionale e quella elettrica colombiana che si esercita tra un elettrone e un protone è pari a 10 e spiega così perché oggetti posti in prossimità e liberi di muoversi non si avvicinano di fatto come avviene per le cariche elettriche.
L'assenza di Gravità è un concetto improprio perché la forza di gravità è un tipo di fenomeno impossibile da schermare o neutralizzare; qualsiasi barriera interposta la potenzia e non esistono in termini dimostrabili forze antigravitazionali a meno di non considerare l'esistenza di oggetti con velocità superluminale. In ogni caso il concetto di forza necessita di un riferimento locale e per il campo gravitazionale terrestre la condizione apparente dell'assenza di gravità si realizza se il corpo è in caduta libera, cioè se cade con la stessa accelerazione di gravità. Un esempio teorico e poi uno pratico chiarirà il concetto. Poniamoci sopra un'alta montagna dell'Antartide e iniziamo a lanciare sassi sempre più distanti verso l'equatore; se gli fornissimo una velocità sufficiente a fargli superare la linea dell'equatore e se l'aria non facesse attrito, dopo alcune ore il sasso ci colpirebbe alle spalle, cioè dalla parte opposta dopo aver fatto il giro della terra. Abbiamo messo in orbita, a bassissima quota, un oggetto; meglio: il sasso è in caduta libera attorno al nostro pianeta. Per evitare l'attrito con l'aria un satellite artificiale viene posto in orbita geostazionaria a centinaia di km di altezza. E ora il secondo esempio facilmente realizzabile. Se ad un bicchiere di plastica pieno di acqua pratichiamo un forellino alla base vediamo subito uscire il liquido ma se lo stesso bicchiere nelle stesse condizioni viene fatto cadere, dal foro non fuoriesce nulla perché l'oggetto è in caduta libera verso il centro della Terra. Il liquido non esercita nessuna pressione sul forellino. In missilistica la condizione di assenza di gravità si simula o in grandi piscine sfruttando il principio di Archimede oppure per brevi tratti dentro aerei in caduta libera verso la terra. Gli oggetti in caduta libera circolare intorno al nostro pianeta, come i satelliti o la stazione spaziale ISS, sono sistemi non inerziali in quanto accelerati dalla forza di gravità che funge da forza centripeta. Se all'improvviso il campo gravitazionale sparisse, tali oggetti si allontanerebbero dalla Terra per inerzia lungo una linea tangente alla traiettoria circolare. I corpi e le persone al loro interno risultano in una condizione apparente di assenza di gravità; dal punto di vista del sistema di riferimento non inerziale, ad esempio l'ISS in orbita attorno alla Terra, ciò è dovuto all'equilibrio tra la forza di gravità del campo terrestre e la spinta della forza centrifuga verso l'esterno dell'orbita, una forza apparente che dipende dalla velocità angolare e dal raggio dell'orbita ed è uguale e opposta alla forza centripeta.
Dal punto di vista di un sistema inerziale, ad esempio le cosiddette stelle fisse, l'astronauta è in "caduta libera" assieme all'ISS, ovvero sono ambedue sottoposti alla stessa accelerazione gravitazionale, un'accelerazione centripeta che li vincola ad orbitare attorno alla Terra e non gli permette di allontanarsi per inerzia lungo una linea tangente all'orbita. La stessa apparente assenza di gravità si riscontrerebbe in un ascensore in caduta libera: i corpi nell'ascensore fluttuerebbero apparentemente privi di peso perché in caduta libera assieme all'ascensore.
I satelliti geostazionari ruotano insieme alla terra su un'orbita con periodo pari 
a T secondo la formula:
dove:
Per cui essi appaiono fissi da un osservatore terrestre e diventano essenziali per le comunicazioni telefoniche e radiotelevisive.
Le maree sono movimenti periodici del mare e nascono dall'equilibrio dinamico tra la forza gravitazionale della Terra e quella dei corpi massivi più vicini come il Sole e la Luna. Caratteristica unica della forza di Gravità è quella di essere sempre attrattiva e di non conoscere ostacoli, anzi un ostacolo diventa un potenziamento e una sommatoria al vettore iniziale. Inoltre la forza di Gravità come il campo elettromagnetico agisce a distanza infinita per cui si tratta la Gravità come una forza elettromagnetica considerando la quasi totale analogia matematica. Le maree sono spostamenti di masse oceaniche che si verificano al passaggio, sul meridiano del luogo, del Sole o della Luna, tutte le altre componenti astrali di fatto non hanno nessuna influenza e questo perché, secondo la formula dettata da Newton, la distanza incide molto più della massa nel generare la forza di marea. Lo spostamento della massa liquida non avviene per diretto sollevamento, questo sarebbe molto esiguo, ma per scivolamento degli strati più superficiali dell'acqua, dalla regola del parallelogramma vettoriale, ed ecco perché in alcune zone favorevoli le maree raggiungono anche i 15 metri mentre nei mari chiusi, come il Mediterraneo, l'oscillazione non supera il mezzo metro. È come se la massa fluida cercasse di seguire lo spostamento apparente del Sole o della Luna.
I dislivelli di marea si possono considerare come un insieme di movimenti periodici che a volte si sommano e altre volte si compensano. Dal punto di vista matematico una marea è la sommatoria di tante onde che si rinforzano del tipo sinusoidale semplice e la formula di base risulta:
dove:
Anche la rotazione della Terra ha quindi un'influenza sul fenomeno, infatti le maree sono doppie perché nel punto opposto in cui si forma la marea dovuta all'attrazione del Sole o della Luna, la forza centrifuga ha la predominanza poiché la distanza è maggiore e si forma anche lì una seconda marea. Anche la litosfera subisce l'attrazione del Sole e della Luna ma i movimenti si apprezzano in pochi millimetri di spostamento. Lo stesso vale per l'atmosfera dove a causa della fluidità si formano onde elastiche simili a quelle oceaniche.
L'affermazione, ormai consolidata, che tutti i corpi attirano gli altri verso il loro centro di massa è in realtà inesatta, essi, infatti, interagiscono fra loro e diventano di fatto un'unica massa in equilibrio dinamico con il loro comune centro di massa, o baricentro, che è spesso difficile da dirimere anche matematicamente (problema dei tre corpi). Questa stessa affermazione avrebbe bisogno anche del calcolo dell'impulso di moto delle masse e del loro versore. Anche lo stesso Newton si pose il problema se al centro della Terra la gravità esistesse ancora e concluse che al centro questa forza si annullava perché da tutti i punti possibili veniva esercitata un'attrazione che veniva poi neutralizzata dalla parte opposta. Quindi, al centro della Terra, la forza di Gravità è zero. A rigor di logica, bisogna ricordare che sul centro della terra interagisce, anche se in modo praticamente ininfluente, tutta la gravità universale e, in particolare, quella della Luna e del Sole, che, per la loro vicinanza, sono capaci di spostare in modo tangibile il centro di massa del sistema Terra, per cui, le due cose, centro della Terra e centro del sistema Terra- Luna -Sole, non coincidono.
Immaginando un tunnel che attraversa il nostro pianeta da una parte all'altra e nel quale viene lasciato cadere un oggetto, questo comincia a precipitare verso il centro della Terra ma nello stesso tempo diminuisce la sua accelerazione fino a raggiungere il centro della Terra e attraversarlo (anche se la forza di gravità è nulla al centro della Terra, l'oggetto attraverserà il centro con una velocità non nulla!) cominciando a sentire una decelerazione che lo porterà all'altro capo della terra a velocità nulla (uguale a quando l'oggetto era stato lasciato cadere). L'oggetto, in assenza di attrito, per inerzia continuerà in un moto oscillatorio perpetuo.
Se si volesse tenere in considerazione la forza di attrito, l'oggetto avrà un moto oscillatorio smorzato.
La legge universale di Newton ci dice che le masse si attraggano fra di loro con una forza proporzionale alla loro grandezza e inversamente al quadrato della loro distanza, il tutto moltiplicato per una costante K che di fatto è un numero piccolissimo. Tale formulazione rimane valida se si considerano i corpi puntiformi e di forma sferica e in pratica incapaci di produrre fenomeni di tipo mareale. Nella realtà le cose non stanno proprio in questi termini perché spesso le masse non sono sferiche e le loro dimensioni sono enormi come le galassie.
Lo stesso Newton aveva anche stabilito, con il secondo principio della dinamica, che una forza dipende dalla massa e dalla accelerazione
In questo caso la massa rappresenta l'inerzia all'accelerazione; ora il quesito è se questa massa inerziale la si può identificare con quella gravitazionale. Dall'esperienza di Henry Cavendish i valori delle masse delle sfere prese a campione per stabilire la forza gravitazionale e in particolare la costante "g" sono masse inerziali per cui le due cose si identificano. Ma anche se numericamente ciò non dà variazioni, è dal punto di vista fenomenologico che emerge un comportamento diverso. Due masse dello stesso peso, a forma discoide, ma con densità diversa avranno una forma spaziale diversa dove la meno densa occuperà più spazio. Ciò vuol dire che a parità di impulso e per il principio della conservazione del moto, una girerà più velocemente dell'altra.
Secondo il principio di equivalenza le due masse debbono coincidere assolutamente perché i corpi in caduta libera devono avere la medesima accelerazione a prescindere dalla loro forma e dalla loro peso. Il principio di equivalenza poi si scinde in forte e debole .
Secondo la fisica classica di Galilei e Newton il campo gravitazionale terrestre era un tipo di energia istantanea che non aveva bisogno di intermediari per agire da un corpo all'altro. Newton infatti nella sua formula non inserisce il tempo ma semplicemente le masse e la loro distanza. Secondo invece la Fisica relativistica einsteiniana ogni forza è generata da un campo e ogni campo energetico deve rispettare le equazioni di Lorentz e quindi non può esistere l'istantaneità ma una qualsiasi forma di energia per agire su un'altra particella ha bisogno di un tempo. Einstein applicando alla forza gravitazionale la trasformazioni del gruppo di Lorentz e il concetto di spaziotempo di Minkowski, lo studioso che per primo affermò che il tempo e lo spazio si potevano annullare singolarmente e perché il concetto potesse continuare a vivere occorreva unirle nella nuova definizione di spazio-tempo, considera il campo gravitazionale uno spazio simile a quello elettromagnetico e per conseguenza ha bisogno di una particella mediatrice – il gravitone o gravifotone – in grado muoversi alla velocità di 300.000 km/s.
Le trasformazioni di Lorentz assumono la forma:
dove
è chiamato fattore di Lorentz e formula_9 è la velocità della luce nel vuoto. Introducendo il quadrivettore:
le quattro equazioni riportate sopra possono essere espresse insieme sotto forma di matrice come:
Da questa formulazione è possibile dimostrare che rimane invariato l'intervallo:
che è quindi un invariante di Lorentz.
Il conflitto relativistico nasce poi quando nella relatività generale si considera il campo gravitazionale non più un vero campo di forza ma un sistema, unico in tutta la storia della scienza, capace di modificare il tempo e lo spazio, secondo il modello matematico di Minkowski e quindi non necessita più di nessuna particella mediatrice.
È pur vero che in matematica si possono percorrere più strade per raggiungere il medesimo risultato ma è anche vero che le due interpretazioni negano o ammettono una particella energetica, in questo caso il gravitone. Secondo perciò la fisica classica se un corpo sparisse all'istante cesserebbe anche la forza di attrazione che questo genera; secondo invece la relatività generale, la forza continuerebbe ad agire fino all'esaurimento della sua corsa verso le altre masse. In pratica se sparisse il Sole, la Terra si accorgerebbe dell'assenza della forza gravitazionale della nostra stella dopo circa otto minuti e mezzo, il tempo cioè che le particelle della gravitazione dal Sole impiegano per raggiungere la Terra. 
 
Empedocle da Agrigento (490 a.C.) individuava nei quattro elementi fondamentali - acqua, aria, terra e fuoco – tutta la realtà che circondava l'uomo. Tali principi venivano da lui denominati infatti "radici". Secondo Talete invece tutto traeva origine dall'acqua ma Aristotele accettò l'impostazione di Empedocle e la convinzione che i quattro elementi costituivano tutta la realtà che circondava l'uomo, resistette quasi fino ai tempi moderni. Gli antichi si erano posti il problema della caduta dei corpi concludendo in modo empirico che gli oggetti più pesanti "cadono" e quelli più leggeri "salgono".
Per spiegare poi come mai dal cielo piovesse l'acqua, che era più pesante dell'aria, si era immaginato un mare sopra le nuvole che ogni tanto si apriva per lasciare cadere la pioggia. Aristotele era dell'idea che oggetti di peso diverso cadevano in modo diverso ma tale convinzione fu contraddetta da Giovanni Filopono nel VI secolo d.C., questi aveva realizzato una serie di esperimenti a proposito dimostrando che oggetti di peso diverso cadono alla medesima velocità introducendo così la costante dell'accelerazione di gravità. Galileo Galilei riprese tali studi e li confermò con una serie di medesimi esperimenti. Newton, costruendo un tubo di vetro dal quale estraeva l'aria, dimostrò che oggetti leggeri come una piuma, non trovando resistenza del mezzo, cadono con la stessa accelerazione di altri corpi più pesanti. Tutti questi esperimenti prendono in considerazione oggetti relativamente piccoli e in assenza di impulso, cioè in quiete. Applicando la legge universale di Newton sulla forza di gravità si dimostra che tutti i corpi si attirano l'un l'altro verso il loro comune centro di massa e per conseguenza salta il concetto che la terra possa attirare tutti i corpi con la medesima accelerazione o velocità. Occorre specificare la grandezza della massa e la presenza dell'impulso e la caduta della cometa Shoemaker-Levy 9 (video) sul pianeta Giove e ripresa dalla sonda Galileo nel 1993 lo dimostra.
</text>
</doc>
<doc id="2090986" url="https://it.wikipedia.org/wiki?curid=2090986">
<title>Energia meccanica</title>
<text>
L'energia meccanica è la somma di energia cinetica ed energia potenziale attinenti allo stesso sistema, da distinguere dall'energia totale del sistema in cui rientra anche l'energia interna.
Quando due sistemi si scambiano tra loro energia meccanica, tale energia in transito è definita lavoro. Dunque l'energia meccanica può essere posseduta da un sistema e scambiata con altri sistemi, mentre il lavoro corrisponde solamente alla parte di energia meccanica che è scambiata.
Per un sistema scleronomo e in presenza di sole forze conservative si dimostra che l'energia meccanica costituisce un integrale di moto, cioè si conserva, e coincide con l'hamiltoniana meccanica. La dimostrazione più semplice discende direttamente dal teorema dell'energia cinetica: se il lavoro compiuto dalle forze è pari alla variazione di energia cinetica del sistema:
Se le forze sono conservative è possibile esprimere il lavoro come variazione di energia potenziale:
si ottiene che la variazione di energia cinetica più la variazione di energia potenziale è identicamente nulla, cioè:
avendo battezzato la quantità "T+U" energia meccanica totale del sistema.
Un corpo in un campo gravitazionale (conservativo) è dotato di una certa energia potenziale dipendente unicamente dall'altezza rispetto ad un punto di riferimento. Se lo lasciamo libero, in assenza di forze dissipative come l'attrito con l'aria, l'energia potenziale iniziale, a mano a mano che cade, si trasforma in energia cinetica (cresce la velocità) mentre la somma delle due energie rimane la stessa.
Chiamando formula_5 e formula_6 rispettivamente la quota rispetto ad un riferimento fisso e la velocità di un corpo all'istante "t", e formula_7 e formula_8 le stesse quantità all'istante iniziale "t"=0, abbiamo:
cioè
che possiamo scrivere come
Il primo membro della precedente esprime l'energia meccanica totale "T + U" del sistema al tempo "t", che è costante ed uguale all'energia meccanica formula_12 del sistema all'istante "t"=0. Quindi:
Alla fine della caduta, quando il corpo urta il pavimento ed è di nuovo fermo, l'energia cinetica è nuovamente nulla, e poiché anche l'energia potenziale è diminuita, concludiamo che in questo evento l'energia meccanica si sia dissipata (in seguito ad un urto anelastico). In realtà l'energia meccanica "scomparsa" risulta essersi convertita in energia termica e, eventualmente, onde sonore: misurando la temperatura dell'oggetto possiamo infatti riscontrarne un lieve aumento, oltre a notare, nel caso in cui sia presente, una perturbazione dell'eventuale mezzo in cui avviene l'urto. Questo è un fatto generale: le leggi di conservazione della fisica implicano la conservazione dell'energia nei sistemi isolati.
Il "pendolo di Maxwell" fornisce un ottimo esempio del principio di conservazione dell'energia meccanica. Il sistema è costituito da un volano. Due fili sono avvolti nello stesso verso attorno all'asse del volano, mentre le estremità opposte sono collegate ad un sostegno orizzontale. Il volano è "caricato" avvolgendo i fili attorno all'asse, in modo tale che il volano si trovi ad una certa altezza rispetto al piano di riferimento. Se lasciato andare, il volano inizia a scendere ed acquista velocità. Arrivato al punto più basso consentito dallo srotolamento dei fili, il pendolo si riavvolge nel verso opposto e risale. In condizioni ideali, esso tornerebbe alla stessa quota di partenza; tuttavia, per la presenza di attriti con i fili e con il mezzo (l'aria), il moto risulta essere invece smorzato, e dopo un certo numero di oscillazioni il pendolo si ferma nel punto più basso consentito dai fili.
Per determinare il periodo di tale pendolo, ovvero il tempo impiegato dal volano per scendere e risalire, si utilizza il principio di conservazione dell'energia:
ovvero le variazioni di energia cinetica, sia di traslazione che rotazionale, compensano le variazioni di energia potenziale. Avendo preso come asse di riferimento l'asse h diretto verso l'alto e come piano di riferimento quel piano orizzontale sul quale giace il punto più basso raggiunto dal volano, alla massima altezza "h" l'energia è tutta potenziale, mentre nel punto più basso ("h" = 0) l'energia è tutta cinetica. Se h e v sono le generiche altezza e velocità all'istante "t", possiamo esplicitare la conservazione dell'energia:
Se esprimiamo "I", il momento di inerzia del volano, come "kmr", con "k" coefficiente adimensionale, la precedente può essere scritta nel seguente modo (ricordando che "v" = ω "r"):
Deriviamo ambo i membri rispetto al tempo (ricordando di includere tutte le dipendenze temporali e di applicare correttamente la regola di derivazione delle funzioni composte):
La legge oraria di un corpo uniformemente accelerato è data da:
Imponendo che Δh sia pari alla massima estensione del volano (cioè "h"("t") = 0), si ricava il tempo "t" nel quale il volano raggiunge il fondo (il periodo "T" è esattamente il doppio):
Quindi, in definitiva:
Consideriamo un corpo di massa "m" con velocità iniziale formula_21 che urti elasticamente un altro corpo, inizialmente fermo, di massa "M".
Dato che l'urto è elastico, l'energia meccanica dell'intero sistema deve conservarsi. Dato che in un urto agiscono forze impulsive è possibile trascurare le altre forze in gioco (es. gravitazionale), quindi l'energia del sistema è data dalla somma delle energie cinetiche dei corpi. Inoltre, dato che in un urto, per definizione, si considera il sistema come isolato, si conserva la quantità di moto. Chiamando "v" la velocità finale del bersaglio, otteniamo il sistema:
Si ricava facilmente, ricavando "v" dalla seconda equazione e sostituendo nella prima:
dove μ è un coefficiente adimensionale che indica il rapporto tra la velocità finale e quella iniziale. Si ricava immediatamente l'energia cinetica finale del proiettile
ovvero
l'energia cinetica del corpo, dopo l'urto, è uguale a quella iniziale per un coefficiente positivo μ detto "di restituzione".
Non sempre le forze che agiscono su un sistema sono conservative, e non sempre l'energia meccanica, dunque, si conserva. Siano allora F e F rispettivamente la somma di tutte le forze conservative e non conservative. Il lavoro da esse compiuto è allora:
Per il teorema dell'energia cinetica, il lavoro corrisponde alla variazione totale di energia cinetica del sistema:
mentre, essendo F forze conservative, è possibile ad esse associare una funzione potenziale "U" tale che il lavoro di tali forze possa essere espresso come:
In questo modo, sostituendo nell'espressione del lavoro, si ha:
Ora a primo membro si riconosce la variazione di energia meccanica del sistema, prova che le variazioni di energia meccanica di un sistema sono dovute esclusivamente al lavoro compiuto dalle forze non conservative sul sistema.
Un esempio di forza non conservativa, preso dall'esperienza di tutti i giorni, è la forza d'attrito. Sebbene in natura non esistano forze non conservative (a livello microscopico), la forza d'attrito è considerata non conservativa, in primo luogo perché essa, in generale, non è costante, perlomeno in direzione e verso; in secondo luogo perché gli effetti che essa produce (generalmente surriscaldamento delle parti a contatto) non sono conteggiati nel computo dell'energia meccanica. Analogamente, non sono conteggiati i contributi del campo elettromagnetico che produce un lavoro non conservativo e dipendente dallo spostamento.
</text>
</doc>
<doc id="2092669" url="https://it.wikipedia.org/wiki?curid=2092669">
<title>Suono</title>
<text>
Il suono (dal latino "sonus") è la sensazione data dalla vibrazione di un corpo in oscillazione. Tale vibrazione, che si propaga nell'aria o in un altro mezzo elastico, raggiunge l'apparato uditivo dell'orecchio che, tramite un complesso meccanismo interno, crea una sensazione "uditiva" correlata alla natura della vibrazione; in particolar modo la membrana timpanica subendo variazioni di pressione entra in vibrazione.
Le oscillazioni sono spostamenti delle particelle intorno alla posizione di riposo e lungo la direzione di propagazione dell'onda; gli spostamenti sono provocati da movimenti vibratori, provenienti da un determinato oggetto, chiamato sorgente del suono, il quale trasmette il proprio movimento alle particelle adiacenti grazie alle proprietà meccaniche del mezzo; le particelle a loro volta, iniziando ad oscillare, trasmettono il movimento alle altre particelle vicine e queste a loro volta ad altre ancora, provocando una variazione locale della pressione; in questo modo, un semplice movimento vibratorio si propaga meccanicamente originando un'onda sonora (o onda acustica), che è pertanto onda longitudinale. Si ha un'onda longitudinale quando le particelle del mezzo in cui si propaga l'onda, oscillano lungo la direzione di propagazione. Le onde meccaniche longitudinali sono anche denominate onde di pressione. Il suono è un'onda che gode delle seguenti proprietà: riflessione, rifrazione e diffrazione, ma non della polarizzazione (a differenza della luce che è un'onda elettromagnetica). Un'onda ha la frequenza e lunghezza d'onda che possono essere messe in relazione con la formula:
dove:
La quantità "x"/"c" è il tempo necessario all'onda per percorrere la distanza "x".
La frequenza "f", in hertz, dell'onda è data da:
Per le onde sonore, l'ampiezza dell'onda è la differenza tra la pressione del mezzo non perturbato e la pressione massima causata dall'onda.
La velocità di propagazione delle onde sonore dipende dalla temperatura e pressione del mezzo attraverso il quale si propagano.
Come tutte le onde, anche quelle sonore sono caratterizzate da una frequenza (che nel caso del suono è in diretta, ma non esclusiva, relazione con la percezione dell'altezza) e un'intensità (che è in diretta, ma non esclusiva, relazione con il cosiddetto "volume" del suono). Inoltre, caratteristica saliente delle onde sonore è la forma d'onda stessa, che rende in gran parte ragione delle differenze cosiddette di timbro che si percepiscono tra suoni di tipo diverso.
Il campo uditivo dell'uomo si estende da una frequenza di circa 20 Hz fino a 20.000 Hz (ossia 20 kHz).
La lunghezza d'onda rappresenta lo spazio percorso dall'onda sonora in un periodo completo di oscillazione. Le relazioni tra periodo T (tempo necessario perché si compia un'oscillazione completa), frequenza f, e lunghezza d'onda L sono date da:
f = c/L; f = 1 /T; c = L/T; c = Lf
dove "c" è la velocità del suono nell'aria (344 m/s; nell'aria, alla temperatura di 20 °C ed alla pressione atmosferica del livello del mare).
La velocità del suono dipende molto dalla densità del mezzo: è circa 1.500 m/s nell'acqua e circa 5.000 m/s nel ferro. Essendo un movimento di materia, nel vuoto non si trasmette, poiché non c'è materia da far oscillare.
Conoscendo la velocità e la frequenza di un suono, possiamo dunque calcolare la sua lunghezza d'onda; alla frequenza di 20 Hz, la lunghezza d'onda è pari a 17 metri, mentre a 20 kHz è pari a soltanto 17 mm.
La velocità di propagazione del suono dipende dalle caratteristiche del mezzo, in particolare l'elasticità e la densità.
È direttamente proporzionale all'elasticità ed inversamente proporzionale alla densità, secondo la relazione dove:
Spesso materiali di elevata densità presentano anche moduli di elasticità elevati e questo ha contribuito al diffondersi della convinzione che la velocità del suono sia più elevata in un mezzo ad alta densità rispetto ad un altro di densità più ridotta.
L'altezza è la qualità che fa distinguere un suono acuto da uno grave. Dipende in massima parte dalla frequenza ma anche dalla intensità. L'orecchio umano percepisce solo i suoni che vanno da 16 a 20.000 oscillazioni al secondo. Al di sotto abbiamo gli infrasuoni, al di sopra gli ultrasuoni. Il sonar, ma anche i delfini ed i pipistrelli, percepisce gli ultrasuoni mentre gli elefanti, i pesci ed i cetacei percepiscono gli infrasuoni.
La pratica musicale copre una gamma di suoni, le cui fondamentali vanno dal do grave che ha circa 65 oscillazioni semplici al secondo al do acuto che ha 8276 oscillazioni semplici. La voce umana ha un registro ancora più limitato. Per calcolare l'altezza dei suoni, è stato scelto come punto di riferimento il La4 (= ottava centrale del pianoforte) che chiamiamo "diapason" o "corista". La frequenza del diapason, che fino al XIX secolo variava di Paese in Paese e anche a seconda del tipo di musica da eseguire (sacra, da camera ecc.) è stata determinata da diversi congressi: nel 1885, il governo austriaco stabilì che il La4 corrispondesse a 870 oscillazioni semplici che, a loro volta, corrispondevano a 435 oscillazioni doppie. Ora invece il valore di riferimento, stabilito dalla Conferenza di Londra del 1939, è 440 vibrazioni doppie, quindi 880 semplici.
Il volume che viene spesso anche chiamato - colloquialmente ed erroneamente - pressione, è la qualità sonora associata alla percezione della "forza" di un suono, ed è determinato dalla pressione che l'onda sonora esercita sul timpano: quest'ultima è a sua volta determinata dall'ampiezza della vibrazione e dalla distanza del punto di percezione da quello di emissione del suono. In particolare, la pressione di un'onda sonora sferica emessa da una sorgente puntiforme risulta essere proporzionale al reciproco della distanza:
p = pressione
Per misurare il volume percepito di un suono si fa spesso riferimento al livello sonoro, che viene calcolato in decibel, come segue:
formula_10.
Come si può vedere l'intensità decresce come il reciproco del quadrato della distanza:formula_11
e quindi molto più rapidamente della pressione formula_12
Valgono anche le relazioni (analogamente al caso della pressione)
I \propto \frac{1}{r^2} \frac{I_1}{I_2} = \frac
</text>
</doc>
<doc id="6247197" url="https://it.wikipedia.org/wiki?curid=6247197">
<title>Sistema di riferimento</title>
<text>
In fisica e geodesia un sistema di riferimento è un sistema rispetto al quale viene osservato e misurato un certo fenomeno fisico o un oggetto fisico oppure vengono compiute determinate misurazioni. La nozione nasce nell'ambito della meccanica classica, in cinematica e dinamica, con la descrizione del moto dei corpi e con la constatazione che il moto è sempre relativo ad un sistema rispetto al quale lo osserviamo.
In fisica la primaria distinzione tra sistemi di riferimento è quella tra sistemi di riferimento inerziali e sistemi di riferimento non inerziali. Dalla teoria della relatività speciale discende il principio secondo il quale le leggi fisiche sono invarianti in tutti i sistemi di riferimento inerziali, cosa che in fisica classica falliva applicando le trasformazioni di Galileo alle equazioni di Maxwell. 
A partire dalla definizione di un sistema di riferimento nell'osservazione di un certo fenomeno è possibile definire o costruire un sistema di coordinate per la misura oggettiva dei parametri fisici o grandezze fisiche di riferimento come ad esempio spazio, tempo, posizione, velocità, accelerazione ecc...
</text>
</doc>
<doc id="1161642" url="https://it.wikipedia.org/wiki?curid=1161642">
<title>Principio di Huygens-Fresnel</title>
<text>
Il principio di Huygens-Fresnel, o più semplicemente principio di Huygens (dal nome del fisico olandese Christiaan Huygens), è un metodo di analisi applicato ai problemi di propagazione delle onde. 
In ottica ondulatoria esso ha la seguente formulazione:
"Ogni elemento dΣ di un fronte d'onda Σ si può considerare formalmente come una sorgente secondaria di onde sferiche in fase con la primaria e di ampiezza proporzionale a quella dell'onda primaria e all'area dΣ. La perturbazione prodotta in un punto dello spazio si può sempre ottenere come sovrapposizione di tutte le onde sferiche secondarie che raggiungono quel punto".
Il principio espresso costituisce uno strumento di calcolo molto utile, in quanto consente di determinare direttamente il fronte d'onda ad un certo istante una volta noto quello ad un qualsiasi istante precedente (o successivo). Il calcolo della figura di interferenza prodotta dall'inviluppo delle onde sferiche secondarie è possibile sia quando l'onda si propaga liberamente, sia quando essa viene limitata da un ostacolo impenetrabile ed è pertanto utilizzabile nella determinazione degli effetti di diffrazione prodotti da una radiazione, visibili su uno schermo.
Data una sorgente (S) generante un'onda sferica nello spazio, ogni punto del fronte d'onda primario si comporta come sorgente secondaria generando altre onde con le stesse caratteristiche dell'onda primaria (lunghezza d'onda, frequenza, velocità) se non vi è un cambio di mezzo (in tal caso la lunghezza d'onda e la velocità si adatteranno al cambio di mezzo); la sovrapposizione di queste onde secondarie genera altri fronti d'onda, detti secondari che a loro volta ne produrranno degli altri determinando l'espansione dell'onda.
Una buona immagine che ci fa capire quanto spiegato poc'anzi si può vedere aprendo il seguente link:
N.B. Il fronte d'onda nuovo da considerarsi è ovviamente la linea nera in grassetto. (Lo schermo in questo caso non ci interessa.)
Le sorgenti elementari di radiazione poste sul fronte d'onda producono sì onde sferiche, ma è anche evidente che tali perturbazioni debbano essere in un certo qual modo proiettate nella direzione di propagazione dell'onda primaria. Questo fatto è preso in considerazione dal cosiddetto "fattore di obliquità", che modula l'ampiezza dell'onda sferica in funzione dell'angolo θ individuato dal versore di propagazione della primaria e da quello che congiunge il centro di emissione con il punto dello spazio "P" in cui si intende valutare il campo
il principio di Huygens-Fresnel per il campo risultante si può dunque esprimere matematicamente, in termini fasoriali, nella seguente forma
dove "k" è il modulo del vettore d'onda della primaria, "r" la distanza che separa il generico punto del fronte d'onda con "P" e "u" l'ampiezza della primaria sul fronte d'onda; la costante a moltiplicare "(iλ)", in cui "λ" è la lunghezza d'onda, deriva dall'imposizione delle condizioni di radiazione all'infinito.
Esiste una spiegazione fisica del principio di Huygens, qui indicata solo a livello qualitativo, descritta da Richard Feynman.
Si immagini, ad esempio, di avere uno schermo opaco con una singola fenditura, su cui incide un'onda piana monocromatica. Secondo il principio di Huygens, il fronte d'onda che si propaga oltre tale fenditura è dovuto all'interferenza creata esclusivamente dai punti nella fenditura, sorgenti secondarie di onde sferiche. Questo fenomeno è interpretabile anche come segue. L'onda incidente sullo schermo opaco è assorbita dagli elettroni presenti sullo schermo. Essi entrano in oscillazione e, di conseguenza, emettono onde elettromagnetiche principalmente alla stessa frequenza con cui oscillano. Si trova che l'effetto dovuto alla somma di tutti i contributi di tali onde coincide con quello ottenuto considerando come sorgenti unicamente i punti della fenditura.
Il principio di Huygens-Fresnel può essere utilizzato nell'analisi delle situazioni in cui la propagazione dell'onda viene limitata dalla presenza di corpi opachi, in particolare nei problemi di diffrazione oppure, tramite il principio di Babinet, in quelli di diffusione.
Le ipotesi semplificative che si avanzano nella soluzione dei problemi in cui un'onda incide su uno schermo per poter applicare esclusivamente questo risultato, senza far ricorso ad altre tecniche quali simulazioni numeriche, riguardano il valore del campo appena oltre lo schermo, che viene assunto nullo, e sulle porzioni delle superfici d'onda che si avrebbero in assenza dell'ostacolo in concomitanza delle fessure (si veda la figura a lato), dove si ipotizza che il campo effettivo assuma lo stesso valore di quello incidente. In realtà, le due ipotesi valgono soltanto approssimativamente e limitatamente a fessure di dimensioni sufficientemente estese rispetto alla lunghezza d'onda: basti pensare che se, ad esempio, valesse la prima si avrebbe una discontinuità nel valore del campo in prossimità del bordo delle aperture. Quello che si produce, appena al di là del bordo, è un'onda evanescente che si propaga sulla superficie dello schermo ma che tende a smorzarsi piuttosto rapidamente; analogamente, il valore del campo vicino al bordo si discosterà in una certa misura da quello incidente. Tutti questi fenomeni si possono però ignorare per lunghezze d'onda molto piccole rispetto alle dimensioni dei fori.
Il principio di Huygens-Fresnel viene applicato integrando su una superficie che comprende tutto lo schermo e le succitate porzioni: per le ipotesi fatte, il primo contributo è nullo, mentre il secondo è noto una volta che si conoscono i dettagli del campo incidente.
Si consideri il caso semplice di diffrazione attraverso una fenditura, e si supponga di voler calcolare in quali punti dello spazio si rileva interferenza distruttiva. Per il principio di Huygens-Fresnel, il problema si riduce a quello di valutare in che modo si sovrappongono le onde sferiche prodotte dai punti della fenditura investiti dal campo. Una generica coppia di queste onde interferisce distruttivamente quando la differenza di cammino ottico è pari a metà della lunghezza d'onda, corrispondente a una differenza di fase di 180°; in maniera analoga si deduce che una terna di sorgenti dà luogo ad un nullo di campo ove le radiazioni da esse prodotte si sovrappongono con differenze di cammino ottico pari a un terzo della lunghezza d'onda.
Tanto per fissare le idee, si assuma di trovarsi in una situazione bidimensionale in cui la fessura si riduce ad un segmento entro il quale il campo è costante: se la fessura è ampia esattamente una lunghezza d'onda, ad ogni punto della fessura corrisponderà uno e un solo punto che dista da esso esattamente mezza lunghezza d'onda. La differenza di cammino ottico che intercorre tra le onde prodotte da quei due radiatori elementari in un punto situato al di là dello schermo, a grande distanza dall'apertura e in posizione perfettamente centrale rispetto a quest'ultima sarà esattamente pari a metà della lunghezza d'onda: le due onde interferiranno distruttivamente. Essendo questo valido per ogni generica coppia di punti, sulla bisettrice della fessura si osserverà quindi un nullo di intensità, in perfetta contraddizione con le leggi dell'ottica geometrica. Nel caso tridimensionale, si otterrà un effetto del genere qualora la fessura abbia la forma di un esagono regolare di lato pari a un terzo della lunghezza d'onda (sempre se il campo è uniforme all'interno del foro).
</text>
</doc>
<doc id="717987" url="https://it.wikipedia.org/wiki?curid=717987">
<title>Carica elettrica</title>
<text>
La carica elettrica, in fisica, è una grandezza fisica scalare dotata di segno, ed è una proprietà fondamentale della materia. La carica elettrica è un tipo di carica ed è responsabile dell'interazione elettromagnetica, essendo sorgente del campo elettromagnetico.
È una grandezza quantizzata, ossia essa esiste solo in forma di multipli di una quantità fondamentale: la carica dell'elettrone, che è definita come negativa ed indicata con −"e". Nel Sistema internazionale di unità di misura l'unità di carica è il Coulomb che corrisponde a circa elettroni. Un elettrone possiede una carica il cui valore, inizialmente determinato da Robert Andrews Millikan tra il 1910 e il 1917, è definito esattamente dal sistema internazionale nel 2019 come:
La carica di un protone, uno dei costituenti fondamentali del nucleo insieme al neutrone, viene considerata positiva ed indicata con +"e".
Se non si considerano i quark, non è stato scoperto alcun oggetto che possiede una carica inferiore a quella dell'elettrone: per tale motivo il valore della sua carica è considerato l'unità di carica elettrica fondamentale, e tutte le quantità di carica sono multiple della carica dell'elettrone. Secondo il modello standard della fisica, tuttavia, le cariche più piccole delle particelle sono ±"e"/3, ±2"e"/3 e ±"e": ad esempio il quark down ha carica -"e"/3, il quark up ha carica 2"e"/3, mentre le loro antiparticelle hanno cariche opposte.
Gli altri quark, di massa maggiore, hanno comunque cariche ±"e"/3 oppure ±2"e"/3. Anche se i quark trasportano una carica elettrica, osservare un quark libero richiede un'energia estremamente elevata che solo da poco è alla portata degli acceleratori di particelle, a causa dell'elevata intensità delle interazioni nucleari forti che li tiene uniti. Si pensa sia possibile l'esistenza di un plasma di quark e gluoni liberi a circa 150 GeV, circa K; i fisici cercano di ottenerlo facendo collidere tra loro nuclei pesanti, come l'oro, ad energie di circa 100 GeV per nucleone.
Oltre alla carica elettrica, si può definire anche una carica di colore, che introduce un ulteriore numero quantico, utilizzato per descrivere quark e gluoni, insieme al sapore, nella teoria della cromodinamica quantistica.
L'elettrone è una particella subatomica che possiede una massa a riposo di , pari a circa 1/1836 di quella del protone. Il momento angolare intrinseco, ovvero lo spin, è un valore semi intero pari ad 1/2 in unità di ħ, che rende l'elettrone un fermione, soggetto quindi al principio di esclusione di Pauli. L'antiparticella dell'elettrone è il positrone, il quale si differenzia solo per la carica elettrica di segno opposto; quando queste due particelle collidono possono essere sia diffuse che annichilite producendo fotoni, più precisamente raggi gamma.
L'idea di una quantità fondamentale di carica elettrica è stata introdotta dal filosofo Richard Laming nel 1838 per spiegare le proprietà chimiche dell'atomo; il termine "elettrone" è stato successivamente coniato nel 1894 dal fisico irlandese George Johnstone Stoney, ed è stato riconosciuto come una particella da Joseph John Thomson e dal suo gruppo di ricerca.
Successivamente il figlio George Paget Thomson ha dimostrato la duplice natura corpuscolare e ondulatoria dell'elettrone, che è quindi descritto dalla meccanica quantistica per mezzo del dualismo onda-particella.
Gli elettroni, insieme ai protoni e ai neutroni, sono parti della struttura degli atomi e, sebbene contribuiscano per meno dello 0,06% alla massa totale dell'atomo, sono responsabili delle sue proprietà chimiche; in particolare, la condivisione di elettroni tra due o più atomi è la sorgente del legame chimico covalente.
La maggior parte degli elettroni presenti nell'universo è stata creata durante il Big Bang, sebbene tale particella possa essere generata tramite il decadimento beta degli isotopi radioattivi e in collisioni ad alta energia, mentre può essere annichilita grazie alla collisione con il positrone e assorbita in un processo di nucleosintesi stellare.
In molti fenomeni fisici, in particolare nell'elettromagnetismo e nella fisica dello stato solido, l'elettrone ha un ruolo essenziale: è responsabile della conduzione di corrente elettrica e del calore, il suo moto genera il campo magnetico e la variazione della sua energia è responsabile della produzione di fotoni.
L'avvento dell'elettronica, a partire dalla quale è nata l'informatica, pone l'elettrone alla base dello sviluppo tecnologico del ventesimo secolo. Le sue proprietà vengono inoltre sfruttate in svariate applicazioni, come i tubi a raggi catodici, i microscopi elettronici, la radioterapia ed il laser.
L'elettrone appartiene inoltre alla classe delle particelle subatomiche dette leptoni, che si ritiene siano componenti fondamentali della materia (ovvero non possono essere scomposte in particelle più piccole).
La carica elettrica è una grandezza fisica conservativa, cioè la carica elettrica totale di un sistema fisico isolato rimane costante. Questa è una legge sperimentale fondamentale della natura, in quanto non è mai stata osservata una sua violazione. Un altro assunto è che la conservazione sia locale, ossia valga il teorema di Noether (v. anche legge di conservazione). Essa afferma che la variazione della densità spaziale di carica formula_2 entro un volume formula_3 è dovuta unicamente a quella che attraversa la superficie frontiera del detto volume essendo in movimento. L'equazione di continuità per la carica elettrica è quindi l'equazione differenziale:
dove formula_5 è la densità di corrente e formula_2 la densità di carica.
Utilizzando il teorema della divergenza si ottiene la forma integrale:
dove formula_8 è la corrente elettrica.
L'equazione di continuità viene considerata nelle equazioni di Maxwell per correggere la legge di Ampère estendendone la validità al caso non stazionario. Infatti, applicando l'operatore divergenza alla quarta (con appunto la correzione di Maxwell):
e sostituendo al suo interno la prima:
si ottiene l'equazione di continuità.
L'equazione di continuità può essere scritta in maniera molto semplice e compatta utilizzando la notazione relativistica. Si definisce in tale contesto il quadrivettore densità di corrente, la cui componente temporale è la densità di carica e quella spaziale è il vettore densità di corrente:
In questo modo l'equazione di continuità diventa:
dove formula_13 è il quadrigradiente, dato da:
L'equazione di continuità si può scrivere anche come:
dove formula_16 denota la derivata covariante.
</text>
</doc>
<doc id="721201" url="https://it.wikipedia.org/wiki?curid=721201">
<title>Ingrandimento</title>
<text>
L'ingrandimento (più raramente magnificazione, per calco dall'inglese "magnification") è il processo che aumenta le dimensioni di un oggetto a livello ottico e non a livello fisico. L'ingrandimento è anche un numero che descrive con quale fattore sia stato ingrandito un oggetto. Quando questo numero è minore di uno si riferisce ad una riduzione del formato, chiamata rimpicciolimento.
L'ingrandimento ha tipicamente come scopo rendere visibili più dettagli, aumentando la risoluzione angolare, usando tecniche ottiche, di stampa o tramite processi digitali. In tutti i casi l'ingrandimento non modifica la prospettiva dell'immagine.
L'ingrandimento ottico è il rapporto tra la misura apparente (o la dimensione in un'immagine) e la misura reale di un oggetto, ed è un numero.
</text>
</doc>
<doc id="93547" url="https://it.wikipedia.org/wiki?curid=93547">
<title>Ottica geometrica</title>
<text>
L'ottica geometrica è la più antica branca dell'ottica: essa studia i fenomeni ottici assumendo che la luce si propaghi mediante "raggi" rettilinei. Dal punto di vista dell'ottica ondulatoria essa è valida quando la luce interagisce solo con oggetti di dimensioni molto maggiori della sua lunghezza d'onda.
Con questa condizione, gli unici fenomeni rilevanti sono la propagazione rettilinea, la riflessione (speculare o diffusa, quest'ultima detta anche diffusione) e la rifrazione ed è possibile dare una spiegazione approssimata, ma sufficiente in molti casi, del funzionamento di specchi, prismi, lenti e dei sistemi ottici costruiti con essi.
La legge afferma che nel vuoto la luce si propaga lungo linee rette. La prima formulazione di questo principio è dovuta a Euclide, anche se il concetto di vuoto associato al moto rettilineo degli atomi venne introdotto circa due secoli prima da Democrito.
Lo stesso comportamento varrà a posteriori all'interno di qualsiasi mezzo omogeneo.
La "riflessione" è il fenomeno per cui una congruenza ortogonale di raggi che incide su una superficie di discontinuità genera una nuova congruenza ortogonale di raggi. Le leggi della riflessione affermano che tale nuovo raggio, detto raggio riflesso, 
La "rifrazione" è il fenomeno per cui una congruenza ortogonale di raggi che attraversa una superficie di discontinuità (contatto tra due materiali diversi) viene deviata.
Le leggi della rifrazione affermano che
dove i valori di "n" (indice di rifrazione) dipendono dai materiali di cui sono costituiti i mezzi e dal colore della luce. 
Nei fenomeni di rifrazione oltre al raggio rifratto, c'è sempre anche un raggio riflesso.
Nel caso in cui il raggio provenga dal mezzo con indice di rifrazione maggiore, con un angolo tale che l'angolo uscente dovrebbe essere maggiore di 90 gradi (θ&gt;arcsen(n/n)), il raggio rifratto non è presente e tutta la luce viene riflessa ("riflessione totale"). È tuttavia necessario che l'indice di inclinazione n1 sia maggiore dell'indice di inclinazione n2.
Tutte e tre le leggi dell'Ottica Geometrica sono deducibili dal principio di Fermat, se si assume che l'indice di rifrazione sia inversamente proporzionale alla velocità della luce nel mezzo considerato.
Gli indici di rifrazione sono definiti dalla legge precedente a meno di una costante moltiplicativa. Essi sono determinati convenzionalmente assumendo uguale a 1 l'indice di rifrazione del vuoto; sapendo che la velocità della luce è massima nel vuoto, ne segue che l'indice di rifrazione di tutte le altre sostanze è maggiore di 1.
Il variare dell'indice di rifrazione in funzione del colore provoca il fenomeno della dispersione cromatica, cioè la separazione di un raggio di luce bianca nel suo spettro sexyllino
La dispersione cromatica è all'origine dell'arcobaleno e dell'aberrazione cromatica.
</text>
</doc>
<doc id="45849" url="https://it.wikipedia.org/wiki?curid=45849">
<title>Resistenza elettrica</title>
<text>
La resistenza elettrica è una grandezza fisica che misura la tendenza di un corpo ad opporsi al passaggio di una corrente elettrica, quando sottoposto ad una tensione elettrica. Questa opposizione dipende dal materiale con cui è realizzato, dalle sue dimensioni e dalla sua temperatura. Uno degli effetti del passaggio di corrente in un conduttore è il suo riscaldamento (effetto Joule).
La resistenza "R" è l'inverso della conduttanza elettrica "G", definita per un conduttore cilindrico come:
Considerando un circuito nel quale vi sia una singola resistenza tale che la resistenza prodotta in questo circuito sia uguale a quella di partenza, si definisce come "resistenza equivalente" il reciproco della somma dei reciproci delle singole resistenze.
la resistenza equivalente si ricava quindi come
La penultima equazione può essere scritta come
Oppure considerando la conduttanza elettrica:
Una resistenza è proporzionale alla sezione trasversale (S+S), di conseguenza vale:
e infine:
In una resistenza che segue la legge di Ohm, esistono le seguenti relazioni tra la Tensione ΔV, la Corrente I e la Potenza elettrica P:
Quando la resistenza non è costante ma dipende dalla tensione e dalla corrente si definisce la resistenza differenziale o resistenza incrementale. Essa è il coefficiente angolare della retta tangente alla curva nel diagramma che rappresenta "V-I" (tensione in funzione della corrente) nel particolare punto che interessa, cioè la derivata della tensione rispetto alla corrente in quel punto della curva:
Talvolta, quella appena definita, viene chiamata semplicemente "resistenza", benché le due definizioni siano equivalenti solo per un componente ohmmico come un resistore ideale che nel piano [V-I] è una retta. Se la funzione "V-I" non è monotòna (cioè ha un picco o un avvallamento) la resistenza differenziale sarà negativa per alcuni valori di tensione e corrente, cosicché con una tensione in aumento l'intensità della corrente decresce, rispettivamente con una tensione decrescente l'intensità della corrente aumenta. Questa proprietà è spesso chiamata "resistenza negativa", anche se è più corretto chiamarla resistenza differenziale negativa, visto che la resistenza assoluta (tensione divisa per la corrente) resta sempre positiva. Una resistenza differenziale negativa può venire utilizzata per la eccitazione di circuiti oscillanti o per la generazione di oscillazioni di rilassamento. La resistenza differenziale occorre per esempio con i diodi a tunnel o nella ionizzazione a valanga.
Al contrario con una resistenza differenziale positiva la corrente aumenta con una tensione in aumento. Tutti gli elementi circuitali reali esistenti hanno in una parte delle loro curve caratteristiche, tuttavia sempre per valori molto grandi una resistenza differenziale per lo più positiva. Per esempio: le resistenze reali, i diodi, i diodi Zener, tutte le ceramiche semiconduttrici.
La forza elettromotrice di un generatore rappresenta la d.d.p. (differenza di potenziale) presente ai capi di un generatore quando il circuito è aperto. Chiudendo il circuito e diminuendo la resistenza del reostato aumenta la corrente che passa nel circuito e diminuisce la tensione formula_10 misurata con voltmetro.
I generatori hanno una piccola resistenza interna che provoca una caduta di tensione formula_13 tanto più grande quanto maggiore è la corrente formula_14.
L'amperometro che è inserito in serie, deve avere la resistenza interna più piccola possibile per rendere minima la caduta di tensione ai suoi capi, mentre il voltmetro che inserito in parallelo deve avere la resistenza interna più grande possibile per rendere minima la corrente I che lo attraversa.
Si è detto che la presenza di una resistenza determina un riscaldamento del componente. Più precisamente la potenza dissipata in calore è data dalla relazione:
dove:
"P" è la potenza misurata in watt (le altre grandezze sono state già definite sopra).
L'espressione si ricava dalla definizione di potenza elettrica, come prodotto di corrente e tensione, sostituendovi la prima legge di Ohm:
dato
Questo effetto è utile in alcune applicazioni come le lampade ad incandescenza oppure negli apparati riscaldanti ad energia elettrica (ad esempio: gli asciugacapelli) ma non è certo voluto nelle linee di distribuzione dell'energia elettrica dove l'effetto Joule provoca perdite di potenza elettrica lungo tali linee che vanno contenute scegliendo opportunamente le dimensioni dei cavi elettrici che trasportano l'energia.
Al di sotto di una temperatura critica specifica e di un campo magnetico critico alcuni materiali (detti superconduttori) assumono un valore di resistenza ohmica nulla. Per ciò tale materiale è chiamato superconduttore.
Varie applicazioni dei superconduttori sono i motori dewar a levitazione, i treni a levitazione, gli SQUID ed apparecchiature elettromedicali.
Alcuni elementi come il germanio e il silicio hanno un comportamento differente a seconda della temperatura, e si comportano come isolanti a temperature molto basse, mentre a temperatura ambiente (circa 20 °C) si comportano come conduttori. Inoltre è possibile aumentare notevolmente la loro conduttività elettrica con il drogaggio, cioè inserendo delle "impurezze" (ad esempio elementi trivalenti o pentavalenti).
I semiconduttori elementari sono elementi tetravalenti, ovvero che possiedono nell'orbitale più esterno quattro elettroni, che permettono di formare un legame covalente con gli atomi adiacenti. A temperatura bassa questa struttura non permette agli elettroni di muoversi liberamente e perciò questi elementi si comportano come isolanti; tuttavia a temperature più elevate alcuni legami covalenti si possono spezzare liberando elettroni che contribuiscono ad accrescere la conduzione elettrica. Nel momento in cui l'elettrone abbandona l'atomo si forma un "buco" chiamato lacuna che può attirare un altro elettrone e via dicendo seguendo un effetto a catena.
Oltre ai semiconduttori elementari esistono semiconduttori composti, ovvero leghe binarie o ternarie che si comportano come un semiconduttore.
Come già detto all'inizio del paragrafo è possibile aumentare la conduttività di questi elementi attraverso il drogaggio. Inserendo un elemento pentavalente (ad esempio l'arsenico) si formano dei legami covalenti tra il semiconduttore stesso e l'elemento aggiunto. Tuttavia un elettrone rimane libero di muoversi e diventa un elettrone di conduzione. L'arsenico in questo caso viene chiamato "donore" e il semiconduttore è detto di "tipo N".
Se il drogaggio avviene per introduzione di un elemento trivalente (ad esempio l'alluminio) si formeranno tre legami covalenti tra gli elettroni di ogni atomo del semiconduttore e quelli di ogni atomo dell'elemento aggiunto. Tuttavia un elettrone per ogni atomo del semiconduttore rimarrà libero e andrà ad aumentare la conduttività elettrica, lasciando libera una lacuna che tende a catturare un altro elettrone dagli atomi vicini del semiconduttore e via dicendo. In questo caso l'alluminio è chiamato "accettore" ed il semiconduttore è denominato di "tipo P".
</text>
</doc>
<doc id="71428" url="https://it.wikipedia.org/wiki?curid=71428">
<title>Moto parabolico</title>
<text>
Il moto parabolico è un tipo di moto bidimensionale esprimibile attraverso la combinazione di due moti rettilinei simultanei ed indipendenti:
Il moto parabolico può essere descritto mediante le relazioni della cinematica che legano i vettori posizione, velocità, ed accelerazione.
La più significativa realizzazione di tale moto è fornita dal "moto del proiettile" in cui si utilizzano le seguenti semplificazioni (approssimazioni della fisica e della geometria del problema):
Si supponga che un corpo sia lanciato all'istante "t=0" nell'origine "O" di un sistema di coordinate cartesiano "Oxy", e che la velocità iniziale abbia modulo "v" e formi un angolo θ con l'asse "x" orizzontale.
Dalle leggi del moto uniformemente accelerato si ha:
Ipotizzando che il corpo si trovi in prossimità della terra, è possibile considerare la funzione formula_2 come costante, con valore pari a formula_3 diretta lungo la perpendicolare al terreno (asse y), per cui si ha:
Come si può notare dalla formula, la velocità giace sempre nel piano formato dai vettori costanti formula_5 e formula_6, ovvero quello su cui si svolge il moto.
Il vettore velocità può essere scomposto lungo le due componenti "x" e "y":
Dalla relazione precedente, si ricava:
Proiettando le velocità sugli assi si ottengono le componenti:
costante nel tempo, e
da cui, integrando, si ricavano le leggi orarie dei moti lungo gli assi "x" e "y":
La traiettoria viene ricavata eliminando la variabile temporale, ossia, esprimendo il rapporto:
e esplicitando il parametro formula_13 dalla legge oraria formula_14:
In tal modo si arriva all'equazione cartesiana:
da cui, moltiplicando per x ambo i membri, si ottiene
che rappresenta una parabola con concavità rivolta verso il basso, il cui grafico è rappresentato in figura. Inoltre se la posizione del lancio del corpo non si trova nell'origine, quindi ad esempio nel punto formula_18 si può approssimare la curva con una traslazione degli assi paralleli agli assi cartesiani con origine in formula_19 (l'approssimazione è dovuta al fatto che stiamo considerando il corpo in prossimità della terra, ergo g è costante)
La gittata è la distanza percorsa in orizzontale dal corpo prima che tocchi terra.
Se consideriamo la traiettoria espressa in un piano cartesiano Oxy, per calcolare la gittata possiamo utilizzare la funzione y(x) vista sopra. Ci interessa sapere a che coordinata x si ha la coordinata y pari a zero, cioè:
Si tratta di una parabola, ci aspettiamo quindi due soluzioni. Se il corpo parte da terra una delle due soluzioni sarà proprio la posizione di partenza e può essere scartata. Se il corpo non parte da terra, una delle due soluzioni si troverà "dietro" la posizione di partenza e non ha significato fisico. Svolgiamo l'equazione di secondo grado per ottenere la gittata x
Studiamo ora il caso in cui l'altezza di partenza non è zero. Non ci servirà altro che riutilizzare la funzione y(x) aggiungendo la costante formula_22. Svolgiamo quest'altra equazione di secondo grado per ottenere:
formula_23
Sono necessarie varie semplificazioni e trasformazioni, ma in questa forma è facile notare come questo risultato vale sia per un corpo lanciato da terra che per un corpo lanciato da un'altezza data. 
A questo punto è possibile ricavare l'angolo di massima gittata. Fissati formula_24 per un punto lanciato da terra, ci si chiede per quale angolo la gittata è massima. formula_25 ha massimo relativo per l'argomento del seno uguale a formula_26 quindi per formula_27°
Siccome il moto parabolico è simmetrico rispetto all'asse passante per il vertice e parallelo all'asse y ("proprietà della parabola"), l'ascissa del punto di atterraggio è due volte l'ascissa del vertice della parabola, ovvero il doppio dell'ascissa del punto di massima altezza. Tale ascissa è dunque:
Sostituendo nell'equazione della parabola esplicitata precedentemente si ha che:
Gli stessi risultati si ottengono considerando il fatto che il punto di altezza massima è un punto di massimo della curva della traiettoria e quindi il punto di massimo della parabola. Trovarlo quindi consiste nel porre la derivata prima dell'equazione della traiettoria uguale a zero e ricavare dall'equazione ottenuta l'ascissa del punto cercato formula_30(Che sarebbe la gittata) sostituendo nell'equazione della traiettoria si ottiene anche l'ordinata formula_31.
Il tempo di volo è il tempo fra l'istante di lancio e quello di arrivo del corpo, che coincide con il tempo necessario a percorrere il tratto OG con la velocità v:
Un tipico esempio di moto parabolico è quello del proiettile, di cui si occupa la balistica. Un proiettile in volo è sottoposto alla forza di gravità della Terra. Nell'ipotesi di attrito dell'aria trascurabile, il secondo principio della dinamica porta ad un'accelerazione che può essere scomposta nel seguente modo:
Se il proiettile viene sparato con velocità iniziale v secondo un angolo θ, si ottengono le seguenti componenti di velocità:
Le componenti della posizione del proiettile sono quindi:
Il moto lungo l'asse x è quindi uniforme, e quello lungo l'asse y accelerato. Se la velocità iniziale fosse stata pari a zero, il moto sarebbe stato di caduta libera.
</text>
</doc>
<doc id="65999" url="https://it.wikipedia.org/wiki?curid=65999">
<title>Energia potenziale</title>
<text>
In fisica, l'energia potenziale di un oggetto è l'energia che esso possiede a causa della sua posizione o del suo orientamento rispetto ad un campo di forze. Nel caso si tratti di un sistema, l'energia potenziale può dipendere dalla disposizione degli elementi che lo compongono. Si può vedere l'energia potenziale anche come la capacità di un oggetto (o sistema) di trasformare la propria energia in un'altra forma di energia, come ad esempio l'energia cinetica.
Il termine "energia potenziale" fu coniato da Rankine nel 1853.
Nel sistema internazionale è misurata in joule ("J").
Si tratta di una funzione scalare delle coordinate dell'oggetto nel sistema di riferimento utilizzato. Dato un campo vettoriale conservativo, l'energia potenziale è la sua capacità di compiere lavoro: il lavoro relativo a una forza che agisce su un oggetto è l'integrale di linea di seconda specie della forza valutato sul cammino compiuto dall'oggetto, e se essa è conservativa il valore di questo integrale non dipende dal tipo di cammino seguito. Quando si ha a che fare con forze conservative si può definire un potenziale scalare definito in tutto lo spazio, usualmente il potenziale è definito come l'energia potenziale fratto la variabile che è responsabile della forza. In particolare, dal punto di vista matematico tale potenziale esiste solo se la forza è conservativa, e del resto si assume che per tutte le forze conservative si può sempre definire fisicamente un'energia potenziale.
L'energia potenziale può essere definita anche per il campo magnetico, che non è conservativo, nelle regioni in cui vi è assenza di correnti elettriche. In tal caso, infatti, il rotore del campo è nullo. L'energia potenziale magnetica di un magnete in un campo magnetico è definita come il lavoro della forza magnetica (il momento meccanico) nel ri-allineare il momento di dipolo magnetico.
Se in una regione di spazio sono presenti una qualche forza e un oggetto che è sensibile alla presenza della forza, l'energia potenziale (associata alla forza) posseduta dall'oggetto è definita come la differenza tra l'energia che esso possiede a causa della forza in una data posizione nello spazio e l'energia posseduta in una posizione scelta come riferimento. Spesso nella posizione scelta come riferimento l'energia potenziale è nulla.
L'energia potenziale è definibile come il lavoro necessario a portare a distanza infinita due molecole, ed è pari a zero quando la distanza tra le molecole è infinita.
Data una forza formula_1, il lavoro formula_2 lungo una curva formula_3 è dato in generale dalla relazione:
che in forma locale si scrive:
Nel caso il campo di forze sia conservativo, il lavoro non dipende dal tipo di percorso compiuto, ma soltanto dall'entità della forza agli estremi del cammino (gli estremi di integrazione): il differenziale formula_6 è allora un differenziale esatto, e il campo conservativo corrisponde (per definizione) al gradiente di un campo scalare, chiamato potenziale. In questo caso, se l'oggetto si sposta da un punto formula_7 a un punto formula_8 la forza esercitata dal campo compie un lavoro pari all'opposto formula_9 della differenza formula_10 tra l'energia potenziale posseduta dall'oggetto nelle due posizioni iniziale e finale:
Il motivo del segno meno, per cui il lavoro è pari all'opposto dell'energia, è il fatto che in questo modo a un lavoro positivo corrisponde una riduzione del potenziale. Poiché è possibile fissare arbitrariamente il livello zero dell'energia potenziale, essa viene definita a meno di una costante additiva. Nel caso più semplice, in cui il moto si svolge in una sola direzione, l'energia potenziale di una forza conservativa è pari a una qualche primitiva della forza, cambiata di segno:
dove formula_13 è la costante additiva. Fissando formula_14 si determina qual è la primitiva, e pertanto si rende necessario imporre delle condizioni al contorno: per le forze nulle all'infinito si utilizza la condizione al contorno di Dirichlet formula_15, detta "condizione di località".
Nel caso tridimensionale, se il dominio è un insieme stellato il lemma di Poincaré fornisce una condizione sufficiente e necessaria affinché nel punto formula_16 la forza sia l'opposto formula_17 del gradiente formula_18 di un potenziale scalare formula_19 (ovvero sia conservativa):
Inoltre, l'integrale si può separare:
dove il punto formula_22 è scelto arbitrariamente e i vettori formula_23, formula_24 e formula_25 sono i versori canonici di formula_26.
formula_32 dove formula_33 è la costante di gravitazione universale e formula_34 la massa del corpo maggiore. In quest'ultima il livello di zero di formula_19 è posto a distanza infinita dal corpo celeste, e di conseguenza i valori di formula_19 sono sempre negativi.
Una forza di posizione agente su un punto materiale qualsiasi dello spazio tridimensionale in un sistema di riferimento viene definita in particolare come:
dove formula_46, formula_49, formula_50 sono le coordinate cartesiane di un generico punto nel riferimento, e agisce su un punto materiale di massa formula_27.
Si nota subito che questa forza è di tipo "non locale", in quanto non è nulla all'infinito:
Il calcolo del lavoro della forza lungo la curva formula_53 parametrizzata da:
avviene tramite un integrale curvilineo, oppure controllando che possa esistere una funzione energia potenziale associata alla forza formula_55. La forza è definita su tutto formula_56. Per il lemma di Poincaré se il campo è irrotazionale esiste una funzione energia potenziale associata. 
Il rotore di formula_55 è:
Il campo è quindi conservativo: ciò significa che il lavoro compiuto dalla forza non dipende dalla traiettoria del corpo. La funzione energia potenziale si calcola nel seguente modo:
Imponendo la condizione di località:
Risulta quindi che il campo di energia potenziale è di tipo "non locale" (come anche la forza che la origina).
Chiamando:
il lavoro compiuto dalla forza lungo la traiettoria formula_53 è funzione dei soli estremi del percorso e pari a:
Come si vede l'imposizione della condizione di località non ha alcun influsso sul lavoro (né l'avrebbe sulla forza). Inoltre, se la forza formula_55 è l'unica forza presente, si conserva l'energia meccanica del sistema formula_66, anche se risulta infinita:
e quindi la conservatività della quantità meccanica:
non dipende dalla condizione di località.
</text>
</doc>
<doc id="63351" url="https://it.wikipedia.org/wiki?curid=63351">
<title>Legge di Ohm</title>
<text>
a legge di Ohm stabilisce che la corrente in un conduttore
tra due punti è direttamente proporzionale alla differenza di potenziale tra i due punti stessi. La costante di proporzionalità è detta resistenza elettrica.
dove "I" è la corrente attraverso il conduttore, "V" è la differenza di potenziale ed "R" è la resistenza. Nel sistema internazionale la corrente si misura in ampere, la differenza di potenziale in volt e la resistenza in ohm. Più specificatamente, la legge di Ohm stabilisce che la "R" in questa relazione è costante, cioè indipendente dalla corrente.
La legge di Ohm è una legge empirica che descrive accuratamente la conduttività della maggior parte dei materiali conduttori al variare della corrente per molti ordini di grandezza. Vi sono alcuni materiali che non obbediscono alla legge di Ohm e vengono chiamati non-ohmici.
Il nome della legge è dovuto al fisico tedesco Georg Ohm, che, in un trattato pubblicato nel 1827, descrive la misura della corrente e della differenza di potenziale attraverso dei semplici circuiti con fili di diversa lunghezza. La formulazione originale è più complessa della forma attuale.
Esiste una espressione locale della legge di Ohm:
Le grandezze vettoriali che intervengono localmente sono J la densità di corrente, E il campo elettrico nello stesso punto e formula_3 una grandezza che dipende dal materiale detta resistività. Questa formulazione è dovuta a Gustav Kirchhoff
La legge di Joule ha un carattere più generale della legge di Ohm.
Gli elementi circuitali che utilizzano la legge di Ohm vengono chiamati resistori.
Nel gennaio 1781, prima del lavoro di Georg Ohm, Henry Cavendish fece degli esperimenti con una bottiglia di Leida e dei tubi di vetro di vari diametri e lunghezza riempiti con soluzioni saline. Egli misurò la corrente annotando lo shock elettrico che percepiva quando chiudeva il circuito con il suo corpo. Cavendish scrisse che la
velocità (corrente) era proporzionale al grado di elettrificazione (differenza di potenziale), ma scrisse tali annotazioni senza comunicarlo alla comunità scientifica del suo tempo il suo risultato rimase sconosciuto fino a quando Maxwell lo pubblicò nel 1879. Nel 1814 F. Ronalds usando una pila a secco trovò la legge di proporzionalità tra corrente e differenza di potenziale.
Qualche anno dopo, tra il 1825 e il 1826, Ohm fece i suoi esperimenti pubblicando i risultati in un libro
. All'epoca era nota la legge di Fourier sulla conduzione termica ed a tale legge si ispirò chiaramente Ohm. Per i suoi esperimenti, utilizzò inizialmente una pila di Volta, ma in seguito usò una termocoppia che forniva una differenza di potenziale più stabile e una resistenza interna costante. Per misurare la corrente usò un galvanometro. La differenza di potenziale della termocoppia era derivata indirettamente dalla misura della differenza di temperatura tra gli estremi della termocoppia. Il circuito era chiuso da fili di vari materiali di diversa lunghezza e sezione.
In termini moderni potremmo mostrare che l'esperimento eseguito è schematizzabile come la figura qui sotto.
Potremmo scrivere il risultato in notazione moderna come:
dove formula_5 è la corrente misurata dal galvanometro, formula_6 è la forza elettromotrice
della termocoppia a circuito aperto ed formula_7 è la resistenza da misurare, formula_8 è la resistenza interna della termocoppia (per Ohm era una costante necessaria per giustificare i risultati). 
La legge di Ohm fu probabilmente una delle più importanti descrizioni quantitative della fisica dell'elettricità, attualmente ci appare banale, ma quando Ohm pubblicò il suo risultato fu criticato dalla società scientifica del tempo tanto da essere definita un rete di fantasie e il ministro tedesco dell'istruzione disse che "un professore che predicava simili eresie non era degno di insegnare scienza".
L'approccio alla scienza da parte della filosofia in Germania, in quegli anni, era che non vi era necessità di fare esperimenti per capire la natura, in quanto la natura è perfettamente ordinata, quindi la verità scientifica
può essere dedotta da ragionamenti. Solo 15 anni dopo la legge di Ohm fu largamente accettata. In ogni caso l'importanza di Ohm fu riconosciuta prima della sua morte.
L'elettrone fu scoperto da J. J. Thomson solo nel 1897, e si capì rapidamente che era il portatore di carica nei conduttori. Nel 1900 fu infatti proposto il modello di Drude che utilizzando la fisica classica spiegava microscopicamente la legge di Ohm. Gli elettroni seguendo tale modello a causa delle collisioni con il reticolo cristallino, che causano una forza viscosa, si muovono con una velocità proporzionale al campo elettrico (la velocità di deriva). Solo nel 1927 Arnold Sommerfeld considerò la natura quantistica degli elettroni per elaborare un nuovo modello che prende il suo nome. Tale modello analogamente a quello di Drude considera gli elettroni nei metalli delle particelle libere ma soggette alla statistica di Fermi-Dirac.
La legge di Ohm è una legge empirica che viene generalizzata nella maggior parte dei materiali. È una legge meno generale delle equazioni di Maxwell e in alcuni materiali non vale. Nei metalli la legge ha un carattere universale, mentre negli isolanti vale solo per campi elettrici locali deboli. Infatti negli isolanti la velocità di deriva degli elettroni può raggiungere valori molto elevati e in questo caso si ha la rottura dielettrica.
La legge di Ohm è stata osservata a diverse scale di dimensioni. All'inizio del 2000 si credeva che la legge di Ohm dovesse perdere la sua validità per dimensioni paragonabili alla spaziatura atomica, ma nel 2012 si è dimostrato
sperimentalmente che una striscia di Silicio larga quattro atomi e di spessore un atomo ancora rispetta la legge di Ohm.
Nei conduttori le cariche libere si muovono
come in un fluido molto viscoso. Come sappiamo dalla meccanica del
punto se la viscosità è molto elevata il sistema raggiunge rapidamente la
condizione di velocità di deriva, in quanto la fase
di accelerazione del moto avviene in un tempo trascurabile. Da un punto di vista della dinamica del punto materiale, a regime, se formula_9 è il campo elettrico presente localmente, formula_10 è
la carica dei portatori di carica (normalmente gli elettroni), la forza di trascinamento
formula_11 viene bilanciata dalla forza di
attrito viscoso formula_12 :
Definendo con formula_14 la velocità di deriva, formula_15 la massa dei portatori di carica e formula_16 è il tempo medio tra gli urti. 
Dalla definizione di densità di corrente elettrica si ha che
Quindi :
da cui risulta la legge di Ohm in forma microscopica:
Dove formula_20 viene detta resistività elettrica che dipende dalle proprietà microscopiche del materiale. 
L'inverso della resistività elettrica viene chiamata conduttività elettrica:
formula_21 
Nel caso di conduttori in cui siano presenti portatori di carica di diversa natura come i semiconduttori, è più semplice usare la conduttività per descrivere i fenomeni di conduzione.
Consideriamo un cilindro conduttore di lunghezza formula_22, sezione normale formula_23 e resistività formula_24. Se applichiamo una d.d.p. formula_25 tra gli estremi:
Inoltre:
Sostituendo tale quantità nella espressione microscopica (1), proiettando nella direzione della velocità di deriva, si ha che:
Da cui se definisco con:
la resistenza del conduttore, posso riscrivere la (2) come:
Che è la legge di Ohm in forma macroscopica.
Se il conduttore non è a sezione costante ed al limite la resistività
varia con la posizione la generalizzazione della eq.(3) porta a:
Spesso invece della resistenza elettrica si utilizza il suo inverso la conduttanza elettrica formula_32, in tale caso la legge di Ohm:
Nel caso di un filo a sezione costante:
La resistività elettrica nei metalli varia approssimativamente in maniera lineare con la temperatura secondo la
legge:
Con formula_36 detto coefficiente di temperatura, formula_37 la resistività alla temperatura di riferimento (comunemente formula_38).
In tabella sono date le resistività
ed i coefficienti di temperatura di alcune sostanze a temperatura ambiente. Volutamente sono state messe nella tabella dei metalli, tutti con resistività molto bassa, ed altri materiali. La figura mostra la resistività dell'Alluminio che in un grande intervallo di temperatura ha una dipendenza lineare con la temperatura, in genere, per altri metalli, la linearità vale in un intervallo più limitato di temperatura. La distinzione tra conduttori ed isolanti diventa quantitativa con la definizione di resistività elettrica come appare chiaro dalla tabella. Mentre la legge di Ohm, vale senza limitazione nei conduttori, purché la temperatura sia mantenuta costante, nelle altre sostanze la validità è limitata al fatto che il campo elettrico localmente sia molto inferiore alla rigidità dielettrica del mezzo. 
Immaginiamo di avere formula_39 resistenze ciascuna di valore formula_40 poste
in parallelo come mostrato in figura.
Definiamo come formula_41 la corrente che scorre in ciascun resistore.
La d.d.p. formula_42 ai capi di ogni resistenza sarà eguale, mentre la corrente totale formula_43 è data dalla somma delle correnti che scorrono nei vari resistori, a causa della prima legge di Kirchhoff:
Ma dalla legge di Ohm applicata ad ogni resistore:
Quindi il parallelo di formula_39 resistori si comporta come una unica
resistenza equivalente di valore eguale a:
Immaginiamo di avere formula_39 resistenze in serie di valore formula_40 come
mostrato in figura. Definiamo con formula_42 la d.d.p ai capi di ogni resistenza. La d.d.p. totale è pari alla d.d.p. ai capi del sistema sarà la somma delle d.d.p. dei singoli elementi. La
corrente che scorre nei vari resistori è eguale a causa di quello che abbiamo visto nelle condizioni stazionarie per i fili percorsi da corrente.
Da cui segue che:
Quindi la serie di formula_39 resistenze equivale ad una resistenza equivalente pari alla somma dei singoli elementi:
La legge di Ohm può essere applicata al caso di corrente alternata, cioè un campo elettrico variabile nel tempo con un andamento sinusoidale. Per i circuiti in corrente alternata tensione e corrente sono date da:
dove formula_55 e formula_56 sono le ampiezze, formula_57 è la pulsazione e formula_58 la fase.
Quando tensione e corrente sono funzioni del tempo, come in questo caso, si deve tenere conto degli effetti capacitivi ed induttivi del materiale o del circuito, e per descrivere l'energia scambiata con il materiale si ricorre all'utilizzo di un numero complesso formula_59, detto impedenza, tale che si abbia:
dove le quantità in gioco sono scalari in formula_61.
L'impedenza può allora essere scritta come:
e la sua parte reale formula_7 è la resistenza, mentre la parte immaginaria formula_64 è detta reattanza:
L'argomento formula_66 quantifica lo sfasamento tra campo elettrico e corrente:
La reattanza tiene conto dei fenomeni di accumulo di energia elettromagnetica all'interno del materiale, che non si verificano in regime stazionario. La legge di Ohm estesa al caso non stazionario descrive quindi il comportamento di un componente circuitale passivo che, oltre a ostacolare il passaggio di corrente, provoca uno sfasamento tra corrente e tensione (nel caso stazionario non vi è sfasamento e l'equazione di Ohm contiene solo numeri reali).
I componenti circuitali passivi fondamentali sono, oltre alla resistenza, la capacità formula_68 e l'induttanza formula_69. La capacità sfasa la corrente di formula_70 rispetto al campo, l'induttanza di formula_71. Denotando con il pedice la rispettiva impedenza, essa vale:
e la legge di Ohm - applicata rispettivamente a resistenza, induttanza e capacità - ha la forma:
</text>
</doc>
<doc id="58606" url="https://it.wikipedia.org/wiki?curid=58606">
<title>Isolante elettrico</title>
<text>
Un isolante elettrico è un materiale incapace di condurre la corrente elettrica. Poiché un campo elettrico è in grado di polarizzare il materiale, genera cioè lo spostamento delle cariche, elettroni e nuclei, delle molecole dalla loro posizione di equilibrio, formando microscopici dipoli elettrici che generano all'interno del materiale un campo aggiuntivo opposto a quello esterno, manifestando una proprietà detta "dielettricità".
I termini "isolante" e "dielettrico" sono comunemente usati come sinonimi, tuttavia i due termini esprimono due concetti profondamente diversi: infatti il primo indica l'impossibilità del materiale di condurre la corrente elettrica, mentre il secondo indica che il materiale si polarizza in opposizione al campo elettrico esterno.
Si tratta di materiali, privi di cariche libere, che possiedono una banda energetica proibita molto ampia ed il campo elettrico esterno non fornisce sufficiente energia per consentire agli elettroni di raggiungere la banda di conduzione. Pertanto essa rimane quindi vuota e la conduzione risulta impossibile. 
Gli isolanti sono sostanze di vario tipo la cui conducibilità elettrica è in generale estremamente bassa, al punto che in alcuni casi si può supporre praticamente nulla. I materiali in cui, al contrario, non si verificano effetti di polarizzazione sono i conduttori caratterizzati da una piccola resistività elettrica. Gli isolanti si distinguono dai semiconduttori, i quali hanno la differenza di energia fra le due bande di 1-4 eV raggiungibile per molti elettroni del materiale (vedi statistica di Boltzmann) con la sola energia termica presente a temperatura ambiente.
Nelle equazioni di Maxwell si tiene conto della presenza di tale campo interno attraverso la definizione del vettore di polarizzazione elettrica e del campo d'induzione elettrica. La polarizzazione di un materiale dielettrico è inoltre quantificata dalla permittività elettrica.
La maggior parte dei materiali isolanti può essere trattata come un dielettrico lineare omogeneo ed isotropo; questo significa che tra il dipolo indotto nel materiale ed il campo elettrico esterno sussiste una relazione lineare. Si tratta di un'approssimazione di largo utilizzo, ed in tal caso i campi formula_1 e formula_2 sono equivalenti a meno di un fattore di scala:
Il numero formula_4 è una costante detta permittività elettrica del vuoto che vale:
</text>
</doc>
<doc id="42688" url="https://it.wikipedia.org/wiki?curid=42688">
<title>Legge di gravitazione universale</title>
<text>
In fisica la legge di gravitazione universale afferma che nell'Universo due corpi si attraggono in modo direttamente proporzionale al prodotto delle loro masse e inversamente proporzionale alla loro distanza elevata al quadrato.
Si tratta di una legge fisica generale derivata per induzione da osservazioni empiriche. Fa parte della meccanica classica ed è stata formulata da Isaac Newton nell'opera "Philosophiae Naturalis Principia Mathematica" ("Principia"), pubblicata il 5 luglio 1687.
Una recente valutazione (di Ofer Gal) sulla storia iniziale della legge dell'inverso del quadrato sottolinea che "verso la fine degli anni 1660", l'ipotesi di una "proporzionalità inversa tra gravità e il quadrato della distanza era abbastanza comune ed era stata avanzata da un certo numero di persone per motivi diversi". Lo stesso autore accredita Robert Hooke di un contributo significativo, anche seminale, ma tratta come poco interessante la pretesa di priorità di Hooke sulla questione dell'inverso del quadrato, dal momento che diverse persone oltre a Newton e Hooke l'avevano almeno accennata, e sottolinea invece, come contributi significativi di Hooke, l'idea della "composizione dei moti celesti" e l'aver convertito il pensiero di Newton dalla 'forza centrifuga' alla 'forza centripeta'.
Nel 1686, quando il primo libro di Newton "Principia" fu presentato alla Royal Society, Robert Hooke accusò Newton di plagio, sostenendo che egli aveva preso da lui la "nozione" de "la regola della diminuzione della Gravità, agendo essa in modo reciproco come i quadrati delle distanze dal Centro". Allo stesso tempo (secondo un resoconto dell'epoca di Edmond Halley) Hooke ammetteva che "la Dimostrazione delle Curve con ciò generate" era completamente di Newton.
Robert Hooke pubblicò le sue idee sul "Sistema del Mondo" negli anni 1660, quando lesse alla Royal Society il 21 marzo 1666 un documento "Sulla gravità", "riguardante la flessione di un moto diretto in una curva da parte di una sopravveniente azione di attrazione", e le pubblicò nuovamente sotto forma più sviluppata nel 1674, come "Tentativo di Dimostrare il Moto della Terra dalle Osservazioni". Hooke annunciò nel 1674 che aveva progettato di "spiegare un sistema del mondo diverso sotto vari aspetti da qualsiasi altro fino allora conosciuto", basato su tre "Supposizioni": che "tutti i corpi celesti indistintamente hanno un'attrazione o forza che gravita verso i propri Centri" e che "essi attirano anche tutti gli altri Corpi Celesti che si trovano nella sfera della loro influenza"; che "tutti i corpi di qualsiasi tipo che vengono messi in un moto diretto e semplice, continueranno il loro moto rettilineo e uniforme, fino a quando non verranno deviati e piegati da qualche altra forza efficace..." e che "queste forze attraenti sono tanto più potenti nell'operare, quanto più vicino ai propri Centri si trova il corpo sul quale agiscono". Così Hooke postulava chiaramente mutue attrazioni tra il Sole e i pianeti, in un modo che aumentava con la vicinanza al corpo attraente, insieme con un principio di inerzia lineare.
Le dichiarazioni di Hooke fino al 1674 non facevano menzione, tuttavia, a una legge dell'inverso del quadrato che si applica, o potrebbe applicarsi, a queste attrazioni. Inoltre, la gravitazione di Hooke non era ancora universale, anche se si avvicinava all'universalità più da vicino delle ipotesi precedenti. Egli inoltre non fornì ulteriori prove o dimostrazioni matematiche. Su questi ultimi due aspetti, Hooke stesso dichiarò nel 1674: "Finora, ciò che questi diversi gradi [di attrazione] sono, non ho ancora verificato sperimentalmente", e per tutta la sua proposta: "Al momento questo è solo un accenno", "avendo io sottomano molte altre cose che vorrei prima completare, e pertanto non posso troppo prendermi cura" (cioè "proseguire questa indagine"). Fu in seguito, il 6 gennaio 1679 in uno scritto a Newton, che Hooke comunicò la sua "ipotesi... che l'Attrazione è sempre in una proporzione duplicata alla Distanza dal Centro Reciproco, e che di Conseguenza la Velocità sarà a sua volta in proporzione duplicata all'Attrazione e di Conseguenza, come Keplero Suppone, Reciproca alla Distanza." (La deduzione sulla velocità non era corretta.)
Nel carteggio con Newton del 1679-1680, Hooke citò non solo la supposizione sull'inverso del quadrato per la diminuzione dell'attrazione con l'aumentare della distanza, ma anche, nella sua lettera a Newton del 24 novembre 1679 a proposito del moto celeste dei pianeti, un approccio alla "composizione di un moto diretto per la tangente con un moto di attrazione verso il corpo centrale".
Nel maggio 1686 Newton, a fronte della rivendicazione di Hooke sulla legge dell'inverso del quadrato, negò che questi dovesse essere accreditato come autore dell'idea. Tra le ragioni addotte, Newton ricordò che l'idea era stata discussa con Sir Christopher Wren precedentemente alla lettera di Hooke del 1679. Inoltre Newton sottolineò e riconobbe la priorità del lavoro di altri, tra cui Bullialdus, (che suggerì, senza peraltro dimostrarlo, che ci fosse una forza attrattiva dal Sole in proporzione inversa al quadrato della distanza), e Borelli (il quale suggerì, anch'egli senza dimostrarlo, che c'era una tendenza centrifuga a controbilanciare un'attrazione gravitazionale verso il Sole, così da far muovere i pianeti lungo ellissi). Whiteside ha descritto il contributo al pensiero di Newton derivato dal libro di Borelli, una copia del quale era nella libreria di Newton alla sua morte
Newton inoltre difese il suo lavoro sostenendo che se anche avesse sentito Hooke parlare di proporzione inversa del quadrato, egli avrebbe ancora dei diritti derivanti dalle sue dimostrazioni circa l'accuratezza dell'idea. Hooke, senza prove a favore della supposizione, poteva solo immaginare che la legge dell'inverso del quadrato a grandi distanze dal centro fosse valida in modo approssimativo. Secondo Newton, mentre i 'Principia' non erano ancora stati pubblicati, c'erano a priori tante ragioni per dubitare della precisione della legge (specialmente nei pressi di un corpo sferico) che "senza le mie (di Newton) Dimostrazioni, alle quali il signor Hooke è estraneo, un Filosofo giudizioso non poteva credere che fosse precisa dappertutto".
Questa osservazione si riferisce tra l'altro alla scoperta di Newton, supportata da dimostrazione matematica, che se la legge dell'inverso del quadrato si applica a piccole particelle, allora anche una grande massa sferica simmetrica attrae masse esterne alla sua superficie, anche da molto vicino, proprio come se tutta la propria massa fosse concentrata nel suo centro. Così Newton dava una giustificazione, altrimenti mancante, per applicare la legge dell'inverso del quadrato a grandi masse sferiche planetarie come se fossero minuscole particelle. Inoltre, Newton aveva elaborato nelle proposizioni 43-45 del Libro 1, e le relative tre sezioni del libro 3, un complesso esame sulla precisione della legge dell'inverso del quadrato, in cui egli dimostrò che soltanto quando la forza è esattamente come l'inverso del quadrato della distanza che le direzioni di orientamento delle orbite ellittiche dei pianeti rimangono costanti, come si è osservato che fanno, a parte piccoli effetti attribuibili a perturbazioni interplanetarie.
Alcuni manoscritti di Newton degli anni 1660 dimostrano che egli era arrivato a provare che, nel caso di moto planetario circolare, 'il tentativo di recedere' (chiamato in seguito forza centrifuga) aveva un rapporto di inverso del quadrato con la distanza dal centro. Dopo il suo carteggio con Hooke degli anni 1679-1680, Newton adottò il linguaggio di forza verso l'interno o forza centripeta. Secondo lo studioso di Newton J Bruce Brackenridge, anche se molto è stato fatto nel cambiamento di linguaggio e di punti di vista, tra forza centrifuga e centripeta, i calcoli reali e le prove sono rimasti gli stessi in entrambi i modi. Esse implicavano anche la combinazione di spostamenti tangenziali e radiali, a cui Newton stava lavorando negli anni 1660. La lezione offerta da Hooke a Newton qui, anche se significativa, era di prospettiva e non cambiava l'analisi. Questo sottofondo dimostra che per Newton c'erano validi motivi per negare la paternità di Hooke sulla legge dell'inverso del quadrato.
D'altra parte, Newton accettò e riconobbe, in tutte le edizioni dei 'Principia', che Hooke (ma non soltanto lui) aveva per proprio conto mostrato apprezzamento per la legge dell'inverso del quadrato nel sistema solare. A questo proposito, Newton ebbe riconoscimenti anche per Wren e Halley alla Proposizione 4 del Libro 1. Ad Halley disse che il suo carteggio con Hooke del 1679-80 aveva risvegliato in lui un latente interesse in campo astronomico, ma ciò non significava che Hooke gli avesse detto qualcosa di nuovo o di originale: "non gli sono grato per avermi illuminato in questo lavoro, ma solo per avermi distolto da altri miei studi per riflettere su queste cose; è l'arroganza dei suoi scritti, come se avesse scoperto il moto nell'Ellisse, che mi ha spinto a studiarlo..."
Dai tempi di Newton e Hooke, gli studiosi si sono chiesti se l'accenno di Hooke del 1679 alla 'composizione dei moti' avesse fornito a Newton qualcosa di nuovo e di valido, anche se, all'epoca, questa in realtà non era una rivendicazione avanzata da Hooke. Come descritto sopra, i manoscritti di Newton degli anni 1660 mostrano che egli effettivamente combina il moto tangenziale con gli effetti della forza diretta radialmente, per esempio nel ricavare la relazione di inverso del quadrato nel caso di moto circolare. Essi mostrano inoltre che Newton esprime chiaramente il concetto di inerzia lineare per la quale era in obbligo verso un'opera di Cartesio pubblicata nel 1644, come probabilmente lo era lo stesso Hooke. Non sembra che Newton avesse appreso questi argomenti da Hooke.
Tuttavia, alcuni autori hanno avuto altro da dire su ciò che Newton aveva acquisito da Hooke, così qualche aspetto rimane controverso. Il fatto che la maggior parte degli scritti privati di Hooke sia scomparsa non aiuta a stabilire la verità.
Il ruolo di Newton in relazione alla legge dell'inverso del quadrato non fu come talvolta viene rappresentato. Egli non la concepì come una semplice idea. Ciò che Newton fece fu di dimostrare come la legge di attrazione dell'inverso del quadrato avesse diverse e indispensabili connessioni matematiche con le caratteristiche osservabili dei moti dei corpi del sistema solare, e che la correlazione era tale per cui le evidenze osservative e le dimostrazioni matematiche, considerate nel loro insieme, davano motivo di credere che la legge non era soltanto approssimativamente vera, ma lo era in modo esatto (con la precisione raggiungibile ai tempi di Newton e per i due secoli successivi, e con alcuni punti non risolti che allora non potevano certamente essere presi in esame, le cui implicazioni teoriche non erano ancora state adeguatamente identificate o calcolate).
Circa trenta anni dopo la morte di Newton nel 1727, Alexis Clairaut, un eminente astronomo matematico a pieno titolo nel campo degli studi gravitazionali, dopo aver esaminato ciò che Hooke aveva pubblicato, scrisse che "Non si deve pensare che questa idea... di Hooke diminuisca il merito di Newton"; e che " l'esempio di Hooke "serve" a mettere in evidenza che distanza c'è tra una verità solo intravista e una verità dimostrata".
In linguaggio moderno, la legge afferma quanto segue:
Considerando le unità di misura SI, "F" è misurata in Newton (N), "m" e "m" in chilogrammi (kg), "r" in metri (m), e la costante "G" è approssimativamente uguale a 6,674 x 10 N m kg.
Il valore della costante "G" è stato accuratamente determinato dai risultati dell'esperimento di Cavendish condotto dallo scienziato britannico Henry Cavendish nel 1798, anche se non fu proprio Cavendish a calcolare il valore numerico di "G".
Questo esperimento fu anche la prima verifica della teoria della gravitazione di Newton tra masse in laboratorio. Ebbe luogo 111 anni dopo la pubblicazione dei "Principia" di Newton e 71 anni dopo la sua morte, quindi nessuno dei calcoli di Newton poteva utilizzare il valore di "G"; egli poteva soltanto calcolare il valore di una forza rispetto ad un'altra.
Il teorema del guscio sferico dimostra che corpi rigidi con distribuzioni di massa sfericamente simmetriche attraggono e sono attratti come punti materiali con tutta la massa situata nei loro centri.
La legge della gravitazione di Newton assomiglia alla legge di Coulomb delle forze elettriche, usata per calcolare la grandezza della forza elettrica tra due corpi elettricamente carichi. Entrambe sono leggi dell'inverso del quadrato, in cui la forza è inversamente proporzionale al quadrato della distanza tra i corpi. La legge di Coulomb ha il prodotto di due cariche al posto del prodotto delle masse, e la costante elettrostatica al posto della costante gravitazionale.
La legge di Newton è stata successivamente sostituita dalla teoria di Einstein della relatività generale, ma continua ad essere utilizzata come un'eccellente approssimazione degli effetti della gravità. La relatività è richiesta solo quando c'è bisogno di estrema precisione, o quando si tratta di gravitazione per oggetti di "notevole" massa e densità.
La legge di gravitazione universale di Newton può essere scritta come un'equazione vettoriale per tener conto della direzione della forza gravitazionale oltre che della sua grandezza. In questa formula, le quantità in grassetto rappresentano vettori.
dove
Si può vedere che la forma vettoriale dell'equazione è la stessa della forma scalare indicata in precedenza, tranne che ora F è una quantità vettoriale, e il lato destro è moltiplicato per l'appropriato vettore unitario. Inoltre, si può vedere che F = −F.
Se i corpi in questione hanno un'estensione spaziale (piuttosto che essere dei punti materiali teorici), allora la forza gravitazionale tra loro viene calcolata sommando i contributi dei punti materiali che costituiscono i corpi. Al limite, quando i punti materiali componenti diventano "infinitamente piccoli", occorre eseguire l'integrazione della forza (sotto forma vettoriale, vedi sotto) sulle estensioni dei due corpi.
In questo modo si può dimostrare che un oggetto con una distribuzione sfericamente simmetrica della massa esercita su corpi esterni un'attrazione gravitazionale come se tutta la massa dell'oggetto fosse concentrata in un punto al suo centro. (Ciò non è in genere vero per corpi non sfericamente simmetrici).
Quando due oggetti solidi entrano in contatto, la forza gravitazionale che li attrae non diventa infinita perché fisicamente la distanza fra due masse non può essere zero. Pertanto la forza non potrà mai essere infinita in condizioni normali.
Per punti "all'interno" di materia con distribuzione sfericamente simmetrica, può essere utilizzato il teorema del guscio sferico per trovare la forza gravitazionale. Il teorema afferma che le diverse parti della distribuzione di massa influiscono sulla forza gravitazionale misurata in un punto situato a distanza r dal centro della distribuzione di massa:
Di conseguenza, ad esempio, all'interno di un guscio di spessore e densità uniformi non vi è alcuna accelerazione gravitazionale netta in alcun punto all'interno della sfera cava.
Inoltre, all'interno di una sfera uniforme la gravità aumenta linearmente con la distanza dal centro; l'aumento dovuto alla massa aggiuntiva è 1,5 volte la riduzione dovuta alla maggiore distanza dal centro. Pertanto, se un corpo con simmetria sferica presenta un nucleo uniforme e un mantello uniforme con una densità che è meno di 2/3 di quella del nucleo, allora la gravità diminuisce inizialmente verso l'esterno al di là del confine e, se la sfera è abbastanza grande, ancora verso l'esterno la gravità aumenta di nuovo, ed alla fine supera la gravità al confine nucleo/mantello. La gravità della Terra potrebbe raggiungere il massimo al confine nucleo/mantello.
Il "campo gravitazionale" è un campo vettoriale che descrive la forza gravitazionale che verrebbe applicata, per unità di massa, su un oggetto in un punto qualsiasi dello spazio. In pratica è uguale all'accelerazione di gravità in quel punto.
Si tratta di una generalizzazione della forma vettoriale, che diventa particolarmente utile se più di due oggetti sono coinvolti (come un razzo tra la Terra e la Luna). Per due oggetti (ad esempio l'oggetto 2 è un razzo, l'oggetto 1 la Terra), ci limitiamo a scrivere r al posto di r e "m" al posto di "m" e di definire il campo gravitazionale g(r) come:
in modo tale che possiamo scrivere:
Questa formulazione dipende dall’oggetto che causa il campo. Il campo ha unità di accelerazione; in SI, questa è m/s.
I campi gravitazionali sono anche conservativi; vale a dire che il lavoro compiuto dalla gravità da una posizione ad un'altra è indipendente dal cammino. Ciò ha la conseguenza che esiste un potenziale campo gravitazionale "V" (r) tale che
Se "m" è un punto materiale o la massa di una sfera con distribuzione di massa omogenea, il campo della forza g(r) all'esterno della sfera è isotropo, cioè, dipende solo dalla distanza "r" dal centro della sfera. Allora
La descrizione della gravità di Newton è sufficientemente accurata per svariati scopi pratici ed è quindi ampiamente utilizzata. Non cambia di molto quando le quantità adimensionali "φ"/"c" e "(v/c)" sono entrambe parecchio minori di uno, dove "φ" è il potenziale gravitazionale, "v" è la velocità degli oggetti analizzati, e "c" è la velocità della luce. Ad esempio, la gravità newtoniana fornisce un'accurata descrizione del sistema Terra/Sole, in quanto
dove "r" è il raggio dell'orbita della Terra attorno al Sole.
In situazioni in cui uno dei parametri adimensionale è grande, allora deve essere usata la relatività generale per descrivere il sistema. La relatività generale si riduce alla gravità newtoniana in presenza di potenziali piccoli e velocità basse, ed è per questa ragione che la legge della gravitazione di Newton è a volte citata come la relatività generale per basse gravità.
Il fatto che la "massa gravitazionale" e la "massa inerziale" sia la stessa per tutti gli oggetti rimane non spiegato nell'ambito delle teorie di Newton. La Relatività Generale considera ciò come un principio fondamentale (vedere il Principio di Equivalenza). Di fatto, gli esperimenti di Galileo, decenni prima di Newton, stabilirono che oggetti che hanno la stessa resistenza dell'aria o di un liquido vengono accelerati dalla forza di gravità della Terra allo stesso modo, indipendentemente dalle loro diverse masse "inerziali". Tuttavia, le forze e le energie necessarie per accelerare masse diverse dipendono strettamente dalle loro diverse masse" inerziali", come si può vedere dalla seconda legge del moto di Newton, F = ma.
Il problema è che le teorie di Newton e le sue formule matematiche spiegano e permettono il calcolo (impreciso) degli effetti della precessione del perielio delle orbite e della deflessione dei raggi luminosi. Tuttavia, esse non spiegano l'equivalenza di comportamento di masse diverse sotto l'influenza della gravità, a prescindere dalle quantità di materia interessate.
Mentre Newton fu in grado di formulare la legge di gravità nella sua monumentale opera, egli era profondamente a disagio con il concetto di "azione a distanza", che le sue equazioni implicavano. Nel 1692, nella sua terza lettera a Bentley, scrisse: ""Che nel vuoto un corpo possa agire a distanza su un altro senza la mediazione di qualsiasi altra cosa, per mezzo e attraverso la quale la loro azione e la loro forza possano essere trasferite dall'uno all'altro, è per me un'assurdità così grande a cui, credo, nessun uomo con competenze in questioni filosofiche potrebbe mai credere"".
Egli non riuscì mai, secondo le sue parole, "a stabilire la causa di questa forza". In tutti gli altri casi, egli usò il fenomeno del moto per spiegare l'origine delle varie forze che agiscono sui corpi, ma nel caso della gravità, non fu in grado di identificare sperimentalmente il moto che produce la forza di gravità (anche se inventò due ipotesi meccaniche nel 1675 e 1717). Inoltre, si rifiutò persino di offrire un'ipotesi alla causa di questa forza, con la motivazione che ciò sarebbe stato contrario alla sana scienza. Deplorò che, per trovare l'origine della forza di gravità, "i filosofi hanno finora tentato la ricerca della natura invano" poiché egli era convinto "per varie ragioni" che c'erano "cause fino ad oggi sconosciute" che erano fondamentali per tutti i "fenomeni della natura".
Questi fenomeni fondamentali sono ancora oggetto di indagine e, anche se le ipotesi abbondano, la risposta definitiva deve ancora essere trovata. E nel "General Scholium" di Newton del 1713 nella seconda edizione del "Principia": ""Non sono stato in grado finora di scoprire la causa di queste proprietà della gravità e hypotheses non fingo... È sufficiente che la gravità esista davvero e agisca secondo le leggi che ho spiegato, e che serva a tenere conto di tutti i moti dei corpi celesti"".
Queste obiezioni sono state spiegate dalla teoria della relatività generale di Einstein, nella quale la gravitazione è un attributo dello spaziotempo curvo invece di essere dovuta a una forza di propagazione tra i corpi. Nella teoria di Einstein, le masse distorcono lo spaziotempo nelle loro vicinanze, e altri corpi si muovono in traiettorie determinate dalla geometria dello spaziotempo. Ciò ha permesso una descrizione dei moti della luce e delle masse in linea con tutte le osservazioni disponibili. Nella relatività generale, la forza gravitazionale è una forza apparente dovuta alla curvatura dello spaziotempo, in quanto l'accelerazione di gravità di un corpo in caduta libera è dovuta alla sua linea di universo, essendo una geodetica dello spaziotempo.
</text>
</doc>
<doc id="69468" url="https://it.wikipedia.org/wiki?curid=69468">
<title>Spostamento (cinematica)</title>
<text>
In cinematica si definisce spostamento il cambiamento di posizione di un punto nello spazio.
Date due posizioni formula_1 e formula_2 dello stesso punto, lo spostamento è dato da:
in pratica è il vettore differenza dei due vettori posizione formula_2 e formula_5 in quanto:
Utilizzando i versori, il vettore spostamento si può ricavare componendo il vettore. Ad esempio in due dimensioni si avrà:
Ovvero abbiamo che la differenza tra due coordinate lungo un asse di riferimento (ad esempio "x") denota un segmento formula_9 che ha un orientamento determinato dal segno della differenza, ovvero un segmento orientato, o anche è un vettore formula_10.
Lo possiamo vedere anche dal fatto che è un prodotto tra uno scalare formula_11 per un vettore formula_12 di lunghezza formula_13, che per il prodotto di uno scalare per un vettore restituisce un vettore lungo formula_14 di lunghezza formula_11 e lungo il verso formula_12.
La composizione dei vettori formula_10 e formula_18
mi dà il vettore spostamento formula_19, come la composizione delle posizioni formula_20, formula_21.
Analogamente in uno spazio a tre dimensioni:
Lo spostamento risulta così indipendente dalla traiettoria effettivamente percorsa per muoversi fra i due punti.
Lo spostamento non è mai tangente alla traiettoria, tranne che nel semplice caso di traiettoria rettilinea.
Sappiamo che la velocità media formula_24 è il rapporto tra lo spostamento, formula_25,
e l'intervallo di tempo impiegato per compierlo formula_26
</text>
</doc>
<doc id="69546" url="https://it.wikipedia.org/wiki?curid=69546">
<title>Posizione</title>
<text>
In fisica, la posizione è l'insieme delle quantità misurabili, dette "coordinate", le quali devono essere di numero pari o superiore ai suoi gradi di libertà, che definisce dove si trovi nello spazio un punto rispetto ad un sistema di riferimento.
L'evidenza sperimentale ha finora dimostrato che per determinare la posizione di un punto materiale nello spazio, quindi un oggetto incapace di ruotare su sé stesso, sono sufficienti tre coordinate, pertanto si ritiene che lo spazio che ci circonda sia tridimensionale, almeno fino alla scala delle dimensioni sondabili sperimentalmente. La ricerca di extradimensioni a scale di dimensioni molto piccole è oggetto di studio della fisica delle particelle.
Per determinare la posizione di un punto vincolato su una curva o su una superficie sono sufficienti rispettivamente una o due coordinate.
In meccanica classica, la posizione di un punto nello spazio è determinata dal vettore posizione definito all'interno di un sistema di riferimento, ad esempio cartesiano.
Il vettore posizione è definito da:
Scegliendo come sistema di riferimento il sistema cartesiano il vettore posizione è un vettore applicato nell'origine degli assi.
Si tratta di un segmento orientato OP che spesso si trova rappresentato anche mediante coordinate del punto P da cui si ottiene la rappresentazione dello stesso per componenti, per esempio nella forma
dove formula_8 e formula_9 rappresentano i versori degli assi cartesiani.
Esistono altre forme di rappresentazione del vettore posizione, ad esempio in coordinate cilindriche e sferiche.
Il vettore posizione è legato alla definizione di spostamento, il quale è in corrispondenza biunivoca con la legge oraria dato che se si rappresenta il vettore in funzione del tempo si ha che
formula_10
In relatività ristretta e in relatività generale le tre coordinate che determinano la posizione di un punto nello spazio insieme all'istante preso in considerazione costituiscono le quattro coordinate che determinano un evento nello spaziotempo. Il concetto di vettore posizione in relatività ristretta può essere generalizzato in quadrivettore con l'aggiunta della quarta coordinata temporale. In relatività generale, tuttavia, l'interpretazione della posizione di un evento nello spaziotempo quadridimensionale richiede l'uso di concetti della geometria differenziale che danno allo spaziotempo proprietà diverse rispetto allo spazio euclideo ipotizzato per la meccanica classica.
</text>
</doc>
<doc id="65715" url="https://it.wikipedia.org/wiki?curid=65715">
<title>Vettore (matematica)</title>
<text>
In matematica, un vettore è un elemento di uno spazio vettoriale. I vettori sono quindi elementi che possono essere sommati fra loro e moltiplicati per dei numeri, detti "scalari".
I vettori sono comunemente usati in fisica per indicare grandezze che sono completamente definite solo quando sono specificati sia una magnitudine (o modulo) che una direzione ed un verso rispetto ad un altro vettore o un sistema di vettori. Le grandezze che possono essere descritte in questo modo sono chiamate grandezze vettoriali, in contrapposizione alle grandezze scalari che sono caratterizzate unicamente dallo loro magnitudine.
Il concetto matematico di vettore nasce dall'idea intuitiva di una grandezza fisica (come ad esempio spostamento, accelerazione e forza) caratterizzata da intensità, direzione e verso nello spazio tridimensionale. A seguito dell'introduzione delle coordinate cartesiane una grandezza di questo tipo poteva essere rappresentata da una terna di numeri reali: le componenti relative a tre direzioni spaziali di riferimento. Nella successiva formalizzazione matematica si è giunti a definire il concetto generale di spazio vettoriale, come insieme in cui è definita l'operazione di combinazione lineare di due o più elementi.
In vari settori della matematica e della fisica, come l'analisi funzionale o la meccanica quantistica, il concetto di spazio vettoriale è applicato agli spazi di funzioni, in cui i vettori sono funzioni, come gli spazi di Hilbert e gli spazi di Banach.
La più semplice e riduttiva rappresentazione di vettore è il segmento orientato. In geometria un segmento orientato formula_1, o "vettore applicato", è un segmento formula_2 dotato di un'orientazione, che rende formula_1 diverso da formula_4. Nello spazio bidimensionale può essere visualizzato con un punto "iniziale" formula_5 e un punto "finale" formula_6, e viene anche denotato con formula_7. Nell'insieme di tutti i segmenti orientati si definisce una relazione di equivalenza, detta di "equipollenza", convenendo che due segmenti orientati sono equipollenti se hanno la stessa direzione, la stessa lunghezza e lo stesso verso. La classe di equivalenza definisce un vettore. La classe di equipollenza individuata da un vettore applicato formula_7 è di solito denotata con il simbolo formula_9; si dice anche che formula_7 è un rappresentante (non certamente unico) del vettore libero formula_9. In questo modo è possibile definire in maniera naturale la somma formula_12.
I vettori sono definiti come facenti parte di uno spazio vettoriale; il piano cartesiano formula_13, inteso come piano affine con un punto fissato formula_14, è un esempio di spazio vettoriale (isomorfo allo spazio tangente in formula_14): un vettore è rappresentato in tal caso come un punto del piano cartesiano determinato da una coppia di numeri reali formula_16. Disegnando una freccia che parte nell'origine formula_17 e arriva in formula_16, si ottiene la "rappresentazione geometrica" del vettore formula_16. Nello spazio tridimensionale un vettore è analogamente una terna di numeri reali formula_20.
In generale, in dimensione formula_21 arbitraria (finita), l'insieme:
è uno spazio vettoriale di dimensione formula_21, i cui vettori sono ennuple di numeri reali:
Numerosi esempi di spazi vettoriali possono essere costruiti sostituendo il campo formula_25 dei numeri reali con un campo qualsiasi formula_26, ad esempio il campo formula_27 dei numeri complessi. Una ennupla di numeri complessi è quindi un vettore dello spazio vettoriale formula_28. Ogni spazio vettoriale formula_29 (sopra il campo formula_26) di dimensione finita è in effetti identificabile con formula_31, dopo aver fissato una opportuna base.
In molti spazi vettoriali di dimensione infinita un vettore può essere descritto come una successione (infinita) di numeri: questo argomento necessita però di strumenti più sofisticati, quali ad esempio la struttura di spazio di Hilbert.
In quanto elementi di uno spazio vettoriale, i vettori possono essere sommati fra loro e moltiplicati per uno scalare secondo le operazioni che definiscono lo spazio vettoriale stesso.
In due dimensioni i vettori possono essere sommati con la regola del parallelogramma, che corrisponde alla somma formula_32 di due vettori formula_33 e formula_34.
In generale, la somma di due vettori formula_35 e formula_36 in formula_31 è definita nel modo seguente:
La somma è associativa, commutativa e possiede l'elemento neutro che è il vettore nullo; inoltre ogni elemento ha un opposto. In altre parole, i vettori con la somma formano un gruppo abeliano.
Il prodotto di uno scalare formula_39 in formula_26 per un vettore formula_41 in formula_31 è definito nel modo seguente:
In particolare, formula_44. Il prodotto è associativo e gode della proprietà distributiva rispetto alla somma.
L'estensione del concetto di coordinate rispetto agli assi di un piano cartesiano è quello di coordinate di un vettore rispetto ad una base. Una base è un insieme di vettori tale per cui ogni elemento dello spazio vettoriale può essere scritto in modo unico come combinazione lineare dei vettori appartenenti a tale insieme. Una base del piano cartesiano sono ad esempio i vettori formula_45, poiché ogni vettore del piano si può scrivere come somma di essi moltiplicati ciascuno per un opportuno scalare.
Nello specifico, dato uno spazio vettoriale formula_29 su un campo formula_26, l'insieme formula_48 di vettori di formula_29 è una base di formula_29 se tali vettori sono linearmente indipendenti in formula_26 e generano formula_29, ovvero:
In particolare, per ogni vettore formula_54 di formula_29 gli scalari formula_56 sono le sue coordinate rispetto alla base scelta.
Data quindi una base formula_57, un qualsiasi vettore formula_58 può essere espresso come combinazione lineare:
La scomposizione di vettori è una procedura utilizzata ad esempio in fisica per scomporre le forze lungo direzioni particolari (ad esempio parallele e perpendicolari a determinati vincoli).
Un caso particolare di sistema di riferimento è quello ortonormale, in cui i vettori scelti come base sono tra loro ortogonali (base ortogonale) e tutti di lunghezza unitaria, cioè versori. Nel caso del piano o dello spazio euclideo, un tale sistema di coordinate è detto cartesiano. Un vettore viene dunque scomposto nelle sue "componenti cartesiane" e, convenzionalmente, i versori sono denominati con i simboli formula_60, formula_61 e formula_62 rispettivamente per gli assi formula_63, formula_64 e formula_65. I versori sono tali che:
con formula_67 il prodotto vettoriale. Un vettore formula_68 può allora essere scritto come combinazione lineare dei versori canonici:
In generale, in un sistema di riferimento cartesiano, le componenti di un vettore coincidono con i coefficienti di Fourier.
Gli elementi di uno spazio vettoriale non hanno automaticamente una "lunghezza", questa è definita solo se si aggiunge un'ulteriore struttura matematica: la norma (o modulo) di un vettore, quindi il modulo non è una proprietà intrinseca del vettore.
Uno spazio vettoriale in cui è definita la norma di un vettore è uno spazio normato. Su un qualsiasi spazio vettoriale è possibile definire vari tipi di norme. Ad esempio la norma euclidea di un vettore formula_70 è il numero reale non negativo:
Questa quantità è geometricamente interpretata come la lunghezza del vettore. Ovviamente è possibile anche definire una norma differente da quella euclidea su formula_72 in tal caso si ottengono geometrie non euclidee.
Un altro esempio è il seguente: lo spazio delle funzioni continue a valori reali definite su un intervallo chiuso formula_73 può essere dotato della norma :
Definendo una forma quadratica formula_75 nello spazio vettoriale considerato si associa ad ogni coppia di vettori formula_76 e formula_77 uno scalare formula_78. Ad esempio la norma formula_79 caratterizza la "lunghezza" del vettore formula_76. Spesso la forma quadratica considerata è un prodotto scalare, che caratterizza la struttura dello spazio euclideo: così due vettori formula_76 e formula_77 sono ortogonali se formula_83, mentre sono paralleli quando formula_84 assume valore massimo.
Il prodotto scalare "standard" di due vettori formula_54 e formula_86 in formula_72 è il numero:
Il prodotto scalare tra due vettori viene indicato usualmente con uno dei simboli seguenti:
dove formula_90 fa riferimento al prodotto matriciale tra un vettore riga e un vettore colonna, con formula_91 la trasposta di formula_54, che è equivalente al prodotto scalare standard.
Importanti spazi muniti di prodotto interno sono lo spazio euclideo (reale) e lo spazio di Hilbert (complesso).
Tramite il prodotto scalare standard è possibile scrivere la norma euclidea come:
Le applicazioni che agiscono su uno spazio vettoriale e restituiscono un numero sono dette funzionali. L'insieme dei funzionali lineari definiti sui vettori di uno spazio vettoriale formula_29 è lo spazio duale formula_95, i cui elementi però, non essendo vettori, non subiscono una trasformazione controvariante al cambiamento di coordinate, bensì una trasformazione covariante (sono quindi covettori). Ad esempio, l'impulso o il momento angolare sono covettori.
Il prodotto scalare definisce in modo naturale un isomorfismo tra vettori e covettori, cioè tra lo spazio vettoriale e il suo duale. Se il prodotto scalare è euclideo e la base è ortonormale allora le componenti di vettori e covettori coincidono, motivo per cui la loro distinzione è spesso trascurata nei testi di fisica più elementari.
Nello spazio tridimensionale sono particolarmente utilizzate alcune operazioni aggiuntive fra i vettori.
Il prodotto vettoriale è un'operazione definita tra due vettori formula_54 e formula_97 di formula_98 che restituisce un terzo vettore formula_99 che ha la direzione della retta perpendicolare al piano individuato da formula_54 e formula_97, e il suo modulo è dato dalla formula:
dove formula_103 è l'angolo fra formula_54 e formula_97. Il verso del vettore formula_86 è dato dalla regola della mano destra: disponendo pollice, indice e medio perpendicolari tra loro, se il pollice indica la direzione di formula_54 e l'indice la direzione di formula_97, allora il medio indica la direzione di formula_86 (si veda la figura a lato).
Esplicitamente il prodotto vettoriale è dato da:
dove formula_112 indica il determinante e formula_113 sono i versori degli assi. Il prodotto vettoriale si indica talvolta anche con la notazione formula_114.
Si nota che il prodotto vettoriale è nullo se almeno uno dei due vettori è il vettore nullo, oppure se i vettori sono tra loro paralleli. Inoltre il prodotto vettoriale soddisfa l'identità ciclica di Jacobi, è distributivo rispetto alla somma:
ed anticommutativo:
Un prodotto misto è un'espressione in cui compaiono contemporaneamente prodotti scalari e vettoriali di vettori. Ad esempio, il prodotto misto di tre vettori formula_117, formula_118, formula_119 è del tipo formula_120 ed è uno scalare. Il valore assoluto di questo scalare non dipende dall'ordine dei tre vettori e misura il volume del parallelepipedo costruito su di essi.
Un prodotto misto che comprende due o più prodotti vettoriali è sempre riconducibile ad una somma di prodotti misti più semplici, ciascuno avente al più un prodotto vettoriale. Ad esempio:
Una matrice costituita da una sola riga, ovvero di dimensione formula_122, viene detta vettore riga; una matrice costituita da una sola colonna, ovvero di dimensione formula_123, viene detta vettore colonna. L'operatore di trasposizione, denotato generalmente con una formula_124 ad esponente (formula_125) trasforma vettori riga in vettori colonna e viceversa. Spesso i vettori di formula_72 vengono descritti come vettori colonna, per poter descrivere le trasformazioni lineari come prodotto con una matrice.
I vettori di formula_31 possono essere considerati delle matrici a una riga o una colonna. Per questo motivo è lecito parlare di moltiplicazioni tra matrici e vettori; in ossequio alle regole della moltiplicazione di matrici, un vettore colonna formula_54 (di dimensione formula_123) sarà moltiplicabile "a sinistra" per una matrice formula_130 a condizione che il numero di colonne di formula_130 sia formula_21.
Generalmente si intende e si usa questo tipo di moltiplicazione, anche se in linea di principio è anche possibile moltiplicare "a destra" un vettore formula_122 per una matrice con formula_21 righe.
Il prodotto di Kronecker, definibile come prodotto tensoriale fra un vettore e uno trasposto rispettivamente in formula_31 e formula_136, è la matrice formula_137:
dove la formula_124 ad apice indica l'operazione di trasposizione. Ad esempio per formula_140:
Più in generale, dati due vettori formula_54 e formula_86 appartenenti a due spazi vettoriali formula_29 e formula_145 sopra lo stesso campo formula_26, il prodotto tensoriale tra i due vettori è un tensore di rango formula_147
Se formula_29 e formula_145 sono spazi vettoriali di dimensione formula_21 e formula_152, fissate due basi, il prodotto tensoriale formula_153 è descrivibile come uno spazio di matrici ed il prodotto tensoriale in coordinate si scrive come sopra.
In uno spazio di dimensione finita un vettore può essere definito come una ennupla di numeri formula_154 che in seguito ad un cambio di sistema di riferimento subiscono una trasformazione controvariante, ovvero passando dal sistema di coordinate formula_155 al sistema formula_156 si ha:
dove si è utilizzata la notazione di Einstein, e formula_158 sono le componenti del vettore nel nuovo sistema di riferimento. Un vettore è dunque un tensore di tipo formula_159. Uno scalare può del resto essere pensato come un vettore di una sola componente, e coincide con un tensore di rango formula_160.
</text>
</doc>
<doc id="79217" url="https://it.wikipedia.org/wiki?curid=79217">
<title>Tensione (meccanica)</title>
<text>
In fisica, la tensione meccanica è una forza esercitata su una unità di superficie. La tensione meccanica generica è formata da componenti "normali" e tangenziali alla superficie (ad esempio quando parliamo di taglio e/o torsione). Per i fluidi si definisce il concetto analogo di pressione (nel caso di componente normale).
Considerato un corpo soggetto a forze esterne in equilibrio, si effettui un taglio lungo un piano che divida il corpo in due sezioni.
Bisogna considerare adesso un punto A appartenente al piano di taglio, quindi appartenente ad entrambe le parti del corpo. La normale n condotta dal punto A sulla prima sezione è chiaramente uguale ed opposta alla rispettiva normale sulla seconda sezione.
Una volta effettuato il taglio, il corpo non si troverà più in uno stato di equilibrio. Per ripristinarlo è infatti necessario che le due sezioni esercitino delle forze uguali ed opposte, dette forze interne, o tensioni.
Considerato un elementino su una delle due facce di superficie formula_1A con la sua normale n e la forza formula_1F possiamo indicare la tensione come:
formula_3
Indicando genericamente la tensione normale con formula_4 possiamo scrivere:
formula_5 dove formula_6 è la forza normale alla superficie, che dà luogo, se il corpo subisce allungamenti, a uno sforzo di trazione, se il corpo subisce accorciamenti ad uno sforzo di compressione; indicando con formula_7 la generica tensione tangenziale possiamo scrivere analogamente a prima:
formula_8 dove formula_9 è la forza tangenziale alla superficie in esame che da luogo, invece, ad uno sforzo di taglio.
Nel caso della corda, nel momento in cui è sotto tensione, esercita una forza sui corpi che sono legati alle sue due estremità. 
La tensione è orientata lungo la corda nel verso di allontanamento dal corpo al quale è legata (nell'ipotesi di considerare le corde e le pulegge prive di massa e attrito).
Dato un pendolo semplice costituito da un punto materiale di massa formula_10 in movimento, appeso tramite un filo inestensibile di massa trascurabile, se formula_11 è l'angolo spazzato dal filo in funzione del tempo, la tensione esercitata sul filo è:
Dati due piani scabri inclinati di angoli rispettivamente formula_12 e formula_13, sui quali si trovano due corpi in movimento di massa formula_14 e formula_15 (con coefficienti di attrito dinamico rispettivamente formula_16 e formula_17) collegati ad un filo inestensibile e di massa trascurabile che passa attraverso ad una carrucola senza attrito, la tensione del filo è:
formula_18
</text>
</doc>
<doc id="81470" url="https://it.wikipedia.org/wiki?curid=81470">
<title>Altezza (acustica)</title>
<text>
L'altezza è la frequenza fondamentale di una nota musicale o suono che viene percepita, ed è una delle caratteristiche principali di un suono. L'altezza è la qualità che permette di distinguere se un suono è acuto o grave e dipende dalla frequenza dell'onda sonora che lo ha generato. In particolare: più la frequenza di un'onda sonora è elevata e più il suono ci sembrerà acuto, mentre più è bassa la frequenza e più il suono ci apparirà grave. Nonostante la frequenza fondamentale reale possa essere determinata con una misura fisica, essa può differire dall'altezza percepita per via degli ipertoni e degli armonici naturali del suono. Il sistema di percezione uditiva umano può avere anche difficoltà a distinguere differenze di altezza fra le note, in alcune circostanze.
I limiti dell'orecchio umano vanno da un minimo di ad un massimo di Hz. La pratica musicale tuttavia si serve di suoni la cui frequenza è compresa in limiti più ristretti e precisamente tra 64 e vibrazioni semplici al secondo.
Uno strumento a suono determinato è uno strumento musicale che emette suoni in riferimento alle note musicali. Può essere strumento a suono determinato un qualsiasi strumento a note come il pianoforte; non lo è uno strumento come una grancassa o un'intera batteria, detti a suono indeterminato.
Allo scopo di uniformare l'altezza dei suoni in tutti i paesi dal 1859 in poi sono stati convocati a più riprese dei congressi internazionali con il compito di stabilire la frequenza di un suono base detto diapason, che è il La (nel secondo spazio in chiave di violino, La secondo la notazione scientifica dell'altezza) alla quale tutti si attengano. Il più recente è stato il congresso di Londra (1951) che ha fissato la frequenza del La in 880 vibrazioni semplici al secondo (pari a ).
Il La sopra al Do centrale suonato su uno strumento qualsiasi ha un'altezza percepita pari a quella di un suono puro a ma non necessariamente ha un'armonica a quella frequenza.
Inoltre una piccola variazione di frequenza potrebbe non comportare una variazione percepibile di altezza, ma una variazione di altezza comporta necessariamente una variazione di frequenza.
Infatti la minima differenza avvertibile, la soglia oltre la quale si percepisce la variazione di frequenza, è di circa cinque cent, cioè cinque centesimi di un semitono equabile; ma questa soglia varia lungo lo spettro delle frequenze udibili ed è minore quando due note sono suonate contemporaneamente.
Come le altre sensibilità umane agli stimoli, la percezione dell'altezza può essere spiegata dalla legge di Weber-Fechner.
L'altezza è influenzata anche dall'ampiezza del suono, specialmente alle basse frequenze. Per esempio una nota grave e forte sembrerà ancora più grave se suonata più piano. Come accade per gli altri sensi, anche la percezione relativa dell'altezza può essere tratta in inganno, creando delle illusioni uditive. Ve ne sono diverse, come il paradosso del tritono o la più nota scala Shepard, dove una sequenza ripetuta (continua o discreta) di toni disposti in modo particolare (parziali separate da un'ottava) può sembrare come una sequenza ascendente o discendente infinita.
Il La sopra il Do centrale al giorno d'oggi è fissato a 440 Hz e spesso è scritto come "A = 440 Hz" (o semplicemente A440) e conosciuto come "diapason da concerto". Questo standard è stato adottato di recente. L'altezza è spesso citata come uno dei fondamentali aspetti della musica.
Le altezze sono spesso classificate usando la notazione scientifica dell'altezza o una qualche combinazione di una lettera e un numero che rappresenta una frequenza fondamentale. Per esempio, ci si può riferire al La sopra al Do centrale con "A" o con "A440". Ma ci sono due problemi con questa pratica. In primo luogo nel sistema temperato equabile la notazione è sovrabbondante: la nota Mi♯ ha la stessa ampiezza della nota Fa. In secondo luogo la percezione umana dell'altezza è logaritmica: la distanza percepita fra le altezze "A220" e "A440" è la stessa distanza percepita fra le altezze "A440" e "A880".
Per evitare questi problemi i teorici musicali talvolta rappresentano le altezze usando una scala basata sul logaritmo della frequenza fondamentale. Per esempio si può adottare lo standard MIDI per mappare la frequenza fondamentale "f" con un numero reale "p":
Questa funzione crea uno spazio dell'altezza lineare in cui le ottave hanno dimensione 12, i semitoni (i tasti adiacenti di una tastiera) dimensione 1 e al Do centrale è assegnato il numero 60. La distanza in questo spazio corrisponde alla distanza musicale così come viene misurata dagli esperimenti psicologici e compresa dai musicisti. Il sistema è flessibile in modo da includere "microtoni" non usati nelle tastiere standard. Per esempio l'altezza a metà fra Do (60) e Do♯ (61) può essere classificata come 60,5.
L'altezza di un suono può essere descritta in molti modi, come alta o bassa, discretizzata o continua, determinata o indeterminata, che varia nel tempo ("chirping") e la maniera in cui questo cambiamento avviene nel tempo: glissando, portamento, vibrato. In base all'estensione della voce si classificano in: soprano, contralto, tenore e basso.
Musicalmente non conta tanto la frequenza assoluta dei suoni, ma è importante la relazione che c'è fra queste altezze, cioè la differenza che può essere espressa da un rapporto o misurata in cent.
Le persone in grado di riconoscere queste relazioni hanno quello che si chiama un orecchio relativo, mentre le persone che riconoscono l'altezza reale di un suono hanno il cosiddetto orecchio assoluto.
Le altezze relative delle singole note che compongono una scala possono essere determinate in base a uno dei vari temperamenti. Nel mondo occidentale, il metodo più comune è quello della scala cromatica, che col temperamento equabile è al giorno d'oggi il sistema di temperamento maggiormente diffuso.
In questo sistema il rapporto d'altezza tra due note consecutive della scala è esattamente la radice dodicesima di due (circa 1,05946). Nei sistemi "ben temperati" (usati al tempo di Johann Sebastian Bach) esistono altri metodi di temperamento musicale. Quasi tutti questi sistemi hanno un intervallo in comune, l'ottava, in cui le altezze degli estremi sono l'una il doppio dell'altra. Per esempio se il La sopra al Do centrale è 440 Hz, il La superiore di un'ottava è 880 Hz.
Nella musica atonale, in quella dodecafonica o nella "musical set theory", l'altezza è una specifica frequenza mentre una "pitch class" è ciascun insieme delle frequenze separate da ottave. Per esempio, Do♯ e Re♭ hanno la stessa altezza mentre Do e Do sono funzionalmente uguali, perché separati di un'ottava.
Al contrario dei suoni che variano con continuità le altezze discretizzate sono praticamente universali, con poche eccezioni fra cui ""tumbling strains"" (Sachs &#38; Kunst, 1962) e ""indeterminate-pitch chants"" (Malm, 1967). Le note glissate sono usate in molte culture, ma sono comunque da mettere in relazione alle note discrete da cui derivano e che abbelliscono.
Storicamente diverse convenzioni sono state impiegate per fissare l'altezza delle note a specifiche frequenze. Vari sistemi di temperamento sono stati applicati per determinare i rapporti fra le frequenze delle note di una scala. Nel 1955, l'Organizzazione Internazionale per le Standardizzazioni fissò la frequenza del La sopra al Do centrale a , ma in passato sono state usate varie frequenze.
Fino al XIX secolo non ci sono stati tentativi di collaborazione per trovare uno standard all'altezza delle note e i livelli in Europa erano i più diversi. Anche all'interno di una singola chiesa, l'altezza usata poteva variare nel tempo per via del modo in cui si accordavano gli organi. Generalmente l'estremità della canna di un organo veniva ribattuta verso l'interno in modo da formare un cono o aperta verso l'esterno per variare la frequenza. Quando le estremità divenivano troppo danneggiate da questo costante processo, venivano tutte tagliate, riducendone la lunghezza e aumentando così l'intonazione generale dell'organo. Il più alto in assoluto è quello dell'organo Stertzing della chiesa di san Pietro a Erfurt, del 1702, il cui La corrisponde ad una frequenza di 519 Hz.
Ci si può fare un'idea della variabilità dell'altezza esaminando i vecchi diapason per accordatura, i tubi degli organi ed altre fonti. Per esempio un vecchio "pitchpipe "(un particolare di fischietto detto corista, utilizzato come riferimento per accordare) utilizzato in Inghilterra nel 1720 suona il La sopra al Do centrale a 380 Hz, mentre gli organi suonati da Johann Sebastian Bach ad Amburgo, Lipsia e Weimar erano calibrati a A = 480, una differenza di circa quattro semitoni. In altre parole il La prodotto dal "pitchpipe" del 1720 aveva la stessa frequenza del Fa di uno degli organi di Bach!
L'altezza non variava solo a seconda del posto o del periodo, il livello poteva variare anche all'interno di una città. L'altezza di un organo di una cattedrale inglese del XVII secolo, per esempio, poteva essere inferiore di cinque semitoni rispetto a quella di uno strumento a tastiera casalingo della stessa città.
Durante quei periodi in cui la musica strumentale divenne preminente rispetto al canto si nota una tendenza continua dell'altezza ad aumentare. Questa "inflazione dell'altezza" sembra dovuta alla competizione fra gli strumentalisti, ognuno teso a produrre un suono più chiaro e brillante di quello dei rivali; il che è particolarmente difficile con gli strumenti a fiato, dove la competizione coinvolge di più i fabbricanti che i musicisti. Bisogna ricordare che l'inflazione dell'altezza è un problema solo quando le composizioni musicali sono fissate secondo una notazione e la combinazione di numerosi strumenti a fiato e della musica scritta ha di conseguenza ristretto quasi completamente il fenomeno dell'inflazione dell'altezza alla tradizione Occidentale.
In almeno due momenti l'inflazione dell'altezza divenne così evidente che si rese necessaria una riforma. All'inizio del XVII secolo Michael Praetorius notava nel suo enciclopedico "Syntagma musicum" che i livelli d'altezza erano diventati talmente elevati che i cantanti soffrivano di problemi alla gola e che liutisti e violisti si lamentavano per le corde rotte. Analizzando le estensioni vocali tipiche citate da Pretorius si può concludere che il livello d'altezza del suo tempo, almeno nella parte di Germania dove viveva, era più alto di oggi di almeno una terza minore (tre semitoni). Le soluzioni che venivano applicate erano sporadiche e locali, ma comportarono in generale la creazione di standard separati per voci e organo da una parte (""Chorton"") e per compagnie da camera dall'altra (""Kammerton""). Quando i due gruppi suonavano insieme, come in una cantata, cantanti e strumentalisti potevano suonare la musica scritta in due chiavi diverse. Questo sistema fu mantenuto per circa due secoli.
Ci si accorse di queste differenze nell'altezza del suono anche attraverso l'osservazione dei diversi diapason per l'accordatura. Ad esempio, un diapason che fa riferimento a Handel, risalente al 1740, è accordato ad un La= 422,5 Hz, mentre uno più tardo del 1780 è accordato ad un La= 409 Hz, quasi un semitono più basso. In generale, verso la fine del XVIII secolo la frequenza del La centrale varia in una gamma di altezze comprese tra il 400 Hz ai 450 Hz.
Anche l'avvento della musica sinfonica e dell'orchestra come strumento indipendente e non più solo d'accompagnamento ripropose il problema dell'altezza ed una tendenza ad aumentare nuovamente. L'aumento si riflesse nei diapason prodotti nel periodo: un diapason del 1815 dalla "opera house" di Dresda dà A = 423,2 Hz, mentre un altro di undici anni dopo della stessa orchestra dà A = 435 Hz. Alla Scala di Milano, il La arrivò fino a 451 Hz.
La maggior parte degli oppositori alla tendenza al rialzo erano i cantanti, che lamentavano il crescente sforzo nel cantare. Principalmente per queste proteste il governo francese fece approvare una legge il 16 febbraio 1859 che definiva il La sopra al Do centrale a 435 Hz. Questo fu il primo tentativo di standardizzazione dell'altezza su un territorio così ampio e fu conosciuto come il "diapason normal". Divenne una convenzione abbastanza popolare anche al di fuori della Francia.
Comunque continuarono ad esserci le variazioni. Uno standard alternativo, conosciuto come "altezza filosofica" o "scientifica", fissava il Do centrale a 256 Hz (cioè 2 Hz) e quindi poneva il La superiore a circa 430,54 Hz. Questa convenzione ottenne una discreta popolarità per via della sua immediatezza matematica (la frequenza di ogni Do è una potenza di due). Ma non ottenne lo stesso riconoscimento ufficiale di A = 435 e non fu molto usato.
Nel 1939 una conferenza internazionale consigliò che il La sopra al Do centrale fosse accordato a 440 Hz. Questo standard fu ufficializzato dalla Organizzazione Internazionale per le Standardizzazioni nel 1955 (e riconfermato nel 1975) come ISO 16. La differenza fra questo standard e il "diapason normal" è dovuta alla confusione su quale fosse la temperatura alla quale deve essere misurato lo standard Francese. Lo standard doveva essere 439 Hz, ma fu cambiato in A = 440 Hz perché più facile da riprodurre in laboratorio, dato che 439 è un numero primo.
Nonostante questa confusione, A = 440 Hz è l'accordatura più usata nel mondo. Le orchestre degli Stati Uniti e del Regno Unito tendono ad aderire a questa convenzione come "concert pitch". In altri paesi, comunque, un'altezza un po' più acuta è diventata la norma: A = 442 è comune nelle orchestre dell'Europa continentale, mentre A = 445 è diffuso in Germania, Austria e Cina.
In pratica, dato che le orchestre continuano ad accordarsi su una nota fornita dall'oboe piuttosto che da un dispositivo di accordatura elettronico e dato che l'oboista potrebbe non avere usato un tale dispositivo per accordare il suo strumento, c'è ancora una piccola varianza nell'esatta altezza usata.
Anche gli strumenti solisti come il pianoforte (con cui un'orchestra si accorda se suonano assieme) non sono tutti accordati A = 440 Hz.
Ci sono tre modi di cambiare l'altezza di una corda che vibra.
Gli strumenti della famiglia degli archi così come numerosi altri cordofoni vengono accordati variando la tensione delle corde perché modificare la lunghezza o la massa per unità di lunghezza non sono metodi di pratica applicazione.
L'altezza viene modificata variando la lunghezza della corda. Una corda più lunga darà un suono più grave, mentre una corda più corta darà un suono più acuto. Il cambiamento di frequenza è inversamente proporzionale alla variazione di lunghezza, e un cambiamento geometrico in lunghezza corrisponde a un cambiamento aritmetico in frequenza:
Una corda lunga il doppio produce un suono più basso di un'ottava.
L'altezza viene modificata variando la tensione. Una corda con minore tensione (più lente) darà un suono più grave, mentre una corda con maggiore tensione (più tesa) darà un suono più acuto.
Il cambio di frequenza è proporzionale alla radice quadrata del cambio di tensione:
L'altezza viene modificata anche variando la densità della corda, intesa come massa per unità di lunghezza.
Una corda più pesante darà un suono più grave, una corda più leggera darà un suono più acuto.
Il cambio di frequenza è inversamente proporzionale alla radice quadrata del cambio di densità:
</text>
</doc>
<doc id="47613" url="https://it.wikipedia.org/wiki?curid=47613">
<title>Energia cinetica</title>
<text>
L'energia cinetica è l'energia che possiede un corpo per il movimento che ha o che acquista.
Dal punto di vista tecnico equivale al lavoro necessario per portare un corpo da una velocità nulla a una velocità nota. Quando un corpo di massa "m" varia la sua velocità, con questa varia anche la sua energia cinetica. Il lavoro equivale a questa variazione di energia cinetica. L'energia cinetica quindi è associata alla massa e alla velocità di un corpo in movimento. L'energia cinetica che possiede un corpo di massa "m" nel suo moto di caduta è uguale al lavoro compiuto per fermarsi.
Nella meccanica newtoniana l'energia cinetica formula_1 di una particella di velocità formula_2 è definita come il lavoro fatto da una forza esterna per aumentare la velocità della particella da zero al valore formula_2. Cioè:
dove formula_5 è il lavoro fatto dalla forza formula_6 nello spostare di formula_7 la particella. Per semplicità, possiamo limitare il moto a una sola dimensione, per esempio la formula_8. Allora, classicamente:
con formula_10 massa della particella.
L'energia cinetica di un punto materiale può essere espressa matematicamente dal semiprodotto della sua massa per il quadrato del modulo della sua velocità; in coordinate cartesiane si esprime di consueto come:
L'energia cinetica di un corpo rigido a simmetria assiale in rotazione attorno all'asse di simmetria con velocità angolare formula_12 e che trasla nello spazio con velocità formula_2 (velocità del centro di massa) è:
dove formula_10 è la massa totale del corpo e formula_16 il momento d'inerzia rispetto all'asse di rotazione.
L'energia cinetica dipende dal sistema inerziale di riferimento. In un sistema di riferimento stazionario l'energia cinetica assume un valore inferiore di quello assumibile in un sistema di riferimento in movimento. L'energia cinetica aggiuntiva è quella corrispondente all'energia cinetica di traslazione della massa m alla velocità v di spostamento del sistema inerziale di riferimento.
Una utile relazione tra l'energia cinetica "E" e il modulo della quantità di moto "p" è data dalle seguenti equazioni:
La dimostrazione è immediata sostituendo nell'espressione di "E" quella di "p".
In determinati casi può essere utile definire la grandezza "energia cinetica specifica" formula_1, definita come energia cinetica per unità di volume:
In meccanica classica, l'energia cinetica di un corpo di massa "m" è il lavoro necessario per portarlo da una velocità iniziale nulla a una velocità finale "v". Questa definizione può essere formalizzata grazie a quello che storicamente prende il nome di teorema delle forze vive, oggi più noto come "teorema dell'energia cinetica".
A livello globale il lavoro compiuto dalla forza F quando il corpo si sposta da uno stato iniziale a uno stato finale è uguale alla variazione dell'energia cinetica del corpo.
In meccanica analitica (non relativistica) è possibile estendere il concetto di energia cinetica, mantenendo al contempo inalterato il suo peculiare aspetto di funzione dipendente dal modulo quadrato della velocità.
Per fare questo, è necessario passare dalle consuete coordinate cartesiane a un sistema generico di coordinate: siano dunque
le coordinate generalizzate, tutte dipendenti dal tempo. Queste coordinate individuano la posizione di un punto materiale in uno spazio formula_22-dimensionale detto spazio delle configurazioni. Formalizzando il concetto, si definisce la funzione
che cioè manda un numero reale nello spazio delle configurazioni e che descrive le traiettoria della particella in tale spazio. È bene notare che non si sta parlando di traiettorie della particella nello spazio-tempo, bensì nello spazio delle configurazioni. Un cambiamento di coordinate è allora una funzione
in generale dipendente sia dal vettore posizione sia dal tempo, con particolari caratteristiche (un diffeomorfismo), che esprime la relazione esistente tra le vecchie coordinate e le nuove.
Introduciamo l'energia cinetica
che a questo punto ha una forma diversa rispetto a quella solitamente usata: la differenza discende dalla nuova forma che assume la velocità, che sebbene sia come al solito definita da
stavolta è una funzione composta, dunque
Calcolando esplicitamente l'energia cinetica grazie alle proprietà di linearità e simmetria del prodotto scalare standard, si ha
Abbiamo così ottenuto una forma quadratica operando le sostituzioni
Il risultato è davvero notevole se si pensa alla generalità da cui si è partiti nella trattazione: è bastato fornire alcune condizioni di regolarità (di norma verificate nel caso di condizioni fisiche) per ottenere una formula che amplia quella di uso comune. Nel caso in cui si tratti di particella libera, perciò, possiamo scrivere immediatamente la lagrangiana:
formula_30
mentre l'eventuale presenza di energia potenziale formula_32 dipendente dalla sola posizione, non fa altro che aggiungere un termine:
Un'altra caratteristica interessante discende dal considerare cambiamenti di coordinate indipendenti dal tempo: in questi casi l'energia cinetica diventa semplicemente un caso particolare di quella già trovata sopra
ma dato che i versori coordinati dello spazio delle configurazioni sono per definizione
i coefficienti formula_36 costituiscono una matrice quadrata che rappresenta il prodotto scalare rispetto alla base coordinata scelta.
La naturale estensione a un sistema costituito da più punti viene eseguita assegnando a ognuno di essi un vettore velocità e un vettore posizione: quindi per formula_37 particelle libere vengono prodotti formula_38 vettori, ciascuno di formula_22 coordinate e poi si procede come si è fatto per la particella singola, ottenendo il risultato che l'energia cinetica totale è la somma delle energie cinetiche delle singole particelle:
Nella meccanica relativistica di Einstein (impiegata particolarmente nelle velocità prossime alla velocità della luce) la massa è sempre costante, ma il lavoro necessario a portare a una velocità "v" una particella di massa (propria) "m" inizialmente in quiete non dipende dal quadrato della velocità come nel caso classico, anzi diverge per formula_41. Posti:
il lavoro "W" necessario per accelerare una particella di massa "m" inizialmente in quiete fino a una velocità "v" è pari a:
in cui formula_48 è il fattore di Lorentz:
Espandendo in serie di Taylor per piccoli formula_50:
Lo sviluppo in serie rende evidente che per valori piccoli della velocità formula_2 tutti i termini superiori al primo sono trascurabili e la serie assume il valore
che, tenendo conto della velocità iniziale nulla, è proprio l'espressione del teorema dell'energia cinetica in meccanica classica. La formula di Einstein generalizza quindi l'energia cinetica alle alte velocità.
È immediato dallo sviluppo in serie notare che quando formula_2 tende a 0 il rapporto tra l'energia cinetica relativistica e quella newtoniana data da formula_55 si approssima a 1:
La teoria della relatività afferma che l'energia cinetica di un oggetto tende all'infinito per velocità che si avvicinano alla velocità della luce, e diventa pertanto impossibile accelerare il corpo fino a raggiungere tale velocità. In altri termini la velocità della luce non può essere raggiunta da alcun corpo materiale mediante accelerazione.
</text>
</doc>
<doc id="83189" url="https://it.wikipedia.org/wiki?curid=83189">
<title>Potenza (fisica)</title>
<text>
In fisica, la potenza è definita operativamente come l'energia trasferita nell'unità di tempo. Viene anche utilizzata per quantificare l'energia prodotta o utilizzata da un sistema fisico. 
A seconda del tipo di energia trasferita, si parla più specificatamente di potenza meccanica (per il trasferimento di lavoro), potenza termica (per il trasferimento di calore) e potenza elettrica (per il trasferimento di energia elettrica). La potenza termica si indica in genere con il simbolo formula_1, mentre la potenza meccanica, la potenza elettrica e altre forme ordinate di potenza in genere si indicano con i simboli formula_2.
Nel sistema internazionale di unità di misura la potenza si misura in watt (formula_3), come rapporto tra unità di energia in joule (formula_4) e unità di tempo in secondi (formula_5):
Per motivi storici, si possono incontrare ancora unità di misura diverse, nate dall'uso di misurare l'energia e il tempo con altre unità di misura, a seconda del campo di applicazione. Ad esempio il cavallo vapore è la potenza necessaria per sollevare alla velocità di , e quindi 1 CV = 735,49875 W = 0,73549875 kW; oppure 1 CV = 0,98631 HP.
La potenza meccanica è definita come il lavoro "L ("chiamato anche "W)" compiuto nell'unità di tempo "t", ovvero come la sua derivata temporale:
In base al principio di uguaglianza tra lavoro ed energia, la potenza misura la quantità di energia scambiata nell'unità di tempo, in un qualunque processo di trasformazione, meccanico, elettrico, termico o chimico che sia.
La terza equazione cardinale è in effetti un'equazione nella potenza generica di un sistema materiale:
dove:
Si calcola il lavoro totale (che non chiamiamo L solo per non confonderlo col momento angolare totale) di un sistema di punti materiali rispetto a un polo formula_14. Chiamiamo formula_15 la posizione del punto i-esimo nel sistema di riferimento del polo. Per la equazione fondamentale della cinematica, e poiché le forze interne non lavorano:
formula_16
Dunque in definitiva la potenza risulta:
che è proprio la nostra tesi: la potenza deriva quindi da tutti i tipi di forze generalizzate, confermando la sintesi della meccanica lagrangiana.
Col concetto di flusso si può definire la potenza termica legandola al flusso termico "q":
In particolare, per una sfera emittente isotropicamente di raggio R come il Sole:
e per un cilindro emittente isotropicamente, come nel caso tipico di un nocciolo nucleare:
Se si considera la corrente termica che fluisce attraverso una superficie chiusa dV:
sfruttando il teorema della divergenza:
si può quindi definire densità di potenza termica la divergenza della densità di corrente termica:
In particolare per una sfera radiante isotropicamente di raggio R si può definire il gradiente radiale di potenza in modo proporzionale alla densità di potenza:
per un cilindro radiante isotropicamente invece si può definire il gradiente assiale di potenza (detto potenza lineica o densità lineica di potenza), sempre in modo proporzionale alla densità di potenza:
Innanzitutto bisogna tener presente che si può svolgere molto lavoro (cioè consumare o produrre energia) anche sviluppando poca potenza. Ciò infatti dipende dalla durata del processo secondo l'espressione integrale data sopra.
Ad esempio in una gara di maratona si consuma più energia rispetto a una gara di cento metri piani; ma certamente la potenza che deve sviluppare il centometrista è enormemente superiore a quella del maratoneta.
Allo stesso modo una lampadina da consuma un decimo di una stufetta (o di un altro elettrodomestico) da 1000 W, ma se utilizziamo la stufetta per un'ora e lasciamo accesa la lampadina per 24 ore, alla fine la stufetta avrà consumato solo un chilowattora mentre la lampadina ne avrà consumati ben 2,4 (il chilowattora è un'unità di misura tollerata di energia, non di potenza).
Ovviamente al fornitore elettrico si paga prima di tutto l'energia consumata e non la potenza; ma la stessa azienda elettrica fa pagare anche una quota base, proporzionale alla potenza nominale ("chilowatt"), cioè alla potenza massima del contatore a cui questo "stacca" la corrente. Ciò per molte ragioni, come il fatto che il fornitore deve garantire all'utente in ogni momento la fornitura della potenza nominale, ma anche il fatto che alla potenza nominale è proporzionale il costo della linea elettrica a monte del contatore.
In un mezzo di trasporto, la velocità massima dipende dalla potenza (in watt), che è data dal prodotto della coppia "T" (in N·m) per la velocità angolare del motore "ω" (in rad/s), spesso espressa come frequenza di rotazione "f" (in n° giri/min).
La potenza del motore delle automobili, motociclette o di qualsiasi mezzo stradale, può variare da pochi chilowatt fino a svariate centinaia.
La relazione che lega la velocità con la potenza erogata dal motore è influenzata da molti fattori, ma in generale la potenza richiesta dall'automobile per avanzare varia con linearità al variare della velocità sino a una certa soglia, indicativamente fino a 30 km/h la resistenza aerodinamica è trascurabile, per poi essere proporzionale al cubo della velocità.
Questo perché la resistenza aerodinamica (forza) è proporzionale al quadrato della velocità e la potenza è data dalla velocità moltiplicata la forza necessaria a vincere la resistenza aerodinamica, quindi la potenza è proporzionale al cubo della velocità.
Si ipotizzi che una motocicletta per viaggiare a 60 km/h necessiti di una potenza di 4 kW: per viaggiare a 70 km/h (1,166 volte la velocità iniziale) sarà necessario un aumento di potenza pari a 1,166^3 ovvero una potenza 1,588 volte la precedente, cioè 1,588*4 kW= 6,35 kW. Un incremento del 16,7% di velocità richiede un aumento del 58,8% di potenza. Per lo stesso incremento di 10 km/h da 150 a 160 km/h la potenza aumenta da 62,50 kW a 75,85 kW, quindi un aumento di 13,35 kW.
Se la velocità passa da 290 km/h a 300 km/h la potenza deve aumentare da 451,65 kW a 500 kW e quindi servono 48,35 kW, sempre per lo stesso incremento di 10 km/h.
Per quanto riguarda le automobili la regola per il calcolo è la stessa. È utile un esempio per avere un'idea delle potenze in gioco. Supponendo che un'automobile debba sviluppare 30 kW per raggiungere 125 km/h, una potenza doppia di 60 kW le permetterà di raggiungere 157,49 km/h e non 250 km/h come erroneamente si potrebbe pensare. 
La regola di calcolo esemplificata è teorica perché tiene principalmente conto soltanto dell'attrito dell'aria, mentre nella realtà sono presenti altri attriti, per cui il calcolo della velocità massima teorica di un veicolo non è semplicemente riconducibile a tale calcolo proposto.Fino alla velocità limite, indicativamente 30 km/h, le forze da vincere per fare avanzare il veicolo sono quelle degli attriti meccanici e dell'attrito volvente degli pneumatici pertanto, essendo la resistenza praticamente costante, la potenza (prodotto di forza per velocità) varia linearmente.
Oltre questa velocità limite la componente di resistenza dovuta all'aerodinamica, prima trascurabile, diviene preponderante: dato che a ogni incremento della velocità corrisponde un incremento elevato al quadrato della resistenza aerodinamica, è sufficiente anche un modesto aumento della velocità per fare aumentare notevolmente la potenza necessaria.
La potenza assorbita viene profondamente influenzata dal peso della vettura e dall'efficienza aerodinamica.
</text>
</doc>
<doc id="73449" url="https://it.wikipedia.org/wiki?curid=73449">
<title>Sistema di riferimento inerziale</title>
<text>
In fisica un sistema di riferimento inerziale è un sistema di riferimento in cui è valido il primo principio della dinamica. Con un'accettabile approssimazione è considerato inerziale il sistema solidale con il Sole e le stelle (il cosiddetto sistema delle stelle fisse), ed ogni altro sistema che si muova di moto rettilineo uniforme rispetto ad esso (e che quindi né acceleri né ruoti): in questo modo si viene a definire una classe di equivalenza per questi sistemi.
Un sistema di riferimento inerziale è un sistema di riferimento caratterizzato dalla seguente condizione: se un punto materiale è libero, cioè non sottoposto a forze oppure sottoposto ad una risultante nulla di forze, allora persevererà il suo stato di quiete o di moto rettilineo uniforme finché esso non viene perturbato. In altre parole un osservatore S o sistema di riferimento si dice inerziale se, di un punto materiale isolato (P, m), misura accelerazione nulla, qualunque sia l'istante t in cui si effettua tale misura e qualunque sia lo stato cinematico (P, Ṗ) del punto nel medesimo istante t.
Si può verificare che gli osservatori, che di un punto materiale isolato misurano accelerazione nulla, sono tutti e solo quelli che si muovono di moto traslatorio rettilineo uniforme rispetto all'osservatore S sopra citato.
Un sistema di riferimento inerziale nella dinamica newtoniana è un sistema di riferimento in cui sia valida la prima legge della dinamica. Questo assunto è basato sull'osservazione e sulla geometria euclidea.
L'espressione (Forza = massa × accelerazione) formula_1 ha un importante valore: non menziona la velocità e quindi questa non fa parte del calcolo; in pratica la variazione di velocità dovuta a una forza è indipendente dalla velocità. Questa proprietà esprime una proprietà fondamentale del moto: le velocità sono indistinguibili. Pertanto un osservatore può scegliere un qualsiasi sistema di riferimento inerziale come sistema di coordinate per i calcoli, e può applicare le leggi del moto. I sistemi di riferimento inerziali sono gli unici sistemi di riferimento in cui sono valide le stesse leggi del moto.
Nel modello concettuale della dinamica newtoniana, è sufficiente definire il moto inerziale come moto a velocità costante lungo una linea retta.
La Terra non è un vero e proprio sistema di questo tipo, a causa dei suoi movimenti di rivoluzione e di rotazione. In particolare, il moto di rotazione sottopone gli oggetti sulla sua superficie lontani dai poli a una piccola forza centrifuga. Tuttavia questa accelerazione è irrilevante in certi casi, per cui la Terra è un sistema di riferimento che approssima un sistema di riferimento inerziale.
Il moto di rotazione sottopone inoltre i corpi lontani dall'equatore alla forza di Coriolis, che devia verso destra il moto di tutti i corpi dell'emisfero nord e verso sinistra quelli dell'emisfero sud, come dimostrato dal famoso pendolo di Foucault.
Gli osservatori posti in un sistema inerziale si dicono osservatori inerziali, ed assumono una particolare importanza nell'ambito della meccanica. Due osservatori in moto relativo uniforme osservano che una medesima massa, sottoposta alla medesima forza, varia nel medesimo modo il suo stato di moto. Nel linguaggio corrente si può dire che presenta la medesima inerzia nei confronti di un tentativo di variarne lo stato di moto. Le leggi della meccanica classica sono invarianti solo per osservatori inerziali. Un osservatore dentro una capsula in caduta libera verso la Terra non può verificare la legge di caduta dei gravi, che pure descrive la sua stessa caduta.
In formule possiamo dire che:
La terza forza si annulla e i due osservatori misurano la medesima forza agente su formula_4
Se questo accade i due osservatori sono detti "inerziali".
La Relatività Speciale non solo ha identificato nelle trasformazioni di Lorentz lo strumento appropriato di trasformazione tra sistemi di riferimento inerziali, ma ha anche introdotto il concetto che le leggi che valgono per tutti i sistemi di riferimento inerziali includono anche le leggi dell'elettrodinamica, non solo quelle della meccanica.
In dinamica relativistica si riconosce che l'assunzione che lo spazio sia euclideo non è giustificata in generale. È comunque possibile creare una dinamica coerente ed accurata sulla base del concetto di "sistema di riferimento inerziale", verificando se ci sia un'accelerazione causata da una forza applicata. Una scatola contenente un peso, collegato a tutte le pareti del contenitore con molle che lo tengono sospeso, agisce come un accelerometro. Quando l'accelerometro viene messo in movimento da una forza, il peso in esso "oscilla" in verso opposto alla forza, a causa della propria inerzia. Se non ci sono manifestazioni di inerzia, questo è un sistema di riferimento inerziale.
In relatività generale esso è così definito, perché l'osservazione che anche gli accelerometri più accurati danno misura "zero" quando sono in caduta libera in un campo gravitazionale è presa come indicazione che "fondamentalmente" non è presente accelerazione nello spazio che circonda un oggetto in caduta libera.
In relatività generale, sia la gravità che l'inerzia sono definite come interazioni di materia con la geometria dello spazio-tempo. Detto molto sinteticamente: la geometria dello spazio-tempo indica alla materia come muoversi, e la materia deforma la geometria dello spazio-tempo.
Quando la materia si sta muovendo attraverso dello spazio-tempo non deformato (in altre parole: spazio-tempo rettilineo), essa segue traiettorie analoghe alle linee rette euclidee. Queste linee sono rettilinee universalmente, nel senso che esse sono diritte anche se osservate a distanza: esse sono universalmente linee rette. Oggetti che si muovono in uno spazio-tempo rettilineo possono muoversi solo in linea retta e a velocità costante: esso è l'unico movimento libero che questa forma di spazio-tempo permette. Quando la velocità cambia in seguito all'esercizio di una forza, allora si manifesta l'inerzia.
Quando la materia si muove liberamente attraverso uno spazio-tempo deformato descrive delle geodetiche. Queste sono percorsi in cui l'inerzia non si manifesta. Quando una forza devia un oggetto dalla propria geodetica si manifesta l'inerzia. In uno spazio-tempo deformato, un sistema di riferimento inerziale è definibile solo localmente, a causa della deformazione dello spazio-tempo causata dalla materia. La materia è riunita nelle stelle, nei pianeti ecc., e nelle vicinanze di questi gravi si osserva una deformazione sferica della geometria dello spazio-tempo. Parte di questa è un'alterazione del ritmo del tempo. Un oggetto in caduta libera verso il centro di gravità di un grave si muove lungo la propria geodetica. Il sistema di riferimento locale, ancorato all'oggetto, è un sistema di riferimento inerziale locale.
Secondo la relatività generale, esiste un solo campo, che può essere chiamato gravito-inerziale.
L'interazione gravitazionale è mediata dalla deformazione di qualcosa che è comunque presente: la geometria dello spazio-tempo.
La proprietà dell'inerzia esiste a causa dell'universale presenza del campo gravito-inerziale; è l'interazione con la geometria dello spazio-tempo a dire a un pianeta rotante quanto si deve rigonfiare all'equatore.
Un accelerometro interagisce con la geometria locale dello spazio-tempo per misurare se sta accelerando rispetto ad essa. È importante notare che un accelerometro non misura la sua "velocità" rispetto a qualche sistema assoluto.
La velocità è fondamentalmente relativa.
Un giroscopio in rotazione interagisce con la geometria locale dello spazio-tempo per misurare se sta ruotando rispetto ad essa. Siccome è molto raro che la geometria locale dello spazio-tempo ruoti significativamente rispetto all'universo, un giroscopio in rotazione mostra effettivamente quale sistema di riferimento non sta ruotando rispetto all'universo.
Il concetto di sistema di riferimento inerziale, come riconosciuto nella relatività generale, viene presentato solitamente discutendo solamente i fenomeni cinematici: accelerometri e giroscopi.
Ma non si tratta solo di dinamica: nei sistemi di riferimento inerziale, come riconosciuti nella relatività generale, valgono tutte le leggi della fisica.
</text>
</doc>
<doc id="1253417" url="https://it.wikipedia.org/wiki?curid=1253417">
<title>Telescopio</title>
<text>
Il telescopio è uno strumento che raccoglie la luce o altre radiazioni elettromagnetiche provenienti da un oggetto lontano, la concentra in un punto (detto fuoco) e ne produce un'immagine ingrandita.
Sebbene col termine "telescopio" si indichi solitamente il telescopio ottico, operante nelle frequenze della luce visibile, esistono telescopi sensibili anche alle altre frequenze dello spettro elettromagnetico.
Il nome, derivato dal greco τηλε ("tēle") che significa «lontano» e σκοπεῖν ("skopein") ovvero «guardare, vedere»), è una parola d'autore coniata dal matematico greco Giovanni Demisiani (), il 14 aprile 1611, nel banchetto offerto a Roma, dal principe Federico Cesi, in onore della cooptazione di Galileo Galilei nell'Accademia dei Lincei.
La nascita del telescopio rifrattore si può far risalire a Galileo il quale ne mostrò la prima applicazione a Venezia nel 1609. In realtà, le prime lenti furono costruite nel 1607 da ottici olandesi che le applicarono a strumenti rudimentali di pessimo potere risolutivo. Le proprietà delle lenti, nondimeno, erano note da tempo e a Galileo deve farsi risalire il merito del perfezionamento e del primo uso astronomico.
L'atmosfera terrestre assorbe buona parte delle radiazioni elettromagnetiche provenienti dallo spazio, con l'importante eccezione della luce visibile e delle onde radio. Per questa ragione, l'osservazione da terra è limitata all'uso dei telescopi ottici e dei radiotelescopi. I primi sono collocati preferibilmente in luoghi alti o isolati (montagne, deserti, ...), in modo da ridurre l'influenza della turbolenza atmosferica e dell'inquinamento luminoso.
Per l'osservazione nelle rimanenti bande dello spettro elettromagnetico (microonde, infrarosso, ultravioletto, raggi X, raggi gamma), che vengono assorbite dall'atmosfera, si utilizzano quasi esclusivamente telescopi orbitali o collocati su palloni aerostatici ad alta quota.
Inizialmente, il sensore usato nei telescopi era l'occhio umano. In seguito, la lastra fotografica prese il suo posto, e fu introdotto lo spettrografo, permettendo agli astronomi di avere informazioni sullo spettro di una sorgente. Dopo la lastra fotografica, varie generazioni di sensori elettronici come i CCD (e ultimamente in campo astrofilo anche le webcam) sono state perfezionate, ognuna con una crescente sensibilità e risoluzione. I sensori CCD permettono di realizzare strumenti con elevata profondità di campo o con elevata risoluzione a seconda delle necessità dello strumento. Il telescopio Pan-STARRS per esempio essendo stato sviluppato per individuare i potenziali asteroidi in rotta di collisione con la Terra necessita di una elevata risoluzione e quindi utilizza una serie di 60 CCD che generano 1,9 gigapixel per scatto.
I telescopi moderni contengono numerosi strumenti tra cui scegliere quello più adatto: camere per immagini, con diversa risposta spettrale. Spettrografi per varie lunghezze d'onda. Polarimetri, che possono rilevare la direzione della luce polarizzata, eccetera.
I telescopi ottici si dividono principalmente in due classi in base al tipo di elementi ottici utilizzati: i "rifrattori" e i "riflettori".
Esistono tuttavia molti schemi ottici misti che, pur utilizzando come elemento principale uno specchio (specchio primario) e per questo rientrano nei telescopi riflettori, sono dotati di elementi correttivi a lenti.
Le grandi aperture oltre i due metri sono di dominio incontrastato dei telescopi riflettori. Oltre una certa dimensione infatti le lenti diventano talmente costose e pesanti da rendere tecnicamente ed economicamente impraticabile il loro utilizzo.
Il telescopio solare è progettato per lo studio del Sole, di solito a lunghezza d'onda visibile. Viene utilizzato di giorno e per evitare che la grande quantità di luce lo surriscaldi, viene tenuto a vuoto o in elio. A differenza dei telescopi ottici hanno una maggiore distanza focale, uno specchio più piccolo per evitare la diffrazione dell'immagine e spesso sono fissi alla base della struttura, mentre un eliostato dirige la luce verso di essi.
I radio telescopi sono antenne radio che, al pari degli specchi dei telescopi che lavorano in ottico, focalizzano la radiazione amplificandola nel fuoco geometrico dell'antenna (dove è posto il detector) che raccoglie "il segnale radio". Le antenne sono a volte costituite da una griglia di fili conduttori, le cui aperture sono più piccole della lunghezza d'onda osservata.
I radio telescopi sono spesso usati a coppie, o in gruppi più numerosi, per ottenere diametri "virtuali" proporzionali alla distanza tra i telescopi (vedi la voce sull'interferometria). I gruppi più grandi hanno collegato telescopi sui lati opposti della Terra.
I radiotelescopi lavorano sulle frequenze radio degli oggetti celesti, compiendo osservazioni in questo settore dell'astronomia che presenta il vantaggio di non dipendere (come nel settore ottico) né dalle condizioni meteorologiche, né dall'alternanza giorno-notte.
I telescopi per raggi X e raggi gamma hanno altri problemi, principalmente derivanti dal fatto che questi raggi possono attraversare il metallo e il vetro. Usano in genere degli specchi a forma di anello, messi quasi paralleli al fascio di luce incidente, che viene riflessa di pochi gradi: questa caratteristica determina una differenza qualità costruttiva e tecnica del telescopio. Gli specchi sono in genere una sezione di parabola ruotata.
Il telescopio Cerenkov rivela la caratteristica radiazione (Radiazione Čerenkov) emessa da particelle gamma che attraversano l'atmosfera. Queste particelle assorbite dall'alta atmosfera terrestre originano un segnale che è da considerare l'equivalente del "bang" supersonico per le onde sonore, le particelle infatti viaggiano ad una velocità maggiore rispetto a quella della luce (della luce nell'aria, ma comunque a velocità inferiore di quella della luce nel vuoto). Il lampo Čerenkov viaggia nella stessa direzione dello sciame, e può essere rivelato dai telescopi Čerenkov. Esso consta di uno specchio primario e di un secondario dove è posta la strumentazione di rivelazione. Questi telescopi vengono denominati "IACT" (Imaging Air Čerenkov Telescopes). Tra gli esperimenti attualmente in funzione che sfruttano tale tecnica spiccano le collaborazioni MAGIC, HESS, CANGAROO e VERITAS.
L'esigenza di aumentare sempre più le dimensioni dei rivelatori (ottici e radio) e quindi di migliorare la risoluzione delle immagini dei corpi celesti, ha sviluppato un sistema che supera i limiti fisici degli strumenti a disposizione. Questo metodo è quello dell'interferometria. Esso sfrutta la possibilità di integrare i segnali di due strumenti posti ad una certa distanza, di elaborarli e di ottenere un'unica immagine contenente le caratteristiche di entrambi gli strumenti: con il vantaggio di considerare la loro distanza come il diametro dell'obiettivo o del rivelatore.
Il metodo interferometrico viene applicato sia in radioastronomia e quindi sulle lunghezze radio, che in campo ottico. Quest'ultimo è un campo di applicazione più recente, più complesso di quello radio, ma che trova già le prime applicazioni pratiche nei nuovi telescopi.
Per "montatura" di un telescopio s'intende la struttura meccanica che si occupa di "sostenere" la componente strumentale ottica e la relativa strumentazione osservativa: fotometro, spettrografo, CCD ecc.
La montatura ha anche la fondamentale funzione di compensare il moto di rotazione della Terra e dunque il moto apparente degli astri da Est verso Ovest, eseguendo un moto di rotazione in senso opposto a quello apparente del cielo. In questo modo l'oggetto da osservare rimarrà sempre al centro del campo d'osservazione.
Una montatura per essere considerata efficiente deve soddisfare i seguenti requisiti:
Le montature per telescopi si dividono in due categorie principali: montature altazimutali e montature equatoriali.
È la montatura più semplice da costruire, costituita dal moto dei due assi principali azimut ed elevazione. Il telescopio, per mantenere l'oggetto osservato al centro del campo, deve eseguire dei moti nei due assi: l'orizzontale e il verticale. Inoltre è presente un altro inconveniente: la rotazione del campo.
Tutto questo è risolto da un sistema di motori controllati da un computer, il quale provvede a mantenere sempre perfetto il puntamento. Questo tipo di montatura è utilizzato nei telescopi amatoriali più economici oppure per i telescopi professionali di grandi dimensioni, a causa della maggior semplicità e leggerezza della stessa: requisito indispensabile per sostenere specchi del diametro di alcuni metri, sorretti da strutture pesanti diverse tonnellate.
La generazione attuale di telescopi presenta diametri di 8-10 metri, ma sono in progetto telescopi da 30, 50 e anche 100 metri: tutti questi telescopi usano montature altazimutali o, in alcuni casi, montature di derivazione altazimutale.
Esistono diversi tipi di montature equatoriali, accomunati però dalla caratteristica fondamentale di avere uno degli assi di rotazione inclinato in funzione della latitudine del luogo. Questa inclinazione consente (a fronte di un puntamento della montatura rispetto al Polo Nord Celeste) di "inseguire" i corpi celesti mediante un solo movimento, semplificando rispetto ad una montatura altazimutale la modalità di inseguimento. La presenza di un solo moto, infatti, consente anche per i telescopi amatoriali di raggiungere il medesimo scopo, senza dover avere l'ausilio di sofisticata attrezzatura e software di supporto: un semplice motorino con un tempo di rotazione di 24 ore è sufficiente. Il più grande telescopio a montatura equatoriale è il famoso Telescopio Hale presso l'Osservatorio di Monte Palomar, del diametro di 5 metri.
I tipi di montature equatoriali sono:
Queste montature si differenziano in base ad alcune differenze costruttive e tecniche, utilizzabili di volta in volta in base alle esigenze. Questo tipo di montatura è la più diffusa in campo amatoriale, per la sua semplicità costruttiva e per la precisione di inseguimento.
Un tipo particolare di montatura è la montatura detta "alt-alt mount" o, più tecnicamente "altitude-altitude". Essa si colloca a metà strada fra la montatura equatoriale e la montatura altazimutale. Si tratta di una montatura all'inglese modificata la cui "culla" principale (la struttura meccanica ove è alloggiato il telescopio) anziché puntare al Nord celeste, è parallela al suolo. La montatura presenta il vantaggio di "scaricare" le masse al centro ideale gravitazionale dello strumento distribuendole in maniera equivalente su due assi (nella montatura inglese tutto il peso gravita sull'asse che punta al Polo Sud) senza dar luogo alle flessioni tipiche della montatura a forcella.
Per contro si ha, come nella montatura altazimutale, la rotazione di campo che è in funzione sia della declinazione strumentale che della latitudine locale ove si trova lo strumento. Tuttavia, quando lo strumento lavora su oggetti che si trovano in prossimità dell'equatore celeste, la rotazione di campo è pressoché eguale a zero. In via teorica la montatura può non essere allineata, ma un allineamento degli assi Nord-Sud o Est-Ovest è essenziale per ridurre il fenomeno della rotazione di campo sopra accennato.
Le montature per telescopi solari differiscono per vari particolari da quelle costruite per i telescopi destinati all'osservazione della volta celeste. I telescopi solari posseggono focali lunghissime ed è impossibile movimentare un tubo ottico di tali dimensioni; lo specchio inoltre non è parabolizzato, ma sferico. La montatura di un telescopio solare è la parte ottica meccanica che serve ad indirizzare la luce del Sole in un tubo che è o coricato sul terreno, o perpendicolare ad esso, o leggermente inclinato e che presenta dimensioni che variano da 30 metri a qualche centinaio di metri.
Tramite un sistema di specchi si opera il rinvio della sorgente luminosa solare all'interno del tubo ottico ove l'immagine subisce il consueto trattamento: ingrandimento, focalizzazione, osservazione e studio. Lo strumento destinato a raccogliere l'immagine del Sole e ad "indirizzarla" nel tubo ottico prende il nome di eliostato.
L'eliostato è composto da uno specchio piano inclinato equatorialmente che ruota per inseguire il Sole e che dirige l'immagine catturata su di un secondo specchio piano che rinvia l'immagine allo specchio principale sferico, che a sua volta provvede ad amplificarla e focalizzarla nel fuoco geometrico dello specchio principale dove si trova la strumentazione.
A motivo della doppia riflessione lo specchio primario (equatorialmente inclinato) non compie un'intera rotazione su se stesso in 24 ore (circa), bensì in 48 ore (circa). Lo specchio principale va spostato durante i diversi periodi dell'anno a motivo della diversa altezza del Sole sull'orizzonte in inverno, primavera, ed estate.
</text>
</doc>
<doc id="2308500" url="https://it.wikipedia.org/wiki?curid=2308500">
<title>Corrente elettrica</title>
<text>
La corrente elettrica, in fisica ed elettrotecnica, indica lo spostamento complessivo delle cariche elettriche. Cioè un qualsiasi moto ordinato definito operativamente come la quantità di carica elettrica che attraversa una determinata superficie nell'unità di tempo.
Con la "corrente elettrica" si ha a che fare solitamente con cariche negative, gli elettroni, che ""scorrono"" in conduttori solidi, solitamente metallici. Ma in altri casi si verifica uno spostamento di carica positiva, come ad esempio ioni positivi di soluzioni elettrolitiche. Dal momento che la direzione delle cariche dipende dal fatto che esse siano positive o negative, si definisce in modo "convenzionale" il verso della corrente come la direzione del flusso di carica positiva. Tale convenzione si deve a Benjamin Franklin. Nelle applicazioni pratiche, comunque, il verso della corrente è importante per il corretto funzionamento dei circuiti elettronici, mentre ha una importanza minore nei circuiti elettrici.
L'intensità di corrente elettrica, indicata col simbolo formula_1 (una I maiuscola), è assunta come grandezza fondamentale nel sistema internazionale (SI). La sua unità di misura è l'ampere ("A"), e da essa si ricava l'unità di misura della carica elettrica, il coulomb, che corrisponde alla quantità di carica trasportata da una corrente con intensità pari ad 1 ampere nell'unità di tempo di 1 secondo (1 C = 1 A•s).
L'intensità della corrente elettrica viene generalmente misurata con l'amperometro, ma per fare questo concorrono due metodi differenti: un metodo richiede l'interruzione del circuito, che talvolta può essere un inconveniente, mentre l'altro metodo è molto meno invasivo ed utilizza il rilevamento del campo magnetico generato dal flusso della corrente, ma in questo caso è necessaria un certa quantità di campo, che non sempre è presente in alcuni circuiti a bassa potenza. Gli strumenti usati per quest'ultimo metodo comprendono i sensori a effetto di Hall o morsetti e spire di Rogowski.
La corrente elettrica costituisce una grandezza fisica di fondamentale importanza nella tecnologia legata alla teoria dei circuiti, all'elettrotecnica ed all'elettronica, avendo un grande numero di applicazioni come ad esempio il trasporto di energia elettrica oppure di informazioni tramite segnali (ad esempio nelle comunicazioni). 
In base ai vari dispositivi, la corrente elettrica per l'alimentazione (trasformazioni di energia) viene prodotta almeno in due possibili modalità:
1 - corrente continua (CC), che presenta tensione e intensità sempre costante nel tempo ed ha un unico verso di percorrenza con una specifica polarità +/- assegnata (ad esempio, le batterie o le pile).
2 - corrente alternata (CA), che presenta tensione e intensità periodicamente variabile nel tempo ed ha due versi di percorrenza alternati, ovvero cambia verso di percorrenza in base ad una frequenza prestabilita (ad esempio la fornitura elettrica energetica civile da 230V a 50Hz).
Si consideri un conduttore di sezione formula_2 attraverso il quale vi sia un moto ordinato di cariche. Si definisce corrente elettrica la quantità di carica elettrica formula_3 che nell'intervallo di tempo formula_4 attraversa la superficie formula_2:
La corrente elettrica, pur avendo un verso di percorrenza, è una quantità scalare perché non possiede una direzione.
Il moto delle cariche che costituisce la corrente è realizzato generando un campo elettrico nel conduttore, la cui intensità è direttamente proporzionale alla forza subita dalle cariche. L'esistenza di un campo elettrico nel conduttore implica la presenza di un potenziale elettrico: considerati due punti del conduttore percorso da corrente, la differenza formula_7 tra i rispettivi potenziali è detta forza elettromotrice. Se nel conduttore vi sono cariche elettriche, la forza elettromotrice è direttamente proporzionale alla differenza tra l'energia potenziale delle cariche nei due punti. Il moto ordinato di carica è quindi dovuto al fatto che le cariche minimizzano la loro energia potenziale spostandosi dal punto a potenziale maggiore al punto a potenziale minore. Il campo elettrico nel conduttore compie pertanto un lavoro sulle cariche, realizzando un trasferimento di potenza dal campo alle cariche in moto. Tale lavoro è dato da:
La potenza sviluppata dal campo elettrico è quindi:
Sia formula_10 la densità di numero dei portatori di carica in un punto, ognuno di essi di carica formula_11. I portatori di carica si muovono ad una velocità istantanea (mediata su tutti i portatori presenti in quel punto a quell'istante) formula_12, detta velocità di deriva, che è parallela o antiparallela alla direzione del campo elettrico e di diversi ordini di grandezza inferiore alla velocità di agitazione termica delle singole particelle.. La densità di carica elettrica in quel punto è:
La densità di corrente in un punto formula_14 al tempo formula_15 è il vettore dato dal prodotto della densità di carica e della velocità di deriva:
La densità di corrente ha la stessa direzione della velocità di deriva dei portatori di carica e verso che dipende dalla carica del portatore stesso: concorde con la velocità di deriva nel caso di carica positiva, discorde nel caso di carica negativa.
La corrente elettrica attraverso una superficie formula_2 (per esempio attraverso la sezione di un conduttore) è il flusso attraverso la superficie della densità di corrente elettrica:
dove il vettore superficie ha per modulo la superficie e per versore quello normale della superficie. Chiaramente si tratta di un parametro globale che non dipende più dalla posizione, ma solo dal tempo. Quindi nelle grandezze originarie:
Questa definizione concorda con la definizione operativa: la carica fluita attraverso una superficie formula_2 nell'intervallo di tempo è infatti:
La legge di conservazione della carica elettrica è rappresentata dall'equazione di continuità per la carica elettrica, ed afferma che la carica che fluisce attraverso una superficie chiusa formula_2 è la stessa quantità di carica che entra o esce dal volume formula_23 delimitato dalla superficie formula_2. La quantità di carica che entra o esce dal volume formula_23 è fornita dalla derivata temporale dell'integrale su tutto formula_23 della densità di carica formula_27, e la legge di conservazione si esprime quindi dicendo che il flusso formula_28 della densità di corrente elettrica attraverso la superficie chiusa formula_2 è pari a:
Il flusso, che è la corrente elettrica formula_1 passante attraverso la sezione, è dato da:
e utilizzando il teorema della divergenza si ottiene:
da cui:
Uguagliando gli integrandi si ottiene così l'equazione di continuità per la carica elettrica in forma locale:
Nel caso stazionario la carica si conserva nel tempo:
e questo implica:
In regime stazionario, quindi, il vettore densità di corrente costituisce un campo vettoriale solenoidale. Dal punto di vista fisico questo significa che il flusso della densità di corrente è costante, e quindi la corrente elettrica attraverso una qualunque sezione del conduttore è sempre la stessa, indipendentemente dalla sezione considerata. Questo fatto va sotto il nome di prima delle leggi di Kirchhoff.
In un conduttore percorso da corrente continua il campo elettrico si propaga ad una velocità prossima a quella della luce, che corrisponde alla velocità con la quale viene trasportata l'informazione associata alla variazione di corrente elettrica nel tempo. La velocità del moto ordinato delle cariche che costituiscono la corrente, invece, risulta molto più bassa. Tale velocità è detta velocità di deriva.
Questo non significa che la velocità reale delle singole cariche sia la stessa della velocità osservabile del moto globale detto di deriva: si considera che il moto globale anche una velocità quadratica media osservabile detta di agitazione termica (senza direzione dato che si tratta di uno scalare) quindi proporzionale alla temperatura e legata alla distribuzione di Maxwell-Boltzmann dalla relazione:
dove formula_39 è la massa di un elettrone, formula_40 è la costante di Boltzmann e formula_41 è la temperatura ambiente. Quindi la velocità di agitazione termica risulta:
La velocità di deriva può essere invece stimata a partire dalla forza di Coulomb come:
dove formula_44 è la durata media del moto libero degli elettroni (cioè del moto tra due urti successivi):
in cui formula_46 è il cammino libero medio degli elettroni.
Quindi anche il vettore densità di carica media è proporzionale al campo elettrico, infatti sostituendo l'espressione della velocità di deriva:
dove e formula_48 è la conduttività elettrica. Questa costituisce la legge di Ohm nella sua formulazione moderna.
Le correnti elettriche nei solidi tipicamente fluiscono molto lentamente, per esempio in un cavo di rame di sezione pari a 0,5 mm con una intensità misurata di 5 Ampere, la velocità di deriva è nell'ordine del millimetro al secondo.
In elettrodinamica, la quadricorrente è un quadrivettore definito come:
dove formula_50 è la velocità della luce, formula_27 la densità di carica elettrica e il suo prodotto per la velocità formula_52 la densità di corrente, mentre formula_53 denota le dimensioni spaziotemporali.
La quadricorrente può essere espressa in funzione della quadrivelocità formula_54 come:
dove la densità di carica formula_27 è misurata da un osservatore fermo che vede muoversi la corrente, mentre formula_57 è misurata da un osservatore posto nel sistema di riferimento in moto delle cariche, che si muove ad una velocità formula_58 pari alla norma della componente spaziale di formula_54.
In relatività speciale la legge di conservazione della carica, che nel limite non relativistico è espressa dall'equazione di continuità, assume la seguente forma tensoriale:
dove formula_61 è il quadrigradiente, dato da:
In relatività generale la quadricorrente è definita come la divergenza del vettore spostamento elettromagnetico, dato da:
La soglia di percezione umana dell'intensità di corrente elettrica è circa di 0,5 mA in modalità di corrente alternata a frequenza di 50÷60 Hz e di 2 mA in corrente continua. Si deve anche tenere conto che l'effetto di una determinata corrente elettrica varia non solo per l'intensità, ma anche per il tempo di persistenza. 
Se teoricamente la tensione non è di per sé rilevante negli effetti sull'uomo, per essere attraversati da una corrente occorre comunque una tensione minima e questo implica che sotto i 50 Vac circa non si corrono rischi.
Con intensità di corrente maggiori a quelle specificate si producono nel corpo umano i seguenti effetti:
Si definisce soglia media di pericolosità (p) per una intensità di corrente pari a:
</text>
</doc>
<doc id="2206607" url="https://it.wikipedia.org/wiki?curid=2206607">
<title>Raggio luminoso</title>
<text>
In ottica geometrica, i raggi di luce definiscono idealmente il percorso compiuto dalla luce.
È possibile schematizzare la luce con dei raggi che si propagano in linea retta, quando essa viaggia in un mezzo omogeneo e la sua lunghezza d'onda è molto inferiore alle dimensioni degli oggetti con cui interagisce. Sotto tali ipotesi un raggio di luce è un sottilissimo fascio di luce rappresentato con una retta.
Alcuni fenomeni ottici, ad esempio quello dell'interferenza o della diffrazione, sono spiegati solo con un modello ondulatorio della luce. Tuttavia, questi fenomeni si manifestano quando la lunghezza d'onda della luce è confrontabile con la dimensione dell'oggetto che causa la diffrazione. Nel caso di oggetti macroscopici e di luce visibile, la lunghezza d'onda della luce è sempre molto minore rispetto alle dimensioni, e pertanto non si osservano fenomeni d'interferenza. Molti fenomeni possono essere, pertanto, studiati ricorrendo al modello dei raggi di luce.
Ad esempio, l'esperimento di Young della doppia fenditura non può certamente essere spiegato con dei raggi che si propagano in linea retta, perché in tale situazione le dimensioni della fenditura sono confrontabili con quelle della lunghezza d'onda, mentre altri fenomeni, come quello della riflessione o della rifrazione, sono descrivibili in termini di raggi.
</text>
</doc>
<doc id="958210" url="https://it.wikipedia.org/wiki?curid=958210">
<title>Campo (fisica)</title>
<text>
In fisica, un campo è una grandezza esprimibile come funzione della posizione nello spazio e del tempo, o nel caso relativistico nello spaziotempo.
Un campo può essere scalare, spinoriale, vettoriale o tensoriale, a seconda che la grandezza rappresentata sia rispettivamente uno scalare, uno spinore di Dirac, un vettore o un tensore. Per esempio, il campo gravitazionale può essere modellato come campo vettoriale dove un vettore indica l'accelerazione esercitata su una massa per ogni punto. Questo intuitivamente, anche se il campo gravitazionale indica la forza che agisce su una massa unitaria e non un'accelerazione (un libro posto su un tavolo non subisce nessun accelerazione ma un campo di forza). Altri esempi possono essere il campo di temperatura o quello della pressione atmosferica, che sono spesso illustrati tramite le isoterme e le isobare collegando i punti che hanno rispettivamente la stessa temperatura o pressione.
Sotto questo punto di vista un campo può essere più semplicemente definito come l'insieme dei valori che una data grandezza fisica, scalare o vettoriale, assume nello spazio. Il teorema di Helmholtz è fondamentale per la comprensione dei campi in quanto fornisce una classe di parametri che li determinano univocamente.
Nel caso di un campo di forze, come il campo gravitazionale e il campo elettrico, il concetto di campo è strettamente correlato con quello di interazione a distanza.
La teoria dei campi descrive la dinamica di un campo, cioè la sua variazione nel tempo. Di solito questa viene descritta da una lagrangiana o una hamiltoniana di campo, trattate come un sistema con infiniti gradi di libertà. La teoria risultante può essere classica o quantistica. In fisica moderna i campi più studiati sono quelli relativi alle forze fondamentali.
La teoria della relatività generale afferma l'impossibilità di fenomeni simultanei (sempre distanziati a meno di un infinitesimo di spazio-tempo) e sostituisce con il concetto di campo le forze simultanee agenti a distanza utilizzate nella fisica newtoniana.
Fenomeni simultanei anche a grandi distanze sono invece possibili per l'entanglement quantistico, che negli esperimenti è misurato e riproducibile solo per singole particelle e che, in accordo con il teorema di non-comunicazione, non è sfruttabile a livello macroscopico per la trasmissione di dati.
Ci sono numerosi esempi di campi classici. La dinamica di questi campi è di solito specificata dalla densità di Lagrangiana in termini di componenti del campo; la dinamica può essere ottenuta usando il principio d'azione.
Michael Faraday per primo capì l'importanza del campo come oggetto fisico, durante la sua ricerca sul magnetismo. Egli capì che il campo elettrico e magnetico non erano solo campi di forza che influenzavano il moto delle particelle, ma avevano un'interpretazione fisica reale, perché essi possono trasportare energia.
Queste idee portarono alla creazione, da parte di James Clerk Maxwell, della prima teoria unificata dei campi con l'introduzione delle equazioni per il campo elettromagnetico. La versione moderna di queste equazioni sono chiamate equazioni di Maxwell. Alla fine del diciannovesimo secolo il campo elettromagnetico fu capito come una collezione di due campi vettoriali nello spazio. Oggi, questo è raggruppato in un singolo campo tensoriale del secondo ordine nello spaziotempo.
La teoria della gravità di Einstein, chiamata relatività generale, è un altro esempio di una teoria di campo. Qui il campo principale è un tensore metrico, un campo tensoriale del secondo ordine nello spaziotempo.
Si pensa attualmente che la meccanica quantistica sia alla base di tutti i fenomeni fisici; cosicché anche la teoria classica dei campi dovrebbe essere riformulata in modo da tenerne conto.
Ciò è stato fatto con la cosiddetta seconda quantizzazione che rende la funzione d'onda della meccanica quantistica (scalare, in quella sede) un operatore.
Tale meccanismo è stato applicato dapprima, con successo, al campo elettromagnetico; la corrispondente teoria di campo quantizzata è nota come elettrodinamica quantistica o QED.
Successivamente si sono ottenute le teorie di campo quantizzato per due delle altre forze fondamentali: la forza forte descritta dalla cromodinamica quantistica (QCD) e quella debole descritta dalla teoria elettrodebole (che mostra come in realtà la forza elettromagnetica e quella debole abbiano un'origine comune).
Queste tre teorie possono esse derivate come casi particolari del cosiddetto modello standard della fisica delle particelle.
A tutt'oggi non è stata invece trovata una soddisfacente teoria di campo quantizzato per il campo gravitazionale.
Le teorie di campo classico rimangono comunque utili per lo studio di fenomeni in cui le proprietà quantistiche della materia non siano rilevanti; ad esempio lo studio dell'elasticità dei materiali o la fluidodinamica.
I campi classici, come quello elettromagnetico, sono usualmente funzioni derivabili a tutti gli ordini, e in ogni caso almeno due volte. Al contrario le funzioni generalizzate non sono continue.
Il metodo dei campi casuali continui deve essere usato per uno studio dei campi classici a temperature finite, poiché un campo classico variabile con la temperatura è ovunque non differenziabile.
I campi casuali sono insiemi di variabili casuali munite di indice; un campo casuale continuo è un campo casuale i cui indici sono funzioni continue. In particolare, conviene talvolta considerare i campi casuali che hanno come insieme degli indici uno spazio di Schwartz di funzioni, nel qual caso il campo casuale continuo è una distribuzione temperata.
In modo (molto) rude possiamo pensare ai campi casuali continui come a funzioni ordinarie che valgano formula_1 quasi ovunque, e tali che quando si effettui una media pesata di tutti questi infiniti in una regione finita, si ottenga un risultato finito, e che può essere ben definito. Possiamo definire un campo casuale continuo in modo più preciso come una mappa lineare dallo spazio delle funzioni in quello dei numeri reali.
Un modo conveniente per classificare i campi (classici e quantistici) sono le simmetrie che possiedono. Le simmetrie sono di due tipi:
I campi sono spesso classificati per il loro comportamento rispetto a trasformazioni dello spaziotempo:
Nella relatività vale una classificazione simile, ad eccezione del fatto che i campi scalari, vettoriali e tensoriali sono definiti rispetto alla simmetria di Poincaré dello spaziotempo.
I campi possono avere simmetrie interne oltre a quelle spaziotemporali.
</text>
</doc>
<doc id="896150" url="https://it.wikipedia.org/wiki?curid=896150">
<title>Urto</title>
<text>
L'urto è il termine fisico con il quale si identifica la collisione di due corpi che si scontrano.
Un'interpretazione più corretta viene fornita dalla meccanica del continuo: i corpi sono dotati di "elasticità" e l'intervallo di tempo durante il quale tali oggetti sono a contatto si compone di un periodo di compressione, nel quale si compie una deformazione spesso impercettibile, e di un periodo di ritorno elastico durante il quale la forma torna allo stato iniziale.
Viene inizialmente presa in considerazione la classe degli urti "normali" a due corpi, cioè quelli in cui la direzione del moto avviene lungo la normale comune per il punto di contatto sia prima che dopo l'urto in quanto moto unidimensionale, e successivamente si può estendere lo studio agli urti "obliqui" a n&gt;2 corpi in d&gt;1 dimensioni.
Per un urto "normale" la velocità relativa dei corpi dopo l'urto è proporzionale a quella precedente l'urto attraverso un coefficiente di ritorno legato alle elasticità dei due corpi:
Se formula_3 l'urto è detto "totalmente anelastico";
se formula_4 l'urto è detto "elastico";
L'applicazione del principio di conservazione:
dove:
al caso di un urto tra due corpi a coefficiente di ritorno qualsiasi si ha:
dove:
La quantità di moto totale dopo l'urto è data dalla quantità di moto iniziale più l'impulso totale. Perciò le forze sono uguali e contrarie per i due corpi, e la loro somma vettoriale è nulla.
Dalle due equazioni precedenti si ricava che:
Che applicata al caso di urto totalmente anelastico si traduce in:
e quindi, :formula_11
in particolare se :formula_12, allora: formula_13
Applicata invece al caso di urto elastico si traduce in:
se poi :formula_16, allora: formula_17
Si capisce perciò meglio come ε sia un fattore legato all'elasticità dell'urto.
Dalle equazioni delle velocità:
dove:
Si può verificare facilmente infatti che :formula_19 il principio di conservazione della quantità di moto che abbiamo imposto precedentemente.
Se si considera un sistema di bersaglio 2 molto più grande del proiettile 1,
ma allora:
dove "formula_22" è l'accelerazione relativa: a parità di questa, la forza impulsiva è perciò massima e doppia per un urto elastico rispetto ad un urto perfettamente anelastico.
Affinché l'energia cinetica totale dei corpi rimanga invariata (e quindi le velocità dei due corpi dopo l'urto abbiano o direzione o verso o intensità diverse tra loro), si deve avere un urto elastico, e viceversa, come dimostra questa catena di doppie implicazioni:
Se l'energia cinetica dei corpi è stata parzialmente dissipata nell'urto, allora si parla genericamente di urto anelastico. In quest'ultimo caso, si può dimostrare analogamente che l'energia cinetica dissipata è la massima possibile (dovendo rispettare la conservazione della quantità di moto totale) nel caso di urto totalmente anelastico, poiché i due corpi procedono alla stessa velocità dopo l'urto.
Secondo il primo principio della termodinamica, la parte di energia cinetica dissipata viene convertita in energia interna dei corpi coinvolti nell'urto, cioè in generale in parte in calore e in parte in lavoro termodinamico dei corpi stessi.
</text>
</doc>
<doc id="837373" url="https://it.wikipedia.org/wiki?curid=837373">
<title>Dispersione ottica</title>
<text>
In ottica la dispersione è un fenomeno fisico che causa la separazione di un'onda in componenti spettrali con diverse lunghezze d'onda, a causa della dipendenza della velocità dell'onda dalla lunghezza d'onda nel mezzo attraversato. È spesso descritta in onde luminose, ma può avvenire in ogni tipo di onda che interagisce con un mezzo o che può essere confinata in una guida d'onda, come le onde sonore. La dispersione è anche chiamata dispersione cromatica per enfatizzare la sua dipendenza dalla lunghezza d'onda. Un mezzo che esibisce queste caratteristiche nei confronti dell'onda in propagazione è detto "dispersivo".
Esistono in generale due sorgenti di dispersione: la dispersione di materiale, che deriva dal fatto che la risposta del materiale alle onde dipende dalla frequenza e la dispersione di guida d'onda, che avviene quando la velocità dell'onda nella guida dipende dalla sua frequenza. I modi trasversi delle onde confinate in una guida d'onda finita in generale hanno velocità (e forme di campo) diverse, che dipendono dalla frequenza (ossia dalla dimensione relativa dell'onda, la lunghezza d'onda, rispetto alle dimensioni della guida).
La dispersione in guide d'onda utilizzate per le telecomunicazioni comporta la degradazione del segnale, poiché il diverso ritardo con cui le differenti componenti spettrali giungono al ricevitore, "sporca" il segnale nel tempo ovvero crea distorsione. Un fenomeno simile è la dispersione modale, causata dalla presenza di più modi in una guida ad una data frequenza, ognuno dei quali presenta una velocità diversa. Un caso particolare è invece la dispersione dei modi di polarizzazione o PMD ("polarization mode dispersion") che deriva dalla composizione di due modi separati in polarizzazione che viaggiano a velocità diverse a causa di imperfezioni randomiche che spezzano la simmetria della guida.
La dispersione della luce nel vetro di un prisma è usata per costruire spettrometri e spettroradiometri. Sono utilizzati anche reticoli olografici, poiché consentono una discriminazione più accurata delle lunghezze d'onda. La dispersione nelle lenti produce l'aberrazione cromatica, un effetto indesiderato che può distorcere la immagini in microscopi, telescopi ed obiettivi fotografici.
In ottica la "velocità di fase" di un'onda "v" in un dato mezzo uniforme è data da
dove formula_2 è la velocità della luce nel vuoto ed formula_3 l'indice di rifrazione del mezzo.
In generale l'indice di rifrazione è una funzione della frequenza formula_4 della luce, quindi formula_5 o, alternativamente, rispetto alla lunghezza d'onda formula_6 La dipendenza dalla lunghezza d'onda dell'indice di rifrazione di un materiale è solitamente quantificato mediante formule empiriche, l'equazione di Cauchy e l'equazione di Sellmeier.
La conseguenza più comunemente osservabile della dispersione in ottica è la separazione di luce bianca in uno spettro di colori per mezzo di un prisma triangolare. Dalla legge di Snell si può vedere che l'angolo di rifrazione della luce in un prisma dipende dall'indice di rifrazione del materiale di cui è composto il prisma. Dato che l'indice di rifrazione varia in dipendenza dalla lunghezza d'onda, ne segue che anche l'angolo con cui la luce viene rifratta varia con la lunghezza d'onda, causando una separazione angolare dei colori nota anche come "dispersione angolare".
Per la luce visibile, la maggior parte dei materiali trasparenti ha:
o alternativamente
</text>
</doc>
<doc id="586797" url="https://it.wikipedia.org/wiki?curid=586797">
<title>Boom sonico</title>
<text>
Il boom sonico, chiamato anche "bang" supersonico, in italiano boato sonico, è il suono prodotto dal cono di Mach generato dalle onde d'urto create da un oggetto (ad esempio un aereo) che si muove, in un fluido, con velocità superiore alla velocità del suono.
Esempi di "boom sonico" si hanno quando un aeroplano vola a velocità superiore a quella del suono nell'aria, o anche quando si fa schioccare una frusta. Il suono è in questo caso prodotto dall'estremità della frusta che supera la barriera del suono.
Nel 1964, la NASA e l'agenzia federale statunitense per l'aviazione civile, la FAA - "Federal Aviation Administration", misero in atto il progetto ""Oklahoma City sonic boom tests"". Questo esperimento consisteva nel generare otto boom sonici al giorno per un periodo di sei mesi. Furono raccolti dati interessanti dall'esperimento, ma 15000 mozioni di protesta sfociarono in una class action contro il Governo degli Stati Uniti, che uscì perdente anche in appello nel 1969.
La NASA continua ad effettuare studi mirati a ridurre l'intensità dei bang sonici.
Nel programma del 2004 denominato ""Shaped Sonic Boom Demonstration project"", un team composto da tecnici NASA del Langley Research Center e del Dryden Flight Research Center insieme alla ditta aeronautica Northrop Grumman, hanno condotto sperimentazioni presso la Edwards AFB sui profili aerodinamici di un F-5E per esplorare i margini di diminuzione del fenomeno, nell'ottica di poter rendere possibile agli aerei di nuova generazione il volo supersonico anche su aree abitate.
Nel mese di Ottobre dell'anno 2005 Israele utilizzò degli aerei F-16 per creare dei boati sonici sulla striscia di Gaza come metodo di guerra psicologica, pratica che venne successivamente condannata da parte delle Nazioni Unite. Una fonte dell'intelligence israeliana riportò che la tattica fu adottata per contrastare il supporto da parte della popolazione civile ai gruppi armati palestinesi, specialmente quelli che lanciavano i razzi Qassam verso i centri abitati israeliani. Nello stesso articolo veniva riportato il controverso rapporto di uno psichiatra palestinese che denunciava come i boom sonici avessero prodotto seri effetti sulla salute dei bambini di Gaza, inducendo ansietà, panico, diminuzione della concentrazione e del rendimento scolastico, oltre a un aumento degli aborti spontanei. Il rapporto è stato contestato dalla parte opposta e accusato di assenza di credibilità scientifica e riscontri clinici.
Lo schiocco prodotto da una frusta, quando viene usata correttamente, in realtà è un bang sonico. L'estremità della frusta si muove a una velocità superiore a quella del suono e crea il rumore caratteristico. 
Le fruste sono realizzate con una struttura che si affina partendo dalla impugnatura fino all'estremità. La punta ha molto meno massa dell'impugnatura, di conseguenza, quando la frusta è fatta schioccare correttamente, l'energia si trasferisce dall'impugnatura all'estremità. La formula per l'energia cinetica formula_1, ci spiega che la velocità della frusta aumenta via via che diminuisce la massa, fino ad arrivare a superare la velocità del suono, creando il caratteristico bang sonico.
</text>
</doc>
<doc id="485282" url="https://it.wikipedia.org/wiki?curid=485282">
<title>Interferenza (fisica)</title>
<text>
l fenomeno dell'interferenza è un fenomeno dovuto alla sovrapposizione, in un punto dello spazio, di due o più onde. Quello che si osserva è che l'intensità (o ampiezza) dell'onda risultante in quel punto può essere diversa rispetto alla somma delle intensità associate ad ogni singola onda di partenza; in particolare, essa può variare tra un minimo, in corrispondenza del quale non si osserva alcun fenomeno ondulatorio, ed un massimo che in generale non coincide con la somma delle intensità. In generale, si dice che l'interferenza è "costruttiva" quando l'intensità risultante è maggiore rispetto a quella di ogni singola intensità originaria, e "distruttiva" in caso contrario.
Il termine viene usualmente utilizzato per parlare di interferenza tra due onde coerenti, di norma provenienti dalla stessa sorgente. I fenomeni di interferenza che si osservano quotidianamente possono essere ad esempio quelli che riguardano le increspature che si formano su uno specchio d'acqua (si veda la figura a destra), oppure i battimenti tra onde sonore.
L'interferenza è un effetto che coinvolge esclusivamente fenomeni ondulatori: quelli riguardanti il trasporto di materia, come ad esempio la conduzione di un fluido all'interno di una tubatura, non risentono dell'interferenza. In tale contesto infatti l'intensità è definita dal flusso di materia attraverso una data superficie e, come noto, le quantità di materia trasportate da due correnti di particelle che si incontrano si sommano (ad esempio, la portata di un fiume è pari alla somma delle portate di tutti i suoi affluenti che si trovano a monte, più quella della sorgente).
Isaac Newton, dall'osservazione delle ombre create dagli oggetti investiti dalla luce, ipotizzò che essa fosse composta da corpuscoli che venivano bloccati dalla superficie illuminata di quei corpi. La congettura di Newton resistette per diverso tempo fino a quando Thomas Young dimostrò nel suo celebre esperimento del 1801, il primo in cui appunto veniva evidenziato il fenomeno dell'interferenza luminosa, la natura ondulatoria della luce, scardinando così la teoria corpuscolare che comunque, già all'epoca di Newton, iniziava ad essere falsificata (lo stesso fisico inglese non riuscì ad esempio a spiegare il fenomeno degli anelli di Newton, che può essere compreso solo ricorrendo a modelli ondulatori). La doppia natura di "onda" e "quanto", della luce, fu in seguito appurata mediante gli studi sul corpo nero, sull'effetto Compton, sull'effetto fotoelettrico e sull'assorbimento della radiazione da parte della materia.
L'esperimento di Young venne ripetuto nel 1961, utilizzando stavolta non radiazioni elettromagnetiche ma fasci di elettroni; anche in quel caso si osservò il fenomeno dell'interferenza, a conferma dell'ormai collaudato formalismo della meccanica quantistica e in particolar modo della cosiddetta ipotesi del dualismo onda-particella.
Due onde generate da sorgenti a frequenza differente non danno luogo ad interferenza, perché oscillazioni con periodo diverso sono disaccoppiate in potenza. Consideriamo allora il caso di due onde che si sovrappongono con la medesima lunghezza d'onda.
I casi estremi sono rappresentati in figura: nel primo, le onde sono in concordanza di fase, cioè si sovrappongono esattamente dando luogo ad un'onda di ampiezza pari alla somma delle singole ampiezze, mentre nel secondo sono in opposizione di fase e dunque si elidono esattamente. Si parla allora rispettivamente di interferenza totalmente costruttiva e di interferenza totalmente distruttiva, a seconda dello sfasamento (nullo nel primo, formula_1 nel secondo). In generale, si verifica facilmente che la sovrapposizione di due onde di ampiezza formula_2 e sfasate di formula_3 genera una nuova onda di ampiezza pari a
A seconda delle relazioni che intercorrono tra le onde che interferiscono, è comunque possibile che gli sfasamenti dipendano dalla coordinata spaziale. Dunque, si potranno osservare regioni in cui l'interferenza è totalmente costruttiva (dette "massimi di interferenza", corrispondenti a frange luminose chiare) alternate ad altre in cui invece l'interferenza è totalmente distruttiva (dette "minimi di interferenza", corrispondenti a frange non illuminate scure).
L'ampiezza di queste regioni è legata sia alla disposizione geometrica delle sorgenti, sia alla lunghezza d'onda; si capisce abbastanza facilmente che, tanto più piccola è la lunghezza d'onda, tanto più piccole e cadenzate saranno queste frange. Questo è uno dei motivi per il quale non si riescono ad osservare quotidianamente fenomeni di interferenza luminosa, ma non è il solo; l'altro è legato alla decoerenza delle sorgenti. Infatti, le più comuni fonti di luce (il sole, le lampadine ad incandescenza e così via) emettono svariati pacchetti di radiazioni che si sovrappongono in maniera completamente casuale, a seconda dell'istante al quale vengono generati: in una situazione del genere quindi, la distribuzione delle frange varierà così rapidamente da non poter essere seguita dall'occhio umano (a causa del fenomeno di persistenza delle immagini sulla retina), che quindi osserverà solo una distribuzione regolare di luminosità. L'unico modo per poter osservare questi fenomeni è disporre di due o più sorgenti coerenti, ad esempio sfruttando il fenomeno della diffrazione come fece Young nel suo esperimento della doppia fenditura.
Definita la differenza di fase formula_5, con formula_6 distanza tra le sorgenti e formula_7 lunghezza d'onda, l'interferenza è costruttiva se si verifica che formula_8, cioè formula_9 (un numero pari di volte mezza lunghezza d'onda), mentre è distruttiva se si verifica che formula_10, cioè formula_11 (un numero dispari di volte mezza lunghezza d'onda).
La figura mostra un metodo usato per produrre onde luminose che interferiscono tra loro. Si tratta di un piano su cui sono state praticate due fenditure: un'onda piana, incidendo sulla superficie, viene parzialmente schermata. Secondo il principio di Huygens le due fenditure, se di dimensioni sufficientemente, piccole rispetto alla lunghezza d'onda della radiazione incidente, a grande distanza dallo schermo si comportano come sorgenti puntiformi di luce coerente ossia in fase tra di loro.
Le onde sferiche emesse dalle fenditure interferiranno: se mettiamo una lastra fotografica oltre lo schermo, osserveremo su di essa una serie alternata di bande illuminate e scure, dette "frange di interferenza", corrispondenti ai massimi e ai minimi di interferenza.
Il discorso può essere esteso al caso generale in cui sono presenti più aperture, ma prima discutiamo quello particolare.
In questa sezione si considera il caso di due fenditure; per semplicità, il problema verrà trattato limitatamente ad una sezione piana ortogonale allo schermo e passante per le due aperture (vedi figura alla fine del paragrafo).
Quello che interessa ai fini della trattazione è come si distribuisce l'intensità luminosa sulla lastra, e quindi capire come questa varia tra i massimi e i minimi. La condizione di campo lontano, necessaria per poter trattare le due fenditure come puntiformi, consente di affermare che i vettori formula_12 congiungenti le due aperture con il punto "P" della lastra in cui si intende valutare l'intensità possono essere considerati paralleli in prossimità delle fenditure. La differenza di cammino ottico, ossia la lunghezza in più che la prima onda percorre rispetto alla seconda prima di giungere in "P", può essere dunque approssimata nel seguente modo
dove α è l'angolo compreso tra i due vettori e la normale allo schermo e "d" la distanza tra le aperture. Prendendo ora in considerazione le leggi che descrivono l'andamento, ad esempio del campo elettrico, per le due onde che partono dalle fenditure si ha
essendo k il numero d'onda, ω la pulsazione e formula_15 l'ampiezza del campo che incide sullo schermo. L'interferenza delle due perturbazioni in "P", ad esempio in "t=0", si deduce subito dalle formule di prostaferesi
dato che si può certamente porre formula_17, si avrà in definitiva
la figura di interferenza è legata all'intensità del campo incidente la lastra, che è direttamente proporzionale al quadrato dell'ampiezza del campo elettrico. Quindi
è la relazione che esprime l'intensità in funzione dell'angolo (o se si preferisce, in funzione della differenza di cammino ottico) e dell'intensità dell'onda che incide sullo schermo. Evidentemente, quando la differenza di cammino ottico è pari ad un multiplo intero della lunghezza d'onda λ
i due campi interferiscono in fase, l'interferenza è costruttiva e si osserva un massimo nella figura di interferenza; viceversa, quando tale differenza coincide con un multiplo dispari di mezza lunghezza d'onda
le perturbazioni interferiscono in controfase, l'interferenza è distruttiva e si osserva un nullo di intensità. In termini della coordinata "x" sulla lastra, calcolata a partire dal centro, considerato che, almeno per piccoli angoli
dove "L" è la distanza tra lo schermo e la lastra, si può quindi affermare che la distanza tra due massimi (o minimi) consecutivi è data da
In conclusione, la distribuzione dell'intensità sullo schermo non è uniforme, ma si manifesta in fasce chiare e scure alternate. Questa ridistribuzione dell'intensità rispetta il principio di conservazione dell'energia, nel senso che la potenza incidente sulla lastra coincide esattamente con quella che transita attraverso le due fenditure. Lo spessore delle fasce, in questo caso uguale per tutte (sempre per piccoli angoli), sarà pari alla metà di "Δx"; ovviamente, si osserverà anche una certa sfumatura ai bordi delle medesime.
Si supponga ora di avere una griglia regolare costituita da un numero molto grande "N" di fenditure, distanziate l'una dall'altra sempre di "d". Il metodo che qui adotteremo è quello dei fasori, più comodo da usare quando si tratta di sommare i contributi di più di due sorgenti; in questo contesto, il fasore associato è un vettore di modulo pari a quello del campo elettrico e di fase pari alla componente spaziale "kr". La somma dei campi elettrici viene così rappresentata nel seguente modo
nell'ipotesi che la distanza "d" sia molto piccola rispetto alla lunghezza d'onda, tanto da poter considerare la differenza di cammino ottico dell"'n+1"-esimo fasore rispetto al primo, formula_26, come una variabile continua. Il risultato dell'integrazione è
usando la formula di Eulero si ottiene
e in definitiva, dato che l'intensità è semplicemente proporzionale al modulo quadro del fasore
il "pattern" di diffrazione coincide quindi con il quadrato di un seno cardinale in funzione della variabile formula_30.
I nulli di intensità corrispondono ai valori di formula_31 per i quali questa quantità è un intero "m" non nullo, cioè
mentre i massimi sono intercalati tra i vari minimi in una qualche maniera; il picco di intensità si trova ovviamente nel centro, cioè per α nullo.
L'estensione al caso tridimensionale è ovvia, richiede solo di osservare che la differenza di cammino ottico è data dalla proiezione del vettore x che congiunge la prima apertura con la seconda sul versore û che individua la posizione di "P"; dunque in generale
essendo "S" la superficie occupata dalla griglia e k il vettore "k"û. La presenza del reticolato dunque non fa altro che eseguire la trasformata di Fourier passa basso del disegno (in funzione di k), con “frequenza di taglio” dipendente dalla lunghezza d'onda.
Un'analisi più accurata, valida per un numero qualsiasi di fenditure e soprattutto per una generica lunghezza d'onda, prevede l'uso della serie geometrica per esprimere la [1]
attraverso questa relazione si ottiene una miglior stima per la figura di interferenza
che si riduce alla precedente per grandi lunghezze d'onda; piccole lunghezze d'onda hanno pertanto l'effetto di creare delle ondulazioni nell'inviluppo del "pattern", che non avrà più un andamento strettamente decrescente ma appunto decadrà oscillando.
La condizione per i massimi principali di intensità, che coincidono con i massimi locali dell'inviluppo, è quella per la quale entrambi i seni si annullano
(si noti che la condizione è indipendente dal numero di fenditure), mentre gli altri massimi, detti secondari, si ottengono in corrispondenza dei punti in cui il seno a frequenza multipla, al numeratore, è massimo in modulo e quello al denominatore è non nullo
Per i minimi si deve infine scegliere di annullare il numeratore escludendo, però, i punti corrispondenti alla condizione di massimo principale
l'estensione al caso multidimensionale è analoga a quella svolta sopra.
In base al principio di Huygens, anche la diffrazione può essere rimandata ad un problema di interferenza. Le approssimazioni fatte sopra trattano le fenditure come sorgenti puntiformi, ma in realtà la loro estensione influenza in qualche maniera il "pattern", soprattutto per piccole lunghezze d'onda; in sostanza, all'effetto interferenziale dovuto all'interazione reciproca tra una fenditura e le altre, è necessario aggiungere quello indotto da ciascuna singola fenditura.
La condizione di massimo di intensità per due fenditure adiacenti è
mentre la condizione di interferenza distruttiva per la singola fenditura è data da
dove "a" è la larghezza della fenditura (qui ci limitiamo per semplicità a trattare il caso monodimensionale): infatti, il minimo si ha se e solo se ad ogni punto della fenditura ne corrisponde un altro che produce un'onda in controfase con quella prodotta dal precedente (con una differenza di cammino ottico pari a mezza lunghezza d'onda, quindi), e ovviamente questo è possibile se e solo se la distanza tra quei due punti coincide con la metà della larghezza della fenditura. I massimi assenti, ad esempio, possono essere dedotti combinando le due formule:
per lunghezze d'onda molto grandi rispetto ad "a", il primo massimo assente si trova all'infinito, come ci si aspettava essendo trascurabili gli effetti di interferenza interni alle singole fenditure.
Il caso multidimensionale è più complesso da trattare; un esempio è quello del disco di Airy, che rappresenta la figura di diffrazione prodotta da un'apertura circolare investita da una radiazione con lunghezza d'onda confrontabile con il diametro della fessura o inferiore.
</text>
</doc>
<doc id="556521" url="https://it.wikipedia.org/wiki?curid=556521">
<title>Momento meccanico</title>
<text>
Il momento meccanico, indicato con formula_1 o, in ambito anglosassone, con formula_2 (dall'inglese "torque"), esprime l'attitudine di una forza a imprimere una rotazione a un corpo rigido attorno a un asse quando questa non è applicata al suo centro di massa, altrimenti si avrebbe moto traslatorio. Costituisce quindi il momento della forza.
Il momento meccanico è uno pseudovettore, non uno scalare come l'energia o il lavoro. Per questo motivo l'unità di misura del momento meccanico nel SI è N·m ("newton per metro"), non il joule, anche se le due unità hanno le stesse dimensioni fisiche.
Una grandezza correlata al momento meccanico è il momento meccanico specifico formula_3 , il quale rappresenta il momento meccanico per unità di massa, ovvero il momento dell'accelerazione.
L'analisi dei momenti meccanici determina la condizione di equilibrio dei corpi estesi e serve allo studio dei moti rotazionali, infatti compaiono nella seconda equazione di Eulero.
Il momento meccanico rispetto a un determinato punto formula_4, detto "polo" o "centro di riduzione", è definito in meccanica newtoniana come il prodotto vettoriale tra il vettore posizione, rispetto al polo stesso, e la forza:
Il modulo di formula_6 è quindi definito da
Il vettore formula_8 è perpendicolare al piano definito da formula_9 e da formula_10 e il verso, come espresso dalla regola della mano destra, è quello di un osservatore che vede ruotare formula_9 in senso antiorario. La grandezza formula_12, distanza dell'asse di rotazione dalla retta su cui giace formula_9, è detta braccio formula_14 della forza formula_9.
Se formula_9 e formula_10 sono ortogonali tra loro, il braccio è esattamente pari al modulo di formula_10 e il modulo del momento è massimo (vedi leva). Il momento può essere nullo se la forza o il braccio sono nulli, oppure se formula_9 è parallela a formula_10.
Se il sistema è composto di più componenti puntiformi, il momento meccanico totale è la somma dei singoli momenti meccanici, ognuno dovuto alla forza sul singolo componente e al relativo braccio:
Nei sistemi continui si estende in modo naturale la definizione introducendo la densità formula_22 e il campo di accelerazioni formula_23:
Si definisce momento meccanico assiale di una forza rispetto a un asse formula_25 passante per un punto formula_4, la componente ortogonale del momento polare su un particolare asse formula_25, detto asse centrale:
dove formula_29 è un versore, vettore di lunghezza unitaria, che identifica l'asse. Il modulo sarà:
dove formula_31 è l'angolo formato dal vettore momento polare formula_32 con l'asse formula_33. In pratica è la proiezione ortogonale del momento polare sull'asse formula_33. Per questo il momento assiale è nullo se l'angolo formula_35 e massimo quando l'asse formula_25 coincide con l'asse di formula_32, in tal caso infatti: formula_38.
Il teorema di Varignon afferma che il momento risultante dalla somma dei momenti meccanici applicati in uno stesso punto, o equivalentemente la somma dei momenti assiali posti alla stessa distanza da un asse di riferimento, corrisponde al momento meccanico della risultante":"
Ciò risulta di particolare utilità nelle equazioni di Eulero.
Derivando rispetto al tempo il momento angolare formula_40 rispetto a un polo formula_41 di un sistema di formula_42 punti materiali si ottiene:
dove formula_44 è la quantità di moto, e formula_45 è la velocità del punto di applicazione, ma poiché:
segue che:
Nel caso in cui il polo formula_41 sia immobile, il momento meccanico è pari alla variazione del momento angolare attorno allo stesso centro o asse del primo:
Prendendo la relazione dimostrata nel precedente paragrafo, nel caso di un corpo rigido rotante, si può osservare che formula_50 rappresenta la velocità tangenziale del corpo rotante, pertanto si ha che:
in questo caso il momento angolare è correlato al moto rotatorio. Infatti, il momento angolare risulta proporzionale rispetto alla velocità angolare formula_52 attraverso il tensore d'inerzia formula_53:
Sostituendo si ottiene:
dove formula_56 è l'accelerazione angolare. Il momento angolare risulta proporzionale anche rispetto alla velocità areolare formula_57 attraverso la massa formula_58:
Sostituendo si ottiene:
dove formula_61 è l'accelerazione areolare.
L'equazione che lega il momento meccanico con la velocità angolare può essere riscritta attraverso la relazione di Poisson; infatti, il vettoriale del prodotto triplo può essere convertito in prodotto ordinario servendosi della matrice antisimmetrica della velocità angolare, in analogia per esempio con la definizione del tensore di Kong, definita per esempio in uno spazio tridimensionale come:
Risulta quindi che:
Si nota allora che il momento meccanico ha in generale due componenti, una a velocità angolare nulla, l'altra ad accelerazione angolare nulla:
Come esempio notevole si consideri un corpo è vincolato a un asse fisso baricentrico in un riferimento in cui è inclinato come l'asse formula_65, come per esempio una manovella:
formula_67 risulta in generale:
In meccanica dei solidi un momento meccanico si traduce in una tensione a seconda che esso sia "flettente", ovvero orientato parallelamente alla sezione, o "torcente, "se orientato perpendicolarmente alla sezione.
In una struttura planare su cui agiscano solo forze complanari ci sono solo "momenti flettenti".
Il lavoro rotazionale, compiuto dal momento meccanico risulta essere:
Come nel caso traslazionale, è possibile quindi per un momento compiere anche lavoro negativo, se si oppone allo spostamento angolare reale, o nullo, nel caso sia normale allo spostamento angolare reale. 
Un momento meccanico, analogamente a una forza, può essere conservativo e ammettere quindi un'energia potenziale in base al lemma di Poincaré:
In tal caso essa risulta per un sistema a un grado di libertà angolare:
Il valore dell'energia potenziale in formula_73 è definito arbitrariamente dal punto di vista matematico; si impone solitamente una condizione al contorno di Dirichlet, a cui non è applicabile la condizione di località dato che in generale l'energia potenziale rotazionale risulta sempre periodica nelle sue variabili angolari con periodo massimo formula_74.
Infine nel caso più generale coi tre gradi di libertà rotazionali:
Nel caso in cui il polo formula_41 sia immobile, la potenza rotazionale, posseduta dal momento meccanico risulta essere:
dove formula_78 è la velocità angolare del punto.
Un problema molto comune è misurare la forza che viene esplicata da qualcosa che gira. Il modo più naturale è fissare una sbarra al rotore e misurare la forza che questa esercita "ortogonalmente" a una certa distanza dal fulcro. Si potrebbe a questo punto definire, per convenzione, la "forza di un rotore" come quella misurata alla distanza, ad esempio, di 1 metro dal fulcro. In tal modo sarebbe possibile confrontare le forze di rotori diversi.
Per le leggi che regolano le leve è tuttavia evidente che il modulo del prodotto vettoriale fra la forza e la distanza dal fulcro, detta "braccio della forza", è una costante. Se si misura la forza esercitata, ortogonalmente alla sbarra, alla distanza di mezzo metro si trova che essa è pari al doppio di quella misurata a 1 metro; a 10 cm è 10 volte più grande; a 2 m la metà e così via. È quindi, in sintesi, rilevante per un corpo rigido solo il prodotto: "braccio × forza", e non i singoli valori delle due componenti.
La coppia è spesso usata nell'industria meccanica per quantificare la potenza generata da un motore secondo la formula:
dove:
Per misurare la coppia viene utilizzato un estensimetro a ponte intero.
</text>
</doc>
<doc id="602818" url="https://it.wikipedia.org/wiki?curid=602818">
<title>Ohmmetro</title>
<text>
L'ohmmetro (su alcuni testi è scritto ohmetro) è uno strumento per la misura della resistenza elettrica esistente di un circuito la cui unità di misura è l'ohm (Ω). L'unità di misura possiede questo nome in onore del fisico tedesco Georg Simon Ohm. L'ohmmetro è, insieme all'amperometro, voltmetro, wattmetro, varmetro, cosfimetro (o fasometro), ecc., uno strumento per misurare le grandezze elettriche.
Come per altri strumenti, i parametri fondamentali di un ohmmetro sono tre (vedi Strumenti di misura per grandezze elettriche):
Un altro parametro non meno importante è la tensione di isolamento.
In base al loro principio di funzionamento esistono quattro tipi diversi di ohmmetri:
Uno strumento che è a volte confuso con l'ohmmetro, anche se ha scopi diversi, è il:
Misurare la resistenza elettrica è di fondamentale importanza pratica. Alcuni esempi possono essere chiarificatori: resistenza degli strumenti elettrici, resistenza di campioni di resistori (indispensabili per i moltissimi metodi di misura delle altre grandezze elettriche come frequenza, induttanza, capacità, ecc.), resistenza di isolamento (di impianti elettrici e di macchine elettriche), ecc.La misura della resistenza di terra è un capitolo a sé stante, (vedi misura della resistenza di terra). Che la misura della resistenza sia di fondamentale importanza si può vedere anche dai tanti metodi diversi esistenti per poterla misurare. I metodi variano in base alla grandezza della resistenza da misurare e alla precisione richiesta. La misura è effettuata sempre con una tensione di alimentazione in continua, se si utilizzasse una tensione di alimentazione in alternata non si misurerebbe la resistenza del resistore ma l'impedenza, visto che l'induttanza (o la capacità) eventualmente presente ne altererebbe il valore.
Si elencano alcuni metodi alternativi all'ohmmetro, per misurare la resistenza elettrica:
Questi ohmmetri sono i più semplici e si trovano comunemente nei tester (o multimetri). Consistono in una pila, una resistenza variabile, e un amperometro magnetoelettrico (più comune di un milliamperometro magnetoelettrico o di un microamperometro magnetoelettrico). Inserendo la resistenza incognita in serie a questo circuito la deviazione dell'ago dello strumento è funzione inversa della resistenza incognita (con resistenza nulla si ha la massima deviazione, con resistenza via via crescente, la deviazione è sempre più piccola). Per compensare eventuali piccole variazioni della tensione della nostra pila bisogna, prima di effettuare la misura, cortocircuitare lo strumento e, attraverso la resistenza variabile, "azzerare" lo strumento. Questo tipo di strumento ha alcuni difetti intrinseci, il più grande è di non poter misurare valori resistivi oltre un certo valore. Valore che dipende dalla resistenza interna allo strumento medesimo e dalle caratteristiche interne dell'amperometro.
Un metodo per poter aggirare questo ostacolo consiste nel variare, in contemporanea, sia la resistenza del circuito, sia la tensione di alimentazione attraverso un apposito circuito elettrico. Gli strumenti così costruiti, in genere, hanno più portate con costanti di moltiplicazione che usualmente sono 1 - 10 - 100 - 1.000. Esistono due metodi per variare la portata a questi strumenti, il primo consiste in un commutatore rotante, il secondo consiste in una serie di boccole dove si devono inserire appositi spinotti che fanno capo a dei puntali. La scala di lettura di questi strumenti varia da infinito a zero. In questi strumenti, in genere, la pila è una comunissima pila commerciale da pochi volt.
Se si usasse, per aggirare l'ostacolo precedentemente accennato, una tensione di alimentazione crescente al crescere della resistenza incognita, non si potrebbe effettuare il necessario azzeramento dello strumento, visto che il circuito sarebbe attraversato, in questa eventualità, da una corrente maggiore della portata dello strumento magnetoelettrico causandone il sicuro danneggiamento.
Questi ohmmetri sono molto simili agli "ohmmetri amperometrici a scala invertita", se ne differenziano per la connessione dello strumento magnetoelettrico: non è in serie alla resistenza incognita, ma è in parallelo. Questi strumenti hanno un valore di fondo scala (la massima resistenza misurabile) ben determinato, al contrario degli "ohmmetri a scala invertita". Anche in questi strumenti è possibile avere più portate resistive ed è anche possibile, attraverso un tasto apposito, che inserisce nel circuito una resistenza di valore noto e molto preciso, effettuare l'indispensabile azzeramento dello strumento in modo da compensare piccole variazioni di tensione della pila interna. La scala di lettura di questi strumenti varia da zero al valore di fondo scala (che è prefissato). Anche in questi strumenti, in genere, la pila è una comunissima pila commerciale da pochi volt. Questi ohmmetri hanno una buona precisione, ma hanno una portata, in genere, limitata.
Questi strumenti sono chiamati usualmente "megaohmmetri" o "misuratori di isolamento Megger". Quando si vuole misurare una resistenza molto grande (per esempio la resistenza di isolamento degli impianti elettrici, o la resistenza di isolamento dei circuiti di una macchina elettrica) si utilizza questo tipo di ohmmetri. Il vantaggio è che solo con questi strumenti si possono utilizzare tensioni di prova molto elevate. Per questo, all'interno dello strumento, esiste un piccolo generatore di corrente continua, a magneti permanenti, comandato attraverso una manovella che l'operatore deve far ruotare. La tensione che si raggiunge con questo generatore varia al variare del tipo di utilizzo di questi ohmmetri. Esistono ohmmetri con tensioni di 100 V, 500 V, 1.000 V ed anche con tensioni di oltre 2.000 V.
Esistono degli artifici meccanici per evitare di far ruotare troppo velocemente il rotore del generatore di corrente continua. Il disinnesco centrifugo interviene quando la velocità supera un valore prestabilito ed a quel punto il generatore fornisce una tensione continua praticamente costante per un certo tempo. È ovvio che la tensione così ottenuta non può certamente essere stabile come è necessario per i tipi precedenti di ohmmetri rendendone inadatti i relativi schemi. Per questa ragione tali ohmmetri hanno un particolare strumento indicatore, detto a bobine incrociate, che permette di rendere la misura quasi indipendente dalla tensione di alimentazione. Sempre per questo motivo non esiste la necessità di fare, prima della misura, l'azzeramento dello strumento, cosa sempre indispensabile per i tipi precedenti.
Per il suo principio di funzionamento questo strumento appartiene alla categoria degli strumenti magnetoelettrici (vedi Strumenti di misura per grandezze elettriche) anche se, rispetto ad essi, ha due bobine incrociate e non ha molle antagoniste o dispositivi con identico scopo (è quindi uno strumento detto astatico). L'ohmmetro possiede un equipaggio fisso che è un magnete permanente con un intraferro (o traferro) costante e un equipaggio mobile composto da due bobine identiche sfalsate di un angolo retto ed il tutto è rigidamente collegato all'asse dello strumento. Ad una bobina è collegata in serie una resistenza nota ed il tutto è alimentato dalla tensione del generatore interno. L'altra bobina è collegata in serie alla resistenza da misurare ed il tutto, anche in questo caso, è alimentato dalla tensione del generatore interno (i due circuiti risultano perciò in parallelo).
Quando lo strumento non è alimentato (cioè è a riposo) l'equipaggio mobile si trova in uno stato di equilibrio indifferente. Quando viene alimentato, su queste due bobine si vengono a creare due coppie contrastanti e l'equipaggio mobile troverà il suo equilibrio quando la tangente dell'angolo di deviazione eguaglia il rapporto tra le correnti che percorrono le due bobine. Ovviamente lo strumento viene tarato direttamente in ohm in modo che la deviazione dell'ago dello strumento dia una misura diretta. La scala di lettura è di tipo logometrico, cioè dipende dal rapporto di due grandezze.
Uno strumento così costruito risente, in maniera notevole, della disuniformità del campo magnetico presente nell'intraferro, richiedendone una taratura empirica. Per poter ovviare a questo problema, anzi, per poter approfittare di questo "difetto", in modo da migliorare la scala di lettura, viene creato un intraferro non uniforme ad espansioni polari cilindriche, dove al suo interno esiste un nucleo di ferro dolce a profilo ellittico rigidamente fissato e le due bobine non sono più sfalsate tra di loro ad angolo retto, ma sono sfalsate con un angolo nettamente inferiore. Questo comporta alcuni vantaggi, il principale è quello di avere una scala più ampia (intesa come angolo di deviazione dell'indice dell'ohmmetro). Anche in questa conformazione lo strumento necessita di una taratura empirica e la scala si addensa notevolmente verso il fondo per valori di resistenza incognita molto alti.
Praticamente lo strumento ha, in serie alla resistenza incognita, una resistenza di protezione con lo scopo di proteggere la corrispondente bobina dello strumento, nel caso la resistenza che si voglia misurare sia nettamente più piccola del previsto, impedendo che la corrente circolante distrugga la bobina. Esiste anche un commutatore, che inserisce una o più resistenze di valore opportuno in parallelo al circuito serie creato dalla bobina e dalla resistenza di protezione, che permette di variare la portata dello strumento ampliandone notevolmente la sensibilità. In genere questo derivatore ha tre posizioni. Una posizione è senza resistenza supplementare, le altre due posizioni inseriscono, alternativamente, due resistenze, permettendo di avere tre portate differenti che sono generalmente 1 - 10 - 100.
Gli "ohmmetri elettronici" possono essere sia con visualizzazione analogica, sia digitale (o numerica). Ovviamente sono, costruttivamente, diversi tra loro, ma come strumenti per misurare la resistenza sono identici. È interessante notare che, a differenza degli strumenti classici, dove lo strumento vero e proprio è un amperometro (bassissima impedenza di ingresso), negli strumenti elettronici lo strumento vero e proprio è un voltmetro (grandissima impedenza di ingresso). Questo fatto comporta l'inversione dello schema di funzionamento. Anche in questi strumenti, come per gli ohmmetri amperometrici, la tensione è fornita da una comunissima pila commerciale di pochi volt. Anche per gli ohmmetri elettronici, come per gli ohmmetri amperometrici, c'è la possibilità di avere più portate ohmmetriche.
Per misurare resistenze non troppo grandi (per esempio resistenze dei resistori, ecc.) il voltmetro elettronico è inserito in parallelo alla resistenza incognita e la scala di lettura di questi strumenti varia da zero al valore di fondo scala (scala diretta). Il campo di applicazione è molto ampio e può variare da pochi millesimi di ohm (mΩ) ad alcuni milioni di ohm (MΩ).
Per misurare resistenze molto grandi (per esempio resistenze di isolamento) il voltmetro elettronico è inserito in serie alla resistenza incognita e la scala di lettura di questi strumenti varia da infinito a zero (scala invertita). Il campo di applicazione è molto ampio e può variare da poche migliaia di ohm (kΩ) ad alcuni miliardi di ohm (GΩ).
La differenza esistente tra gli ohmmetri e i rivelatori di continuità è molto semplice. Il primo strumento dà un valore alla resistenza di un circuito, il secondo dice solamente se il circuito, che dovrebbe essere a resistenza abbastanza piccola, è integro (resistenza trascurabile) oppure è interrotto (resistenza elevatissima). È evidente, da quello appena detto, che un ohmmetro può essere utilizzato anche come rivelatore di continuità, ma non è possibile fare l'inverso.
Esistono diversi rivelatori di continuità, uno dei più semplici è dato dal "cacciavite cercafase". Collegando la fase della tensione di rete (nei circuiti che sono in grado di sopportare tale tensione) è possibile, con il "cacciavite cercafase" verificare se all'altro estremo si rileva la fase (circuito non interrotto), in caso contrario ho un circuito interrotto. Un caso pratico dove si utilizza il cacciavite cercafase è nella ricerca dei guasti degli impianti elettrici civili e industriali. In questo caso non si ha la necessità di conoscere la resistenza di un tratto di circuito, ma più semplicemente verificare se il cavo è integro (e allora si rileva tensione) oppure è interrotto (ed in questo caso non si rileva nessuna tensione).
Un'ulteriore rivelatore di continuità utilizzato nella ricerca di cortocircuiti presenti su schede elettroniche, consiste in un milliohmmetro a più portate, dotato di un piccolo altoparlante, il quale emette un tono con frequenza proporzionale al valore resistivo presente ai capi della coppia di puntali usati per l'individuazione del punto di cortocircuito.
</text>
</doc>
<doc id="511435" url="https://it.wikipedia.org/wiki?curid=511435">
<title>Caduta libera</title>
<text>
Un corpo si definisce in caduta libera quando esso è sottoposto alla sola forza gravitazionale.
Tale definizione implica che il corpo possa anche non cadere nel senso comune del termine, in quanto a seconda delle condizioni iniziali può, ad esempio, orbitare (i pianeti intorno al sole sono in caduta libera) oppure anche allontanarsi all'infinito.
In relatività generale la definizione perde significato perché la gravità è considerata la curvatura stessa dello spazio-tempo e quindi un corpo in caduta libera non ha forze che agiscono su di lui.
Il moto di un corpo in caduta libera può essere studiato usando la sola cinematica: quest'ultima, infatti, ci permette di studiare il fenomeno senza considerare le cause che lo determinano.
Nell'ipotesi di caduta libera, un corpo è soggetto a un'accelerazione che si manifesta in direzione radiale verso il centro di un pianeta. Per i corpi che cadono liberamente per brevi percorsi (come nel caso di cadute da piccole altezze), l'accelerazione può essere ritenuta costante, sia in modulo che in direzione. In tal caso, il moto di caduta libera può essere considerato un moto rettilineo uniformemente accelerato.
Scelto il sistema di riferimento in modo che l'asse "z" sia rivolto verso il basso, l'accelerazione ha la forma:
dove "g" è l'accelerazione di gravità (indipendente dalle coordinate, secondo l'ipotesi formulata). La soluzione è quella di trovare le equazioni del moto del corpo. A tale scopo si può integrare la (1) rispetto ad un intervallo di tempo generico:
dove formula_3. Otteniamo:
che è l'equazione della velocità (anch'essa rettilinea e diretta verso il basso) per il nostro corpo. Integrando nuovamente la (2):
Questa è l'equazione del moto per il corpo in caduta libera. Ovviamente formula_6 è l'altezza cui il corpo viene lasciato e, poiché la scelta del sistema di riferimento è arbitraria, possiamo sempre fare in modo che essa coincida con zero, oppure con "h", cioè la quota iniziale. D'altra parte formula_7 è la velocità iniziale del corpo: se esso viene lasciato cadere, allora formula_8; se invece il corpo viene lanciato verso il basso, o verso l'alto, allora formula_9 (formula_7 sarà positivo o negativo a seconda dei due casi).
Si noti come da un certo punto in poi non si sia usata la notazione vettoriale perché il moto si svolge lungo una linea retta: dunque si abusa legittimamente della notazione, come ad esempio formula_11, intendendo che è la derivata rispetto al tempo della coordinata spaziale. Così anche formula_12.
Dalla (2) si può ricavare il tempo di caduta del corpo:
Una variante del moto in caduta libera è il lancio di un corpo verticalmente verso l'alto. In tal caso scegliamo un sistema di riferimento composto dall'asse 'z' ma rivolto verso l'alto. In tal caso le equazioni (1), (2), (3) rimangono invariate eccetto che per il segno dell'accelerazione di gravità che stavolta è negativo.
In questo caso il corpo raggiunge una certa quota partendo da formula_14, e questo implica innanzitutto che la velocità iniziale non è mai nulla (altrimenti significherebbe che il corpo non si è mai mosso), e successivamente raggiunge la massima quota "h" per poi ridiscendere verso terra. Questo implica che la velocità si annulla nel punto di inversione del moto, esattamente al tempo:
(ottenuto dalla (2) per formula_16) a cui corrisponde un'altezza massima (cioè la quota massima):
ottenuta sostituendo il valore del tempo di inversione nella (3), per formula_8.
L'istante di caduta al suolo è quello per cui formula_19 e risolvendo la (3):
le cui soluzioni sono due: una di queste è da scartare perché negativa (significherebbe che il tempo è negativo), l'altra soluzione sarà semplicemente:
con velocità finale corrispondente: formula_22.
Dunque in questo caso il moto è uniformemente decelerato per formula_23 e uniformemente accelerato per formula_24.
</text>
</doc>
<doc id="110720" url="https://it.wikipedia.org/wiki?curid=110720">
<title>Funzioni pari e dispari</title>
<text>
In matematica, le funzioni pari e le funzioni dispari sono funzioni che soddisfano delle particolari relazioni di simmetria riguardo ai valori negativi. Sono importanti in molte aree dell'analisi matematica, in particolare nella teoria delle serie di potenze e delle serie di Fourier.
Sia formula_1 una funzione a valori reali di variabile reale e sia formula_2 il suo dominio. Allora formula_3 è pari se per ogni formula_4 vale l'equazione:
Geometricamente, il grafico di una funzione pari è simmetrico rispetto all'asse formula_6.
Il nome pari deriva dal fatto che le serie di Taylor di una funzione pari centrate nell'origine contengono solo potenze pari.
Esempi di funzioni pari sono formula_7
Esempio pratico: formula_8
Ancora sia formula_9 una funzione a valori reali di variabile reale e sia formula_2 il suo dominio. Allora formula_11 è dispari se per ogni formula_4 sussiste l'equazione:
Geometricamente, il grafico di una funzione dispari è simmetrico rispetto all'origine degli assi.
Il nome dispari deriva dal fatto che le serie di Taylor di una funzione dispari centrate nell'origine contengono solo potenze dispari.
Esempi di funzioni dispari sono formula_15
Esempio: formula_16
Mentre l'unione degli interi pari e dispari corrisponde all'intero insieme degli interi, l'unione delle funzioni pari e dispari su un intervallo è incluso propriamente nell'insieme delle funzioni su quell'intervallo.
Una funzione pertanto può essere pari, oppure dispari, oppure essere né pari né dispari.
ma il prodotto di una funzione pari e una dispari è una funzione dispari:
e quindi:
Inoltre dato che l'unica funzione pari e dispari è formula_17 lo spazio delle funzioni pari è in somma diretta con quello delle funzioni dispari.
</text>
</doc>
<doc id="111154" url="https://it.wikipedia.org/wiki?curid=111154">
<title>Valore assoluto</title>
<text>
In matematica, il valore assoluto o modulo di un numero reale formula_1 è una funzione che associa a formula_1 un numero reale non negativo secondo la seguente definizione: se formula_1 è non negativo, il suo valore assoluto è formula_1 stesso; se formula_1 è negativo, il suo valore assoluto è formula_6. Ad esempio, il valore assoluto sia di formula_7 che di formula_8 è formula_7. Il valore assoluto di un numero formula_1 si indica con formula_11.
Nel caso di numeri reali, il valore assoluto si definisce come la seguente funzione a tratti:
oppure
oppure come composizione di 2 funzioni algebriche
o mediante le parentesi di Iverson:
Se rappresentiamo i numeri reali sulla retta reale, allora il valore assoluto di un numero può essere visto come la sua "distanza" dallo zero. Concetti che generalizzano quest'idea sono la nozione matematica di distanza e quella di norma, che talvolta usa la stessa notazione del valore assoluto.
Il valore assoluto ha le seguenti proprietà:
Le ultime due proprietà sono spesso sfruttate nella soluzione delle disequazioni del tipo:
Per argomenti reali, la funzione valore assoluto formula_34 è pari, continua ovunque e derivabile per formula_35. Tale funzione non è invertibile, in quanto non iniettiva: per ogni valore del codominio ci sono due numeri (un numero ed il suo opposto) con lo stesso valore assoluto (tranne che nel caso dello zero).
Una funzione formula_36 può essere trasformata a seconda di cosa sia l'argomento del modulo: in una funzione del tipo formula_37 il modulo rende positivo ciò che è negativo e lascia positivo ciò che è positivo; pertanto il grafico della funzione risulterà simmetrizzato rispetto all'asse formula_1 negli intervalli in cui formula_39. Se l'argomento del modulo è anche la variabile indipendente della funzione, ovvero è del tipo formula_40, la funzione diviene pari; ne consegue che la parte di grafico a sinistra dell'asse formula_41 (formula_42) viene cancellata e rimpiazzata dalla simmetria rispetto all'asse formula_41 della parte di grafico a destra di questi (formula_44).
Il termine "valore assoluto" è solitamente utilizzato in ambito reale. Generalizzando tale nozione ai numeri complessi, ai vettori e a più generali spazi metrici, si utilizza più frequentemente il termine "modulo".
Nel caso di un numero complesso formula_45 il modulo è definito come
dove formula_47 è la parte reale del numero e formula_48 la parte immaginaria. Dunque formula_49 è la distanza fra l'origine e formula_45 nel piano complesso. Questa definizione coincide con la precedente se il numero complesso formula_45 è un numero reale.
In maniera equivalente si può definire il modulo di formula_52 come formula_53, dove formula_54 è il complesso coniugato di formula_45.
Questa definizione di modulo su formula_56 soddisfa le proprietà dalla 1 alla 7 sopra indicate: infatti, identificando il campo complesso con lo spazio formula_57, essa non è altro che la norma euclidea del vettore formula_58.
Per argomenti complessi, la funzione modulo formula_59 è sempre continua ma non è mai differenziabile (si può vedere mostrando che non soddisfa le equazioni di Cauchy-Riemann).
Il modulo di un vettore formula_60-dimensionale formula_61 è generalmente dato da:
Si noti che formula_63 è la distanza del vettore formula_64 dall'origine degli assi e che oltre al termine "modulo" si utilizza spesso il termine "norma euclidea" o "pitagorica" (in quanto in 2 dimensioni questa formula è proprio il teorema di Pitagora).
L'utilizzo di questo termine si spiega con il fatto che il modulo come scritto qui può considerarsi un caso particolare, all'interno dello spazio euclideo, della nozione di norma di un vettore di uno spazio normato o di una matrice: l'insieme dei reali e l'insieme dei complessi si possono infatti considerare spazi normati unidimensionali e insiemi di matrici formula_65.
Nel linguaggio C il valore assoluto di un numero è calcolato dalle funzioni codice_1, codice_2, codice_3 (in C99), codice_4, codice_5, e codice_6. Scrivere la versione della funzione per i numeri interi è banale, se non si considera il caso limite in cui venga immesso il più grande numero intero negativo:
Le versioni per numeri a virgola mobile sono più complesse, in quanto devono tener conto dei codici speciali per l'infinito e not-a-number.
</text>
</doc>
<doc id="161418" url="https://it.wikipedia.org/wiki?curid=161418">
<title>Fuoco (geometria)</title>
<text>
In geometria, il fuoco è un punto particolare usato nel descrivere sezioni coniche.
Una sezione conica può essere definita come un luogo di punti la cui distanza dal fuoco è uguale all'eccentricità moltiplicata per la distanza alla direttrice corrispondente.
Anche nel caso in cui ci siano due fuochi, il luogo descritto, su una combinazione un fuoco - direttrice, rappresenta la sezione conica.
Da notare è che l'ellisse (non circolare) e l'iperbole hanno due fuochi ciascuna.
Un'ellisse può essere descritta con il luogo dei punti per i quali la somma delle distanze dai fuochi è costante, mentre l'iperbole è il luogo dei punti per i quali il valore assoluto della differenza delle distanze dai fuochi è costante.
Nel problema gravitazionale dei due corpi, le orbite dei corpi sono descritte da sezioni coniche dove i fuochi sono posizionati nel centro di massa.
</text>
</doc>
<doc id="131917" url="https://it.wikipedia.org/wiki?curid=131917">
<title>Complesso coniugato</title>
<text>
In matematica, si definisce complesso coniugato (o coniugio) di un numero complesso il numero ottenuto dal primo cambiando il segno della parte immaginaria. Pensando il numero complesso come punto del piano complesso, il suo complesso coniugato è il punto riflesso rispetto all'asse reale.
Dato il numero complesso
dove "x" e "y" sono numeri reali ed "i" è l'unità immaginaria, il complesso coniugato di formula_2 si indica con formula_3 o formula_4 ed è definito da
Per un numero complesso dato in forma esponenziale
con formula_7, il complesso coniugato è
La coniugazione complessa è un automorfismo del campo dei numeri complessi formula_9, in altre parole: l'applicazione formula_10 è una funzione biettiva dei numeri complessi con le seguenti proprietà:
Si hanno inoltre le seguenti relazioni fra complesso coniugato, inverso, valore assoluto e parte reale ed immaginaria: per ogni formula_14,
Inoltre se un polinomio formula_20 a coefficienti reali ha una radice (complessa) formula_21 allora anche formula_22 è una radice di formula_20. Infatti per quanto detto in precedenza si ha che
</text>
</doc>
<doc id="195731" url="https://it.wikipedia.org/wiki?curid=195731">
<title>Limite di una successione</title>
<text>
In matematica, il limite di una successione è il valore a cui tendono i termini di una successione. In particolare, se tale limite esiste finito, la successione si dice convergente. Si tratta di un concetto fondamentale per la costruzione rigorosa dell'analisi matematica.
Tramite la nozione di limite viene formalizzata rigorosamente l'idea intuitiva di "punto variabile che si avvicina arbitrariamente a un punto dato". Tale "punto mobile" potrebbe "muoversi" nell'insieme dei numeri razionali, sulla retta reale, sul piano o anche (via via generalizzando) in uno spazio euclideo, in uno spazio metrico o in uno spazio topologico.
L'esempio più semplice è dato dalla successione dei reciproci degli interi positivi formula_1:
successione che si può descrivere meccanicamente come un numero variabile che si avvicina sempre di più allo zero.
La nozione di limite di una successione può essere generalizzata a quella di limite di una funzione. Infatti una successione è una funzione avente come dominio l'insieme dei numeri naturali.
Un numero reale formula_3 è il limite di una successione di numeri reali formula_4 se la distanza fra i numeri formula_5 ed formula_3, data dal valore assoluto formula_7, è arbitrariamente piccola quando formula_8 è sufficientemente grande.
In altre parole, formula_3 è il limite della successione se formula_10 e in tal caso si scrive:
e si dice che la successione "converge" a formula_3.
Se formula_13, la successione è detta "infinitesima". Questa definizione chiarisce il fatto che l'espressione "infinitesima" non è appropriata per una grandezza ben determinata (anche se molto piccola), ma ha senso solo in riferimento a una grandezza variabile.
La definizione di limite può essere estesa al caso formula_14 e formula_15 nel modo seguente. La successione formula_16 ha limite formula_17 se raggiunge e mantiene valori arbitrariamente alti, cioè se per ogni formula_18 esiste un numero naturale formula_19 tale che formula_20 per ogni formula_21.
Analogamente, la successione ha limite formula_22 se formula_23 per ogni formula_21. In entrambi i casi si dice che la successione è "divergente".
Per il teorema di unicità del limite il limite di una successione (che sia finito o infinito) se esiste è unico.
In uno spazio metrico formula_25, dove formula_26 è la funzione distanza, un punto formula_27 di formula_28 è il limite di una successione formula_29 se:
Questa definizione coincide in formula_31 con quella descritta sopra, se formula_32 è considerato con la usuale metrica euclidea, definita da formula_33.
In uno spazio topologico formula_34, un punto formula_27 è limite di una successione formula_29 se:
Per il teorema di limitatezza, una successione formula_38 convergente ad un limite finito formula_3 è limitata, ovvero esiste un formula_40 tale che formula_41 per ogni formula_8.
D'altra parte, una successione limitata non è necessariamente convergente: si veda ad esempio la successione formula_43.
Una successione divergente (cioè con limite formula_44) può essere limitata o solo inferiormente o solo superiormente. D'altro canto, esistono però successioni non limitate che non sono divergenti. Ad esempio, la successione formula_45 data da:
oppure la successione:
In entrambi i casi, le successioni non hanno limite e quindi non sono divergenti.
Per il teorema della permanenza del segno, se una successione formula_16 converge ad un limite strettamente positivo formula_49 (che può essere anche formula_50), questa ha "definitivamente" soltanto termini positivi. In altre parole, esiste un formula_19 tale che formula_52 per ogni formula_53.
Analogamente, una successione che converge ad un limite strettamente negativo ha definitivamente soltanto termini negativi. Una successione che converge a zero può avere infiniti termini di ambo i segni, ad esempio formula_54:
D'altro canto, non è vero in generale che una successione formula_16 di termini positivi formula_52 convergente debba avere un limite strettamente positivo formula_58: ad esempio, la successione formula_59 è fatta di termini positivi, ma converge a zero.
È però vero che una tale successione debba avere un limite formula_60: se infatti avesse un limite negativo formula_61, per la permanenza del segno appena descritta dovrebbe avere infiniti termini negativi.
Se una successione formula_16 converge ad un limite (finito o infinito) formula_3, la successione dei valori assoluti formula_64 converge al valore assoluto del limite formula_65.
Non è vero l'enunciato opposto: esistono successioni non convergenti, i cui valori assoluti però convergono. Ad esempio, la successione formula_66.
Per il teorema di esistenza del limite di successioni monotone, una successione monotona formula_67 converge sempre ad un limite (che può essere infinito). Il limite è dato dall'estremo superiore (se è monotona crescente) o inferiore (se è decrescente) dei valori della successione. In altre parole, nel caso crescente:
Tale limite è finito quindi se e solo se la successione è limitata.
Il fatto che formula_69 sia monotona e converga ad un limite formula_3 è spesso espresso con una freccia:
oppure:
Una sottosuccessione di una successione formula_16 è ottenuta prendendo un sottoinsieme infinito di questa, e si indica con formula_74. Vale la proprietà seguente: una successione è convergente se e solo se ogni sua sottosuccessione è convergente.
Se formula_16 e formula_76 sono successioni convergenti, con:
limiti finiti, allora:
Queste proprietà sono valide in alcuni casi anche per limiti formula_82 infiniti, purché l'operazione richiesta non sia una forma indeterminata. Ad esempio:
e se formula_84, anche:
con i segni opportuni calcolati con la usuale regola del prodotto.
Un metodo classico per ottenere informazioni sulla convergenza di una successione consiste nel confrontare questa con un'altra, il cui comportamento è già noto.
Se due successioni formula_5 e formula_87 convergono ai limiti formula_88 e formula_89, e se formula_90 per ogni formula_8, allora formula_92.
Per mostrare questo fatto, basta prendere la successione formula_93, che è fatta di termini maggiori o uguali a zero, e per le proprietà dei limiti rispetto alle operazioni converge a formula_94: quindi per il teorema della permanenza del segno formula_95, ovvero formula_96.
Il teorema del confronto per le successioni asserisce che una successione "stretta fra due successioni" convergenti allo stesso limite converge anch'essa a questo limite. Formalmente, se formula_97 e formula_98 sono tre successioni tali che:
per ogni formula_8, e se:
allora anche:
Ad esempio, la successione:
è "stretta" fra le successioni formula_104 e formula_105, poiché:
per ogni formula_8. Poiché entrambe formula_5 e formula_109 sono infinitesime (convergono cioè a zero), anche formula_87 è infinitesima.
Una successione di Cauchy è una successione formula_16, i cui valori "si avvicinano sempre di più" fra loro. Formalmente, per ogni formula_112 esiste formula_19 tale che:
Per il criterio di convergenza di Cauchy, una successione di numeri reali è convergente se e solo se è di Cauchy.
La proprietà essenziale dei numeri reali che rende possibile questo fatto è la completezza. Infatti il criterio non vale per i numeri razionali, che non sono completi: una successione di numeri razionali di Cauchy non è necessariamente convergente ad un numero razionale (ma lo è ad un numero reale). Ad esempio:
è una successione di Cauchy di numeri razionali convergenti al numero irrazionale formula_116 di Nepero.
Se si considerano due successioni a valori reali di cui una formula_117 è positiva, strettamente crescente, illimitata, ed esiste il seguente limite:
allora esiste anche il limite:
</text>
</doc>
<doc id="123837" url="https://it.wikipedia.org/wiki?curid=123837">
<title>Sottrazione</title>
<text>
In matematica, la sottrazione è una delle quattro operazioni aritmetiche fondamentali. È normalmente denotata con un segno meno infisso ("−"). 
La sottrazione tra due numeri naturali può essere definita in termini di addizione. Dati due numeri naturali "n" ed "m", il primo detto minuendo ed il secondo sottraendo, si dice differenza il numero naturale "d", se esiste, che aggiunto ad "m" dà come somma "n". In simboli, 
La sottrazione viene utilizzata per modellare i tre processi fisici seguenti.
Matematicamente è spesso utile vedere la sottrazione non come un'operazione separata, ma come addizione dell'opposto del sottraendo. Così, 7-3 diventa la somma di 7 e di "−3". In questo modo, si possono applicare alla sottrazione tutte le regole familiari e la nomenclatura dell'addizione. Si consideri inoltre che la sottrazione non è commutativa né associativa, ma l'addizione di quantità con segno sì; questo significa che un matematico non userà spesso le parole "minuendo" e "sottraendo" ma considererà 7-3 come la somma degli addendi "7" e "−3”.
Prendiamo un segmento di lunghezza "b" disegnato per terra con l'estremo di sinistra chiamato "a" e quello destro "c".
Partendo dalla posizione "a", saranno necessari "b" passaggi per raggiungere la posizione "c". Questo movimento verso destra, chiamato addizione, può essere scritto come:
Dalla posizione "c", saranno necessari "b" passaggi per ritornare all'estremo "a". Questo movimento verso sinistra, chiamato "sottrazione", può essere scritto come:
Immaginiamo ora un segmento le cui posizioni siano contrassegnate dai numeri 1, 2 e 3.
Dalla posizione 3, per rimanere alla posizione 3 non è necessario nessun passaggio, quindi
Dalla posizione 3, per andare alla posizione 2 è necessario 1 passaggio, quindi
Dalla posizione 3, per andare alla posizione 1 sono necessari 2 passaggi, quindi
Cosa succederebbe se si continuasse nel processo andando per 3 volte verso sinistra dalla posizione 3? Per il nostro esempio, si andrebbe oltre la linea disegnata, cosa che non sarebbe permessa. Quindi per fare questo la linea deve essere estesa.
Per la sottrazione dei numeri naturali, la linea dovrebbe avere tutti i numeri naturali (0, 1, 2, 3, 4, ...) su di essa.
Usando la linea dei numeri naturali, dalla posizione 3, tornando per 3 volte verso sinistra si raggiungerebbe la posizione 0, quindi
Ma per i numeri naturali, 3 − 4 sarebbe una operazione non valida. Per eseguirla dobbiamo ulteriormente estendere la linea.
Usando la linea dei numeri interi (…, −3, −2, −1, 0, 1, 2, 3, …), dalla posizione 3, togliendo 4 arriveremmo alla posizione −1, quindi
Per fare una sottrazione in colonna bisogna scrivere prima il minuendo e sotto il sottraendo: 86 - 34 = 52
Si prende il primo numero da destra e gli si sottrae quello che ha sotto (6-4=2). Si fa la stessa cosa con quello a sinistra (8-3=5). Si scrivono i due risultati sotto le corrispondenti sottrazioni.
Aggiungendo o sottraendo uno stesso termine al minuendo e al sottraendo la differenza non cambia. Cioè se
allora si ha anche
</text>
</doc>
<doc id="134503" url="https://it.wikipedia.org/wiki?curid=134503">
<title>Teorema binomiale</title>
<text>
In algebra il teorema binomiale (o anche formula di Newton, binomio di Newton e sviluppo binomiale) esprime lo sviluppo della potenza formula_1-esima di un binomio qualsiasi con la formula seguente:
in cui il fattore formula_3 rappresenta il coefficiente binomiale ed è sostituibile con 
formula_4. Tali coefficienti sono peraltro gli stessi che si trovano nel noto triangolo di Tartaglia.
Lo sviluppo vale per ogni coppia di numeri reali o complessi, ma più in generale vale in ogni anello commutativo.
Come esempio di applicazione della formula, riportiamo i casi piccoli, formula_5, formula_6 ed formula_7:
Nel caso in cui formula_1 sia un numero reale o complesso, la somma finita è sostituita da una serie infinita. Questa formula generalizzata, nel caso di formula_1 reale positivo, fu realizzata da Isaac Newton (da cui il nome). 
In base all'ultimo teorema di Fermat, (per formula_13) la serie dei prodotti intermedi più un termine n-esimo non può essere la potenza n-esima di "un numero intero". 
Per assurdo, infatti, dati (x , y) interi, se ad esempio per formula_14 potessi raccogliere qualche formula_15 "intero", posto formula_16 al primo membro, potrei scrivere formula_17 con x, s, z interi.
È possibile, secondo il teorema, sviluppare una qualunque potenza intera di formula_18 in una sommatoria nella forma
dove formula_20 rappresentano i coefficienti binomiali. Utilizzando la notazione di sommatoria, la stessa formula può essere scritta:
Una variante di questa formula binomiale può essere ottenuta sostituendo formula_22 ad formula_23 e formula_23 a formula_25, considerando quindi una sola variabile. In questa forma, si ha:
o, in maniera equivalente,
Il teorema binomiale può essere dimostrato per induzione. Infatti è possibile introdurre per tale teorema un passo base per cui esso risulta banalmente vero
e provare con il passo induttivo la veridicità del teorema per un esponente n qualsiasi. Infatti presa per corretta l'espressione 
sicuramente vera per formula_30, si ha 
moltiplicando la sommatoria per formula_18 si ha
da cui, essendo 
ed inoltre
Utilizzando nel primo passaggio la proprietà del coefficiente binomiale
si ha che
essendo infine 
e 
si ha che
e si ottiene l'espressione formale dello sviluppo della potenza successiva del binomio
che conferma la tesi.
Se scriviamo formula_51 come il prodotto 
formula_52 
con formula_1 fattori, è evidente che il numero delle volte in cui compare nello sviluppo il termine formula_54 è pari al numero di combinazioni che si possono ottenere prendendo formula_55 volte formula_23 e formula_57 volte formula_25 dai fattori del prodotto, numero che è dato proprio da formula_3. 
Poiché per la proprietà distributiva il prodotto è dato dalla somma di questi termini al variare di formula_57 da formula_61 a formula_1, si ha subito la tesi.
La definizione fornita del binomio di Newton è valida solo per formula_1 numero naturale. È tuttavia possibile fornire una generalizzazione valida per formula_64, nonché approssimarla in un intorno destro dello 0 con una serie di Taylor.
Nella pratica si usano spesso solo i primi due termini della serie, ossia 
formula_65 dove il resto formula_66 indica un infinitesimo di ordine superiore al primo.
Lo sviluppo completo è
dove formula_68 è il coefficiente binomiale generalizzato, dato da
Lo sviluppo attorno all'origine della funzione formula_70 è
e, poiché
si ottiene
che è la formula di cui sopra. Troncando la serie al formula_57-esimo termine, l'errore che si ottiene è un infinitesimo di ordine formula_77.
</text>
</doc>
<doc id="135026" url="https://it.wikipedia.org/wiki?curid=135026">
<title>Asintoto</title>
<text>
Una retta è detta "asintoto" del grafico di una funzione quando la distanza di un punto qualsiasi della funzione da tale retta tende a 0 al tendere all'formula_1 dell'ascissa o dell'ordinata del punto.
Il termine asintoto è utilizzato in matematica per designare una retta, o più generalmente una curva, alla quale si avvicina indefinitamente una funzione data. Con il termine "asintoto", senza ulteriori specificazioni, si intende, genericamente, una retta, a meno che dal contesto non emerga un altro significato, quando si vuole essere più specifici si parla di retta asintotica o, più in generale, di curva asintotica.
In matematica espressioni come "avvicinarsi indefinitamente" (o l'equivalente "tendere a") non sono definite rigorosamente, se non utilizzando in modo esplicito il concetto di limite. Volendo adottare un linguaggio più conforme a quello che si impiega nello studio dei limiti, si può dire che "la curva A è un asintoto della curva C" se, comunque si fissi una distanza minima, esiste un tratto contiguo, non limitato, della curva C che dista dall'asintoto A meno della distanza minima fissata.
In generale, la curva C può intersecare anche più volte il suo asintoto A. Tuttavia storicamente e in modo intuitivo, l'asintoto era considerato una curva A alla quale la nostra curva C si avvicina senza mai raggiungerla. Questo rende ragione della etimologia del termine, che deriva dal greco ἀσύμπτωτος "a-sým-ptōtos", dove "a-" ha un valore privativo, mentre "sým-ptōtos" è composto da "syn-", "con", e "ptōtós", un aggettivo che connota ciò che "cade". Dunque "sým-ptōtos" descrive ciò che "cade assieme", ovvero ciò che "interseca", e "a-sým-ptōtos" etimologicamente descrive ciò che "non interseca", nel senso che si diceva poco fa. Volendo si può fare ricorso ad un linguaggio figurato e dire che c'è una "intersezione all'infinito" fra A e C. È questa particolare "intersezione all'infinito" che rende A "asintoto" di C.
La retta di equazione formula_2 è asintoto verticale alla curva rappresentativa della funzione formula_3, se vale almeno una delle seguenti relazioni
La retta di equazione formula_2 può essere asintoto verticale ascendente o discendente a seconda che formula_7 tenda a più infinito o a meno infinito. In generale la ricerca degli asintoti verticali per una funzione si effettua calcolando i limiti destro e sinistro (o uno di questi), e, in tal caso, vale comunque la definizione data.
Per esempio la funzione tangente ha un numero infinito di asintoti verticali in corrispondenza dei valori formula_8 con formula_9, cioè le rette formula_10 sono asintoti verticali.
Un altro esempio è il logaritmo naturale il quale ha come asintoto verticale la retta formula_11.
La retta di equazione formula_12 è asintoto orizzontale alla curva di equazione formula_3, se:
In generale, si ha un asintoto orizzontale quando la funzione è scrivibile nella forma: formula_15
dove formula_16 è una funzione infinitesima nell'intorno dell'infinito (tende a zero per formula_17 tendente ad infinito) e formula_18 è un valore finito.
A volte può esistere un asintoto obliquo, ovvero la funzione tende asintoticamente ad una retta di equazione formula_19.
Questo accade quando si ha 
e una condizione analoga si ha per i limiti a formula_21.
Esiste un teorema che afferma che la condizione necessaria e sufficiente affinché formula_19 sia asintoto obliquo del grafico di formula_7 per formula_24 è che esista finito:
e che esista finito anche:
L'enunciato per formula_29 è identico.
Come esempio notevole consideriamo la funzione
il cui grafico è contenuto in una iperbole. Si può facilmente verificare che le rette formula_31 sono asintoti rispettivamente a formula_32.
Le tre situazioni precedenti ne formano solo una in geometria proiettiva, con un asintoto visto come tangente all'infinito.
Un esempio è la spirale.
Una curva di equazione formula_33 ammette una parabola asintoto di equazione formula_34 e un'iperbole asintoto di equazione formula_35. La figura costituisce un tridente di Newton.
</text>
</doc>
<doc id="208048" url="https://it.wikipedia.org/wiki?curid=208048">
<title>Disuguaglianza</title>
<text>
In matematica una disuguaglianza (o "diseguaglianza") è una relazione d'ordine totale sull'insieme dei numeri reali o su un suo sottoinsieme.
Può essere intesa in senso "largo" o in senso "stretto", a seconda che la relazione d'ordine sia riflessiva o meno. Nei due casi vengono utilizzate le coppie di simboli formula_1 e formula_2, oppure formula_3 e formula_4.
Gli stessi simboli possono essere utilizzati per "confrontare" due funzioni a valori reali.
La disuguaglianza in senso largo si indica con le scritture equivalenti formula_5 e formula_6, che si leggono ""a" è maggiore o uguale a "b"" e ""b" è minore o uguale ad "a"".
La disuguaglianza in senso stretto si indica invece le scritture equivalenti formula_7 e formula_8, si verifica una e una sola relazione tra: formula_9 e formula_10.
Se la disuguaglianza è larga:
Ogni coppia di elementi è confrontabile, ovvero:
formula_11 si verifica una relazione tra: formula_12 e formula_6.
Se la disuguaglianza è stretta, allora vale la tricotomia:
Le disuguaglianze vengono preservate se ad entrambi i termini viene aggiunto o sottratto uno stesso numero:
Lo stesso vale con la disuguaglianza in senso largo.
Questa proprietà indica che confrontare due numeri "a" e "b" è equivalente a verificare se la loro differenza "a-b" è positiva o negativa, ovvero a confrontare "a-b" e "0". Inoltre "a&gt;0" equivale a "-a&lt;0", così come "a&gt;b" equivale a "-a&lt;-b".
Questa proprietà in generale descrive i gruppi ordinati.
Le disuguaglianze vengono preservate se entrambi i termini vengono moltiplicati o divisi per uno stesso numero strettamente positivo. Moltiplicando o dividendo per un numero strettamente negativo, invece, le disuguaglianze si scambiano:
A volte si abusa della notazione per la disuguaglianza, scrivendo "f&gt;0" anche quando "f" è una funzione a valori reali. Con questa notazione si intende che "f" assume solo valori strettamente positivi, ovvero che "f(x)&gt;0" per ogni "x" nel dominio di "f". Nello stesso modo, "f&gt;g" indica che "f-g&gt;0", ovvero che "f(x)&gt;g(x)" per ogni "x" nel comune dominio di "f" e "g". Lo stesso capita con la disuguaglianza in senso largo.
Quando il dominio delle funzioni non viene specificato, si parla di disequazione.
Alcune "famose" disuguaglianze in matematica sono elencate di seguito.
</text>
</doc>
<doc id="136524" url="https://it.wikipedia.org/wiki?curid=136524">
<title>Sezione conica</title>
<text>
In matematica, e in particolare in geometria analitica e in geometria proiettiva, con sezione conica, o semplicemente conica, si intende genericamente una curva piana che sia luogo dei punti ottenibili intersecando la superficie di un cono circolare con un piano.
Le sezioni coniche sono state studiate accuratamente in epoca ellenistica, in particolare da Menecmo ed Apollonio di Perga intorno al 200 a.C.; questi diede anche i nomi tuttora in uso per i tre tipi fondamentali di sezioni coniche: ellisse (la circonferenza ne è un caso degenere), parabola e iperbole.
Si consideri il cono circolare retto costituito dalle rette generatrici che, con il suo asse, formano un angolo di ampiezza θ.
Si tenga presente che i punti del cono si tripartiscono in tre sottoinsiemi: uno costituito solo dal suo vertice e due sottoinsiemi separatamente connessi dette "falde" o "nappe".
A seconda del tipo di piano che interseca il cono si hanno due tipi di curve: le cosiddette "non degeneri" e le "degeneri". Per quanto riguarda le prime si può avere:
Le cosiddette "coniche degeneri" si ottengono, invece, per intersezioni con piani passanti per il vertice del cono:
Il grafico di ogni equazione quadratica in due variabili reali, se i coefficienti soddisfano determinate condizioni che preciseremo, individua una sezione conica di un piano cartesiano, cioè di un piano riferito ad un sistema di coordinate cartesiane. Si trova inoltre che tutte le sezioni coniche si possono ottenere in questo modo.
Se si considera l'equazione quadratica nella forma 
si ha la seguente casistica:
Condizione necessaria affinché la curva sia una circonferenza è che formula_8.
Quello che si sta dicendo è che se formula_9 l'equazione data non può rappresentare una circonferenza, se invece formula_10 allora l'equazione potrebbe rappresentare una circonferenza. Ciò implica che, ad esempio, formula_11 non può essere l'equazione di una circonferenza, mentre invece formula_12 potrebbe esserlo, tuttavia formula_13, perciò non esiste nessun punto che soddisfi l'equazione data.
Una definizione alternativa delle sezioni coniche viene data a partite da una retta formula_14, la "direttrice," un punto formula_15 esterno a formula_14, detto "fuoco", e un numero formula_17, che prende il nome di "eccentricità". A tali enti si fa corrispondere la sezione conica consistente in tutti i punti la cui distanza da "formula_15" è uguale al prodotto di formula_19 per la rispettiva distanza da "formula_14". Per formula_21 si ottiene una circonferenza, per formula_22 un'ellisse, per formula_23 una parabola e per formula_24 un'iperbole.
Per una ellisse e una iperbole si possono assumere due coppie fuoco + direttrice, ciascuna fornendo la stessa intera curva. La distanza del centro dalla direttrice è formula_25, dove formula_26 denota il semiasse maggiore dell'ellisse, oppure la distanza del centro da ciascuno dei punti di distanza minima dell'iperbole. La distanza del centro da un fuoco è formula_27.
Nel caso della circonferenza si deve immaginare la retta direttrice a distanza infinita dal fuoco, cioè la retta si trova all'infinito del piano. Questo caso non si può trattare a partire dalla richiesta che la circonferenza sia il luogo dei punti la cui distanza dal centro sia "formula_19" volte la distanza da formula_14, in quanto si avrebbe una forma indeterminata della forma zero per infinito; questo caso va trattato come caso limite di ellissi.
Si può dunque affermare che l'eccentricità di una sezione conica dia una misura di quanto essa si allontani dall'essere circolare.
Per una data lunghezza formula_26 del semiasse maggiore, quanto più formula_19 si avvicina a 1, tanto più piccolo è il semiasse minore.
Sia formula_32 l'equazione associata alla conica tale che formula_33.Ad essa si associano due matrici formula_34 e formula_35 simmetriche tali che:
formula_36
È possibile distinguere i diversi tipi di conica studiando il determinante delle due matrici:
Si definisce semilato retto di una sezione conica C un segmento ortogonale all'asse maggiore che ha una estremità nel suo fuoco singolo o in uno dei suoi due fuochi e l'altra in un punto della C; la sua lunghezza di solito si denota con "l". Questa grandezza viene collegata alle lunghezze dei semiassi "a" e "b" dall'uguaglianza formula_43.
In coordinate polari, una sezione conica con un fuoco nell'origine e, se dotata di un secondo fuoco, con questo sul semiasse positivo delle "x", è determinata dall'equazione
Le sezioni coniche sono importanti in astronomia: le orbite di due corpi (con masse elevate) che interagiscono secondo la legge di gravitazione universale sono sezioni coniche rispetto al loro comune centro di massa considerato a riposo. Se tra di loro si esercita una attrazione sufficiente, entrambi percorrono un'ellisse; se l'attrazione reciproca è insufficiente si muovono con la possibilità di allontanarsi illimitatamente percorrendo entrambi parabole o iperboli. Si veda in proposito problema dei due corpi.
In geometria proiettiva le sezioni coniche nel piano proiettivo sono considerate equivalenti, nel senso che possono essere trasformate l'una nell'altra mediante una trasformazione proiettiva.
In epoca ellenistica la conoscenza delle coniche permise la costruzione di specchi parabolici, forse applicati in attività belliche (v. Specchi ustori) e nella costruzioni di fari di grande portata (v. Faro di Alessandria).
Per una trattazione breve e abbastanza semplice delle sezioni coniche che mostra come esse si possono caratterizzare equivalentemente come intersezioni di un piano con un cono e in termini di fuochi o di un fuoco e una direttrice vedi Sfere di Dandelin.
Consideriamo un cono avente come asse l'asse delle "z" e il vertice nell'origine. Esso è determinato dall'equazione 
dove
e formula_47 denota l'angolo che ogni generatrice del cono forma con l'asse. Si noti che questa equazione individua due superfici una posta al di sopra e l'altra al di sotto del vertice; nel parlare comune ciascuna di queste superfici viene detta cono; i matematici preferiscono parlare di due nappe la cui unione costituisce il cono e la cui intersezione si riduce al vertice del cono.
Consideriamo un piano P che interseca il piano "Oxy" in una retta parallela all'asse delle "y" e che interseca il piano "Oxz" in una retta con una certa pendenza; la sua equazione è
dove
e formula_50 è l'angolo che P forma con il piano "Oxy".
Ci proponiamo di individuare l'intersezione del cono con il piano P: questo richiede la combinazione delle due equazioni (1) e (2). Queste si possono risolvere nella variabile "z" e le espressioni trovate si possono uguagliare. 
L'equazione (1) per la "z" fornisce
di conseguenza 
Elevati al quadrato i due membri e sviluppato il binomio del membro a destra si ottiene 
Raggruppando le variabili si giunge alla
Si noti che questa è l'equazione della proiezione della sezione conica sul piano "Oxy"; Quindi questa equazione fornisce una figura ottenuta dalla sezione conica mediante una contrazione nella direzione dell'asse delle "x".
Si ottiene una parabola quando la pendenza del piano P è uguale alla pendenza delle generatrici del cono. In questo caso gli angoli
formula_47 
e
formula_50
sono complementari. Questo implica che
di conseguenza 
Sostituendo l'equazione (4) nell'equazione (3) si fa scomparire il primo termine nell'equazione (3) e rimane l'equazione
Moltiplicando entrambi i membri per "a",
a questo punto si può trovare un'espressione per la "x":
L'equazione (5) descrive una parabola il cui asse è parallelo all'asse delle "x". Altre versioni della equazione (5) si possono ottenere ruotando il piano intorno all'asse delle "z".
Si individua un'ellisse quando la somma degli angoli formula_62 e formula_50 è inferiore ad un angolo retto:
In tal caso la tangente della somma dei due angoli è positiva.
Ricordiamo ora la identità trigonometrica 
questa implica
Ma "m + a" è positivo, in quanto è la somma di due numeri positivi; quindi la disuguaglianza (6) è positiva se anche il denominatore è positivo:
Dalla disuguaglianza (7) si deducono:
Riprendiamo ancora l'equazione (3),
ma questa volta assumiamo che il coefficiente di "x" non si annulli ma sia invece positivo. Risolviamo per la "y": 
Questa equazione descriverebbe chiaramente un'ellisse, se non fosse presente il secondo termine sotto il segno di radice, "2 m b x": sarebbe l'equazione di una circonferenza dilatata proporzionalmente secondo le direzioni dell'asse delle "x" e dell'asse delle "y". L'equazione (8) in effetti individua un'ellisse ma in modo non evidente; quindi occorre manipolarla ulteriormente per convincersi di questo fatto. Completiamo il quadrato sotto il segno di radice:
Raccogliamo i termini in "b":
Dividiamo per "a" ed eleviamo al quadrato entrambi i membri:
Un'ulteriore manipolazione delle costanti finalmente conduce a 
Il coefficiente del termine in "y" è positivo (per un'ellisse). Cambiando i nomi dei coefficienti e delle costanti ci conduce a 
che è chiaramente l'equazione di un'ellisse. In altri termini, l'equazione (9) descrive una circonferenza di raggio "R" e centro "(C,0)" che viene poi dilatata verticalmente per un fattore formula_82. Il secondo termine del membro a sinistra (il termine nella "x") non ha coefficiente ma è un quadrato, quindi deve essere positivo. Il raggio è un prodotto di quadrati e quindi deve essere anch'esso positivo. Il primo termine del membro a sinistra (il termine in "y") ha un coefficiente positivo, e dunque l'equazione descrive un'ellisse.
L'intersezione del cono con il piano P fornisce un'iperbole quando la somma degli angoli formula_83 e formula_84 è un angolo ottuso, maggiore di un angolo retto. La tangente di un angolo ottuso è negativa e tutte le disuguaglianze trovate per l'ellisse vengono cambiate nelle loro opposte. Quindi si ottiene
Di conseguenza per l'iperbole si trova l'equazione che differisce da quella trovata per l'ellisse solo per avere negativo il coefficiente "A" del termine in "y". Questo cambiamento di segno fa passare da un'ellisse ad un'iperbole. Il collegamento fra ellissi e iperbole può descriversi anche osservando che l'equazione di un'ellisse con coordinate reali può interpretarsi come l'equazione di un'iperbole con una coordinata immaginaria e, simmetricamente, che l'equazione di un'iperbole con coordinate reali può interpretarsi come l'equazione di un'ellisse con una coordinata immaginaria (vedi numero immaginario). Il cambiamento di segno del coefficiente "A" equivale allo scambio fra valori reali e immaginari della funzione della forma "y=f(x)" che si legge nell'equazione (9).
Una ellisse non ha punti impropri. Una parabola ha un solo punto improprio. Una iperbole ha due punti impropri.
</text>
</doc>
<doc id="124991" url="https://it.wikipedia.org/wiki?curid=124991">
<title>Moltiplicazione</title>
<text>
La moltiplicazione è una delle quattro operazioni fondamentali dell'aritmetica. È un modo rapido per rappresentare la somma di numeri uguali. Il risultato di una moltiplicazione è chiamato "prodotto", mentre i due numeri moltiplicati sono detti "fattori" se considerati insieme, e rispettivamente "moltiplicando" e "moltiplicatore" se presi individualmente.
Nella scrittura matematica, esistono due diversi simboli utilizzati per indicare la moltiplicazione: entrambe le seguenti notazioni significano "cinque moltiplicato per due volte" ed entrambe si leggono "cinque per due":
Qualora i due moltiplicandi non siano scritti in cifre, e quindi non ci sia il rischio di equivoco, è possibile anche semplicemente giustapporli, come in:
anche per leggere queste formule vale lo stesso principio: se non c'è rischio di equivoco si può omettere il "per", come nella prima ("due zeta"), altrimenti verrà detto, come nella seconda ("due per, aperta parentesi, zeta più due, chiusa parentesi" o "due per, tra parentesi, zeta più due") o infine "due che moltiplica zeta più due".
Nei linguaggi di programmazione e nelle calcolatrici, la moltiplicazione viene solitamente indicata con l'asterisco (*).
Dati due numeri interi positivi formula_4 e formula_5, detto il primo "moltiplicando" ed il secondo "moltiplicatore", la definizione di moltiplicazione non è altro che:
ovvero "addizionare il numero formula_4 per formula_5 volte".
Usando una formula più ristretta, con il simbolo di sommatoria:
Quindi, per esempio:
Data la proprietà commutativa della moltiplicazione (vedi più in basso), talvolta si dà la seguente definizione (equivalente) di moltiplicazione:
A partire dalla definizione, si può dimostrare che la moltiplicazione ha le seguenti proprietà:
Per la moltiplicazione nel campo dei numeri razionali (v. sotto) vale anche
Nel libro "Arithmetices principia, nova methodo exposita", Giuseppe Peano propose un sistema assiomatico per i numeri naturali; due di questi assiomi riguardano la moltiplicazione:
Qui "b"' rappresenta l'elemento dei numeri naturali successivo di "b". Con gli altri nove assiomi di Peano, è possibile provare le regole comuni della moltiplicazione, come la proprietà distributiva e associativa. I due assiomi elencati forniscono una definizione ricorsiva della moltiplicazione.
Estendiamo l'operazione di moltiplicazione al caso dei numeri negativi, definendo quanto segue: dato x numero naturale
dove con − x si intende l'inverso additivo di x : 
Da qui abbiamo che la moltiplicazione di interi qualunque si riduce alla moltiplicazione di interi positivi e di formula_24. 
Lo schema che ne deriva è detto "regola dei segni":
Quest'ultima regola pratica ha un'interpretazione anche nella vita reale. Supponiamo di guadagnare "m" euro l'anno; tra "n" anni avremo "mn" euro (un numero positivo), mentre se questo guadagno era iniziato nel passato allora "n" anni fa (cioè "tra meno "n" anni") avevamo "mn" euro in meno (un numero negativo). Se invece perdessimo "m" euro l'anno (cioè guadagnassimo "meno "m" euro"), tra "n" anni ne avremo "mn" in meno, ma "n" anni fa ne avevamo "mn" in più di quanti ne abbiamo ora.
La definizione di moltiplicazione si può infine estendere ai numeri razionali, ai numeri reali, e ai numeri complessi.
Per i numeri razionali abbiamo che 
verificando che la definizione è indipendente dai rappresentanti scelti.
Per i numeri reali, una definizione di moltiplicazione si può ottenere prendendo il modello di numero reale come sezione di Dedekind: dati due numeri reali positivi, rappresentati come sezioni in campo razionale, moltiplicando (con opportuni accorgimenti) i minoranti tra loro e i maggioranti tra loro si ottiene ancora una sezione, che rappresenta il prodotto dei due numeri. La definizione si può poi estendere a tutti i numeri reali seguendo la regola dei segni indicata nella sezione precedente.
Per i numeri complessi, infine, si ha:
</text>
</doc>
<doc id="120737" url="https://it.wikipedia.org/wiki?curid=120737">
<title>Limite (matematica)</title>
<text>
In matematica, il concetto di limite serve a descrivere l'andamento di una funzione all'avvicinarsi del suo argomento a un dato valore (limite di una funzione) oppure l'andamento di una successione al crescere illimitato dell'indice (limite di una successione). I limiti si utilizzano in tutti i rami dell'analisi matematica; sono usati ad esempio per definire la continuità, la derivazione e l'integrazione.
Il concetto di limite di una funzione, più generale del limite di una successione, può essere generalizzato da quello di limite di un filtro.
Il concetto di limite era già presente in modo intuitivo nell'antichità, per esempio in Archimede (nel suo metodo di esaustione), e fu utilizzato, anche se non in modo rigoroso, a partire dalla fine del XVII secolo da Newton, Leibniz, Eulero e D'Alembert.
La prima definizione abbastanza rigorosa di limite risale al XIX secolo con Cauchy, seguita da una miglior formalizzazione di Weierstrass.
Una completa teoria del limite si ha con Heine, che nel 1872 pubblicò un lavoro che creò molto interesse all'epoca e nel quale stilò regole e proprietà del limite. Molti altri studiosi si sono interessati al problema del limite, approfondendo l'argomento con lo studio dell'analisi infinitesimale, tra cui Bolzano, Dedekind e Cantor.
Ma solo nel 1922 Eliakim Hastings Moore ed H.L. Smith diedero una nozione generale (topologica) di limite, ed è quella attualmente utilizzata in matematica. Nel 1937, Henri Cartan ne fornì una versione equivalente, basata sul concetto di filtro.
Una successione formula_1 di numeri reali ha come limite il numero formula_2 se al crescere di formula_3 i termini della successione "sono arbitrariamente vicini" al valore formula_4. Formalmente, questa nozione è resa chiedendo che per ogni formula_5 piccolo a piacere esista un numero naturale formula_6 tale che formula_7 per ogni formula_8.
Una successione può non avere limite, ad esempio formula_9, data da:
non ha limite. D'altra parte, se esiste un limite formula_2, si dice che la successione converge ad formula_2; in questo caso, il limite è unico (una successione non può convergere a due valori distinti). Ad esempio, la successione formula_13, data da:
converge a zero.
Considerando uno spazio topologico formula_15, una successione formula_16 con formula_17 tende al limite formula_18 se, comunque si prenda un intorno formula_19 di formula_4, esiste un formula_21 tale per cui formula_22 per tutti gli formula_23, e si scrive:
Se formula_15 è uno spazio di Hausdorff il limite di formula_16 con formula_17, se esiste, è unico.
Il limite di una funzione generalizza il limite di una successione di punti in uno spazio topologico formula_28; si considera la successione una funzione formula_29 nello spazio topologico formula_30 con la topologia discreta. In tale definizione, un intorno di formula_31 ha la forma formula_32.
Siano dati una funzione formula_33 definita su un sottoinsieme formula_15 della retta reale formula_35 ed un punto di accumulazione formula_36 di formula_15. Un numero reale formula_38 è il limite di formula_39 per formula_40 tendente a formula_41 se la distanza fra formula_39 ed formula_38 è arbitrariamente piccola quando formula_44 si avvicina a formula_41.
La distanza fra i punti è misurata usando il valore assoluto della differenza: quindi formula_46 è la distanza fra formula_40 e formula_41 e formula_49 è la distanza fra formula_39 ed formula_38. Il concetto di "arbitrariamente piccolo" è espresso formalmente con i quantificatori "per ogni" (quantificatore universale) ed "esiste" (quantificatore esistenziale).
Formalmente, formula_38 è limite se per ogni numero reale formula_5 piccolo a piacere esiste un altro numero reale positivo formula_54 tale che:
In questo caso si scrive:
La definizione di limite di una funzione è necessaria per formalizzare il concetto di funzione continua.
Dato uno spazio topologico formula_60, un punto formula_61 è il limite di un ultrafiltro formula_62 su formula_15 se ogni intorno di formula_64 appartiene a formula_62.
Il limite di una funzione rispetto ad un filtro è definito considerando una funzione formula_66 tra spazi topologici e un filtro formula_62 su formula_19. Il punto formula_69 è il limite di formula_70 in formula_61 rispetto ad formula_62 se formula_64 è il limite di formula_62 e formula_75 è il limite di formula_76. Si scrive in tal caso:
Il concetto di limite si estende anche alle successioni di insiemi attraverso le nozioni di limite superiore e limite inferiore: data una successione di insiemi formula_78, l'insieme limite è definito come l'insieme che intuitivamente contiene gli elementi che stanno nel maggior numero di insiemi della successione. Formalmente, una successione di insiemi si dice possedere limite se vale la seguente uguaglianza:
</text>
</doc>
<doc id="121221" url="https://it.wikipedia.org/wiki?curid=121221">
<title>Addizione</title>
<text>
L'addizione (denotata normalmente dal simbolo del più, "+") è una delle quattro operazioni fondamentali dell'aritmetica, insieme alla sottrazione, alla moltiplicazione e alla divisione. L'addizione di due numeri naturali può essere definita in termini insiemistici. Per sommare due numeri naturali "a" e "b", si considerano due insiemi "A" e "B" che abbiano, rispettivamente, "a" e "b" come numero di elementi, e che siano disgiunti (cioè non abbiano elementi in comune). Allora il risultato dell'addizione di "a" e "b" è il numero di elementi dell'insieme unione di "A" e "B" (l'insieme "A" ∪ "B").
Ad esempio, se in un sacchetto abbiamo tre mele e in un altro sacchetto abbiamo due mele, mettendo insieme il contenuto dei due sacchetti avremo cinque mele. Questa osservazione è equivalente all'espressione matematica "3 + 2 = 5", ovvero "3 più 2 è uguale a 5".
L'addizione può essere definita anche su quantità più astratte, quali i numeri interi relativi, i numeri razionali, i numeri reali e i numeri complessi, e su altri oggetti matematici quali i vettori e le matrici.
L'addizione gode di alcune proprietà basilari. È commutativa, ovvero cambiando l'ordine degli addendi la somma non cambia. È associativa, ovvero quando si sommano più di due numeri, il risultato è lo stesso indipendentemente dall'ordine in cui vengono effettuate le addizioni. Lo zero è l'elemento neutro dell'addizione, ovvero sommare zero ad un numero lascia quel numero invariato.
I numeri o termini coinvolti nell'addizione sono collettivamente detti addendi, e il risultato dell'addizione è la loro somma.
Se gli addendi sono scritti individualmente, l'addizione è rappresentata dal carattere "+", che si interpone tra un termine e l'altro. Tra la sequenza degli addendi e la loro somma si interpone il simbolo dell'uguaglianza, "=". Sono addizioni valide:
e la prima di queste si legge, indifferentemente,
Negli scritti precedenti al XVI secolo è possibile trovare un altro simbolo indicante l'addizione. Si tratta di una "P" in corsivo che rimpiazzava la parola "più".
Se i termini non sono scritti individualmente, ma la sequenza degli addendi si ricava facilmente dalla scrittura, la somma si può indicare con un'ellissi ("...") per indicare i termini mancanti: la somma dei numeri naturali da 1 a 100 si può dunque scrivere come 1 + 2 + … + 99 + 100 = 5050.
In alternativa, la somma può essere rappresentata con il simbolo di sommatoria, rappresentato dalla lettera greca Sigma maiuscola. In particolare, data una sequenza di numeri denotati con formula_4, la somma degli "n-m+1" compresi fra quello di posizione "m" e quello di posizione "n" può essere espressa con la scrittura
Il simbolo "+" è un'abbreviazione della parola latina "et", che significa "e". Il suo uso in opere a stampa è attestato sin dal 1486.
Una delle interpretazioni dell'addizione è quella della combinazione di insiemi: 
Questa interpretazione, facile da visualizzare, è anche alla base della definizione formale di addizione tra numeri cardinali.
Una seconda interpretazione dell'addizione è data dall'estensione di una lunghezza (intera) di partenza in termini di una lunghezza (intera) data: 
Questa interpretazione è alla base della definizione formale di addizione tra numeri ordinali.
L'addizione è un'operazione "commutativa", ovvero cambiando l'ordine degli addendi il risultato non cambia: 
Ad esempio: formula_7
L'addizione è un'operazione "associativa", ovvero quando si sommano tre o più addendi, l'ordine delle operazioni non influisce sul risultato:
Ad esempio: formula_9
Aggiungere zero ad un numero qualunque lascia quel numero invariato; in altre parole, lo zero è l'elemento neutro dell'addizione. In formule,
Per eseguire velocemente un'addizione fra numeri naturali, si allineano in colonna gli addendi, partendo dalla cifra delle unità a destra. Dopodiché si sommano le cifre di ciascuna colonna, da destra verso sinistra, e nell'eventualità che il risultato della somma su di una colonna sia maggiore o pari a dieci, si "riportano" le decine in eccesso come ulteriore addendo sulla colonna immediatamente a sinistra a quella appena calcolata.
Ad esempio, nell'addizione 27 + 59,
7 + 9 = 16, e la cifra 1 è il riporto.
Il metodo discusso sopra può essere applicato per sommare due qualunque numeri naturali ad "n" cifre "A"=a...aa e "B"=b...bb (nel caso il numero di cifre di "A" e "B" sia diverso, è sufficiente aggiungere il numero appropriato di zeri in testa al numero con meno cifre). In pseudocodice, l'algoritmo è il seguente:
Seguendo un metodo simile a quello descritto sopra si possono sommare anche due frazioni decimali. Si allineano i due numeri decimali in modo tale che la virgola sia nella stessa posizione. Se necessario, si aggiungono degli zeri in coda al numero più corto perché abbia lo stesso numero di cifre di quello più lungo. Infine, si segue il procedimento visto sopra, con l'unica accortezza di aggiungere la virgola al risultato nella stessa posizione in cui compare nei due addendi. Ad esempio, 45,1 + 4,34 si svolge nel seguente modo:
Questa estensione del metodo fu divulgata da Simone Stevino.
Nella teoria dell'algebra astratta si può chiamare "addizione" una qualunque operazione associativa e commutativa definita su un insieme, che sia dotata di un elemento neutro. Una struttura algebrica che includa una simile operazione è detta "monoide commutativo".
Già gli antichi Egizi avevano un semplice metodo per sommare numeri naturali. Nella notazione geroglifica, i numeri venivano indicati ripetendo una quantità appropriata di simboli rappresentanti le unità, le decine, le centinaia, eccetera. Per ottenere la somma di due numeri, quindi, era sufficiente combinare le unità, poi le decine, poi le centinaia, e così via, con l'accortezza di rimpiazzare eventuali gruppi di dieci simboli identici con un simbolo del tipo successivo. 
Ad esempio, per sommare 783 e 275, si combina
Il metodo di addizione posizionale moderno era sicuramente già noto in India intorno all'anno 600 d.C., ma è assai plausibile che lo stesso genere di algoritmo fosse già utilizzato dai Babilonesi in Mesopotamia nel 1700 a.C., anche se in un sistema sessagesimale anziché decimale.
In Europa, l'algoritmo di addizione moderno fu introdotto nel XII secolo attraverso la traduzione degli scritti del matematico persiano al-Khwarizmi.
</text>
</doc>
<doc id="187594" url="https://it.wikipedia.org/wiki?curid=187594">
<title>Limite di una funzione</title>
<text>
In matematica, il limite di una funzione in un punto formula_1 di accumulazione per il suo dominio è un modo per esprimere la quantità a cui tende il valore assunto dalla funzione all'avvicinarsi del suo argomento a formula_1. Indicando con formula_3 la funzione, il limite viene indicato con la notazione:
In altri termini, formula_5 significa che quando il valore di formula_6 si avvicina a formula_1, esprimibile con formula_8, il valore formula_9 assunto dalla funzione si avvicina a formula_10, cioè formula_11. Il valore formula_10 può essere finito (formula_13), infinito (formula_14) o non esistere affatto. Il limite rappresenta in un certo senso il comportamento di un oggetto matematico quando una o più variabili del suo dominio tendono ad assumere un determinato valore.
Il concetto di limite di una funzione viene generalizzato da quello di limite di un filtro, mentre un caso particolare è quello di limite di una successione di punti in uno spazio topologico.
Siano dati una funzione formula_15 definita su un sottoinsieme formula_16 della retta reale formula_17, e un punto di accumulazione formula_18 di formula_19. Un numero reale formula_20 è il limite di formula_3 per formula_22 tendente a formula_1 se, fissato arbitrariamente un valore formula_24 della distanza fra formula_3 e formula_20, si riesce a trovare, in corrispondenza di questo, un valore formula_27 della distanza tra formula_28 ed formula_1 per il quale per tutti gli formula_28, escluso formula_1, che distano da formula_1 meno di formula_27, si ha che formula_3 disti da formula_20 meno di formula_24.
La distanza fra i punti è misurata usando il valore assoluto della differenza: quindi formula_37 è la distanza fra formula_22 e formula_1 e formula_40 è la distanza fra formula_3 e formula_20. I concetti di "fissato arbitrariamente" e "si riesce a trovare" sono espressi formalmente, rispettivamente, con i quantificatori "per ogni" (quantificatore universale) ed "esiste" (quantificatore esistenziale).
Formalmente, formula_20 è il limite di formula_9 per formula_6 che tende a formula_46 se per ogni numero reale formula_47 esiste un altro numero reale positivo formula_48 tale che se formula_49 allora formula_50, o con formalismo puramente matematico
che è riassunto dalla scrittura:
Una definizione equivalente che usa gli intorni è la seguente: formula_20 è limite se per ogni intorno formula_54 di formula_55 in formula_56 esiste un intorno formula_57 di formula_1 in formula_56 tale che formula_3 appartiene a formula_54 per ogni formula_62 in formula_63. Il punto formula_46 non è necessariamente contenuto nel dominio di formula_65. Il punto è comunque escluso nella definizione di limite, poiché il limite deve dipendere soltanto dai valori di formula_65 in punti arbitrariamente vicini a formula_46 ma non dal valore che formula_65 assume in formula_46: per questo motivo si chiede che formula_70 sia maggiore di zero.
La definizione di cui sopra è quella maggiormente utilizzata al giorno d'oggi. Tuttavia, nella seconda metà del XX secolo una revisione dei concetti basilari di topologia ha indotto alcuni illustri studiosi a proporre una definizione modificata di limite. Se infatti formula_46 è più in generale punto di aderenza per l'insieme formula_19, allora si dice che formula_10 è limite se per ogni numero reale formula_74 esiste formula_75 tale che formula_76 ogni volta che formula_77. La condizione formula_78 viene quindi a mancare. La definizione riformata non modifica i limiti tradizionali come ad esempio la definizione di derivata, ma tratta in modo diverso alcuni casi "patologici". Si osservi che la condizione di aderenza di formula_46 a formula_19 è condizione necessaria e sufficiente affinché il limite, inteso con la definizione riformata, sia unico. Inoltre, utilizzando questa definizione la continuità diventa un caso particolare di limite a tutti gli effetti: infatti si vede facilmente che formula_81 continua in formula_46, punto del suo dominio, equivale a dire che formula_81 ammette limite formula_84 in formula_46. Vari altri classici risultati assumono una forma più semplificata assumendo la definizione riformata di limite: ad esempio il teorema del passaggio al limite in una funzione composta vale sotto le ipotesi più naturali possibili.
La definizione di limite viene normalmente estesa per considerare anche i casi in cui formula_1 e/o formula_20 sono infiniti.
La funzione formula_65 ha limite formula_89 in un punto finito formula_1 se per ogni numero reale formula_91 esiste un altro numero reale formula_92 tale che formula_93 per ogni formula_22 in formula_95 con formula_96, ovvero
che in maniera più sintetica si scrive:
Analogamente si definisce il limite formula_99 sostituendo formula_100 con formula_101.
Per definire il limite per formula_102, è ancora necessario che formula_89 sia un "punto di accumulazione" per il dominio formula_95: questo si traduce nella richiesta che formula_95 contenga valori arbitrariamente grandi, cioè che il suo estremo superiore sia infinito:
In questo caso, un numero finito formula_107 è limite di formula_65 per formula_109 se per ogni numero reale formula_47 esiste un altro numero reale formula_111 tale che formula_50 per ogni formula_22 in formula_95 con formula_115, ovvero
che in maniera più sintetica si scrive:
Analogamente si definisce il limite per formula_118, sostituendo formula_119 con formula_120.
Resta quindi da esaminare il caso in cui entrambi formula_1 e formula_20 sono infiniti. La funzione formula_65 ha limite formula_124 per formula_125 se per ogni numero reale formula_91 esiste un altro numero reale formula_111 tale che formula_93 per ogni formula_22 in formula_95 con formula_115, ovvero
che in maniera più sintetica si scrive:
In maniera analoga si definiscono i casi in cui formula_134 e/o formula_135.
Tutte queste definizioni possono essere raggruppate elegantemente in una sola proposizione: per questo scopo, è sufficiente estendere la retta reale formula_56 alla retta reale estesa:
ottenuta aggiungendo due punti formula_138 e formula_139. La retta reale estesa è un insieme ordinato e uno spazio topologico. Il concetto di intorno si estende quindi alla retta reale estesa: gli intorni di formula_89 sono tutti gli insiemi che contengono una semiretta formula_141, per qualche formula_142.
In questo modo, si possono riunire tutte le definizioni precedenti in una sola proposizione, ottenuta sostituendo formula_56 con formula_144 nella definizione che usa gli intorni. Sia quindi formula_145 una funzione definita su un insieme formula_95 di formula_144, e sia formula_1 un punto di accumulazione per formula_95. Un valore formula_20 in formula_144 è "limite" di formula_65 in formula_1 se per ogni intorno formula_54 di formula_55 in formula_144 esiste un intorno formula_57 di formula_1 in formula_144 tale che formula_3 appartiene a formula_54 per ogni formula_62 in formula_63.
Per il teorema di unicità del limite, una funzione può avere un limite (finito o infinito) in formula_1 oppure nessuno (non può quindi averne più di uno).
Se il limite per formula_8 di formula_9 è 0, formula_9 si dice "infinitesima" o "convergente" in formula_1. D'altro canto, se formula_9 tende a formula_170 è detta "divergente". Se formula_1 è contenuto nel dominio formula_95 di formula_65, e se vale:
allora la funzione è continua in formula_1. La nozione di continuità è molto importante in matematica: intuitivamente, una funzione continua in formula_1 ha il grafico che "non fa salti" intorno al punto, quindi può essere disegnato manualmente senza staccare mai la penna dal foglio: in ogni punto formula_46 del suo dominio, la formula_81 assume in formula_46 il valore del suo limite per formula_8. Altrimenti, la funzione ha in formula_1 un punto di discontinuità.
Sono qui elencati alcuni esempi.
Per avere informazioni più precise è a volte utile utilizzare i concetti di "limite destro" e "limite sinistro", definiti tramite la nozione di "intorno destro" e "sinistro".
Un "intorno destro" di un punto formula_1 della retta estesa formula_201 è un intervallo del tipo formula_202 con formula_203. Analogamente, un "intorno sinistro" è un intervallo del tipo formula_204. In particolare, gli intorni di formula_138 sono tutti destri e quelli di formula_139 sono sinistri.
A questo punto, sia formula_207 con formula_1 punto di accumulazione per formula_95. Un valore formula_20 della retta estesa è limite destro per formula_65 in formula_1 se per ogni intorno formula_213 di formula_55 esiste un intorno destro formula_215 di formula_18 tale che formula_217 appartiene a formula_54 per ogni formula_28 in formula_220.
Il limite sinistro è definito in modo analogo. I limiti sinistro e destro (se esistono) vengono descritti rispettivamente come:
Vale il risultato seguente: una funzione ha limite in formula_1 se e soltanto se ha limite destro e sinistro, e questi due limiti sono finiti e coincidono.
Ad esempio, la funzione gradino formula_65 mostrata in figura ha limite sinistro e destro in formula_224, ma questi non coincidono: quindi non ha limite in formula_224:
Le nozioni di "limite per difetto" e "per eccesso" vengono definite in modo analogo, sostituendo l'intorno formula_54 di formula_228 con intorni destri e sinistri. I limiti per difetto e per eccesso (se esistono) possono essere indicati con un piccolo abuso di linguaggio nel modo seguente:
Per il teorema di limitatezza locale, una funzione che ha limite finito in formula_1 è limitata in un intorno di formula_1, ovvero esistono un numero formula_232 e un intorno formula_57 di formula_1 tale che formula_235 per ogni formula_22 del dominio contenuto in formula_57.
D'altra parte, una successione limitata in un intorno di formula_1 non ha necessariamente limite in formula_1: ad esempio la funzione gradino è ovunque limitata, ma non ha limite in zero.
Per il teorema di permanenza del segno, se una funzione ha limite formula_240 strettamente positivo in formula_1, allora assume valori strettamente positivi per ogni formula_22 sufficientemente vicino a formula_1. In altre parole, esiste un intorno formula_57 di formula_1 tale che formula_246 per ogni formula_22 del dominio in formula_57 diversa da formula_1.
Analogamente, una funzione che ha limite formula_250 strettamente negativo ha valori strettamente negativi per tutti gli formula_22 sufficientemente vicini a formula_1. Una funzione che ha limite formula_253 può assumere vicino a formula_1 valori di entrambi i segni (ad esempio la funzione formula_255 con formula_256).
Siano formula_65 e formula_258 due funzioni definite su un dominio formula_95, con formula_1 punto di accumulazione per formula_95. Se formula_262 per ogni formula_28 del dominio in un intorno formula_57 di formula_1, e se entrambe le funzioni hanno limite in formula_1, allora vale:
Questo risultato è ottenuto applicando il teorema di permanenza del segno alla differenza formula_268.
Il teorema del confronto (o "dei carabinieri") asserisce che una funzione "stretta fra due successioni" convergenti allo stesso limite converge anch'essa a questo limite. Formalmente, se formula_269 e formula_270 sono tre funzioni definite su un dominio formula_95 con punto di accumulazione formula_1, tali che:
per ogni formula_274 del dominio in un intorno di formula_1, e tali che:
allora anche:
Viene detto ""dei carabinieri"" perché formula_9 e formula_279 vengono immaginati come i due carabinieri che portano in cella formula_280 cioè il criminale, oppure perché si immaginano due carabinieri che cercano di catturare un criminale da due lati opposti, esso tenderà, insieme ai carabinieri (le funzioni esterne), allo stesso punto.
Funzioni aventi lo stesso dominio possono essere sommate o moltiplicate. In molti casi è possibile determinare il limite della funzione risultante dai limiti delle singole funzioni.
Siano formula_281 e formula_282 due funzioni con lo stesso dominio formula_95, e formula_1 un punto di accumulazione per formula_95. Se esistono i limiti:
allora:
Alcune delle uguaglianze elencate sono estendibili ai casi in cui formula_292 e/o formula_293 sia infinito.
Il concetto di limite è generalizzato a ogni funzione formula_294 fra spazi metrici formula_95 e formula_296 nel modo seguente. Se formula_1 è un punto di formula_95, un valore formula_299 di formula_296 è limite di formula_3 per formula_302 se formula_3 si avvicina arbitrariamente a formula_299 quando formula_22 si avvicina a formula_1. Formalmente, se per ogni formula_307 esiste formula_308 tale che formula_309 per ogni formula_22 con formula_311. In questo caso si scrive:
Continua a valere il teorema di unicità del limite: una funzione non può tendere a due limiti diversi in un punto.
Siano formula_313 e formula_314 due spazi topologici e siano formula_315 , formula_1 un elemento della chiusura di formula_317 in formula_95 , formula_319. 
Data formula_320 un'applicazione si dice che formula_20 è un limite di formula_3 per formula_302 in formula_317, e si scrive formula_325 se:
formula_326
è continua in formula_1 con formula_328 dotato della topologia indotta da formula_329 e formula_296 dotato della topologia formula_331.
Inoltre se formula_1 punto di accumulazione di formula_317 in formula_95 e lo spazio formula_314 è di Hausdorff allora l'insieme formula_336 ha al più un elemento (unicità del limite).
Lo spazio euclideo formula_337 è uno spazio metrico, con la metrica euclidea. Quindi la definizione di limite per spazi metrici si applica a qualsiasi funzione:
dove formula_95 è un qualsiasi sottoinsieme di formula_337.
Una funzione complessa formula_341 può essere interpretata come funzione:
In questo modo è quindi anche definito il limite per funzioni fra insiemi di numeri complessi.
</text>
</doc>
<doc id="103296" url="https://it.wikipedia.org/wiki?curid=103296">
<title>Base (algebra lineare)</title>
<text>
In matematica, e più precisamente in algebra lineare, la base di uno spazio vettoriale è un insieme di vettori linearmente indipendenti che generano lo spazio. In modo equivalente, ogni elemento dello spazio vettoriale può essere scritto in modo unico come combinazione lineare dei vettori appartenenti alla base.
Se la base di uno spazio vettoriale è composta da un numero finito di elementi allora la dimensione dello spazio è finita. In particolare, il numero di elementi della base coincide con la dimensione dello spazio.
Sia formula_1 uno spazio vettoriale su un campo formula_2. L'insieme formula_3 di elementi di formula_1 è una base di formula_1 se valgono entrambe le seguenti proprietà:
Si dice anche che i vettori formula_16 appartenenti a una qualsiasi base di formula_1 costituiscono un sottoinsieme massimale di vettori linearmente indipendenti dello spazio. Questo significa che i vettori formula_16 sono tali che esistono formula_9 tali che:
ossia l'aggiunta al sottoinsieme massimale di un qualsiasi altro elemento dello spazio determina la dipendenza lineare degli elementi del sottoinsieme.
Una base è dunque composta da un minimo numero di vettori generatori dello spazio. Uno spazio vettoriale non banale con un campo infinito possiede infinite possibili basi diverse.
Uno spazio vettoriale in generale non ha una sola base, e solitamente si trattano spazi con infinite basi possibili. Il teorema della dimensione per spazi vettoriali afferma che tutte le possibili basi di uno stesso spazio hanno la stessa cardinalità, sono formate cioè sempre dallo stesso numero di vettori. Questo numero è la dimensione dello spazio, e permette di definire spazi di dimensione arbitrariamente alta. La dimensione dello spazio è inoltre uguale sia al massimo numero di vettori indipendenti che esso contiene, sia al minimo numero di vettori necessari per generare lo spazio stesso.
Qualsiasi sia lo spazio vettoriale formula_1, è sempre possibile trovarne una base. La dimostrazione richiede l'uso del lemma di Zorn nel caso generale, mentre nel caso particolare degli spazi finitamente generati esistono dimostrazioni più semplici.
Si consideri la collezione formula_22 dei sottoinsiemi di formula_1 linearmente indipendenti. È immediato dedurre che l'inclusione è un ordine parziale su formula_22, e che per ogni catena formula_25 l'insieme formula_26 ne è un maggiorante (è linearmente indipendente in quanto unione di elementi di una catena ordinata per inclusione). Applicando il lemma di Zorn, esiste un insieme massimale linearmente indipendente formula_27 in formula_22. Dunque formula_27 è una base, infatti se formula_30 ma non appartiene a formula_27 allora per la massimalità di formula_27 l'insieme formula_33 deve essere linearmente dipendente, cioè esistono degli scalari formula_9 non tutti nulli tali che
con formula_36, dal momento che se fosse nulla allora anche gli altri formula_37 dovrebbero esserlo, essendo gli elementi di formula_27 linearmente indipendenti. Quindi formula_13 può essere scritto come combinazione lineare finita di elementi di formula_27, che oltre a essere linearmente indipendenti generano formula_1. Dunque formula_27 è una base.
Per esprimere un vettore in modo unico attraverso una base è necessario definire un ordinamento nell'insieme dei vettori che costituiscono la base. Una "base ordinata" è una successione di vettori linearmente indipendenti che generano lo spazio. In particolare, se la successione formula_3 di elementi è una base ordinata di formula_1, allora l'insieme di tali vettori è una base di formula_1.
Ogni vettore formula_46 si può scrivere in modo unico come combinazione lineare dei vettori di base:
Si definisce l'insieme delle coordinate di formula_48 rispetto alla base data il vettore:
Si tratta del vettore che ha come componenti i coefficienti della combinazione lineare di vettori di base attraverso i quali si può scrivere formula_48. Tale vettore dipende dalla base scelta.
La mappa formula_51 che associa ad ogni vettore formula_13 le sue coordinate formula_53 è un isomorfismo di spazi vettoriali, cioè è una applicazione lineare biettiva.
Sia formula_2 un campo. L'insieme formula_55 è uno spazio vettoriale di dimensione formula_56. Si definisce base canonica di formula_55 l'insieme di vettori:
Ogni vettore formula_62 si può allora scrivere come combinazione lineare dei vettori di base:
Il vettore:
è il vettore delle coordinate di formula_48 rispetto alla base canonica. Solitamente si identifica un vettore attraverso le sue coordinate rispetto alla base canonica, ovvero formula_66.
Ad esempio, i vettori formula_67 ed formula_68 sono una base di formula_69, infatti ogni vettore formula_70 si scrive come:
Il concetto di base in spazi di dimensione infinita (in cui cioè esista un insieme infinito di vettori linearmente indipendenti) è più problematico. Per tali spazi esistono due nozioni differenti di base: la prima, detta "base di Hamel", è definita algebricamente, mentre la seconda, detta "base di Schauder", necessita della presenza di una topologia.
Una base di Hamel per uno spazio vettoriale formula_72 è un insieme formula_73 di vettori linearmente indipendenti, parametrizzato da un insieme ordinato formula_74 di indici, tale che ogni vettore formula_75 di formula_72 è combinazione lineare di un insieme finito di questi. 
Nel caso in cui formula_74 è un insieme finito, la definizione coincide con quella data precedentemente.
Grazie al lemma di Zorn ogni spazio vettoriale ha una base di Hamel, ed inoltre due basi di Hamel qualsiasi di uno stesso spazio vettoriale hanno la stessa cardinalità, che è pari alla dimensione (di Hamel) dello spazio vettoriale. Infine, continua a rimanere vero il fatto che ogni vettore dello spazio formula_72 si scrive in modo "unico" come combinazione lineare dei vettori di una base di Hamel.
Ad esempio, una base di Hamel per lo spazio vettoriale formula_79 formato da tutti i polinomi a coefficienti in un campo formula_80 è data dall'insieme di tutti i monomi:
Infatti ogni polinomio formula_82 è combinazione lineare di un insieme finito di questi.
L'insieme dei numeri reali può essere considerato uno spazio vettoriale su formula_83. Ne consegue che ogni numero reale può essere espresso come combinazione lineare finita di elementi presi da un sottoinsieme proprio di formula_84: tale sottoinsieme non potrà essere finito o numerabile poiché formula_84 ha la potenza del continuo (analoghe considerazioni possono essere fatte considerando formula_86 come spazio vettoriale su formula_83).
Più generalmente per uno spazio topologico è possibile estendere la definizione di Hamel in modo diverso, ammettendo somme infinite di vettori. Il senso di queste somme infinite è infatti dato dalle nozioni di limite di una successione e di serie.
Se formula_72 è uno spazio vettoriale topologico (ad esempio uno spazio di Hilbert o di Banach), un insieme ordinato formula_73 di vettori linearmente indipendenti è una "base di Schauder" (o "topologica") se lo spazio da essi generato è denso in formula_72. In altre parole, se ogni vettore formula_75 di formula_72 può essere approssimato da somme (finite) di vettori in formula_73, e quindi come limite di una somma infinita di questi:
dove formula_95 è un sottoinsieme numerabile.
Si pone il problema dell'esistenza di una base di Schauder in spazi di Hilbert o di Banach. La risposta, in generale, è negativa: infatti, dalla definizione consegue, in particolare, che uno spazio di Hilbert o di Banach che possiede una base di Schauder deve necessariamente essere separabile (infatti, dallo spazio generato dai formula_73, che è denso in formula_72 è sempre possibile estrarre un sottoinsieme denso e numerabile utilizzando le combinazioni lineari a coefficienti in formula_83)
In uno spazio di Hilbert, è di particolare importanza la nozione di base ortonormale: in uno spazio di Hilbert separabile, una base ortonormale è una base di Schauder.
L'esistenza di una base di Schauder in uno spazio di Banach non è, in genere, assicurata nemmeno aggiungendo l'ipotesi (peraltro necessaria) che si tratti di uno spazio separabile: un controesempio è stato fornito nel 1973 da Per Enflo. Un teorema di Stanisław Mazur mostra che in uno ogni spazio di Banach (a dimensione infinita) esiste sempre un sottospazio di dimensione infinita che possiede una base di Schauder.
L'esistenza di una base di Schauder consente di estendere alcuni teoremi .
Le due nozioni di basi sono generalmente molto differenti, e anche le loro cardinalità possono differire, portando a due concetti diversi di dimensione, chiamati rispettivamente "dimensione di Hamel" e "dimensione di Schauder". La dimensione di Hamel può avere cardinalità superiore a quella di Schauder (pur essendo entrambe infinite).
Ad esempio, sia formula_72 lo spazio delle funzioni continue reali definite sull'intervallo formula_100. Questo è uno spazio di Banach con la norma:
Come conseguenza della teoria delle serie di Fourier, una base di Schauder per formula_72 è costruita a partire dalle funzioni trigonometriche:
ed ha cardinalità numerabile. Una base di Hamel ha invece cardinalità non numerabile, ed è molto più difficile da costruire (e scarsamente utilizzata).
</text>
</doc>
<doc id="152903" url="https://it.wikipedia.org/wiki?curid=152903">
<title>Funzione cubica</title>
<text>
In matematica per funzione cubica si intende una funzione data da un'espressione della forma 
dove "a" è un numero reale o complesso diverso da zero; in altre parole una funzione cubica è una funzione data da un polinomio di terzo grado. La derivata di una funzione cubica è una funzione quadratica, mentre l'integrale indefinito di una funzione cubica è una funzione di quarto grado.
La derivata della funzione cubica, formula_2 e la richiesta formula_3 implicano 
Questa espressione simile alla formula per la soluzione dell'equazione quadratica, può essere usata per trovare i punti critici di una funzione cubica. Si trova quindi che 
La curva di equazione 
viene chiamata cubica bipartita. Essa si incontra nella teoria delle curve ellittiche.
Si può ottenere il suo grafico con qualche strumento per la raffigurazione delle funzioni reali applicato alla funzione 
corrispondente alla metà superiore della cubica bipartita. Essa è definita nell'insieme dell'asse reale 
La formula generale che consente di trovare i valori esatti delle radici delle funzioni cubiche è piuttosto complicata. Quindi può essere opportuno servirsi in alternativa del test della radice razionale o ricercare una soluzione numerica.
Riferiamoci alle costanti che compaiono nell'espressione 
Valutiamo 
e successivamente 
Le soluzioni sono date da
</text>
</doc>
<doc id="209254" url="https://it.wikipedia.org/wiki?curid=209254">
<title>Curva (matematica)</title>
<text>
In matematica, una curva è un oggetto "unidimensionale" e "continuo", come ad esempio la circonferenza e la retta. Una curva può giacere su un piano, nello spazio euclideo, o in uno spazio topologico più generale.
Una "curva" può essere pensata intuitivamente come la traiettoria descritta da un oggetto puntiforme che si muove con continuità in qualche spazio.Per definire la curva si fa ricorso alle nozioni di funzione continua e funzione differenziabile.
In topologia, una curva è una funzione vettoriale continua
dove formula_2 è un intervallo della retta reale e formula_3 è un qualsiasi spazio topologico.
Ad esempio, formula_3 può essere il piano cartesiano formula_5, lo spazio euclideo formula_6 o un generico spazio formula_7. L'intervallo formula_2 può essere ad esempio un intervallo chiuso formula_9, un intervallo aperto formula_10, una semiretta formula_11, ecc.
L'immagine di una curva formula_12 viene anche chiamata "sostegno", o "supporto", della curva. Spesso, con un abuso di linguaggio, per "curva" si intende il sostegno e non la funzione. In topologia, quando l'intervallo di partenza formula_2 è quello unitario formula_14 si parla di "cammino" o "arco".
Ad esempio, una circonferenza è il sostegno della curva
Una curva formula_16 che coincide sui suoi estremi, cioè tale che formula_17, è una "curva chiusa" o un "laccio".
Una curva formula_16 si dice "semplice" se è tale che presi due punti distinti formula_19, di cui almeno uno appartenente all'intervallo formula_10, risulta formula_21. In altre parole la funzione formula_22 è quasi iniettiva e la curva non ha autointersezioni con un'unica eccezione ammessa: formula_23
Una curva piana chiusa e semplice è anche detta curva di Jordan, quindi una circonferenza è una curva di Jordan.
Una curva piana è una curva a valori nel piano cartesiano formula_5. 
Se formula_26 è un omeomorfismo crescente dell'intervallo, ad esempio una funzione derivabile e biettiva con derivata positiva, allora formula_27 ottenuta componendo formula_28 e formula_22 è un'altra curva avente lo stesso sostegno di formula_22. Si dice che formula_31 è un'altra "parametrizzazione" della curva formula_22.
Una curva topologica, per quanto sembri rispondere all'esigenza di rappresentare oggetti "filiformi" e "senza spessore" che localmente sembrano una retta incurvata, può essere molto bizzarra se non si fissano delle condizioni aggiuntive. Ad esempio nel 1890 il matematico Giuseppe Peano scoprì una curva, nota ora come curva di Peano, avente come sostegno un quadrato. La curva di Koch è invece un frattale con dimensione di Hausdorff compresa tra uno e due, un oggetto dimensionalmente intermedio tra la retta e il piano.
Una condizione aggiuntiva che garantisce l'aspetto "filiforme" del sostegno è la differenziabilità: se formula_3 è il piano o un altro spazio euclideo, è possibile chiedere che formula_22 sia differenziabile in ogni punto e in questo caso si parla di "curva differenziabile" o "regolare". In una curva differenziabile, per ogni formula_35 è definita una tangente alla curva in formula_36: la tangente è il vettore delle derivate di formula_22.
Se si immagina di percorrere la curva nel tempo, la lunghezza del vettore tangente è la "velocità" della curva nel punto. La velocità può cambiare tramite riparametrizzazione della curva: data una curva, c'è sempre un'unica parametrizzazione tale che la velocità sia costantemente uno e questa parametro è la lunghezza d'arco.
In molti contesti è utile parlare di curve "lisce" che . Per questo scopo si definisce una "curva regolare a tratti" come una curva il cui dominio formula_2 è unione di intervalli successivi, su ciascuno dei quali la curva è regolare. Formalmente, si chiede che esista una partizione di un intervallo formula_39 in alcuni intervalli formula_40 tali che la restrizione della curva su ciascun formula_41 sia regolare.
Due modi utilizzati per rappresentare una curva in tre dimensioni sono la forma cartesiana e la forma parametrica.
È possibile rappresentare una curva tridimensionale in forma implicita identificando il suo supporto con il luogo di zeri di un campo vettoriale formula_42, ovvero i punti di coordinate formula_43 che verificano il sistema:
dove formula_22 e formula_31 sono funzioni di classe almeno formula_47 a valori reali. Questa rappresentazione può essere pensata come curva intersezione di due superfici in forma implicita.
Condizione sufficiente per la regolarità locale di una curva così rappresentata nell'intorno di un suo punto formula_48 è che la jacobiana:
abbia rango massimo
Una curva in forma parametrica è una funzione vettoriale di una sola variabile formula_50 del tipo:
Si può scrivere anche:
La variabile formula_35 si chiama "parametro". Una curva è una funzione di classe formula_54 in un intervallo se le funzioni formula_55, formula_56 e formula_57 hanno derivate continue in questo intervallo. Una curva formula_54 si dice "regolare" in un punto formula_59 se:
e regolare in formula_2 se ciò vale in ogni punto di formula_62. Un punto in cui si abbia formula_63 si dice "punto singolare" per la curva.
Se formula_64 è uno spazio metrico (ad esempio, il piano o uno spazio euclideo) si può usare la metrica stessa per definire la lunghezza di una curva. Sia data una curva formula_65 e una partizione dell'intervallo formula_9 cioè un insieme finito di punti formula_67 tale che:
Allora si può definire la poligonale, cioè una curva che è l'unione dei segmenti aventi vertici l'immagine degli elementi della partizione tramite formula_69. In pratica la poligonale è una curva spezzata i cui vertici appartengono alla curva originale. Più i vertici della poligonale sono numerosi e più la sua lunghezza approssimerà quella della curva.
Si può definire la "lunghezza" della curva formula_22 come estremo superiore della lunghezza della poligonale al variare della partizione formula_71:
Se questo valore non è infinito, la curva si dice "rettificabile". Le curve di Peano e di Koch non sono rettificabili.
La lunghezza di una curva non dipende dalla sua parametrizzazione, cioè non varia se si considerano "parametrizzazioni equivalenti".
Una curva derivabile è rettificabile: per ogni punto formula_73 dell'intervallo è definita una velocità, e si può dimostrare che la lunghezza definita come sopra è uguale all'integrale di questa velocità su formula_74
usando la nozione di integrale di linea si può scrivere anche:
</text>
</doc>
<doc id="205766" url="https://it.wikipedia.org/wiki?curid=205766">
<title>Dominio e codominio</title>
<text>
In matematica il dominio e il codominio di una funzione sono gli insiemi su cui è definita la funzione la quale associa a ogni elemento del dominio uno e un solo elemento del codominio.
In matematica una funzione è il dato di tre oggetti: un "dominio" formula_1, un "codominio" formula_2 e una "legge" formula_3 che associa ad ogni elemento formula_4 di formula_1 uno e un solo elemento di formula_2 che viene indicato formula_7. Una funzione viene definita indicando tutti e tre questi oggetti, che vengono raccolti nella notazione
o nella notazione equivalente
È importante notare che il dominio e il codominio devono essere definiti "prima" della legge di applicazione, e che "tutti" assieme questi oggetti definiscono una funzione. In particolare, senza indicare il dominio e il codominio non può essere definita alcuna funzione.
Ad esempio, per ogni insieme formula_10 è ben definita una funzione "identità su formula_10", con dominio formula_10, codominio formula_10 e legge di applicazione formula_14:
Omettendo dominio e codominio, la sola legge di applicazione formula_14 non è ben definita e non definisce alcuna funzione.
In alcuni ambienti si usa sottintendere il dominio e il codominio di una funzione reale di variabile reale (cioè con dominio e codominio contenuti nell'insieme dei numeri reali) quando il dominio è pari all'insieme di definizione della funzione e il codominio è l'intero insieme dei numeri reali.
Ad esempio,
Dunque nel sottintendere dominio e codominio, ci si limita a sottoinsiemi dei numeri reali e si rinuncia a studiare le proprietà di una funzione (come iniettività, suriettività, morfismo).
Come il dominio, anche il codominio è parte integrante della definizione di funzione e senza di esso non è possibile definire una legge di applicazione.
Da un punto di vista puramente computazionale, ovvero se ci si interessa alle sole immagini formula_7 dei singoli elementi del dominio, si considera il solo insieme delle immagini, o immagine formula_27, che è un sottoinsieme del codominio.
È sempre possibile definire una "nuova" funzione
che è talvolta identificata con la funzione stessa, pur avendo diverse proprietà (come suriettività o morfismo).
Ad esempio, nel calcolo di formula_29 vengono identificate le due funzioni
anche se solo la seconda è un isomorfismo tra il gruppo formula_32 e il gruppo formula_33.
In analisi complessa con dominio solitamente si indica un sottoinsieme aperto e connesso di formula_34.
In topologia per dominio si intende la chiusura di un insieme aperto. Inoltre, se il suddetto aperto manifesta la proprietà della connessione, anche il dominio può dirsi "connesso".
</text>
</doc>
<doc id="197780" url="https://it.wikipedia.org/wiki?curid=197780">
<title>Associatività</title>
<text>
In matematica, l'associatività (o proprietà associativa) è una proprietà che può avere un'operazione binaria. Significa che l'ordine di valutazione è irrilevante se l'operazione appare più di una volta in una espressione. Detta in altro modo, non sono richieste parentesi per un'operazione associativa. Si consideri ad esempio l'uguaglianza
Sommando 5 e 2 si ottiene 7, e sommando 1 si ottiene il risultato 8 per il membro a sinistra. Per valutare il membro a destra, si inizia a sommare 2 e 1 ottenendo 3, e quindi si somma 3 e 5 per ottenere 8 ancora. Quindi l'uguaglianza è verificata. Di fatto è verificata per "tutti" i numeri reali, non solo per 5, 2, e 1. Diciamo che "l'addizione nell'insieme dei numeri reali è un'operazione associativa".
Le operazioni associative sono frequenti in matematica, e infatti molte strutture algebriche richiedono esplicitamente che le loro operazioni binarie siano associative. Tuttavia, molte operazioni importanti non sono associative; un esempio comune è il prodotto vettoriale.
Formalmente, un'operazione binaria formula_1 su un insieme "S" è detta associativa se soddisfa la legge associativa:
L'ordine di valutazione non influisce sul valore di tale espressione, e si dimostra che lo stesso vale per le espressioni che contengono un numero arbitrario di operazioniformula_1. Quindi, quando formula_1 è associativa, l'ordine di valutazione può essere lasciato non specificato senza causare ambiguità, omettendo le parentesi e scrivendo semplicemente:
Seguono alcuni esempi di operazioni associative.
formula_7
Un'operazione binaria formula_1 su un insieme "S" che non soddisfa la legge associativa è detta non associativa. In simboli,
Per tale operazione l'ordine di valutazione "è" importante. La sottrazione, la divisione e l'esponenziazione sono esempi ben noti di operazioni non associative:
In generale, le parentesi devono essere usate per indicare l'ordine di valutazione, se un'operazione non associativa appare più di una volta in un'espressione. Tuttavia i matematici si accordano su un particolare ordine di valutazione per molte operazioni non associative comuni. Questa è una convenzione, e non una verità matematica.
Una operazione associativa a sinistra è un'operazione non associativa che viene valutata convenzionalmente da sinistra a destra, cioè,
mentre un'operazione associativa a destra è valutata convenzionalmente da destra a sinistra:
Esistono sia operazioni associative a sinistra che operazioni associative a destra; sotto sono dati alcuni esempi.
Le operazioni associative a sinistra includono:
Le operazioni associative a destra includono le seguenti:
Operazioni non associative per cui non è stato definito nessun ordine convenzionale di valutazione includono le seguenti:
</text>
</doc>
<doc id="198146" url="https://it.wikipedia.org/wiki?curid=198146">
<title>Distributività</title>
<text>
In matematica, e in particolare nell'algebra, la distributività (o proprietà distributiva) è una proprietà delle operazioni binarie che generalizza la ben nota legge distributiva valida per somma e prodotto tra numeri dell'algebra elementare.
Dato un (insieme) "S" e due operazioni binarie * e + su "S", diciamo che:
Si osservi che quando * è commutativa, allora le tre condizioni precedenti sono logicamente equivalenti.
La distributività si trova negli anelli e nei reticoli distributivi.
Un anello ha due operazioni binarie (chiamate comunemente "+" e "*"), e uno dei requisiti per un anello è che * sia distributiva rispetto a +.
Molti tipi di numeri (esempio 1) e di matrici (esempio 4) formano anelli.
Un reticolo è un altro tipo di struttura algebrica con due operazioni binarie, ∧ e ∨.
Se una delle due operazioni (diciamo ∧) è distributiva rispetto all'altra (∨), allora anche ∨ deve essere distributiva rispetto a ∧, e il reticolo è detto distributivo. Si veda anche la teoria degli ordini.
Gli esempi 4 e 5 sono algebre booleane, che possono essere interpretate come un tipo particolare di anello (un anello booleano) oppure come un tipo particolare di reticolo distributivo (un reticolo booleano). Ciascuna interpretazione è responsabile di differenti leggi distributive nell'algebra booleana. Gli esempi 6 e 7 sono reticoli distributivi che non sono algebre booleane.
Gli anelli e i reticoli distributivi sono entrambi tipi speciali di semianelli, una generalizzazione degli anelli.
I numeri nell'esempio 1 che non formano anelli formano comunque semianelli.
I quasi-semianelli sono un'ulteriore generalizzazione dei semianelli, e sono distributivi a sinistra ma non distributivi a destra; l'esempio 2 è un quasi-semianello.
In molte aree della matematica si considerano leggi distributive generalizzate. Questo può coinvolgere l'indebolimento delle condizioni della definizione oppure l'estensione a operazioni infinitarie. Soprattutto nella teoria degli ordini, si trovano numerose importanti varianti della distributività, alcune delle quali includono operazioni infinitarie, altre sono definite in presenza di una "sola" operazione binaria. Dettagli sulle definizioni e sulle loro relazioni si trovano nell'articolo distributività (teoria degli ordini). È inclusa anche la nozione di reticolo completamente distributivo.
In presenza di una relazione d'ordine, si può indebolire la condizione precedente sostituendo = con ≤ oppure ≥. Naturalmente questo porta a concetti sensati solo in alcune situazioni. Un'applicazione di questo principio è la nozione di sottodistributività.
</text>
</doc>
<doc id="201604" url="https://it.wikipedia.org/wiki?curid=201604">
<title>Progressione geometrica</title>
<text>
In matematica, una progressione geometrica o successione geometrica (detta talvolta, impropriamente, anche serie geometrica, vedi sotto) è una successione di numeri tali che il rapporto tra un elemento ed il suo precedente sia sempre costante. Tale costante è detta "ragione" della successione. 
In generale sarà
dove "r" ≠ 0 è la ragione e formula_2 è il primo termine della successione. 
Le progressioni geometriche hanno il vantaggio di fornire alcune semplici formule per il calcolo dei termini che le compongono. 
Il termine "n"-esimo può essere infatti definito come
La ragione è di conseguenza
e il primo termine della successione vale
Una successione di ragione 2 e fattore di scala 1 è
Una successione di ragione 2/3 e fattore di scala 729 è
Una successione di ragione −1 e fattore di scala 3 è
Una progressione geometrica non nulla mostra una crescita esponenziale o un decadimento esponenziale. In particolare se
Si confrontino questi risultati con quelli di una progressione aritmetica, la quale mostra una crescita (o una diminuzione) lineare (es. 4, 15, 26, 37, 48, ...). Si noti che i due tipi di progressione sono strettamente connessi: applicando il logaritmo ai termini di una progressione geometrica si ottiene una progressione aritmetica.
Si osserva facilmente che una progressione geometrica soddisfa la seguente condizione
interpretabile come una equazione alle differenze finite, di cui una progressione di rapporto comune "r" è soluzione.
L'equazione precedente si ritrova in molti modelli di crescita esponenziale. Ad esempio, il numero di individui in una colonia di batteri che si duplicano ad intervalli di tempo costanti segue una progressione geometrica di ragione 2.
Il termine serie geometrica è riservato alla somma di "infiniti" termini di una progressione geometrica (con "fattore di scala" unitario)
mentre la scrittura sottostante è detta "somma parziale" dei primi "n" termini della serie o "ridotta n-esima" della serie:
La formula chiusa che esprime la somma della ridotta n-esima di una serie geometrica di ragione "r" può essere ottenuta nel seguente modo: si moltiplica l'espressione per il fattore (1-"r") ottenendo
poiché tutti i termini del membro a destra dell'equazione, ad eccezione di 1 e formula_17, si annullano fra loro, posto formula_18, si può dividere per (1-"r"), ottenendo
Quindi, nel caso in cui formula_20, per formula_21 si ha formula_22, pertanto per una "serie geometrica" ("convergente") si può scrivere
</text>
</doc>
<doc id="128038" url="https://it.wikipedia.org/wiki?curid=128038">
<title>Calcolo infinitesimale</title>
<text>
Il calcolo infinitesimale è la branca fondante dell'analisi matematica che studia il "comportamento locale" di una funzione tramite le nozioni di continuità e limite, usato in quasi tutti i campi della matematica e della fisica, e della scienza in generale.
Le funzioni a cui si applica sono a variabile reale o complessa. Tramite la nozione di limite, il calcolo infinitesimale definisce e studia le nozioni di convergenza di una successione o di una serie, continuità, derivata e integrale.
Il calcolo infinitesimale poggia sull'algebra, la geometria analitica e la trigonometria. Tra le nozioni che vi appartengono e di cui si avvale vanno ricordate quelle di successione e serie, spazio metrico, funzione di variabile reale, funzione analitica. Sue branche o prodotti sono la teoria dell'integrazione e la teoria della misura, le funzioni speciali (a partire da esponenziale, logaritmo e funzioni trigonometriche), l'analisi armonica.
Fornisce la base concettuale e metodologica per lo sviluppo del modello di un qualsiasi sistema continuo riguardante per esempio fenomeni e processi fisici, astronomici, tecnologici, economici e statistici. La conoscenza del calcolo infinitesimale costituisce quindi un bagaglio culturale di primaria importanza e sul piano storico il suo sviluppo può a buon diritto considerarsi uno dei processi fondamentali per la storia del pensiero scientifico, e più in generale, per la storia della filosofia occidentale. È significativo a questo proposito osservare che nella lingua inglese, in cui più si è sviluppato in partenza, il calcolo infinitesimale viene chiamato per antonomasia "calculus".
Il calcolo infinitesimale è stato inizialmente sviluppato nel mondo scientifico greco ed ellenistico del IV e del III secolo a.C. per opera di Eudosso (metodo di esaustione), di Euclide e Anassagora fino al raggiungimento di risultati di piena maturità con Archimede.
Con il successivo progressivo decadimento della scienza nell'area mediterranea, occorre attendere l'opera dei matematici indiani Aryabhata (476-550), Bhaskara (1114-1185), Madhava (1350-1425) e della scuola del Kerala per avere innovazioni come il teorema noto come teorema di Rolle, il passaggio al limite per una variabile tendente all'infinito e la manipolazione di alcune serie.
Per uno sviluppo sistematico del calcolo infinitesimale occorre attendere il periodo del recupero europeo dello spirito scientifico ellenistico nel secolo XVI (Tartaglia) e soprattutto nel secolo XVII. Dopo gli avanzamenti dovuti a Cavalieri, Barrow, Cartesio, Fermat, Huygens e Wallis, negli anni dal 1670 al 1710, ad opera principalmente di Pietro Mengoli, Newton e Leibniz furono posti i fondamenti del calcolo infinitesimale moderno e fu raggiunta la piena consapevolezza della sua portata per lo sviluppo di metodi e modelli per lo studio quantitativo degli oggetti dell'indagine scientifica. Nel secolo XVIII si assiste all'ampliamento dei metodi e delle applicazioni, con i Bernoulli, Eulero, Lagrange, Laplace, pur nella mancanza di fondamenti rigorosi. Una prima revisione critica dei fondamenti fu sviluppata da Cauchy intorno al 1821 sulla base della nozione di limite introdotta da d'Alembert nel 1765. In Giappone fu invece Kōwa Seki che per primo sviluppò i metodi fondamentali del calcolo integrale.
Per opera dello stesso Cauchy, e di matematici come Poisson, Liouville, Fourier gli obiettivi dell'analisi infinitesimale si ampliano a comprendere l'analisi complessa, le equazioni alle derivate parziali e l'analisi armonica. Intorno al 1850 Riemann introduce la teoria dell'integrale che porta il suo nome.
Intorno al 1860 Dedekind precisa la nozione di numero reale (altro recupero di una nozione ellenistica, ben chiara negli Elementi di Euclide). Questa consente che, intorno al 1870, sia precisata la definizione delle basi del calcolo infinitesimale per opera di Weierstrass e di vari altri matematici (Eduard Heine, Georg Cantor, Charles Méray, Camille Jordan...). Da allora le idee e le tecniche di calcolo infinitesimale - diventate analisi matematica o “analisi standard”, evitando di fare riferimento al concetto oscuro di infinitesimo - sono bagaglio essenziale per chi si dedica alla scienza e alla tecnologia.
All'inizio del XX secolo sono sviluppate teorie che forniscono basi (o “fondamenti”) più generali, astratte ed efficaci per lo studio dei problemi infinitesimali. Basti ricordare la teoria assiomatica degli insiemi (scuola di Hilbert), la teoria della misura (Lebesgue), la nozione di spazio di Hilbert, la nozione di spazio normato e quindi la definizione dell'analisi funzionale principalmente per opera di Banach. Infine Robinson tentò di rifondare l'analisi sugli infinitesimi, recuperando su basi logiche più rigorose la semplicità del metodo di Leibniz introducendo l'analisi non standard.
</text>
</doc>
<doc id="126866" url="https://it.wikipedia.org/wiki?curid=126866">
<title>Intervallo (matematica)</title>
<text>
, un intervallo è un sottoinsieme dei numeri reali formato da tutti i punti della retta reale che sono compresi tra due estremi formula_1 e formula_2. Gli estremi possono (ma non devono necessariamente) appartenere all'intervallo e possono essere infiniti.
Formalmente, un sottoinsieme formula_3 dei numeri reali formula_4 o di un altro insieme ordinato è un intervallo se per ogni coppia di elementi formula_5 e formula_6 di formula_3, ogni altro elemento formula_8 tale che formula_9 sta anch'esso in formula_3. In formula_4 gli intervalli corrispondono agli insiemi convessi.
Gli intervalli di formula_4 sono quindi gli insiemi seguenti (dove formula_1 e formula_2 sono due numeri reali tali che formula_15 (intervallo chiuso)
I punti formula_1 e formula_2 sono gli estremi dell'intervallo. Quindi una parentesi quadra formula_24 formula_25 indica che l'estremo appartiene all'intervallo, mentre una parentesi tonda formula_26 formula_27 indica che non vi appartiene. Una notazione alternativa usa formula_25 e formula_24 rispettivamente al posto di formula_26 e formula_27. Entrambe le notazioni fanno parte dello standard ISO 31-11 e del successivo ISO 80000-2 come equivalenti sebbene la notazione che prevede l'utilizzo delle parentesi tonde per indicare gli intervalli aperti sia in assoluto la più utilizzata.
I primi quattro intervalli hanno lunghezza formula_32, i cinque seguenti hanno lunghezza infinita, il punto e l'insieme vuoto hanno lunghezza formula_33.
L'intervallo unitario è l'intervallo chiuso formula_34.
Raramente, il simbolo ÷, chiamato obelo, viene usato in Italia per indicare un intervallo numerico. Ad esempio 3 ÷ 7 vuol dire 'da tre a sette', estremi compresi.
</text>
</doc>
<doc id="110461" url="https://it.wikipedia.org/wiki?curid=110461">
<title>Serie geometrica</title>
<text>
In matematica, una serie geometrica è una serie tale per cui il rapporto tra due termini successivi è costante.
La serie geometrica è una serie del tipo formula_1. In modo equivalente, può essere definita come il limite della successione delle somme parziali formula_2, in cui:
La somma parziale formula_4-esima di una serie geometrica è dunque la somma per formula_5 che va da zero ad formula_4 di formula_7. Il rapporto di ogni termine della somma rispetto al termine precedente è costantemente uguale a formula_8 ed è detto "ragione della serie".
Questo tipo di serie ricorre con una particolare frequenza nell'analisi degli algoritmi; in molti casi il valore di questi ultimi può essere calcolato direttamente con le formule illustrate successivamente. Una delle espressioni più comuni è proprio la somma parziale della nota serie geometrica.
Possiamo dimostrare che formula_9 in diversi modi.
Osserviamo che per formula_10 si ottiene formula_11 pertanto la base induttiva è verificata. Supponiamo che la formula sia vera per formula_4, ovvero che la somma dei primi formula_4 termini valga proprio formula_14, allora la somma dei primi formula_15 termini vale
Pertanto la formula, supposta vera per i primi formula_4 termini, è vera anche per i primi formula_15 termini, pertanto è stato dimostrato per induzione matematica che: formula_19
Osserviamo che tale formula è valida per formula_20, se formula_21 la somma vale banalmente formula_22.
Se la serie non parte da formula_23, ma da un altro termine formula_24, allora
Derivando la somma rispetto a formula_8 si possono trovare formule per somme del tipo
ad esempio:
La serie ha il seguente carattere:
Se infatti formula_39 la somma della serie esiste e vale
Quest'ultima formula è valida in ogni algebra di Banach con la condizione che la norma di formula_8 sia minore di formula_36, e anche nel campo dei numeri p-adici se formula_43. In particolare è valida nel campo dei numeri complessi con l'usuale definizione di valore assoluto.
Come nel caso delle somme finite, possiamo derivare la serie per trovare le formule di somme analoghe. Ad esempio:
Questa formula naturalmente è valida solo per formula_39.
Per effettuare la stima della somma geometrica finita conoscendo quella infinita, spezziamo la serie come segue
ricordando che la serie geometrica ha somma pari a formula_47 otteniamo che
Se si pone che formula_49 si ha che:
La funzione formula_51 viene chiamata serie geometrica troncata. La serie geometrica troncata è alla base delle stime di somme molto complesse. Utilizzando l'operatore formula_52 (dove con formula_53 si indica la derivata) si ha che 
riconducendosi alla serie geometrica troncata. Quindi si ha
Si vuole calcolare la seguente sommatoria:
Consideriamo la funzione 
e osserviamo che la sua derivata è data da 
questo significa che 
e quindi il nostro problema si riduce a valutare la derivata di formula_60 in formula_61. Poiché formula_62 per ogni formula_63 otteniamo 
e di conseguenza 
</text>
</doc>
<doc id="130404" url="https://it.wikipedia.org/wiki?curid=130404">
<title>Divisione (matematica)</title>
<text>
La divisione è l'operazione aritmetica inversa della moltiplicazione.
Più specificamente, se 
dove "b" è diverso da zero, allora 
(da leggersi ""c" diviso "b"").
Ad esempio, 6 : 3 = 2, dato che 2 × 3 = 6.
La divisione per zero non viene definita.
Nell'espressione sopra, "a" rappresenta il "quoziente" ("quoto" nel caso di divisione senza resto), "b" il "divisore" (cioè la quantità che divide) e "c" il "dividendo" (cioè la quantità da dividere).
La divisione gode della "proprietà invariantiva" ovvero il quoziente non cambia se dividendo e divisore sono moltiplicati per una stessa quantità diversa da zero (il resto invece risulta moltiplicato per quella quantità).
L'espressione "c" : "b" viene anche scritta ""c"/"b"" (letta ""c" su "b"", o ""c" fratto "'b"", o ""c" "b"-esimi" ; se "b" è un intero positivo diverso da 2 questo si legge come ordinale, plurale se "c" è diverso da 1, es. 2/3 si legge "due terzi" ma 3/2 si legge "tre mezzi"), specialmente nelle matematiche superiori, incluse le applicazioni alla scienza e all'ingegneria, e nei linguaggi di programmazione.
Tale forma viene anche spesso usata come forma finale di una frazione.
La divisione fra due numeri interi "a" e "b", con "b"≠0, consiste invece nel trovare una coppia di interi "q" ed "r", detti "quoziente" e "resto", tali che "a" = "b" × "q" + "r" e 0 ≤ "r" &lt; | "b" |. (Si dimostra che tale coppia di interi esiste ed è unica). Quando "r" = 0, il risultato della divisione "q" viene talvolta detto "quoto".
In inglese e sulle calcolatrici elettroniche, il simbolo della divisione è l'obelo che ha una barretta orizzontale tra i due punti: "c" ÷ "b". In italiano questo uso non è mai stato attestato: l'obelo è solo usato a volte in ingegneria e chimica per indicare un intervallo. Nell'uso inglese, invece, i due punti si utilizzano solo per il concetto correlato di rapporto.
A/B = C indica che il denominatore B è contenuto nel numeratore A una quantità di volte pari a C.
infatti come si evince dall'esempio 0,5 è contenuto 2 volte dentro il numeratore 1.
Un secondo modo di leggere la divisione: A/B = C indica che ad una unità del denominatore (B) corrispondano C unità del numeratore A.
cioè: a ogni unità del denominatore spetteranno 2 unità del numeratore.
Questo secondo metodo di lettura della divisione è un approccio particolarmente utile per comprendere il calcolo delle percentuali e il concetto di derivata. Infatti:
Cioè a ogni euro del prezzo della maglietta che si trova al denominatore corrispondono venti centesimi dei 7 euro sconto. Perciò moltiplicando 0,2 per 100 si ha la percentuale dello sconto pari al 20%.
Utilizzando la tavola pitagorica, si possono dividere due numeri interi con carta e penna.
Se il dividendo ha una parte frazionaria espressa come frazione decimale, si può continuare l'algoritmo dopo le unità; se è il divisore ad avere una parte frazionaria, basta spostare la virgola a destra dello stesso numero di posizioni - aggiungendo se necessario degli zeri a destra del dividendo - fino a che il divisore diventa un numero intero. Pertanto, per fare la divisione 245,7 : 3,78 si eseguirà quella equivalente 24570 : 378.
Un'altra possibilità che si ha per semplificare i conti è vedere se dividendo e divisore abbiano un fattore comune, ed eliminarlo; la divisione di cui sopra è dunque equivalente a 12285 : 189 (eliminando il fattore 2) e ancora a 1365 : 21 (eliminando un fattore 9), 455 : 7 (con un fattore 3), da cui si ottiene subito il risultato finale 65.
Si può calcolare la divisione con un abaco, scrivendo ripetutamente il dividendo e sottraendo man mano il divisore, spostato a sinistra quanto più possibile. Ogni volta che occorre riportare a destra il divisore, si passerà a una nuova cifra anche per il quoziente. Il procedimento risulta pertanto abbastanza simile a quello della divisione su carta, anche se in quel caso c'è la scorciatoia di utilizzare delle moltiplicazioni per ridurre il numero di sottrazioni necessarie.
Nella aritmetica modulare, alcuni numeri hanno un inverso moltiplicativo rispetto al modulo: ad esempio, in base 7, 3 ha come inverso 5. In questo caso, la divisione per 3 può essere calcolata moltiplicando per 5; questo approccio è utile in computer che non hanno una istruzione di divisione veloce.
La divisione tra interi - anche non considerando la divisione per zero, che non è definita, non è un'operazione chiusa; vale a dire, esistono coppie di numeri "a" e "b" tali che non esiste alcun intero "c" per cui "a" : "b" = "c". In questi casi si possono dare diverse possibili risposte:
Quando si fa una divisione tra interi in un linguaggio di programmazione occorre verificare attentamente la definizione. Nel linguaggio C, ad esempio, la divisione tra interi è definita come nel caso 4 qui sopra, e il risultato sarà pertanto un intero troncato; in altri linguaggi come MATLAB, invece, si inizia a convertire gli interi in numeri reali (o più correttamente numeri di macchina), e si ottiene così un numero reale come risposta, come nel caso 2 sopra.
A differenza del caso precedente, i numeri razionali sono chiusi rispetto alla divisione, se il divisore non è 0. Si può definire il risultato della divisione tra due numeri razionali "p"/"q" e "r"/"s" come il valore
Tutti e quattro i valori sono interi, e solo "p" può valere 0. Questa definizione assicura che la divisione è l'operazione inversa della moltiplicazione: si ricava infatti subito che
I casi particolari riguardano l'operazione di divisione quando 0 è il dividendo o il divisore. Il risultato di queste operazioni può essere indeterminato oppure impossibile. Detto "a" un qualsiasi numero reale diverso da zero, vale che:
Si verifica facilmente grazie alla definizione: l'unico numero che moltiplicato per un numero non nullo dà zero è lo zero stesso in quanto vale la legge di annullamento del prodotto.
Anche in questi due ultimi casi, il metodo per risolvere l"'impasse" è considerare la definizione dell'operazione di divisione, cioè come l'inverso della moltiplicazione. Risolvere i casi 2 e 3 equivale a cercare la soluzione dell'equazione 0 × "x" = "a"; questa ha:
In entrambe i casi non è possibile determinare il risultato che, come sappiamo, deve essere unico. Per tale motivo la divisione per zero non viene definita ed è priva di significato.
Il risultato della divisione di due numeri reali è un altro numero reale, se il divisore non è 0. Dati "a" e "b", si definisce "a"/"b" = "c" se e solo se "a" = "cb" e "b" ≠ 0.
Anche per i numeri complessi, così come per i numeri reali, la divisione è un'operazione chiusa eccetto il caso in cui il divisore sia zero, nel qual caso l'operazione non è definita.
Se si esprimono i numeri complessi con le comuni coordinate, il risultato della divisione di formula_4 per formula_5, dove "p", "q", "r" e "s" sono numeri reali e "r" e "s" non possono essere entrambi nulli, è dato da
Se i numeri complessi sono espressi mediante le coordinate polari, l'espressione è più semplice da esprimere e ricordare: il risultato della divisione tra formula_7 e formula_8, con "p", "q", "r" e "s" reali e finiti e "r" diverso da zero, è dato da
La divisione tra due polinomi (a coefficienti interi) può anch'essa venire definita come l'operazione inversa della moltiplicazione: come nel caso dei numeri interi, generalmente si otterrà un polinomio quoziente e un resto. Per maggiori informazioni su come viene condotta l'operazione, si veda la regola di Ruffini.
Nelle algebre astratte, come quella delle matrici e nei quaternioni, frazioni come formula_10 sono tipicamente definite come formula_11 oppure formula_12, quando "b" è un elemento invertibile (vale a dire, esiste un altro elemento "c" tale che "bc" = "cb" = 1, dove 1 è l'identità moltiplicativa; l'elemento "c" viene generalmente scritto come "b"). In un dominio d'integrità, in cui possono non esistere gli inversi, più che di "divisione" si può parlare di "cancellazione", nelle equazioni della forma "ab" = "ac" o "ba" = "ca", dove "a" viene cancellato da entrambi i membri.
La funzione quoziente di due funzioni "f" e "g" è la funzione "h" il cui valore in un punto "x" è dato da formula_13. È definita nell'intersezione dei due domini di "f" e "g", ad eccezione dei valori "x" che annullano la "g".
La derivata del quoziente di due funzioni è data dalla regola del quoziente:
Non esiste una regola generale per integrare il quoziente di due funzioni.
</text>
</doc>
<doc id="130415" url="https://it.wikipedia.org/wiki?curid=130415">
<title>Uguaglianza (matematica)</title>
<text>
In matematica l'uguaglianza indica comunemente una relazione binaria di equivalenza fra due enti, detti membri dell'uguaglianza. 
Rappresenta uno dei concetti più importanti e fondamentali introdotti a livello della logica di una teoria.
Più formalmente per uguaglianza/Identità, in una teoria del primo ordine, si intende una relazione, di solito definita col simbolo "=", che verifichi i seguenti assiomi:
dove formula_3 è un predicato contenente la variabile libera formula_4
L'assioma di riflessività e lo schema degli assiomi caratterizzano formalmente l'idea intuitiva di uguaglianza: il primo assioma afferma che ogni oggetto è uguale a se stesso, mentre il secondo afferma che due oggetti uguali verificano le stesse proprietà. In particolare lo schema degli assiomi afferma che due oggetti uguali sono essenzialmente la stessa cosa, infatti non c'è modo di distinguerli dato che: "" tutto ciò che è vero per uno, è vero per l'altro"".
In particolare in una logica del secondo ordine, gli assiomi di uguaglianza si possono riformulare nel seguente modo:
dove formula_7 è un predicato.
Si noti, che in questo caso, si hanno solo due assiomi, mentre nell'altra definizione abbiamo uno schema di assiomi, questo è dovuto al fatto che nella logica del primo ordine non si può quantificare sulle proprietà diversamente da quanto avviene nella logica del secondo ordine.
L'uguaglianza è una relazione di equivalenza; tuttavia essa è molto particolare in quanto tutte le sue classi di equivalenza hanno cardinalità 1, e ciò è dovuto al fatto che l'uguaglianza gode di proprietà più restrittive rispetto alle consuete proprietà delle relazioni di equivalenza. 
Una relazione di uguaglianza definita su un dato universo ha per grafico, nel prodotto cartesiano formula_8, l'insieme formula_9 
</text>
</doc>
<doc id="208737" url="https://it.wikipedia.org/wiki?curid=208737">
<title>Diagonale principale</title>
<text>
In matematica, e più in particolare in algebra lineare, la diagonale principale di una matrice quadrata è la diagonale che va dall'angolo in alto a sinistra a quello in basso a destra. Ad esempio, la seguente matrice ha valori non nulli solo nella diagonale principale:
Una matrice di questo tipo è detta matrice diagonale. Un caso particolare di matrice diagonale, in cui tutti i valori della diagonale principale sono uguali ad 1, è la matrice identità. La somma di tutti i valori che si trovano sulla diagonale principale è detta traccia della matrice.
La diagonale opposta, dall'angolo in alto a destra a quello in basso a sinistra, è detta antidiagonale o diagonale secondaria.
Talvolta risulta utile considerare la diagonale di una matrice che non è quadrata:
</text>
</doc>
<doc id="114522" url="https://it.wikipedia.org/wiki?curid=114522">
<title>Funzione razionale</title>
<text>
In matematica, una funzione razionale è una funzione esprimibile come rapporto fra polinomi, in modo analogo ad un numero razionale che è un numero esprimibile come rapporto fra interi.
Una funzione razionale in una variabile è una funzione del tipo:
dove formula_2 e formula_3 sono due polinomi. Ad esempio:
è una funzione razionale a una variabile.
Una funzione è detta "razionale intera" quando al secondo membro figura un polinomio. Per ottenere il valore della variabile dipendente formula_5, si svolgono operazioni costituite da somme, differenze e prodotti. Alla formula_6 può quindi essere assegnato qualsiasi valore.
Una funzione è detta "razionale fratta" quando al secondo membro figura una frazione il cui numeratore e denominatore sono polinomi. In questo caso, per ottenere il valore della variabile dipendente formula_5, oltre alle operazioni costituite da somme, differenze e prodotti, occorre eseguire l'operazione di divisione. Alla formula_6 può quindi essere assegnato qualsiasi valore che non annulli il denominatore.
Una funzione razionale può essere reale o complessa, a seconda che i coefficienti dei polinomi siano numeri reali o complessi. Più in generale, i coefficienti devono essere elementi di un campo formula_9 (che può essere appunto formula_10 oppure formula_11).
Il dominio (anzi, più precisamente l'insieme di definizione) della funzione è l'insieme di tutti i valori formula_12 di formula_9 che non sono radici di formula_14. Ovvero, tutti gli formula_12 tali che il denominatore formula_3 è diverso da zero. Infatti solo per questi valori ha senso dividere formula_2 per formula_3.
Ad esempio, la funzione razionale descritta sopra, se considerata sui numeri reali, è definita su tutto formula_10 meno il punto formula_20. Se considerata sui numeri complessi, è definita su tutto formula_11 meno le tre radici cubiche dell'unità
Per comodità, nella discussione che segue si suppone che i polinomi formula_2 e formula_24 non abbiano radici in comune.
Una funzione è "irrazionale" quando la variabile indipendente formula_6 figura sotto segno di radice:
L'espressione "funzione razionale" è anche usata per descrivere un rapporto fra polinomi con più variabili, come ad esempio:
Come sopra, la funzione è definita su tutti i punti di formula_27 (dove formula_28 è il numero di variabili) per cui il denominatore non si annulla. Tale insieme non è però generalmente un numero finito di punti: si tratta di una più generale varietà affine.
Se considerata sui numeri reali, una funzione razionale può avere asintoti, che possono essere agevolmente individuati nel modo seguente. 
Se considerata sui numeri complessi, una funzione razionale presenta un polo su ogni radice di formula_24, di ordine pari all'ordine della radice. Una funzione razionale è quindi una particolare funzione meromorfa formula_45 definita sulla sfera di Riemann formula_46. Tra queste, le trasformazioni di Möbius:
giocano un ruolo importante in analisi complessa ed in geometria proiettiva. Sono le uniche funzioni meromorfe che inducono una corrispondenza biunivoca sulla sfera di Riemann.
La decomposizione in fratti semplici di una funzione razionale è la scrittura della frazione tramite un polinomio (che può essere nullo) sommato ad una o più frazioni con un denominatore più semplice. Tale metodo fornisce un algoritmo che consente di valutare le primitive di una funzione razionale.
Per illustrare l'idea del procedimento, sia data una funzione razionale formula_48, in cui formula_49 e formula_50 sono polinomi, e si consideri la fattorizzazione formula_51 del denominatore. Per ogni fattore che ha la forma formula_52 si considerano le frazioni formula_53, mentre per ogni fattore che ha la forma formula_54 si considerano le frazioni:
Si ottiene così la scrittura:
e calcolando i coefficienti formula_57 e formula_58 si trova una decomposizione che consente, analizzandone ogni singolo termine, di integrare la frazione di partenza. Essa conduce quindi formula_59 ad un'espressione del tipo:
dove formula_61 e formula_62 sono polinomi di grado inferiore rispetto a formula_49 e formula_50.
Se si applica la decomposizione fin dove è possibile si ottiene che il denominatore di ogni termine è una potenza di un polinomio non fattorizzabile e il numeratore è un polinomio di grado inferiore di quello del polinomio non fattorizzabile.
</text>
</doc>
<doc id="1543724" url="https://it.wikipedia.org/wiki?curid=1543724">
<title>Analisi matematica</title>
<text>
L'analisi matematica è il ramo della matematica che si occupa delle proprietà che emergono dalla scomposizione infinita di un oggetto denso. Si fonda sul calcolo infinitesimale, con il quale, attraverso le nozioni di limite e continuità, studia il comportamento locale di una funzione utilizzando gli strumenti del calcolo differenziale e del calcolo integrale.
Introducendo per il "calcolo" concetti problematici, quali quello di infinito e di limite, si può passare all"'indagine" che le ha permesso di divenire basilare in diverse discipline scientifiche e tecniche (dalle scienze naturali all'ingegneria, dall'informatica all'economia), dove viene spesso coniugata con l'analisi numerica.
L'analisi matematica nasce durante la seconda metà del XVII secolo, grazie a Isaac Newton e Gottfried Leibniz che indipendentemente introdussero i concetti fondamentali del calcolo infinitesimale. Inizialmente l'analisi matematica puntava alla rappresentazione geometrica nel piano cartesiano delle funzioni, nel tentativo di rispondere a quesiti su calcolo di aree e caratteristiche geometriche di una curva. Lo sviluppo dell'analisi nel XVIII secolo fu anche fortemente motivato dalla fisica portando allo sviluppo e all'elaborazione della meccanica razionale.
Dalla fine del XVIII secolo si introdusse il concetto di limite, passando da un'interpretazione intuitiva basata su suddivisioni successive, già introdotta, nel V secolo a.C., dal filosofo eleatico Zenone nella formulazione delle sue aporie (Paradossi di Zenone), fino ad arrivare all'analisi matematica dei giorni nostri, che introdusse metodologie per il calcolo di un valore del limite. Questo portò ad una rivoluzione completa della materia che rianalizzò nozioni e teoremi senza più avvalersi di giustificazioni geometriche ma basandosi su concetti di numero e di insieme. Questo permise l'analisi più approfondita di geometrie non euclidee e di spazi a dimensione maggiore di tre.
Il concetto di insieme costituisce l'elemento fondante di quella parte della matematica che è la teoria degli insiemi. Con questo termine si indica ogni raggruppamento, collezione, aggregato di elementi, in modo indipendente dalla loro natura.
Per definire alcune proprietà di notevole interesse e diffuso uso (quali la continuità e la derivabilità) sono necessari i concetti di base della topologia, in particolare quello di intorno, e il concetto di distanza in uno spazio metrico.
Il concetto di funzione è fondamentale per gli scopi dell'analisi matematica. Attraverso operazioni più avanzate (come quella di limite) viene definita una serie di proprietà fondamentali di notevole utilità negli sviluppi teorici e nelle applicazioni pratiche. Tra di esse si possono elencare:
Un importante ruolo è svolto dalle cosiddette funzioni elementari, quali:
Di particolare importanza, nel XX secolo, sono stati gli avanzamenti nello studio degli spazi di funzioni, visti come particolari spazi vettoriali topologici infinito-dimensionali, nell'ambito dell'Analisi funzionale.
Il concetto di limite, fondamentale in analisi, è stato definito coerentemente solo nell'800 ma esso era stato compreso intuitivamente da matematici del calibro di Wallis, Eulero, Bernoulli, Newton, Leibniz e addirittura sembra che già Archimede l'avesse compreso intuitivamente. Il limite è, in parole povere, un valore a cui il valore di una funzione si avvicina sempre di più (senza necessariamente raggiungerlo) man mano che l'argomento si avvicina a zero o a infinito o a qualsiasi altro numero. Per esempio formula_6. Infatti se facciamo aumentare sempre di più formula_7, formula_8 sarà sempre più vicino a zero.
Il limite di una funzione o successione può:
Attraverso il concetto di limite di una successione è possibile definire la somma di un numero infinito di elementi. Ad esempio, è possibile dare un senso all'espressione
che è uno dei tanti modi per descrivere il numero di Nepero formula_13.
Una somma infinita di elementi è detta serie e viene indicata, in genere, con la seguente notazione:
formula_14 oppure con formula_15 .
Dunque, ponendo formula_16, il numero di Nepero formula_17, con le precedenti notazioni, può essere scritto in uno nei seguenti modi
formula_18 oppure formula_19 .
Analogamente a quanto accade per i limiti, la somma di infiniti elementi può essere finita, infinita, o addirittura non essere definita come nel caso della serie formula_20 , detta serie di Grandi.
Il concetto di derivata occupa un ruolo fondamentale nel calcolo infinitesimale e in tutta l'analisi matematica. Definita come limite del rapporto incrementale, la derivata quantifica il tipo di crescita di una funzione, ed ha applicazione in tutte le scienze.
Tramite la nozione di derivata si definiscono e studiano i concetti di massimo e minimo di una funzione, di concavità e convessità: la derivata è quindi uno strumento fondamentale per lo studio di una funzione.
Tramite una lista di regole di derivazione è possibile calcolare la derivata di qualsiasi funzione definita combinando funzioni elementari.
Il concetto di derivata si estende anche a funzioni a più variabili tramite la nozione di derivata parziale.
L'integrale è un altro strumento fondamentale del calcolo infinitesimale. Viene utilizzato soprattutto per calcolare aree e volumi di figure curve, quali ad esempio l'ellisse o la parte del piano cartesiano delimitata da una funzione.
Per il teorema fondamentale del calcolo integrale, l'integrale risulta essenzialmente essere una operazione inversa a quella della derivata. Se ne differenzia però poiché, contrariamente a quanto accade per la derivata, non ci sono degli algoritmi che permettano di calcolare l'integrale di qualsiasi funzione definita a partire da funzioni elementari. Vi sono comunque numerosi metodi di integrazione con cui risolvere buona parte degli integrali più semplici, spesso riassunti in opportune tavole.
A partire dal XIX secolo, il concetto di integrale si è legato sempre più al concetto di misura. La definizione stessa di integrale è legata ad un problema fondamentale di come "misurare" lunghezze, aree e volumi di sottoinsiemi della retta, del piano, dello spazio. Ciascuna possibile risposta a questa domanda fornisce una definizione di integrale: le definizioni più utilizzate sono l'integrale di Riemann e l'integrale di Lebesgue.
La serie di Taylor di una funzione analitica permette di scrivere la funzione come una serie di potenze. Per una funzione analitica formula_21 si ha che:
dove formula_23 è il fattoriale di formula_24 e formula_25 è la derivata formula_7-esima della formula_27 nel punto formula_28 Se formula_29 la serie viene chiamata serie di Maclaurin ed è
Lo studio di funzione è lo studio dell'andamento o grafico di una funzione evidenziandone massimi e minimi (relativi ed assoluti), asintoti (orizzontali e verticali), flessi (orizzontali e verticali), concavità e area sottesa, attraverso l'uso di strumenti propri dell'analisi matematica sopraesposti ovvero limite, derivata e integrale.
Il calcolo infinitesimale è il fondamento dell'analisi matematica: comprende la nozione di limite e varie applicazioni legate allo studio delle funzioni, che possono essere a variabile reale o complessa. Tramite la nozione di limite, il calcolo infinitesimale definisce e studia le nozioni di convergenza di una successione o di una serie, continuità, derivata e integrale.
Il calcolo infinitesimale è alla base dell'analisi matematica ed è uno strumento usato in quasi tutti i campi della matematica e della fisica e della scienza in generale.
</text>
</doc>
<doc id="1539729" url="https://it.wikipedia.org/wiki?curid=1539729">
<title>Algebra</title>
<text>
L'algebra (dall'arabo الجبر, "al-ğabr", 'completamento') è una branca della matematica che tratta lo studio di strutture algebriche, relazioni e quantità.
Il termine algebra (dall'arabo الجبر, "al-ǧabr" che significa "unione", "connessione" o "completamento", ma anche "aggiustare") deriva dal libro del matematico persiano , intitolato "Al-kitāb al-muḫtaṣar fī ḥīsāb al-ǧabr wa l-muqābala" ("Compendio sul calcolo per completamento e bilanciamento"), conosciuto anche nella forma breve "Al-kitāb al-ǧabr wa l-muqābala", che tratta la risoluzione delle equazioni di primo e di secondo grado.
Ci sono anche alcune testimonianze su problemi algebrici semplici dell'Antico Egitto, della Grecia arcaica e della Mesopotamia, di matematici che fecero uso di proprietà attinenti all'algebra elementare.
Algebra totalmente priva di simboli, i passaggi sono descritti a parole, secondo la tradizione di Muḥammad ibn Mūsā al-Ḫwārizmī.
Algebra descrittiva, ma con notazioni simboliche, come quella usata dal greco Diofanto di Alessandria.
Algebra in cui i concetti sono rappresentati in simboli, utilizzata oggi in tutto il mondo è nata nell'antica India e poi sviluppata nel XVI secolo dai matematici europei.
Un numero è un oggetto astratto, usato per misurare una quantità. I numeri più utilizzati sono i numeri naturali:
Aggiungendo a questi i numeri negativi, tramite il segno meno, si ottengono tutti i numeri interi:
Aggiungendo a questi le frazioni si ottengono tutti i numeri razionali:
Infine, i numeri reali contengono molti altri numeri che non possono essere espressi come frazioni, quali ad esempio:
Aggiungendo a questi un elemento formula_5, chiamato unità immaginaria, tale che formula_6, si ottengono i numeri complessi:
Gli insiemi formati dai numeri naturali, interi, razionali, reali e complessi sono indicati con le lettere:
Ciascun insieme è contenuto nel successivo, come indicato dal simbolo formula_9 di inclusione insiemistica. Ad esempio, il numero formula_10 non è un numero naturale, ma è un numero intero: quindi è anche razionale, reale e complesso.
Con le operazioni aritmetiche di addizione, sottrazione, prodotto e divisione è possibile manipolare i numeri e scrivere espressioni del tipo
Lo stesso numero può essere scritto in modo diverso, ad esempio:
L'algebra elementare è un'evoluzione dell'aritmetica: oltre ai numeri e alle quattro operazioni, in algebra si fa uso di simboli letterali che (a seconda del contesto) possono essere considerati numeri "costanti" o "variabili". Ad esempio:
Usando simboli letterali è possibile enunciare dei teoremi che sono validi in contesti molto generali. Ad esempio, il quadrato del binomio
è una uguaglianza valida per qualsiasi valore di formula_15 e formula_16.
Una equazione è una uguaglianza che può contenere alcune variabili, dette "incognite". L'equazione è verificata solo per alcuni valori delle incognite, detti "soluzioni". Determinare le soluzioni di una equazione è un problema centrale in algebra. Ad esempio, nell'equazione di primo grado
la lettera formula_15 è una costante, mentre formula_19 è l'incognita da determinare. Questa equazione ha una sola soluzione; data da
Un polinomio è una espressione algebrica ottenuta manipolando alcune costanti e variabili con le operazioni di addizione, sottrazione e moltiplicazione (ma "non" la divisione). Ad esempio:
è un polinomio con variabile formula_19. Un polinomio può avere più di una variabile, ad esempio
ha tre variabili formula_24.
Una radice di un polinomio con una sola variabile formula_19 è un valore numerico formula_26 per cui vale
Determinare le radici di un polinomio equivale quindi a risolvere una equazione, in cui il polinomio viene posto uguale a zero. Esistono delle formule generali per determinare le radici di un polinomio di grado 1, 2, 3 o 4. Ad esempio, un polinomio di secondo grado
può avere al massimo due radici reali, determinate dalla formula
Se l'argomento del radicale formula_30 è negativo, il polinomio non ha radici reali. Per il teorema di Abel-Ruffini, non esistono formule risolutive generali per equazioni di grado maggiore o uguale a 5.
Un polinomio può non avere radici reali. Il teorema fondamentale dell'algebra asserisce però che ne esiste sempre (almeno) una radice complessa.
Un numero reale (o complesso) è algebrico se è radice di un polinomio a coefficienti interi. Ad esempio, ogni numero razionale formula_31 è algebrico, perché radice del polinomio
che ha coefficienti formula_33 e formula_34 interi. La radice formula_35-esima reale formula_36 di un intero formula_37 è anch'esso un numero algebrico, radice del polinomio
Più in generale, tutti i numeri ottenibili a partire dagli interi usando le quattro operazioni ed i radicali sono algebrici. Ad esempio:
è un numero algebrico. Esistono però algebrici non scrivibili in questa forma, per il teorema di Abel-Ruffini. Fra i numeri complessi, l'unità immaginaria formula_5 è algebrica perché radice del polinomio formula_41.
Un numero reale (o complesso) è trascendente se non è algebrico. I numeri pi greco formula_42 e la costante di Nepero formula_43 sono trascendenti.
Una struttura algebrica è un insieme dotato di una o più operazioni che soddisfano determinati assiomi. Sulla base di questi assiomi è quindi possibile dimostrare vari teoremi che risultano validi in contesti molto generali. Le strutture algebriche hanno un ruolo centrale nell'algebra astratta e in tutta la matematica moderna.
Un gruppo è un insieme formula_44 dotato di una operazione binaria, che può essere indicata con il simbolo formula_45, che soddisfa gli assiomi seguenti.
Ad esempio, i numeri interi formano un gruppo con l'operazione formula_46 di addizione. L'insieme formula_44 e l'operazione formula_45 sono entrambi importanti nella struttura di gruppo: per identificare il gruppo degli interi con l'addizione si scrive la coppia
Un anello è un insieme formula_50 dotato di due operazioni binarie, generalmente indicate con gli usuali simboli formula_46 e formula_52 dell'addizione e della moltiplicazione, che soddisfa alcuni assiomi. L'operazione formula_46 deve soddisfare gli assiomi di gruppo già elencati; inoltre devono valere
Ad esempio, i numeri interi formano un anello con le usuali operazioni di addizione e moltiplicazione, e si scrive:
L'elemento neutro per l'operazione formula_46 viene solitamente indicato con il simbolo formula_56.
Un campo è un anello che soddisfa alcuni assiomi aggiuntivi per l'operazione formula_52, e cioè:
I numeri interi "non" formano un campo perché 2 non ha un inverso rispetto al prodotto. I numeri razionali formano un campo e si scrive:
Altri campi importanti sono i numeri reali formula_59 ed i numeri complessi formula_60.
Uno spazio vettoriale è una struttura algebrica lievemente più complessa. Formalmente, consiste di una quaterna
in cui formula_62 è un insieme di oggetti detti "vettori", formula_63 un campo, e formula_64 due operazioni binarie che soddisfano una lunga lista di assiomi. Come i vettori del piano cartesiano, i vettori di formula_62 possono essere sommati e riscalati, cioè moltiplicati per un elemento del campo formula_63 detto "scalare". La nozione di spazio vettoriale è centrale in tutta la matematica moderna.
L'algebra elementare può essere introdotta come generalizzazione ed estensione dell'aritmetica, tramite l'introduzione di oggetti simbolici, chiamati "variabili" e "costanti", denotati solitamente con lettere dell'alfabeto.
Alle espressioni costruite con l'uso delle variabili e delle costanti, si applicano le operazioni aritmetiche di addizione, differenza (più generalmente, somma algebrica), moltiplicazione e divisione. In questo modo vengono introdotti e studiati oggetti come i polinomi e le equazioni, e studiati i metodi per trovarne le eventuali radici dei primi e soluzioni delle seconde.
L'algebra astratta è un'estensione dell'algebra elementare, nata verso la fine del XIX secolo e sviluppatasi enormemente nel XX secolo. L'algebra astratta definisce e studia le strutture algebriche: insiemi muniti di operazioni che soddisfano determinati assiomi. Esempi molto particolari di strutture algebriche sono costituiti dagli usuali insiemi numerici, quali i numeri interi, i razionali, i reali e i complessi con le loro ordinarie operazioni di somma o prodotto, o anche con una sola di queste operazioni.
Esempi di strutture algebriche sono i gruppi, gli anelli, i campi e gli spazi vettoriali.
Le operazioni di cui sono dotate queste strutture soddisfano leggi molto simili a quelle valide negli esempi numerici menzionati sopra. Esempi di strutture le cui operazioni soddisfano altre leggi, a volte apparentemente controintuitive, sono i reticoli, l'Algebra di Boole, le Algebre di Lie.
L'algebra lineare studia le matrici e gli spazi vettoriali.
Uno spazio vettoriale è una generalizzazione astratta della nozione dell'insieme dei vettori del piano (o dello spazio) in senso fisico. Uno dei suoi principali vantaggi è la possibilità di introdurre spazi di qualunque dimensione (anche infinita). Viene applicata anche per studiare le equazioni lineari, cioè le equazioni omogenee di primo grado.
Le applicazioni dell'algebra lineare sono di importanza fondamentale in fisica, in molte branche (anche non algebriche) della matematica e in altre discipline scientifiche.
Un gruppo è una struttura algebrica dotata di una singola operazione binaria che soddisfa alcune ben determinate proprietà (gli assiomi di gruppo). Esempi di gruppi sono i numeri interi, con l'operazione di somma, oppure l'insieme delle simmetrie di un particolare oggetto geometrico (con l'operazione di composizione di funzioni). È da notare che, mentre nel primo caso vale la proprietà commutativa formula_67 (il gruppo si dice "abeliano"), la proprietà analoga non vale, in generale, nel secondo caso, perché non è necessariamente vero che formula_68.
La teoria dei gruppi studia le strutture di gruppo. Oltre ad avere un profondo interesse intrinseco, la teoria dei gruppi ha importanti applicazioni in quasi tutti i settori della geometria, e in particolare alla topologia, e allo studio delle simmetrie. Ha anche una forte correlazione con la combinatoria: l'insieme delle permutazioni di un insieme è ad esempio un gruppo rispetto alla composizione di funzioni. Ha anche notevoli applicazioni in teoria dei numeri, e talvolta in analisi.
Un anello è una struttura algebrica con due operazioni, la prima delle quali soddisfa agli assiomi di un gruppo commutativo. Considerando anche la seconda operazione, si richiede che vengano soddisfatte molte delle proprietà valide per i numeri interi, con le operazioni di somma e prodotto. Ma, ad esempio, in un anello generico può capitare che formula_69, senza che necessariamente uno degli elementi formula_15 o formula_16 sia uguale a formula_56 (questa proprietà è invece verificata negli interi). Tra gli insiemi che risultano essere degli anelli, troviamo l'insieme dei polinomi a coefficienti in un dato anello, quello delle matrici (con opportune operazioni di somma e prodotto), e l'insieme dei numeri razionali.
La teoria degli anelli studia queste strutture, e ha applicazioni in algebra e in molte altre branche della matematica, in particolare in geometria algebrica.
Un campo è un anello che deve soddisfare degli assiomi ulteriori, che, intuitivamente, asseriscono la possibilità di effettuare le divisioni (ovviamente solo per un elemento non nullo). Ad esempio gli interi non sono un campo, mentre i razionali sì.
La teoria dei campi studia queste strutture. I campi sono l'oggetto base necessario per la definizione degli spazi vettoriali e quindi per tutta l'algebra lineare. La teoria di Galois è una teoria che mette in relazione i campi, e le loro possibili estensioni, coi gruppi finiti, e i loro possibili sottogruppi. La teoria di Galois fornisce metodi estremamente potenti per lo studio della risolubilità delle equazioni; in particolare, è fondamentale per dimostrare che non esiste una formula generale (che faccia uso solo di radicali) per la risoluzione delle equazioni di 5º grado o superiore.
L'algebra computazionale studia gli algoritmi per la manipolazione simbolica di oggetti matematici.
Oltre alle strutture già descritte, l'algebra ne studia molte altre, tra cui semigruppi, reticoli, moduli, algebre su campo, bialgebre, algebre di Hopf, superalgebre.
Il termine "algebra" viene usato per indicare varie specie di strutture algebriche composite:
</text>
</doc>
<doc id="1855883" url="https://it.wikipedia.org/wiki?curid=1855883">
<title>Variabili dipendenti e indipendenti</title>
<text>
In matematica una variabile è dipendente da altre variabili se esiste una relazione tra di esse che la coinvolge, altrimenti è indipendente da esse. Due o più variabili indipendenti l'una dall'altra sono dette variabili indipendenti. In assenza di una relazione, le variabili sono solitamente supposte indipendenti.
Ad esempio, le coordinate formula_1 dei punti nel piano sono variabili indipendenti, mentre le coordinate dei punti su una circonferenza di raggio formula_2 sono variabili dipendenti: formula_3 (alcuni valori che possono essere scelti singolarmente per le due variabili non possono essere presi contemporaneamente).
Questa terminologia è di frequente uso nell'ambito della teoria delle funzioni. Data una funzione formula_4 la variabile formula_5 argomento della funzione è detta "variabile indipendente", mentre la variabile formula_6 che rappresenta il valore della funzione viene detta "variabile dipendente", poiché dipende dalla variabile formula_5. Nel caso di funzioni in più variabili la dicitura è analoga. Data formula_8 le variabili formula_9 sono dette "indipendenti" mentre la variabile formula_6 è detta "dipendente".
In teoria delle probabilità due variabili aleatorie formula_11 e formula_12 sono stocasticamente indipendenti quando la loro probabilità congiunta è uguale al prodotto delle probabilità marginali:
In statistica la denominazione non è così precisa. La scelta di quali variabili sono dipendenti o indipendenti in un modello statistico dipende da motivi non strettamente matematici, ma che si basano sul contesto dell'esperimento e sulla più estesa realtà del fenomeno oggetto di studio. Solitamente sono necessari dei criteri logico-causali per scegliere quale variabile sia dipendente e quale sia indipendente. In particolare, per due variabili dipendenti formula_11 e formula_12 (come ad esempio l'"età" e il "titolo di studio") è possibile scegliere quale considerare "indipendente" e quale "dipendente" a seconda dell'ambito di studio.
In base al contesto si usano come sinonimi
</text>
</doc>
<doc id="1815927" url="https://it.wikipedia.org/wiki?curid=1815927">
<title>Reciproco</title>
<text>
In matematica, con reciproco di un numero formula_1 si indica il numero che moltiplicato per formula_1 dia come risultato 1; e può essere indicato come formula_3 ("frazione unitaria") o anche formula_4.
Generalmente quando si fa riferimento ai reciproci, si intendono soltanto i reciproci dei numeri interi:
formula_5 , ma in realtà è utilizzato anche per indicare il reciproco di un numero decimale, ad esempio il reciproco di formula_6 è formula_7
Ci sono infinite coppie di reciproci che hanno la proprietà di avere la stessa parte decimale; noto è l'esempio del numero aureo formula_8. Tali numeri si ricavano come le soluzioni positive dell'equazione di secondo grado
che può essere riscritta in modo più standard come formula_10, con formula_11 appartenente all'insieme dei numeri interi.
Utilizzando la formula risolutiva, si trova che questi numeri sono della forma
Vengono qui visualizzati i numeri risultanti per i primi valori di formula_13 insieme ai loro reciproci.
</text>
</doc>
<doc id="6897847" url="https://it.wikipedia.org/wiki?curid=6897847">
<title>Funzione definita a tratti</title>
<text>
In matematica una funzione definita a tratti (o semplicemente funzione a tratti) è una funzione definita da varie sottofunzioni, ciascuna delle quali è definita su un certo sottodominio, cioè su un sottoinsieme del dominio della funzione definita a tratti. Questi sottodomini formano una partizione del dominio della funzione definita a tratti.
Consideriamo, ad esempio, la funzione valore assoluto:
formula_1
La funzione è definita dalle sottofunzioni formula_2 e formula_3, negli intervalli formula_4 e formula_5 rispettivamente.
Una funzione definita a tratti è continua su un dato intervallo se rispetta le seguenti condizioni:
La funzione in figura, ad esempio, è continua nei sottointervalli formula_6 e formula_7 in cui è definita a tratti, ma non è continua nell'intero dominio, dato che contiene un punto di discontinuità a salto: il punto formula_8.
Le seguenti funzioni sono definite a tratti:
</text>
</doc>
<doc id="365697" url="https://it.wikipedia.org/wiki?curid=365697">
<title>Equazione logistica</title>
<text>
Una funzione logistica o curva logistica descrive una curva ad "S" di crescita di alcuni tipi di popolazioni "P". All'inizio la crescita è quasi esponenziale, successivamente rallenta, diventando quasi lineare, per raggiungere una posizione asintotica dove non c'è più crescita. (vedi grafico a lato)
La libera evoluzione di una popolazione "P" può essere modellata con un termine di crescita +"rKP," una percentuale di "P," ma quando la popolazione cresce alcuni membri di P, descritti mediante il termine -"rP²", interferiscono l'un l'altro ponendosi in competizione per le risorse facendo diminuire così il tasso di crescita, finché la popolazione" P" cessa di crescere perché raggiunge quel che è chiamato "maturità". K è la capacità portante, il fattore che limita la crescita e che può essere considerato "il collo di bottiglia."
Una funzione logistica è definita mediante la seguente formulazione:
con i seguenti parametri reali "a", "m", "n", e formula_2. Queste funzioni trovano applicazioni in una vasta gamma di campi, dalla biologia all'economia.
Per esempio, nello sviluppo dell'embrione la divisione dell'uovo fecondato comincia con una crescita esponenziale: 1, 2, 4, 8, 16, 32, 64, ecc. Il feto può crescere solo quanto l'utero gli consente; questo e altri fattori cominciano a rallentare l'aumento del numero delle cellule e il fattore di crescita diminuisce anche se il bambino continua a crescere. Dopo il tempo di gravidanza, il bambino nasce e riprende a crescere. Nell'ultimo periodo prima del parto il numero di cellule è pressoché stabile su un valore asintotico.
Altro esempio è la concentrazione di reagenti e prodotti nelle reazioni autocatalizzanti che seguono la funzione logistica.
In questi esempi sono modellati "i rapporti" tra le variabili. Una funzione importante logistica è il modello di Rasch, che è un modello generale stocastico di misura. Questo modello è usato come un "sostegno" per la misura piuttosto che per modellare i rapporti tra le variabili per cui sono state fatte le misure, come nell'esempio precedente. In particolare, il modello di Rasch forma una base per la stima della probabilità massima delle posizioni di oggetti che possono essere misurati in uno spazio continuo, basato sulla raccolta di dati categorici.
L'equazione logistica, anche nota come modello di Verhulst o curva di crescita logistica, è stata inizialmente proposta come modello di crescita della popolazione.
Questo modello assume che:
Così il secondo termine modella la competizione per le risorse disponibili, che tende a limitare la crescita della popolazione.
Assumendo che formula_3 rappresenti la misura di popolazione (in ecologia è usualmente indicato con formula_4) e formula_5 rappresenti il tempo, questo modello è formalizzato dall'equazione differenziale:
dove la costante formula_7 definisce il tasso di crescita e formula_8 il termine asintotico della popolazione (definito dalle risorse disponibili per la popolazione, noto in ecologia come "carrying capacity", o "capacità portante"). Il termine formula_9 rappresenta la competizione intraspecifica. La soluzione generale di queste equazioni è una funzione logistica.
Nell'ecologia, la specie sono riferite a volte alle r-strategie o K-strategie dipendendo dai processi selettivi che hanno modellato le loro strategie di vita.
La soluzione della equazione (dove formula_10 è la popolazione iniziale) è:
ovvero, raccogliendo e semplificando il termine formula_12
dove si è posto
da tale formulazione è facile ricavare il limite asintotico:
L'equazione di Verhulst fu pubblicata per la prima volta da Pierre F. Verhulst nel 1838, dopo aver letto il libro di Thomas Malthus' "An Essay on the Principle of Population".
Verhulst derivò la sua "équation logistique" ("equazione logistica") per descrivere le auto-limitazioni di crescita di una popolazione biologica. L'equazione viene talvolta chiamata "equazione di Verhulst-Pearl" dopo che è stata riscoperta nel 1920. Alfred J. Lotka dedusse l'equazione ancora nel 1925, chiamandola "legge di crescita di una popolazione".
Il caso speciale della funzione logistica con formula_16, cioè
È chiamato funzione sigmoide o curva sigmoidale. Il nome è dovuto alla forma del suo grafico analogo ad un "S". Questa funzione è anche chiamata "funzione logistica standard" ed è spesso incontrata in molti ambiti tecnici, soprattutto nelle reti neurali come funzione di trasferimento, in probabilità, statistica, biomatematica, psicologia matematica e in scienze economiche.
Data l'equazione logistica/sigmoidale in una forma più generale:
con:
La funzione sigmoide (standard) è la soluzione dell'equazione differenziale del primo ordine non lineare
con condizioni al contorno formula_36. L'equazione (2) è la versione continua della mappa logistica.
La curva sigmoide mostra prima crescita esponenziale per "t" negativo, che rallenta verso una crescita lineare di pendenza 1/4 nell'intorno di "t" = 0, poi si avvicina a "y" = 1 (asintoto orizzontale) con un decadimento esponenzialmente.
La funzione logistica è l'inverso della funzione di "logit" naturale e può essere usata così per convertire il logaritmo di probabilità in una probabilità; la conversione dal "rapporto di log-probabilità" di due alternative porta anche la forma di una curva sigmoidale.
Avendo supposto che il numero di individui di una popolazione sia una funzione continua del tempo formula_37 che ammette derivata continua, si ha che l'incremento della popolazione al variare del tempo può essere rappresentato dalla derivata di formula_37, che in un modello elementare si può supporre direttamente proporzionale al numero di individui della popolazione stessa.
Si ha pertanto la seguente equazione differenziale:
con formula_7: parametro di crescita malthusiana (tasso massimo di crescita della popolazione).
Pertanto se formula_7 è una costante la popolazione cresce in maniera esponenziale con pendenza dipendente da formula_7.
Invece in un ambiente la cui disponibilità di risorse è limitata si può descrivere l'evoluzione della popolazione utilizzando un coefficiente formula_7 che decresce all'aumentare della popolazione: il modello più semplice è formula_44 con formula_45 e formula_46 costanti. Sostituendo tale funzione nella precedente equazione differenziale si ottiene:
formula_47
che può essere posta nella forma:
formula_48
con formula_49 che è la cosiddetta popolazione massima sostenibile ed a uguale al parametro di crescita malthusiana.
Questa è l'equazione logistica di Verhulst.
Separando le variabili si ottiene:
formula_50
risolvendo gli integrali, scegliendo come primitive quelle tali che formula_51 e utilizzando le proprietà dei logaritmi si ottiene la soluzione:
formula_52
Si nota che a causa del sovraffollamento la popolazione non cresce più in maniera esponenziale ma converge al valore asintotico k indipendentemente da formula_53.
La soluzione dell'equazione si può anche scrivere nelle forme:
È immediato verificare che per questa soluzione ha due asintoti orizzontali:
Si ha un differente comportamento nel caso formula_57 allora il secondo limite tenderebbe a formula_58, presentando anche un asintoto verticale, ma queste soluzioni non sono considerate nel modello di crescita (descrivono evidentemente una popolazione in rapida decrescita in quanto inizialmente in eccesso rispetto alle risorse presenti).
Se la popolazione chiusa è soggetta a "catastrofi" periodiche, cioè viene fatto un prelievo formula_59 costante nel tempo (si imagini un lago con dei pesci di cui ne viene pescata una quota fissa giornaliera) l'equazione di Verhulst diventa:
formula_60
Questa equazione è di difficile soluzione, ma è possibile analizzarla qualitativamente considerando che la derivata di N(t) si annulla in:
formula_61 ed formula_62 con formula_63 da cui formula_64 .
Posto formula_65 e formula_66 l'insieme delle funzioni N(t) al variare di formula_53 che soddisfa l'equazione di Verlhust con prelievo costante si ha che:
Pertanto in caso di prelevamento non solo deve essere formula_64 ma è necessario che la popolazione iniziale non sia minore di formula_83 come si evince dalla (3).
Si nota inoltre che formula_84 cioè che in caso di prelevamento nell'ipotesi (1) e (2) la popolazione converge ovviamente ad un valore più piccolo rispetto al caso in cui non ci sia prelevamento.
Per meglio descrivere il caso in cui la popolazione si possa estinguere si può modificare l'equazione:
formula_85
dove formula_86 rappresenta il livello minimo di popolazione al di sotto del quale questa si estingue (pensando sempre al lago di specie, gli adulti non riesco ad accoppiarsi).
Un ulteriore passo è l'introduzione di un certo ritardo nel raggiungimento dell'asintoto orizzontale (fase di maturità); questa nuova situazione è descritta mediante la seguente equazione:
con questa equazione si introduce una oscillazione, come un sistema molla-smorzatore, che oscilla intorno alla posizione di equilibrio in modo decrementale ma infinito.
La funzione logistica può essere utilizzata per illustrare il progresso della diffusione di un'innovazione tecnica, lungo il suo ciclo di vita. Storicamente quando vengono introdotti nuovi prodotti si investe molto in ricerca e sviluppo; ciò conduce a notevoli miglioramenti qualitativi e riduce i costi. Tutto questo comporta un periodo di crescita rapida dell'industria. Ecco alcuni beni e servizi coinvolti in tal fenomeno: ferrovie, lampade a incandescenza, elettrificazione, Ford Model T, trasporto aereo e computer. Infine i drastici aumenti d'efficienza, nonché le associate opportunità di riduzione dei costi, si esauriscono; al contempo il prodotto o processo in questione si diffonde saturando il mercato, restando pochi potenziali nuovi acquirenti.
La funzione logistica è stata usata negli articoli di diversi ricercatori dell'IIASA ("International Institute of Applied Systems Analysis"). In queste pubblicazioni vengono studiati argomenti come: la diffusione di varie innovazioni e infrastrutture; la sostituzione di fonti di energia; il ruolo del lavoro fisico in economia, ovvero nei cicli produttivi di lungo periodo. Robert Ayres (1989) e Cesare Marchetti (1988, 1996) si sono occupati delle cosiddette Onde di Kondratiev, cicli produttivi macroeconomici sinusoidali, e della diffusione delle innovazioni. Un libro di Arnulf Grübler (1990) fornisce un resoconto dettagliato della diffusione di infrastrutture, tra cui canali, ferrovie, autostrade e compagnie aeree, dimostrando che essa è ben rappresentata da una opportuna curva logistica.
Carlota Perez (2002) ha scelto la curva logistica per spiegare e sviluppare le succitate Onde K, introducendo alcuni termini chiave: "irruzione", per l'inizio di un'era tecnologica; "frenesia", per indicare la sua diffusione iniziale; "sinergia", ossia il suo rapido sviluppo; "maturità", per denotarne la diffusione completa.
Malgrado la sua popolarità persistente come modello per la crescita della popolazione nel campo della dinamica di popolazione, quest'uso della funzione logistica è stato pesantemente criticato. Il demografo e professore Joel E. Cohen ("How Many People Can The Earth Support", 1995), uno dei critici, spiega che Verhulst ha tentato di adattare la curva logistica, basata sulle ipotesi di funzione logistica, a tre censimenti separati della popolazione degli Stati Uniti d'America per predire la crescita futura in questo Paese. Tutte e tre le serie di predizioni hanno fallito.
Nel 1924 i professori Ray Pearl e Lowell J. Reed hanno usato il modello di Verhulst per predire un limite superiore di 2 miliardi per la popolazione mondiale. Questo limite è stato superato nel 1930. Nel 1936 un nuovo tentativo di Pearl e di un suo associato, Sophia Gould, ha prodotto un limite superiore di 2,6 miliardi. Questo limite è stato superato nel 1955.
Un'analisi di queste critiche è stata effettuata dal professor Peter Turchin ("la Dinamica di Popolazione Complicata", 2003) che, nonostante tutto, conclude che questo tipo di equazioni fornisce una struttura utile per la dinamica di una sola specie (anche grazie a modelli generalizzati) e può contribuire all'elaborazione di modelli per le interazioni di più specie.
Nonostante le critiche, storicamente la curva logistica è stata un punto di unione tra modelli matematici e sociologici, come ad esempio la teoria della trasformazione di George Land, che usa il concetto della curve a "S" per predire un corretto modello affaristico-industriale nei vari scenari di un processo di crescita tecnologica.
</text>
</doc>
<doc id="367434" url="https://it.wikipedia.org/wiki?curid=367434">
<title>Matrice elementare</title>
<text>
In algebra lineare, con matrice elementare si indica generalmente una matrice quadrata di un certo tipo, utile in alcuni algoritmi come l'algoritmo di Gauss o le fattorizzazioni LU e QR.
Nella più grande generalità, una matrice elementare è una matrice quadrata a coefficienti reali o complessi, del tipo
dove formula_2 è la matrice identità e formula_3 è una matrice con rango al più uno. In altre parole, le colonne (o le righe) di formula_3 sono tutte multiple una dell'altra, ad esempio:
Equivalentemente, formula_6 è il prodotto di due vettori, il primo formula_7 colonna ed il secondo formula_8 riga (perché formula_8 indica la trasposta di formula_10). Nell'esempio, abbiamo
Risulta quindi comodo esprimere una matrice elementare come
dove formula_13 è un coefficiente (reale o complesso) e formula_14 sono vettori non nulli.
Le principali proprietà delle matrici elementari sono:
Le matrici elementari di Gauss sono matrici elementari molto semplici, definite per interpretare le mosse di Gauss come moltiplicazione per una matrice. Sono di tre tipi, ciascuno corrispondente ad un tipo di mossa.
La matrice formula_15 è ottenuta dalla matrice identità scambiando le righe formula_16-esima e formula_17-esima:
Può essere anche definita come 
dove 
è l'formula_21-esimo vettore della base canonica.
Analogamente, formula_22 è ottenuta dalla matrice identità moltiplicando la riga formula_16-esima per un numero formula_24.
Può anche essere definita come
La matrice formula_27 è ottenuta dalla matrice identità aggiungendo alla riga formula_16-esima la riga formula_17-esima moltiplicata per formula_24.
Può anche essere definita come
Se formula_33 è una matrice qualsiasi con formula_34 righe, allora le matrici formula_35 sono le matrici ottenute da formula_33 operando le corrispondenti mosse di Gauss.
Una matrice di Householder è una matrice elementare del tipo formula_37 dove formula_10 è un vettore di norma uno. 
Le matrici elementari di Householder sono utili per definire le trasformazioni di Householder e quindi la fattorizzazione QR.
</text>
</doc>
<doc id="319137" url="https://it.wikipedia.org/wiki?curid=319137">
<title>Circonferenza unitaria</title>
<text>
In matematica, una circonferenza unitaria è una circonferenza di raggio unitario, cioè una circonferenza il cui raggio è formula_1. Frequentemente, specialmente in trigonometria, la circonferenza unitaria è centrata nell'origine formula_2 in un sistema di coordinate cartesiane nel piano euclideo.
La circonferenza unitaria è spesso indicata con formula_3; la generalizzazione a più dimensioni è la sfera unitaria.
Se formula_4 è un punto della circonferenza unitaria del primo quadrante, allora formula_5 e formula_6 sono le lunghezze dei lati di un triangolo rettangolo la cui ipotenusa ha lunghezza 1. Quindi, per il teorema di Pitagora, formula_5 e formula_6 soddisfano l'equazione
Poiché formula_10 per ogni formula_5, e poiché la riflessione di ogni punto della circonferenza unitaria sull'asse formula_5 (o formula_6) appartiene ancora alla circonferenza unitaria, l'equazione precedente vale per ogni punto formula_4 della circonferenza unitaria, non solo nel primo quadrante.
Si può anche usare la nozione di "distanza" per definire altre "circonferenze unitarie".
Ovvero le si può definire come il luogo dei punti che hanno distanza unitaria (modulo uguale a formula_1) dall'origine. In coordinate polari l'equazione sarà
Vedere la voce sugli spazi normati per alcuni esempi.
Il cerchio unitario è il luogo dei punti del piano aventi una distanza minore o uguale all'unità da un punto detto centro del cerchio. In altri termini il cerchio unitario comprende la circonferenza unitaria e la parte di piano racchiusa dalla circonferenza stessa. Esso è indicato dalle disequazioni:
Le funzioni trigonometriche coseno e seno possono essere definite sulla circonferenza unitaria come segue. Se formula_4 è un punto della circonferenza unitaria, e se il raggio dall'origine formula_2 a formula_4 forma un angolo formula_22 con l'asse formula_5 positivo, (l'angolo misurato nel verso antiorario), allora
Per definizione delle funzioni seno e coseno, l'equazione formula_26 fornisce la relazione
che è vera per ogni formula_22 reale.
formula_22 è definito come un angolo orientato, che cioè assume un segno positivo in un verso e negativo nell'altro, a seconda della convenzione oraria o antioraria adottata. Solitamente si adotta la convenzione antioraria, e si suppone che l'angolo sia positivo spostandosi dall'asse delle ascisse in senso antiorario. Una circonferenza con tale angolo orientato è detta circonferenza goniometrica.
La circonferenza trigonometrica è una circonferenza goniometrica di raggio unitario (ossia goniometrica e unitaria). Essa è detta trigonometrica perché per definire seno, coseno, e da essi tutte le altre funzioni trigonometriche, servono un angolo orientato e un raggio unitario. Gli altri elementi presenti nei disegni sono una costruzione di geometria euclidea.
La circonferenza unitaria fornisce un modo intuitivo per visualizzare il seno e il coseno come funzioni periodiche, con le identità
Queste identità discendono dal fatto che le coordinate formula_5 e formula_6 di un punto sulla circonferenza unitaria rimangono le stesse incrementando o decrementando l'angolo formula_22 di un numero qualsiasi di giri (1 giro = 2π radianti).
Quando si lavora con triangoli rettangoli, seni, coseni, e altre funzioni trigonometriche ha senso parlare di misura di angoli maggiore di zero e minore di π/2. Tuttavia, usando la circonferenza unitaria, queste funzioni hanno un significato intuitivo per ogni misura di angolo reale.
In effetti, non solo seno e coseno, ma tutte le sei funzioni trigonometriche standard — seno, coseno, tangente, cotangente, secante, e cosecante, come anche le funzioni arcaiche come senoverso e exsecante — possono essere definite geometricamente in termini della circonferenza unitaria.
Prendendo in considerazione solo la parte della circonferenza descritta dall'equazione formula_36 che la rappresenta nel 1° e nel 2º secondo quadrante, l'area di questa si calcolerà
con un integrale formula_37. Allo stesso modo prendendo in considerazione la parte formula_38,che descrive la circonferenza nel 3° e nel 4° quadrante, l'integrale che ne definisce l'area sarà formula_39. Si può dire pertanto che l'area della circonferenza unitaria ha come valore formula_40, visto che si può considerare come la somma dei due integrali
formula_41.
Si può inoltre dimostrare la veridicità di questa formula attraverso l'utilizzo della formula per calcolare l'area è formula_42.
Sapendo che formula_43 otteniamo che formula_44 C.V.D.
Ogni numero complesso può essere identificato con un punto del piano euclideo, chiamando il numero complesso formula_45 esso è identificato con il punto formula_46. Con questa relazione la circonferenza unitaria è un gruppo sotto la moltiplicazione, chiamato anche gruppo circolare. Questo gruppo ha importanti applicazioni in matematica e nelle scienze.
</text>
</doc>
<doc id="411683" url="https://it.wikipedia.org/wiki?curid=411683">
<title>Minore (algebra lineare)</title>
<text>
In matematica, in particolare in algebra lineare, un minore di una matrice formula_1 è il determinante di una matrice quadrata ottenibile da formula_1 eliminando alcune righe e/o colonne di formula_1.
I minori sono uno strumento utile per calcolare il rango di una matrice, e quindi per risolvere i sistemi lineari.
Una sottomatrice di una matrice formula_4, con formula_5 e formula_6 interi non negativi, è una matrice formula_7, con formula_8 e formula_9 interi tali che formula_10 e formula_11, ottenuta da formula_1 rimuovendo formula_13 righe e formula_14 colonne.
Un minore è il determinante di una sottomatrice (quadrata, cioè con formula_15). Il numero formula_8 è definito "ordine" del minore.
Un minore complementare è un minore di formula_17 ottenuto togliendo una sola riga e una sola colonna da formula_17. Si nota subito che i minori complementari sono definiti solo per matrici formula_17 quadrate, altrimenti la matrice risultante non sarebbe più quadrata e non se ne potrebbe calcolare il determinante. Il minore complementare rispetto all'elemento formula_20 di una matrice quadrata formula_17 si ottiene togliendo l'formula_22-esima riga e la formula_23-esima colonna e si indica con formula_24 o con formula_25. Se il minore complementare formula_24 viene considerato con il segno formula_27 esso è detto "complemento algebrico" o "cofattore" di formula_20.
Talvolta con "minore" si intende "sottomatrice quadrata", ma questo uso è meno comune e alcuni risultati potrebbero dover essere enunciati in modo differente. Qui e nel seguito si userà la definizione di minore come determinante.
Sia formula_17 una matrice formula_30 e siano formula_31 un sottoinsieme di formula_32 con formula_33 elementi e formula_34 un sottoinsieme di formula_35 con formula_33 elementi. Indicando con formula_37 il minore formula_38 di formula_17 che corrisponde alle righe con indice in formula_31 e colonne con indice in formula_34:
Il rango di una matrice formula_49 è uguale al massimo ordine di un minore non nullo di formula_49. Questo risultato fornisce uno strumento frequentemente utilizzato nel calcolo del rango di una matrice, ma non è molto efficiente per matrici di con elevato numero di righe e/o colonne.
La matrice dei cofattori è un'importante matrice associata ad una matrice quadrata ed è definita a partire dai suoi minori complementari.
Data una matrice ad elementi reali formula_51 e rango formula_8, allora esiste almeno un minore di ordine formula_8 non nullo e tutti i minori di ordine maggiore sono nulli.
Si consideri la matrice formula_54:
Allora alcune delle sue sottomatrici sono:
I minori di ordine formula_61 sono:
Alcuni dei minori di ordine formula_63 sono:
Infine i minori di ordine formula_65:
</text>
</doc>
<doc id="323938" url="https://it.wikipedia.org/wiki?curid=323938">
<title>Elemento inverso</title>
<text>
In matematica, e in particolare in algebra astratta, dato un gruppo formula_1, e un suo elemento "g", si definisce elemento inverso (o semplicemente "inverso") di "g" un elemento "h" appartenente a formula_2 tale che:
dove formula_4 indica l'elemento neutro del gruppo.
La "h" si indica anche con il simbolo formula_5.
L'elemento inverso di un dato elemento è unico e se "h" è l'inverso di "g" allora "g" è l'inverso di "h".
In notazione additiva, dato il gruppo formula_6 l'elemento inverso associato a "g" si indica con formula_7 e si chiama di solito opposto. Nella notazione moltiplicativa, nei casi di gruppi numerici, l'elemento inverso si denota anche come reciproco.
Le funzioni trigonometriche (con dominio opportunamente ristretto) sono il reciproco l'una dell'altra: la cotangente è il reciproco della tangente; la secante è il reciproco del coseno; la cosecante è il reciproco del seno. (Inverse rispetto all'operazione di prodotto tra due funzioni a valori reali non nulli, ovvero nel gruppo moltiplicativo dei numeri reali.)
Negli insiemi numerici considerati con l'addizione, l'opposto non esiste qualora l'insieme considerato non contenga numeri negativi. Ad esempio non esiste in R.<br/>
In formule, dato un numero x, il suo opposto x' è quel numero tale che:
L'opposto di un numero ha sempre il segno contrario a quello del numero stesso: l'opposto di un numero negativo è un numero positivo e viceversa. L'opposto di zero è zero stesso.
Il reciproco o inverso di un numero "x" è il numero che, quando moltiplicato per "x", dà 1. Viene denotato con 1/"x" oppure con "x":
Il reciproco dello zero non esiste.
</text>
</doc>
<doc id="340513" url="https://it.wikipedia.org/wiki?curid=340513">
<title>Curva piana</title>
<text>
In matematica una curva piana è una curva che giace interamente in un (unico) piano ed è identificabile da una funzione continua formula_1, dove formula_2 è un intervallo nell'insieme dei numeri reali. Ad esempio, una curva su uno spazio euclideo di dimensione maggiore di 2 è piana se il suo supporto giace su un piano contenuto nello spazio euclideo in cui è definita.
L'immagine di una curva viene anche chiamata "supporto" della curva. Talvolta si usa l'espressione "curva" anche per indicare il supporto di una curva.
Le curve piane sono oggetti geometrici ampiamente studiati, fin dall'antichità, con obiettivi non solo di tipo matematico. La collezione delle curve che sono state studiate in termini matematici è molto varia e complessa, e conviene rilevare subito alcune distinzioni.
Una curva piana si dice "semplice" se non si autointerseca, ovvero se per ogni formula_3 si ha formula_4. In caso contrario si dice dotata di punti doppi, tripli, e così via.
Un'altra distinzione riguarda il fatto che una curva piana sia "limitata", cioè abbia come supporto un insieme limitato di punti di formula_5, oppure sia "illimitata". Curve piane limitate sono le ellissi e le lemniscate, mentre sono illimitate le iperboli e le spirali.
Un tipo di rappresentazione della curva piana è l'equazione:
tale che ad ogni punto formula_7 corrisponde un punto formula_8, e in modo che ogni punto formula_9 del piano rappresenti il supporto della curva. Una curva di questo tipo si dice anche grafico in riferimento al grafico delle funzioni reali. In effetti la rappresentazione si può anche scrivere come:
cioè come funzione di una variabile indipendente. Questa rappresentazione ha molti limiti geometrici derivanti dal fatto che una curva molto spesso ha una descrizione molto complessa in questa forma, non adatta allo studio delle proprietà geometriche.
Una curva si può rappresentare anche nella forma:
cioè come funzione di due variabili indipendenti. Sebbene questa rappresentazione sia per alcune finalità migliore di quella esplicita si possono incontrare problemi quando è necessario esplicitare una variabile in funzione dell'altra, cosa che non è nemmeno sempre possibile.
La migliore rappresentazione è sicuramente quella parametrica, del tipo:
oppure:
dove formula_14 si chiama "parametro". La condizione di continuità non basta per rappresentare e studiare le curve intese come oggetti filiformi ad una dimensione con le caratteristiche di regolarità volute. La condizione aggiuntiva è che la curva piana sia differenziabile entro formula_2.
Una curva piana parametrica formula_16 si dice differenziabile in ogni punto se le funzioni formula_17 e formula_18 hanno derivate continue in ogni punto. Una curva piana differenziabile si dice "regolare" in un punto formula_19 se formula_20 e regolare in I se formula_21 in ogni punto di I. Un punto in cui si abbia formula_22 si dice che è un punto "singolare" per la curva.
La regolarità della curva permette di definire la retta tangente alla curva. Sia formula_23 una curva differenziabile e formula_24 un punto regolare. Si può definire la retta tangente alla curva in quel punto come la retta passante per formula_25 parallela al vettore formula_26.
La retta tangente ha equazione cartesiana nel punto formula_19:
e equazioni parametriche:
Nel caso di curva rappresentata esplicitamente da un'equazione formula_6, la retta tangente nel punto formula_31 è data:
mentre nel caso di una curva rappresentata da un'equazione implicita formula_11 la retta tangente nel punto formula_31 è data da:
La regolarità della curva permette di definire anche la "retta normale" alla curva nel punto formula_19 di equazione cartesiana:
Nel caso di curva rappresentata esplicitamente:
mentre per il caso di curva rappresentata implicitamente:
Dalla definizione stessa di derivata si ottiene:
che geometricamente rappresenta la pendenza della retta tangente, cioè la tangente goniometrica dell'angolo che la retta tangente forma con l'asse orizzontale "x". Da questa relazione si possono estrarre i coseni direttori della retta tangente:
Data una curva formula_42 differenziabile e una funzione formula_43 definita sull'intervallo formula_44 allora la curva:
tale che per ogni formula_46 si ha formula_47 è una riparametrizzazione della curva formula_48. La riparametrizzazione è regolare se formula_49 e formula_50.
Si mostra che se formula_51 è una riparametrizzazione di formula_52 tramite formula_53 allora:
Infatti, se formula_55 allora formula_56 e per la regola di derivazione delle funzioni composte si ottiene:
e così si ha:
Sia data formula_55 differenziabile e formula_60. Allora la lunghezza dell'arco di curva compreso tra formula_61 vale:
Si aggiunga che, se formula_63 è una riparametrizzazione della curva, allora:
Se la curva è rappresentata in forma cartesiana esplicita:
cioè:
allora, sapendo che:
e che:
applicando il teorema di Pitagora ad elementi infinitesimali, ed integrando nell'intervallo di variazione dell'ascissa, la lunghezza della curva è data da:
Una forma di parametrizzazione che assume importanza notevole nello studio della matematica, della geometria e in molti campi di applicazione della matematica è quella in coordinate polari piane. Data una curva che ha parametrizzazione in coordinate polari piane in forma cartesiana:
e in forma parametrica con parametro formula_71:
allora sue derivate sono:
di modo che la lunghezza della curva sia uguale a:
Si definisce "ascissa curvilinea" oppure "parametro lunghezza arco" la riparametrizzazione particolare ottenuta fissando l'estremo inferiore di integrazione formula_75 in modo che l'integrale:
dipenda solo dall'estremo superiore formula_77 inteso come variabile. Questa funzione è la lunghezza dell'arco di curva a partire da un punto fisso formula_75 e può avere segno. Si può sempre riparametrizzare la curva nell'ascissa curvilinea. In tal modo se si vuole calcolare la retta tangente in un punto, si sa che essa è parallela ad un vettore tangente unitario, cioè ad un versore. Si dimostra che si può sempre riparametrizzare una curva tramite l'ascissa curvilinea nel modo seguente:
dato che formula_79 allora si può invertire formula_80 e se la sua inversa è formula_43 allora si ha la riparametrizzazione ascissa curvilinea data da:
Si dimostra poi che il vettore tangente è unitario nel modo seguente:
Sia formula_63 una curva parametrizzata secondo l'ascissa curvilinea e formula_85 il suo versore tangente. Si considera la funzione formula_86 che associa ad ogni formula_46 il valore formula_88. La funzione formula_89 è la curvatura della curva.
Se la curva è rappresentata esplicitamente, la sua curvatura è:
mentre per una curva rappresentata da un'equazione implicita:
Una curva (sufficientemente regolare) nello spazio ha in ogni suo punto un sistema di riferimento, detto "triedro di Frenet", dato da una terna di vettori "tangente", "normale" e "binormale". Tale curva è piana precisamente quando il vettore binormale è sempre costante.
Sia formula_92 una curva parametrizzata secondo l'ascissa curvilinea. Il versore tangente è dato da:
Il versore normale è dato da:
dove formula_2 è l'unità immaginaria. Sfruttando la definizione di curvatura si può dare un'altra forma al versore normale:
Si dimostra che il vettore formula_97 è ortogonale a formula_98 e quindi parallelo ad formula_99.
In definitiva le formule di Frenet e la curvatura per una curva piana con parametrizzazione qualsiasi formula_100 sono:
</text>
</doc>
<doc id="4996029" url="https://it.wikipedia.org/wiki?curid=4996029">
<title>Mereologia</title>
<text>
In filosofia la mereologia (composizione del greco μερος, "meros", "parte" e -λογία, "logia", "discorso", "studio", "teoria") è uno dei "cosiddetti" «sistemi di Leśniewski», ossia è la teoria, o scienza, delle relazioni parti-tutto; presentata da Achille Varzi come teoria «delle relazioni della parte al tutto e da parte a parte con un tutto» (o «teoria delle parti e dell'intero»), da Hilary Putnam come «"il calcolo delle parti e degli interi"» e da Claudio Calosi come la «teoria formale delle parti e delle relazioni di parte». Per Maurizio Ferraris tale relazione parte-intero può essere tra oggetti concreti, regioni spazio-temporali, processi ("parti temporali"), eventi e oggetti astratti.
Stanisław Leśniewski creò il termine "mereologia" nel 1927 per denominare la teoria (che gli si presentò tramite un ragionamento di Husserl) delle relazioni tra le parti e il tutto a partire dalla differenziazione — il cui principale fine era "evitare" l'antinomia di Russell — tra interpretazione distributiva (un oggetto come elemento di una classe) e interpretazione collettiva (un oggetto come parte di un intero) dei simboli di classe. Leśniewski elaborò poi la teoria in un sistema assiomatico deduttivo entro cui poter esprimere il calcolo proposizionale e il calcolo delle classi.
Anche se cronologicamente è il primo dei sistemi di Leśniewski la mereologia contiene gli altri due: 
Con la mereologia si presenta una differente definizione d'insieme. Esso non è definito distributivamente ma collettivamente (mereologicamente): l'insieme è una concreta totalità di elementi, un aggregato e dunque un oggetto fisico composto di parti, che è solo se, e finché, esse sono (v. "dipendenza ontologica"). Da ciò risultano varie differenze dalla "normale" teoria degli insiemi tra le quali che in mereologia è "insensato" ammettere l'esistenza di un insieme vuoto; indi insiemi di un solo elemento sono tale elemento e la proprietà, unico termine primitivo della mereologia, di «essere un elemento» è transitiva e antisimmetrica e riflessiva.
Gli assiomi di base della mereologia sono il "principio della riflessività" della nozione di parte (R), il "principio dell'asimmetria" della nozione di parte propria (aS) e il "principio di transitività" della nozione di parte (T).
</text>
</doc>
<doc id="3180524" url="https://it.wikipedia.org/wiki?curid=3180524">
<title>Funzione quadratica</title>
<text>
In algebra, una funzione quadratica è una funzione in una o più variabili definita in modo esplicito attraverso un polinomio di secondo grado. Ad esempio, una funzione quadratica nelle variabili "x", "y", "z" ha la seguente forma generale:
formula_1 con almeno uno tra formula_2 diverso da 0.
Una funzione quadratica in una variabile ha forma:
formula_3
Il suo grafico è una parabola con l'asse di simmetria parallelo all'asse "y". Uguagliando a zero una funzione quadratica si ottiene una equazione di secondo grado; le soluzioni dell'equazione di secondo grado sono dette radici del polinomio associato.
Una funzione quadratica in due variabili ha forma:
formula_4 con formula_5 non contemporaneamente nulli. Il grafico di una funzione quadratica è, in generale, una ipersuperficie detta quadrica. 
Il sottoinsieme di formula_6 descritto da formula_7 è una sezione conica (ellisse, circonferenza, parabola, iperbole).
I coefficienti del polinomio che definisce la funzione possono essere reali o complessi, perché un polinomio può essere definito su qualunque anello.
Nel caso in cui tutti i coefficienti dei termini di secondo grado siano uguali a zero, si parla di caso degenere della funzione.
I polinomi di secondo grado (e quindi anche le funzioni quadratiche) sono generalizzate sugli spazi vettoriali dal concetto di forma quadratica.
L'aggettivo "quadratico " deriva dal latino "quadratum" (quadrato). Un termine di secondo grado formula_8 è detto quadrato perché rappresenta l'area di un quadrato di lato formula_9.
Una funzione quadratica in una variabile può essere espressa in tre forme:
La conversione dalla forma normale a quella fattorizzata si effettua calcolando le radici del polinomio; la conversione dalla forma normale a quella del vertice si effettua attraverso il completamento del quadrato; la forma normale si ricava dalle altre due eseguendo le operazioni indicate.
A prescindere dalla forma dell'espressione, il grafico di una funzione quadratica in una variabile formula_3 è una parabola. Da questo si ha, equivalentemente, che una parabola può essere descritta come formula_16.
Se formula_17, la parabola volge la concavità verso l'alto; se formula_18, la parabola volge la concavità verso il basso.
Il coefficiente formula_19 controlla la curvatura del grafico: maggiore è il suo valore assoluto, più stretta è la parabola. I coefficienti formula_19 e formula_21 concorrono a definire la posizione dell'asse di simmetria della parabola, quindi la coordinata formula_22 del vertice, data da formula_23. Il coefficiente formula_24 controlla l'altezza della parabola; in particolare essa intercetta l'asse "y" nel punto di coordinate formula_25.
Il vertice è il massimo o minimo assoluto della parabola. Se la funzione è nella forma del vertice, le sue coordinate sono formula_14.
Attraverso il completamento del quadrato, la forma normale
formula_3
può essere trasformata in
formula_28;
ponendo formula_29 (discriminante)
allora il vertice ha coordinate formula_30
quindi l'asse di simmetria passa per il vertice.
Se la funzione è in forma fattorizzata, sfruttando la simmetria della parabola, si dimostra che le coordinate del vertice possono essere calcolate equivalentemente come formula_31.
Siccome il punto di vertice è un massimo o un minimo della funzione quadratica, esso può essere trovato attraverso i teoremi dell'analisi matematica. Quindi, il punto di vertice deve essere radice della derivata:
formula_32
in questo punto la funzione vale
formula_33
quindi le coordinate del vertice sono:
formula_30
in accordo con quanto trovato prima.
Le radici (o zeri) di una funzione in una variabile sono i valori di formula_9 per i quali formula_36. Per il teorema fondamentale dell'algebra per una funzione quadratica le radici sono due (eventualmente coincidenti). Attraverso il completamento del quadrato si trova che:
formula_37.
Quindi a seconda del segno del discriminante si possono avere tre casi:
Il modulo delle radici non può essere più grande di formula_42, dove formula_43è la sezione aurea.
La funzione data dalla radice quadrata di una funzione quadratica in una variabile ha forma formula_44 ed ha come grafico una ellisse o una iperbole.
Se formula_17 il grafico è un'iperbole. La direzione dell'asse dell'iperbole è determinata dall'ordinata del vertice: se è negativa l'asse trasverso è verticale, se è negativa l'asse trasverso è orizzontale.
Se formula_18 il grafico è un'ellisse se esistono due radici reali e distinte; altrimenti è un punto (radici coincidenti), oppure non esiste grafico sul piano cartesiano (radici complesse).
Iterare una funzione significa applicarla ripetutamente, sostituendo alla variabile indipendente il valore della funzione trovato nella iterazione precedente. L'iterazione "n"-esima viene indicata con formula_47; la notazione può essere estesa ai numeri negativi se è possibile iterare la funzione inversa (se esiste) di formula_48. Non è sempre possibile scrivere l'espressione analitica di formula_47. Di seguito sono trattati due casi di funzioni quadratiche iterate in cui può essere scritto la forma analitica in modo esplicito.
Per la funzione formula_50 (con formula_51 parametri reali) la forma iterata è
formula_52
ponendo formula_53
allora formula_54
quindi per induzione formula_55
sempre per induzione si ha che formula_56
allora formula_57 è la soluzione esplicita.
La mappa logistica formula_58 con parametro formula_59 può essere risolta solo in alcuni casi, almeno uno dei quali è caotico e uno non lo è. Nel caso caotico formula_60 la soluzione è
formula_61 dove la condizione iniziale formula_62 è data da formula_63.
Per formula_62 razionale, dopo un numero finito di iterazioni, formula_65 entra in una sequenza periodica. Per formula_62 irrazionale formula_65 non si ripete mai con sensibile dipendenza dalle condizioni iniziali; siccome la maggior parte dei formula_62 è irrazionale, il comportamento è caotico.
La soluzione della mappa logistica con formula_69 è formula_70 per formula_71.
Se formula_72, per ogni valore di formula_22 diverso dal valore instabile formula_74, il termine formula_75 per formula_76, quindi formula_77.
Una funzione quadratica in due variabili è una funzione definita da un polinomio di secondo grado della forma:
formula_4
dove formula_2 sono costanti e formula_80 non sono contemporaneamente nulli. Il grafico di questa funzione è una superficie(quadrica). L'insieme descritto da formula_7 è l'intersezione tra la superficie e il piano formula_82 ovvero una sezione conica.
Se formula_83 la funzione non ha massimi né minimi; il grafico è un paraboloide iperbolico.
Se formula_84 la funzione ha un punto di massimo (formula_17) o di minimo (formula_17); il suo grafico è un paraboloide ellittico. Le coordinate del punto di massimo o minimo sono formula_87.
Se formula_88 e formula_89 la funzione non ha massimi né minimi; il suo grafico è un cilindro parabolico.
Se formula_88 e formula_91 la funzione raggiunge un punto di massimo (formula_18) o minimo (formula_17); il suo grafico è un cilindro parabolico.
</text>
</doc>
<doc id="2573122" url="https://it.wikipedia.org/wiki?curid=2573122">
<title>Radice (matematica)</title>
<text>
In matematica, una radice di una funzione formula_1 è un elemento formula_2 nel dominio di formula_1 tale che formula_4. La definizione quindi generalizza la nozione di radicale, che è in questa chiave la radice delle funzioni della forma:
Questa definizione è molto importante in algebra quando formula_1 è un polinomio, per cui si parla anche di "zero".
In analisi complessa le radici di un polinomio sono dette zeri. Il teorema fondamentale dell'algebra garantisce l'esistenza di un numero di radici (contate con molteplicità) uguale al grado del polinomio.
Tra i casi non polinomiali più studiati, l'ipotesi di Riemann è una famosa congettura riguardante gli zeri della funzione zeta di Riemann.
Sia formula_7 una funzione fra due insiemi, tale che formula_8 contiene un elemento "zero". Ad esempio, formula_8 può essere l'insieme dei numeri reali, interi, o un qualsiasi altro gruppo. Un elemento formula_10 è una radice di formula_1 se
in altre parole, se l'immagine di formula_2 tramite formula_1 è zero (vedi la voce nucleo per una trattazione da un punto di vista algebrico).
Denotiamo con formula_15 l'insieme dei numeri reali. Si consideri la funzione polinomiale formula_16 data da:
Il numero 3 è radice di formula_1, perché formula_19. Più in generale, le radici di una funzione formula_16 sono i punti in cui il grafico di formula_1 interseca l'asse formula_2. Tra queste, la funzione esponenziale non ha radici, mentre la funzione seno ne ha infinite.
Si definisce la molteplicità di una radice formula_23 di un polinomio formula_24 come il numero naturale formula_25 tale che
dove formula_27 è diverso da zero. In altre parole, per il teorema di Ruffini, formula_25 è il numero di volte in cui possiamo dividere formula_29 per formula_30.
Se il polinomio formula_29 si "spezza" come
allora la molteplicità di formula_23 è il numero di volte che compare fra i vari formula_34. La molteplicità è però definita in generale, anche nel caso in cui il polinomio non si possa fattorizzare, perché siamo nel campo dei numeri reali, o semplicemente perché non riusciamo a farlo: ad esempio si vede subito che il polinomio
ha la radice zero con molteplicità 2, infatti
e 0 non è radice di formula_37.
Usando il teorema di Ruffini si dimostra facilmente per induzione che un polinomio formula_24 di grado formula_25 ha al più formula_25 radici, nel modo seguente: 
Sempre usando il teorema di Ruffini, si vede che formula_29 ha formula_25 radici se e solo se possiamo scrivere
dove formula_61 sono numeri reali distinti (le radici di formula_29).
Radici multiple e valore della derivataIl teorema di Ruffini permette di osservare facilmente che se formula_63 è una radice con molteplicità superiore a 1, allora la derivata del polinomio si annulla in formula_63 ,cioè formula_65.
Basta osservare che il polinomio si scompone come formula_66 e che calcolando la derivata, si ottiene un polinomio multiplo di formula_67
Equazione formula_68 (con formula_69 polinomio di grado formula_25)L'equazione è equivalente a formula_71. Poiché formula_72 è un polinomio di grado formula_25, l'equazione ammette sempre "n" radici (tenuto conto delle radici multiple). È possibile dimostrare che esistono al massimo formula_47 valori di formula_75 per cui l'equazione ammette radici multiple (equivalentemente: esistono al massimo formula_47 valori di formula_75 per cui la controimmagine formula_78 ha cardinalità inferiore a formula_25).
La dimostrazione utilizza quanto detto sopra rispetto al fatto che se formula_23 è una radice con molteplicità superiore a 1, allora la derivata formula_81 si annulla.
Un polinomio in una variabile a coefficienti reali è interpretabile come una particolare funzione formula_82. Lo studio delle radici di un dato formula_29 è stato sempre un problema centrale nello sviluppo della matematica, che equivale a risolvere l'equazione formula_50, il cui grado è pari al grado di formula_29. Il teorema di Niels Henrik Abel e Paolo Ruffini asserisce che non esistono sempre formule analoghe per le equazioni di grado maggiore al quarto, per cui è necessario l'ausilio della teoria dei gruppi. Alcune di queste sono tuttavia riconducibili con la Regola di Ruffini a equazioni di grado minore o uguale al quarto, per cui la soluzione sotto forma di radicale esiste sempre.
Un polinomio a coefficienti reali di grado dispari ha sempre una radice reale, mentre esistono polinomi di grado pari (arbitrariamente alto) che non ne hanno. In particolare:
Un polinomio reale può non avere radici: ad esempio formula_86 non ne ha, perché formula_87 per ogni formula_88. Per questo motivo sono stati introdotti i numeri complessi, che soddisfano molte proprietà mancanti ai numeri reali. Visto nel campo dei numeri complessi, lo stesso polinomio formula_86 ha due radici: formula_90.
Il teorema fondamentale dell'algebra asserisce infatti che un qualsiasi polinomio formula_29 a coefficienti complessi ha almeno una radice (il campo complesso è "algebricamente chiuso"). Usando il teorema di Ruffini come sopra, si dimostra come conseguenza che formula_29 si può sempre scrivere come
dove formula_61 sono numeri complessi non necessariamente distinti.
Viene in aiuto, per calcolare gli zeri di funzioni non polinomiali, l'analisi numerica, che ha sviluppato vari metodi iterativi che, seppur non fornendo il valore esatto del punto, vi si avvicinano con approssimazioni accettabili. I metodi principali sono:
</text>
</doc>
<doc id="263861" url="https://it.wikipedia.org/wiki?curid=263861">
<title>Funzione monotona</title>
<text>
In matematica, una funzione monotòna è una funzione che mantiene l'ordinamento tra insiemi ordinati. Queste funzioni sono state dapprima definite in analisi e successivamente sono state generalizzate nell'ambito più astratto della teoria degli ordini. I concetti di monotonia nelle due discipline sono in effetti gli stessi, anche se la terminologia è un po' differente. In analisi spesso si parla di funzioni monotone crescenti e monotone decrescenti, la teoria degli ordini invece preferisce i termini monotona e antitona oppure che conserva l'ordine (order-preserving) e che inverte l'ordine (order-reversing).
Sia formula_1 una funzione tra due insiemi formula_2 e formula_3, entrambi dotati di ordinamento parziale, denotato col simbolo formula_4 per entrambi gli insiemi. Di solito in analisi si pone l'accento su funzioni tra sottoinsiemi dei numeri reali e la relazione d'ordine formula_4 è la relazione d'ordine usuale dei numeri reali, ma questa posizione non è necessaria ai fini di questa definizione.
La funzione formula_6 si dice monotona se, per ogni formula_7, allora formula_8. Detto in altri termini, una funzione monotona "conserva l'ordinamento".
In analisi matematica di solito non è necessario utilizzare i metodi astratti della teoria degli ordini. Come già sottolineato, le funzioni di solito operano tra sottoinsiemi dei numeri reali, ordinati secondo l'ordinamento naturale.
Prendendo spunto dalla forma che ha il grafico di una funzione monotona sui reali, una funzione che possiede la proprietà sopra enunciata viene anche detta monotona crescente (o "monotona non decrescente").
Analogamente, una funzione viene detta monotona decrescente (o "monotona non crescente") se, per ogni formula_9 si ha che formula_10, cioè se "inverte l'ordinamento".
Se la relazione d'ordine formula_4 nella definizione di monotonia è sostituita dalla relazione d'ordine stretto formula_12, allora si richiede una proprietà più forte. Una funzione che gode di questa proprietà viene detta strettamente crescente. Anche in questo caso, invertendo il simbolo di ordinamento, si può ottenere il concetto di funzione strettamente decrescente.
Le funzioni strettamente crescenti o decrescenti sono dette strettamente monotone e sono iniettive (perché formula_13 implica formula_14) e dunque invertibili restringendo il codominio all'immagine.
I termini "non decrescente" e "non crescente" evitano ogni possibile confusione con "strettamente crescente" e "strettamente decrescente", rispettivamente.
In analisi, ognuna delle seguenti proprietà di una funzione
formula_15 implica la successiva:
Dimostriamo che la seconda affermazione implica la terza.
Sia l'intervallo formula_20 l'insieme di definizione della funzione formula_6 e sia formula_22 un punto di discontinuità della funzione. Dimostriamo per esclusione che questa deve essere "di prima specie".
Si consideri formula_6 ad esempio monotona non decrescente (un discorso analogo vale per una funzione non crescente). Data la proprietà precedente, formula_6 ammette limite sinistro e destro in formula_22:
E deve essere, per la monotonia, formula_27, perciò i limiti devono esistere finiti. Questo significa che la discontinuità "non può essere di seconda specie".
Poiché formula_22 è di discontinuità non può essere formula_29, perciò formula_30 e formula_31 non sono eguali, il che "esclude anche la discontinuità "eliminabile"".
Per esclusione, allora, in formula_22 si ha una "discontinuità di prima specie".
Dimostriamo ora che la terza affermazione implica la quarta.
Valgano le stesse ipotesi della precedente dimostrazione, e sia formula_33 un altro punto di discontinuità tale che, ad esempio, formula_34. Per la monotonia e per il risultato di cui sopra abbiamo formula_35 dove diciture come formula_36 sono state definite come nella dimostrazione precedente. Gli intervalli non vuoti formula_37 e formula_38 sono evidentemente disgiunti; poiché i razionali sono densi nei reali, ciascuno di questi intervalli ne contiene almeno uno, il quale non è contenuto nell'altro. Posso costruire una funzione che associ biunivocamente un numero razionale formula_39 a ogni intervallo del tipo formula_40 che lo contiene, il quale intervallo rappresenta il salto della funzione nel punto di discontinuità formula_41:
Poiché i numeri razionali sono numerabili, il numero di punti di discontinuità in formula_20 è al più numerabile.
Q.E.D.
Queste proprietà sono la ragione per la quale le funzioni monotone sono utili nel lavoro tecnico dell'analisi matematica. Due proprietà riguardanti queste funzioni sono:
Un'importante applicazione delle funzioni monotone la si ha nella teoria della probabilità. Se formula_57 è una variabile casuale, la sua funzione di distribuzione cumulativa
è una funzione monotona crescente.
Una funzione è unimodale se è monotona crescente fino a un certo punto (la "moda") e poi è monotona decrescente.
Nella teoria degli ordini non ci si restringe ai numeri reali, ma si ha a che fare con insiemi parzialmente ordinati arbitrari o addirittura con insiemi preordinati. In questi casi le definizioni date sopra di monotonia rimangono valide, anche se i termini "crescente" e "decrescente" vengono evitati, dal momento che perdono il loro significato grafico non appena si ha a che fare con ordinamenti che non sono totali. Inoltre le relazioni strette formula_12 e formula_80 sono poco usate in molti ordinamenti non totali e quindi non viene introdotta altra terminologia addizionale per esse.
Il concetto duale è spesso chiamato antitonia, anti-monotonia o order-reversing. Perciò una funzione antitona formula_6 soddisfa alla proprietà seguente:
per ogni formula_49 e formula_84 nel suo dominio. È facile vedere che la composizione di due funzioni monotone è a sua volta monotona.
Una funzione costante è sia monotona che antitona; inversamente, se formula_6 è sia monotona che antitona, e se il dominio di formula_6 è un reticolo, allora formula_6 deve essere costante.
Le funzioni monotone sono di primaria importanza nella teoria degli ordini. 
Alcune funzioni monotone degne di nota sono le 
immersioni d'ordine (order embedding)
(funzioni per le quali formula_88 e gli 
isomorfismi d'ordine (immersioni suriettive).
La monotonia dell'implicazione è una proprietà di molti
sistemi logici che afferma che le ipotesi di ogni fatto derivato possono essere liberamente estese con assunzioni addizionali. Ogni affermazione che era vera in una logica con questa proprietà, sarà ancora vera dopo l'aggiunta di un qualunque nuovo assioma (consistente). Logiche con questa proprietà possono essere chiamate monotone allo scopo di essere distinte dalle logiche non monotone.
</text>
</doc>
<doc id="224968" url="https://it.wikipedia.org/wiki?curid=224968">
<title>Sommatoria</title>
<text>
La sommatoria è un simbolo matematico che abbrevia, in una notazione sintetica, la somma di un certo insieme di addendi. La notazione prevede:
Nel caso più generale possibile abbiamo quindi una scrittura del tipo
dove formula_6 e formula_7 sono dei numeri interi, detti rispettivamente "limite inferiore della sommatoria" e "limite superiore della sommatoria". La scrittura si legge "sommatoria per formula_2 che va da formula_6 a formula_7 di formula_11". Con questa notazione si indica la somma di tutti gli addendi che si ottengono sostituendo all'indice formula_2 di formula_11 tutti i valori interi che vanno dal numero formula_6 al numero formula_7 compresi.
formula_16
Se formula_17
Oppure se formula_19
È anche possibile utilizzare questa notazione per somme di un numero infinito di termini; esse sono chiamate serie infinite. Al posto di formula_7 sopra il simbolo di sommatoria si usa il simbolo di infinito (formula_22). La somma di una serie siffatta è definita come il limite della somma dei primi formula_7 termini, al crescere di formula_7 oltre un qualsivoglia valore. In formule, 
Si può anche rimpiazzare formula_6 con un infinito negativo, e avere 
per un intero a scelta formula_6, ammesso che entrambi i limiti esistano.
È in uso lo stesso simbolo anche per descrivere somme i cui addendi non sono in corrispondenza con i numeri interi, ma soddisfano condizioni più generali, come ad esempio
dove la somma si estende a tutti i numeri che dividono un dato numero formula_7,
la somma di formula_32 su tutti gli formula_33 interi nell'intervallo specificato,
la somma su tutti gli formula_33 appartenenti all'insieme formula_36.
Nella matematica del continuo, l'equivalente della somma è l'integrale, il cui simbolo nasce appunto dalla deformazione del simbolo di sommatoria.
Albert Einstein introdusse per matrici e serie una notazione semplificata che da lui prende nome.
Nella notazione di sommatoria vale la seguente uguaglianza:
Si noti che perché l'uguaglianza sia valida, i limiti superiori ed inferiori delle due sommatorie devono essere uguali, altrimenti l'uguaglianza non è valida.
Nella notazione di sommatoria vale la seguente uguaglianza:
questo vuol dire che un fattore che si trova all'interno di una sommatoria può essere estratto da essa e, viceversa, un fattore esterno alla sommatoria può essere portato al suo interno.
Dalla dimostrazione si può dedurre che questa proprietà è equivalente alla proprietà distributiva della moltiplicazione rispetto all'addizione.
Ovviamente questa proprietà vale anche nel caso in cui un rapporto abbia una sommatoria al numeratore, infatti:
Scomposizione:
Traslazione di indici:
Nel caso in cui il termine della sommatoria sia un polinomio la traslazione dei limiti superiore e inferiore può essere fatta alterando opportunamente i soli termini dipendenti dall'indice:
Riflessione di indici:
La formula per la somma di tutti gli interi da formula_6 a formula_7 è 
Quindi in particolare la somma dei primi formula_7 interi positivi è 
La formula della somma dei primi formula_7 quadrati invece è
Da queste formule si può anche ricavare quella relativa alla somma dei primi formula_7 cubi.
Una relazione che lega i primi formula_7 cubi ai primi formula_7 numeri è la seguente:
</text>
</doc>
<doc id="283011" url="https://it.wikipedia.org/wiki?curid=283011">
<title>Piano complesso</title>
<text>
In analisi complessa, il piano complesso (chiamato anche piano di Argand-Gauss) è un modo per visualizzare lo spazio dei numeri complessi. Può essere pensato come un piano cartesiano modificato, con la parte reale rappresentata sull'asse formula_1 e la parte immaginaria rappresentata sull'asse formula_2. L'asse formula_1 è chiamato anche l'asse reale e l'asse formula_2 asse immaginario.
Il piano complesso è a volte chiamato piano di Argand per il suo uso nei diagrammi di Argand. La sua creazione è generalmente attribuita a Jean-Robert Argand, in parallelo con Gauss, per cui viene da alcuni anche definito "Piano di Gauss". Per non sminuire uno o l'altro matematico viene anche definito Piano di Argand-Gauss anche se fu descritto per la prima volta nel 1799 dal matematico norvegese-danese Caspar Wessel.
Il concetto del piano complesso consente una interpretazione geometrica dei numeri complessi. Sotto addizione, i numeri complessi si sommano come vettori, mentre la moltiplicazione di numeri complessi può essere geometricamente espressa usando le coordinate polari, dove il modulo del prodotto è il prodotto dei moduli dei fattori e l'argomento del prodotto (angolo dall'asse reale) è la somma degli angoli dei fattori.
I diagrammi di Argand sono frequentemente usati per graficare la posizione dei poli o di zeri di una funzione nel piano complesso.
Un numero complesso può essere separato in parte reale e immaginaria:
dove formula_1 e formula_2 sono numeri reali, e formula_8 è l'unità immaginaria. I numeri reali sono in corrispondenza biunivoca con i punti della retta reale euclidea. In questa notazione, il numero complesso formula_9 corrisponde al punto formula_10 del piano cartesiano. L'ascissa è data da formula_11 (la parte reale, l'asse delle formula_1) e da formula_13 (la parte immaginaria, l'asse delle ordinate).
Nel piano cartesiano, il punto formula_10 può anche essere rappresentato in coordinate polari come:
dove il "modulo" formula_16 e la "fase" formula_17 sono ricavate (per formula_18) dalle formule
Per il calcolo della fase si può usare la funzione arcotangente2.
</text>
</doc>
<doc id="226926" url="https://it.wikipedia.org/wiki?curid=226926">
<title>Matrice quadrata</title>
<text>
In matematica, in particolare in algebra lineare, una matrice è detta quadrata se ha un numero uguale di righe e colonne, detto "ordine" della matrice. Viene altrimenti detta "matrice formula_1".
Si tratta del tipo più comune e più importante di matrice, l'unico su cui sono definiti concetti come determinante, traccia, autovalore.
Le matrici quadrate sono utili a modellizzare le trasformazioni lineari di uno spazio vettoriale in se stesso (più precisamente, i suoi endomorfismi), le forme bilineari ed i prodotti scalari.
L'insieme di tutte le matrici quadrate a valori in un campo formula_2 fissato (ad esempio, i numeri reali o complessi) costituisce, rispetto alle operazioni di somma e di prodotto fra matrici, un anello. Eccetto il caso formula_3, tale anello non è commutativo. Viene indicato generalmente con formula_4. 
L'elemento neutro per la somma è la matrice nulla, avente zeri ovunque. L'elemento neutro per la moltiplicazione è la matrice identità formula_5, contenente elementi pari a 1 nella diagonale principale e elementi nulli altrove. Per esempio, se formula_6:
Considerato anche con l'operazione di moltiplicazione per scalare, l'insieme formula_8 è anche uno spazio vettoriale su formula_2, di dimensione formula_10.
Le due strutture di anello e spazio vettoriale formano insieme una struttura di algebra su campo.
Gli elementi invertibili nell'anello si dicono matrici invertibili. Una matrice quadrata formula_11 è invertibile se e solo se esiste una matrice quadrata formula_12 tale che:
In tal caso, formula_12 è la matrice inversa di formula_15, ed è indicata con formula_16.
L'insieme di tutte le matrici invertibili di tipo formula_17, dotato dell'operazione di moltiplicazione, è un gruppo, chiamato gruppo generale lineare: si tratta di un particolare gruppo di Lie.
Se formula_18 è un numero in formula_19 e formula_20 è un vettore non nullo in formula_21 tali che 
si dice che formula_23 è un autovettore di formula_11 e formula_18 è l'autovalore ad esso associato. Lo studio degli autovalori e autovettori è di fondamentale importanza in algebra lineare, e porta al concetto di diagonalizzabilità. Gli autovalori di una matrice sono le radici del suo polinomio caratteristico, definito come
Il determinante di una matrice quadrata è una quantità importante che può essere definita in numerosi modi diversi, tutti equivalenti fra di loro. I determinanti caratterizzano l'invertibilità di una matrice quadrata: una matrice quadrata è invertibile se e solo se il suo determinante è non nullo.
La traccia di una matrice quadrata è la somma degli elementi della sua diagonale principale.
Il polinomio caratteristico, oltre ad essere uno strumento utile per il calcolo degli autovalori, è anche un oggetto che ha fra i suoi coefficienti il determinante, la traccia ed altri valori numerici simili.
Quando una matrice è diagonalizzabile, determinante e traccia sono rispettivamente il prodotto e la somma degli autovalori della matrice.
La funzione esponenziale di matrice è definita per matrici quadrate attraverso una serie di potenze.
</text>
</doc>
<doc id="227182" url="https://it.wikipedia.org/wiki?curid=227182">
<title>Grafico di una funzione</title>
<text>
In matematica, il grafico di una funzione è l'insieme delle coppie ordinate costituite dagli elementi del dominio e dalle rispettive immagini.
Data una funzione formula_1, si definisce grafico di formula_2 il sottoinsieme del prodotto cartesiano formula_3 (cioè una relazione tra gli insiemi formula_4 e formula_5) dato da:
Per una funzione reale di variabile reale formula_7, il grafico formula_8 è il sottinsieme di formula_9 dato da formula_10. Per funzioni continue su un intervallo il grafico può essere visto come una curva in formula_9; la curva è inoltre «liscia» sugli intervalli in cui la funzione è regolare (ossia differenziabile).
Nel caso di una funzione reale di due variabili reali formula_12 definita su un sottinsieme del piano "x"-"y", il grafico è dato da:
La sua rappresentazione è tridimensionale per cui ad ogni punto del piano corrisponde un'ordinata formula_14 nello spazio. In alternativa si può usare il metodo delle curve di livello. In tal caso le curve di livello della funzione formula_15 sono date dall'insieme:
dove formula_17 è una costante in generale intera. La sua rappresentazione è quindi una famiglia di curve in cui ogni curva rappresenta un'altezza diversa del grafico. In pratica le curve sono le curve di intersezione del grafico formula_14 con i vari piani formula_19.
Si supponga che formula_4 e formula_5 siano spazi di Banach, e che formula_22 sia un operatore lineare. Il teorema del grafico chiuso afferma che formula_23 è continuo (e dunque limitato) se e solo se il suo grafico è chiuso nello spazio formula_3 dotato della topologia prodotto.
La restrizione sul dominio è necessaria a causa dell'esistenza di operatori lineari chiusi illimitati, che non sono necessariamente continui.
</text>
</doc>
<doc id="302769" url="https://it.wikipedia.org/wiki?curid=302769">
<title>Eccentricità (matematica)</title>
<text>
L'eccentricità in matematica è un parametro numerico non negativo "e" che caratterizza le sezioni coniche a meno di similitudine: ellissi per "e&lt;1", parabole per "e=1", iperboli per "e>1". L'eccentricità può essere interpretata come una misura di quanto una sezione conica è lontana dall'essere una circonferenza.
L'eccentricità può essere definita come un parametro che interviene nella costruzione di una conica, oppure in funzione degli angoli del cono e del piano che lo seziona, rispetto all'asse di rotazione del cono. Siccome il "tipo" di conica (la sua classe di similitudine) e le sue caratteristiche sono definiti in funzione dell'eccentricità, questa può essere ricavata indirettamente dalle formule.
Fissati nel piano una retta "r" ("direttrice") e un punto "F" ("fuoco") esterno a "r", una conica di eccentricità "e&gt;0" è il luogo dei punti "P" che hanno distanza dal fuoco pari a "e" volte la loro distanza dalla direttrice:
Fissati nello spazio un cono circolare retto di apertura α (l'angolo tra l'asse di rotazione e la retta generatrice del cono) e un piano non passante per il vertice, che forma un angolo β con l'asse di rotazione del cono; l'eccentricità della sezione conica è definita come:
Per formula_3, ovvero formula_4, si ha un'ellisse, che ha "F" come uno dei due fuochi.
Scrivendo l'equazione dell'ellisse in forma canonica
l'eccentricità "e", l'asse maggiore "2a", l'asse minore "2b" e la distanza interfocale "2c" sono legati tra loro dalle formule
Invertendo le formule si può esprimere l'eccentricità come
L'eccentricità fornisce dunque una misura di quanto l'ellisse sia "schiacciata", anche se in maniera meno diretta del rapporto "a/b" tra i semiassi.
In particolare per formula_11, ovvero "a=b", l'ellisse diventa una circonferenza (solo come sezione conica: con la costruzione geometrica si ottiene il solo punto "F").
Per formula_12, ovvero formula_13 si ottiene una parabola avente fuoco "F" e direttrice "r": è il luogo dei punti equidistanti da "F" e da "r".
Per formula_14, ovvero formula_15, si ha un'iperbole, uno dei cui due fuochi è "F".
Scrivendo l'equazione dell'iperbole in forma canonica
con asintoti
l'eccentricità "e", la distanza tra i vertici "2a", i coefficienti angolari "±b/a" degli asintoti e la distanza interfocale "2c" sono legati tra loro dalle formule
Invertendo le formule si può esprimere l'eccentricità come
L'eccentricità fornisce dunque una misura di quanto l'iperbole sia "schiacciata", anche se in maniera meno diretta dei coefficienti angolari "±a/b" degli asintoti.
In particolare per formula_20, ovvero "a=b", l'iperbole è equilatera (questo è possibile solo se "cos(α)=cos(β)/√2≤1/√2=cos(π/4)", ovvero per "α≥π/4".)
</text>
</doc>
<doc id="222191" url="https://it.wikipedia.org/wiki?curid=222191">
<title>Formula</title>
<text>
Una formula (dal latino "formula" diminutivo di "forma": modo, norma, regola) è un'espressione matematica utilizzata per esprimere in maniera concisa ed inequivocabile le relazioni quantitative.
Una formula stabilisce un procedimento di calcolo combinando due o più grandezze mediante l'uso di opportuni operatori. Il linguaggio utilizzato nelle formule è quindi quello della matematica, che, per conferire generalità ai propri enunciati, si serve di simboli al posto di numeri.
Molte delle formule utilizzate più frequentemente sono uguaglianze o disuguaglianze. Esempi ben noti sono:
ove con p si intende il semiperimetro. 
Ci sono metodi diversi per ottenere una formula. È possibile per esempio partire da presupposti fondamentali (assiomi) per dedurne logicamente conseguenze esprimibili con formule. È anche possibile ricavare una formula dall'osservazione di dati ottenuti sperimentalmente, o dalla valutazione sistematica di informazioni con i metodi della statistica; in questo caso si parla di "formula empirica".
In chimica la formula è semplicemente una sequenza codificata di abbreviazioni. Il caso più tipico è costituito dalla chimica, dove le abbreviazioni indicano gli elementi che concorrono alla formazione di un composto, per esempio HO, formula chimica dell'acqua, in cui H indica l'idrogeno e O l'ossigeno.
</text>
</doc>
<doc id="246174" url="https://it.wikipedia.org/wiki?curid=246174">
<title>Funzione inversa</title>
<text>
In matematica, una funzione formula_1 si dice invertibile se esiste una funzione formula_2 tale che
più formalmente,
dove formula_9 indica la funzione composta e formula_10 indica la funzione identità su formula_11.
Se formula_12 è invertibile, allora la funzione formula_13 della definizione è unica; quest'unica funzione formula_13 è detta funzione inversa di formula_12 e viene indicata con formula_16 (coerentemente con la notazione per l'elemento inverso rispetto alla composizione).
Se una funzione è invertibile, allora è biiettiva, ovvero è sia iniettiva che suriettiva. Infatti, con le notazioni di cui sopra
Viceversa, se formula_12 è una biiezione, allora possiamo definirne un'inversa formula_13, stipulando che formula_26 sia quell'unico elemento formula_4 tale che formula_28; infatti tale formula_29 esiste per la suriettività, ed è unico per l'iniettività. Inoltre risulta formula_30 per ogni formula_31 e formula_32 per ogni formula_33.
Una funzione formula_34 ammette un'inversa destra (in alcuni contesti sezione) se esiste una funzione formula_35 tale che
Con l'assioma della scelta, una funzione ammette un'inversa destra se e solo se è suriettiva.<br/>
L'inversa destra di una funzione non è unica: ad esempio la funzione formula_37 definita da formula_38 ammette come inversa destra qualunque funzione formula_39 che per ogni formula_40 soddisfi formula_41 oppure formula_42.
Una funzione formula_34 ammette un'inversa sinistra (in alcuni contesti retrazione) se esiste una funzione formula_44 tale che
Una funzione ammette un'inversa sinistra se e solo se è iniettiva.<br/>
L'inversa sinistra di una funzione non è unica: ad esempio la funzione formula_46 definita da formula_47 ammette come inversa sinistra qualunque funzione formula_48 la cui restrizione agli interi sia l'identità, ovvero che per ogni formula_49 soddisfi formula_50.
Se formula_12 ammette sia un'inversa destra formula_13 che un'inversa sinistra formula_53, allora formula_12 è invertibile con inversa formula_55:
Applicando le proprietà precedenti, risulta:
Nel linguaggio delle categorie, la funzione inversa formula_16 è il morfismo inverso di formula_12 all'interno della categoria degli insiemi.
Nel linguaggio dei gruppi, se formula_59 è invertibile, allora la funzione inversa formula_16 è l'elemento inverso di formula_12 nel gruppo delle permutazioni di formula_62.
Se formula_63 e formula_64 sono invertibili, allora l'inversa della loro composizione è data da
cioè si compongono le inverse a ordine invertito. Infatti
e
Ad esempio, la funzione
ha come inversa la funzione
Se una funzione è l'inversa di se stessa si dice che è un'involuzione. Un esempio è il coniugio complesso,
Se formula_34 è invertibile, allora per ogni coppia formula_72 sono equivalenti le affermazioni
Infatti ogni funzione formula_34 è una relazione formula_80 tra i due insiemi formula_62 e formula_82, che può essere identificata con l'insieme delle coppie che sono in relazione, formula_83, ovvero con il grafico della funzione. La relazione inversa è semplicemente la simmetrica, formula_84 se e solo se formula_85; dunque
In particolare, per funzioni di variabile reale, il grafico della funzione inversa formula_16 è simmetrico del grafico di formula_12 rispetto alla "diagonale" formula_89 ovvero la retta bisettrice del primo e del terzo quadrante.
In analisi matematica se una funzione reale è invertibile e derivabile in un punto con derivata non nulla, allora anche la sua inversa è derivabile e risulta
Il teorema della funzione inversa è inoltre un importantissimo teorema che afferma che una funzione con derivata non nulla in un punto è localmente invertibile (cioè la sua restrizione in un opportuno intorno del punto è invertibile).
Se una funzione è espressa come composizione di funzioni invertibili, allora la sua inversa può essere ricavata come descritto nel relativo paragrafo.
In particolare, si può ottenere rapidamente un'espressione esplicita per la funzione inversa ricordando che formula_91 è equivalente a formula_92. Dunque è sufficiente esprimere formula_29 in funzione di formula_94
Per esempio, l'inversa della funzione
può essere determinata esplicitamente ricavando
Quindi
In ogni caso è necessario definire una funzione inversa: la sottrazione, la divisione e l'estrazione di radice applicate nell'esempio precedente sono definite come le funzioni inverse rispettivamente della somma, della moltiplicazione e dell'elevamento a potenza. Se una funzione invertibile non è esprimibile come composizione di funzioni delle quali sono già state definite le funzioni inverse, allora la funzione inversa non potrà essere espressa come composizione di inverse note e dovrà essere definita "ex-novo".
Ad esempio, la funzione
ha un'inversa definita appositamente: il logaritmo prodotto.
Ogni funzione può essere "resa" biiettiva, quindi invertibile, restringendo il suo dominio e il suo codominio, ovvero sostituendo ad essa una nuova funzione con dominio e codominio "più piccoli" e che mantiene una parte delle associazioni.
Ad esempio, è sempre possibile restringere il dominio ad un singolo elemento formula_29 ed il codominio al singolo elemento formula_91: la funzione così definita,
è invertibile:
Con questo procedimento si ottiene una funzione diversa da quella di partenza, e la sua funzione inversa non è funzione inversa della funzione originale. Poiché su alcuni elementi si comporta come una funzione inversa, viene considerata una inversa parziale.
Ogni funzione può essere "resa" iniettiva restringendo il suo dominio: se nel dominio sono presenti due elementi formula_103 tali che formula_104, allora la funzione non può essere iniettiva. "Togliendo" formula_105 o formula_106 dal dominio, quest'ostacolo viene eliminato.
Ad esempio, la funzione
non è iniettiva, ma la funzione
è iniettiva.
Non esiste un'unica restrizione del dominio che renda iniettiva la funzione: per ogni coppia di elementi formula_103 tali che formula_104, si può scegliere di escludere dal dominio formula_105, o formula_106, o entrambi.
Nell'esempio indicato, si ottengono funzioni iniettive anche prendendo come dominio formula_113, o formula_114.
Nel caso di funzioni reali continue, dove sia possibile applicare una nozione di continuità e di separazione, si usa scegliere come dominio un intervallo massimale e parlare di "rami" della funzione, e viene convenzionalmente scelto un ramo principale.
Ogni funzione può essere "resa" suriettiva restringendo il suo codominio: se nel codominio è presente un elemento formula_94 che non è immagine di alcun elemento del dominio, allora la funzione non può essere suriettiva. "Togliendo" formula_94 dal codominio, quest'ostacolo viene eliminato.
Ad esempio, la funzione
non è suriettiva, ma la funzione
è suriettiva.
Non esiste un'unica restrizione del codominio che renda suriettiva la funzione, ma esiste un'unica restrizione massimale, che contiene tutte le altre: l'immagine, ovvero l'insieme di tutte le immagini degli elementi del dominio,
Combinando i due metodi indicati, ovvero restringendo tanto il dominio quanto il codominio di una funzione, questa può essere resa sia iniettiva che suriettiva, ovvero biiettiva (e di conseguenza invertibile).
Ad esempio, la funzione
non è invertibile, ma la funzione
è invertibile.
Non tutte le funzioni sono invertibili, ma ad ogni elemento del codominio può essere associata la sua controimmagine (o "fibra"), indicata talvolta con abuso di notazione
Quest'associazione definisce una funzione, detta funzione inversa generalizzata, tra il codominio e l'insieme delle parti del dominio
Ogni funzione è una relazione tra due insiemi, ed è invertibile nel senso delle relazioni: formula_85 se e solo se formula_84.
La relazione inversa non è una funzione, se la funzione di partenza non è invertibile.
Se però la funzione di partenza è suriettiva, allora per ogni elemento formula_33 del codominio esiste almeno un elemento del dominio formula_31 tale che formula_91, ovvero formula_92. Questo elemento non è necessariamente unico, se formula_12 non è iniettiva. In questo caso formula_16 non è una funzione (non è "univoca"), ma è una "funzione multivoca", o "multifunzione".
</text>
</doc>
<doc id="245984" url="https://it.wikipedia.org/wiki?curid=245984">
<title>Moltiplicazione di matrici</title>
<text>
In matematica, e più precisamente in algebra lineare, la moltiplicazione di matrici è il prodotto righe per colonne tra due matrici, possibile sotto certe condizioni, che dà luogo ad un'altra matrice. Se una matrice rappresenta una applicazione lineare, il prodotto fra matrici è la traduzione della composizione di due applicazioni lineari. Quindi se due matrici 2 x 2 rappresentano ad esempio due rotazioni nel piano di angoli α e β, il loro prodotto è definito in modo tale da rappresentare una rotazione di angolo α + β.
Sia formula_1 un anello. Siano date una matrice formula_2 di dimensione formula_3 ed una seconda matrice formula_4 di dimensioni formula_5 a valori in formula_1. Siano formula_7 gli elementi di formula_2 e formula_9 gli elementi di formula_4. Si definisce il prodotto matriciale di formula_2 per formula_4 la matrice formula_13 a valori in formula_1 e di dimensioni formula_15 i cui elementi formula_16 sono dati da:
per ogni valore di riga "i" e di colonna "j".
Due matrici possono essere moltiplicate fra loro solo se il numero di colonne della prima è uguale al numero di righe della seconda, ed il prodotto tra due matrici non è commutativo.
Una matrice può essere moltiplicata con se stessa solo se è quadrata. In questo caso, il prodotto formula_18 si denota con formula_19. Più in generale, la potenza formula_20-esima di una matrice è:
dove formula_20 è un numero naturale, anche se spesso per esponenti molto maggiori dell'ordine della matrice è più semplice calcolarle servendosi della teoria della funzione di matrice, che permette inoltre di generalizzare la definizione di potenza fino ad ammettere un esponente non solo complesso, ma addirittura arbitrario.
Un'altra definizione informale della moltiplicazione matriciale, atta a permetterne una più rapida e immediata memorizzazione, è "moltiplicazione riga per colonna", infatti, per ottenere l'elemento della i-esima riga e j-esima colonna della matrice prodotto basta porre un indice sulla riga i della prima matrice, l'altro sulla colonna j della seconda e moltiplicare gli elementi indicati, quindi scorrere di un posto con le dita e moltiplicare, fino a raggiungere la fine della colonna e della riga, infine sommare i vari prodotti ottenuti.
Una matrice con una sola riga, cioè di dimensione formula_63, è un vettore riga. Analogamente, una matrice con una sola colonna, cioè di dimensione formula_64 è un vettore colonna. Nell'operazione di moltiplicazione questi due oggetti si comportano in modo differente. 
Siano formula_65 una matrice formula_3 e formula_67 un vettore colonna formula_68.
Il prodotto di formula_2 per il vettore formula_70 è il prodotto di matrici:
Le componenti di formula_72 sono:
Un algoritmo per la moltiplicazione matrice per vettore è:
Questo prodotto è ampiamente usato in algebra lineare perché descrive una applicazione lineare. Ad esempio, il prodotto:
rappresenta una rotazione di angolo formula_75 nel piano cartesiano.
In alcuni casi può essere utile effettuare il prodotto formula_76: il risultato è un altro vettore riga.
La moltiplicazione di una matrice formula_77 per uno scalare formula_78, cioè un elemento dell'anello cui appartengono gli formula_79, è ottenuta moltiplicando ogni elemento di formula_80 per lo scalare:
Se l'anello di partenza non è commutativo, questa viene indicata come "moltiplicazione sinistra", e può differire dalla "moltiplicazione destra":
Se l'anello di partenza non è commutativo, ad esempio se è l'anello dei quaternioni, le due moltiplicazioni non sono equivalenti. Ad esempio:
Sono stati definiti nel tempo altri tipi di prodotto tra matrici, meno fortunati in quanto a utilizzo dell'usuale prodotto righe per colonne. In particolare si può nominare il "prodotto di Hadamard" o "prodotto puntuale", in cui il prodotto di formula_86 e formula_87 è dato da formula_88. Ad esempio:
Un'altra costruzione è data dal prodotto di Kronecker, che trova applicazioni nel calcolo tensoriale, dato da:
espressa sotto forma di matrice a blocchi, in cui ogni blocco formula_91-esimo è dato dalla matrice formula_4 moltiplicata per lo scalare formula_7.
</text>
</doc>
<doc id="221185" url="https://it.wikipedia.org/wiki?curid=221185">
<title>Topologia prodotto</title>
<text>
La topologia prodotto è una topologia naturale definita sul prodotto cartesiano di alcuni spazi topologici. 
Sia "I" un insieme (anche infinito) di indici, e "X" uno spazio topologico, per ogni "i" in "I". Sia "X" = Π "X" il prodotto cartesiano degli insiemi "X". Per ogni "i" abbiamo una proiezione "p:X" → "X".
La topologia prodotto su "X" è definita in uno dei seguenti modi (tutti equivalenti):
Le proiezioni "p", oltre ad essere continue, sono aperte, cioè la proiezione di un aperto è un aperto. Non sono invece in generale chiuse: si prenda ad esempio la proiezione di R su uno dei due assi; un ramo di iperbole (che è chiuso nel piano) è proiettato su una semiretta aperta di equazione "x" &gt; 0.
La topologia prodotto è spesso chiamata in analisi la topologia della convergenza puntuale per il fatto seguente: una successione in "X" converge se e solo se convergono tutte le sue proiezioni. In particolare, nello spazio "X" = R delle funzioni da "I" in R, una successione di tali funzioni converge se converge puntualmente.
Elenchiamo qui altre proprietà.
</text>
</doc>
<doc id="218726" url="https://it.wikipedia.org/wiki?curid=218726">
<title>Insieme chiuso</title>
<text>
atematica, in particolare in topologia, un sottoinsieme formula_1 di uno spazio topologico formula_2 è chiuso se il suo complementare è aperto. Intuitivamente se un insieme è chiuso vuol dire che il "bordo" dell'insieme appartiene all'insieme stesso, infatti una definizione equivalente alla precedente è la seguente: formula_1 è "chiuso" se contiene la sua frontiera.
Gli insiemi chiusi hanno quindi le seguenti proprietà, "complementari" a quelle degli insiemi aperti, valide in un qualsiasi spazio topologico:
Si possono usare queste proprietà come assiomi per definire una topologia su formula_4 a partire dai chiusi, che coincide con quella generata nel modo usuale dalla famiglia formula_6 degli aperti complementari.
Sono insiemi chiusi della retta reale con l'usuale topologia indotta dalla metrica euclidea i seguenti sottoinsiemi:
Non sono insiemi chiusi della retta reale con l'usuale topologia indotta dalla metrica euclidea i seguenti sottoinsiemi:
Altri esempi di insiemi chiusi sono:
</text>
</doc>
<doc id="217228" url="https://it.wikipedia.org/wiki?curid=217228">
<title>Razionalizzazione (matematica)</title>
<text>
In matematica, la razionalizzazione del denominatore di una frazione è un procedimento algebrico che consente di eliminare dal denominatore le espressioni irrazionali, ossia quelle contenenti radicali algebrici. Ciò viene ottenuto tramite la moltiplicazione del numeratore e del denominatore della frazione per un opportuno fattore. La razionalizzazione è utile, fra l'altro, per semplificare il calcolo numerico di tali espressioni, dal momento che la divisione per un numero irrazionale comporta un calcolo laborioso e potenzialmente più soggetto ad errori di approssimazione.
Si consideri una frazione della forma:
In questo caso, certamente il più semplice, si riesce a razionalizzare il denominatore moltiplicando semplicemente numeratore e denominatore per formula_2:
Nel caso più generale della forma: 
con "n" &gt; "m", il fattore razionalizzante è formula_5. Infatti:
Le frazioni della forma normale:
ricordando il prodotto notevole formula_8, si razionalizzano come segue:
Lo stesso accorgimento funziona ovviamente anche quando nel denominatore vi sono somme come formula_10 o somme di 3 o più radicali quadratici. Ad esempio:
Le frazioni della forma:
si risolvono facilmente ricorrendo ai prodotti notevoli:
Infatti:
Anche in questo caso, il procedimento funziona bene anche per denominatori della forma formula_15 o simili.
Sebbene sia più frequente la necessità di razionalizzare il denominatore di una frazione (riconducendosi ad un'espressione equivalente con i radicali al numeratore), talvolta può essere utile applicare tecniche del tutto analoghe al numeratore. Per esempio, in analisi matematica, ciò consente di risolvere alcune forme indeterminate che possono comparire nel calcolo dei limiti. Ad esempio:
Questo limite si presenta nella forma indeterminata formula_17. In questo caso risulta conveniente razionalizzare il numeratore della frazione:
A questo punto, osservando che formula_19, il limite si può riscrivere come segue:
Si può notare come la razionalizzazione del numeratore abbia consentito di eliminare a denominatore il fattore formula_21 che rendeva impossibile il calcolo immediato del limite.
La razionalizzazione si effettua anche per determinare il reciproco di un numero complesso e, di conseguenza, per effettuare la divisione. Infatti, dato un numero complesso formula_22, si può determinare il reciproco moltiplicando il numeratore e il denominatore per il coniugato di formula_23:
</text>
</doc>
<doc id="229601" url="https://it.wikipedia.org/wiki?curid=229601">
<title>Composizione di funzioni</title>
<text>
In matematica, la composizione di funzioni è l'applicazione di una funzione al risultato di un'altra funzione.
Più precisamente, una funzione formula_1 tra due insiemi formula_2 e formula_3 trasforma ogni elemento di formula_2 in uno di formula_3: in presenza di un'altra funzione formula_6 che trasforma ogni elemento di formula_3 in un elemento di un altro insieme formula_8, si definisce la composizione di formula_1 e formula_6 come la funzione che trasforma ogni elemento di formula_2 in uno di formula_8 usando prima formula_1 e poi formula_6. Il simbolo Unicode dell'operatore è ∘ (U+2218).
Formalmente, date due funzioni formula_15 e formula_16 definiamo la funzione composta
applicando prima formula_1 ad formula_20 e quindi applicando formula_6 al risultato formula_22.
Ad esempio, supponiamo che l'altezza di un aereo al tempo formula_23 sia data da una funzione formula_24 e che la concentrazione di ossigeno nell'atmosfera all'altezza formula_20 sia data da un'altra funzione formula_26. Allora formula_27 descrive la concentrazione di ossigeno nella posizione in cui sta l'aereo al tempo formula_23.
Per ragioni storiche la composizione è scritta "da destra verso sinistra", in contrasto con la normale lettura "da sinistra a destra" delle lingue europee. Per questo motivo alcuni autori preferiscono usare una notazione invertita, e scrivere formula_29 invece di formula_30.
Per comporre due funzioni è strettamente necessario che il dominio di formula_6 coincida con il codominio di formula_1. In alcuni ambiti, tuttavia, identificando impropriamente due funzioni che hanno la stessa legge di applicazione, ma diversi domini e codomini, si ritiene sufficiente che l'immagine di formula_1 e il dominio di formula_6 abbiano un'intersezione non vuota.
La composizione di funzioni è sempre associativa. In altre parole, se formula_1, formula_6 e formula_37 sono tre funzioni con domini e codomini opportuni, allora formula_38. Per questo motivo si possono omettere le parentesi nella composizione di più funzioni.
La composizione di due funzioni iniettive è iniettiva, e di due funzioni suriettive è suriettiva. Quindi la composizione di due funzioni biettive è biettiva. Ma non vale il viceversa.
L'insieme delle funzioni biettive formula_39, con l'operazione di composizione, è un gruppo. La proprietà associativa è garantita per quanto detto sopra, l'elemento neutro è la funzione identità formula_40 per ogni formula_20) e un inverso esiste sempre perché le funzioni sono biettive.
Questo gruppo è detto anche gruppo delle permutazioni di formula_2.
Se l'insieme formula_2 contiene più di due elementi, tale gruppo non è commutativo: generalmente due funzioni biettive non commutano.
La derivata della funzione composta è il prodotto tra la derivata della funzione "esterna" moltiplicata per la derivata della funzione "interna":
dove le notazioni formula_45 e formula_46 indicano il medesimo significato di derivata.
La formula è valida anche per funzioni di più variabili reali e per funzioni vettoriali. Il teorema di derivazione delle funzioni composte afferma che se:
è un vettore di formula_48 le cui componenti sono funzioni derivabili:
e se formula_1 è una funzione differenziabile in formula_51, allora la funzione composta:
è differenziabile nella variabile formula_23 e si ha:
dove formula_55 è il gradiente di formula_1 e formula_57 è il prodotto scalare euclideo standard.
Infine, se formula_58 e formula_59 sono due funzioni vettoriali differenziabili componibili, allora:
dove formula_61 è la moltiplicazione di matrici e formula_62 è la matrice jacobiana di formula_58.
Una funzione formula_39 (non necessariamente biettiva) può essere composta con sé stessa formula_65 volte, ed il risultato, detto iterata formula_65-esima di formula_1, può essere scritto formula_68 quando non genera ambiguità. Ad esempio con formula_69 si denota comunemente il quadrato del seno di formula_20, cioè formula_71, anziché il valore in formula_20 della composizione del seno con se stesso, cioè formula_73.
Lo studio delle composizioni iterate di una funzione è argomento comune nell'ambito dei sistemi dinamici discreti e in particolare nella definizione dei frattali, che si possono trovare iterando "infinite volte" una funzione.
</text>
</doc>
<doc id="229624" url="https://it.wikipedia.org/wiki?curid=229624">
<title>Funzione costante</title>
<text>
In matematica una funzione costante (a volte anche chiamata collasso) è una funzione i cui valori non variano, e rimangono quindi costanti al variare della variabile indipendente nel suo dominio. 
Una funzione formula_1 tra due insiemi è costante se e solo se esiste un formula_2 in formula_3 per cui formula_4 per ogni formula_5 in formula_6. La funzione formula_7 assume cioè lo stesso valore formula_2 su tutti gli formula_5 in formula_6.
Ad esempio, la funzione formula_11 definita sui numeri reali data da formula_12 (indipendentemente da formula_5) è costante.
In termini più astratti, una funzione formula_1 è costante se e solo se vale la seguente "proprietà universale": 
Questa proprietà dice che la funzione costante è un "morfismo costante" nella categoria delle funzioni.
</text>
</doc>
<doc id="229756" url="https://it.wikipedia.org/wiki?curid=229756">
<title>Radicale (matematica)</title>
<text>
In matematica, la radice formula_1-esima o radicale formula_1-esimo, con formula_3, di un numero reale formula_4, scritto come formula_5, è un numero reale formula_6 tale che formula_7. Il numero reale formula_8 è detto radicando, il numero formula_1 è detto indice e il numero formula_10 è detto radice formula_1-esima di formula_8.
Una radice con indice 2 è indicata con il nome di radice quadrata e una radice con indice 3 con il nome di radice cubica o radice terza, ma esistono e si possono creare radici con qualsiasi indice.
Le condizioni di esistenza sono quell'insieme dei valori delle variabili contenute nel radicale per i quali esso esiste nel campo dei numeri reali.
La funzione radice formula_1-esima è una funzione definita da formula_14, perciò formula_5 resta definita formula_16
Per esempio, i seguenti radicali esprimono numeri reali: 
Si può ottenere un risultato analogo alla radice ennesima attraverso l'elevamento a potenza con esponente frazionario:
Tuttavia la funzione potenza è definita da formula_19, perciò essa permette di definire due sottocasi:
Ciò implica che equazioni del tipo formula_22, con formula_1 pari e formula_24 non hanno soluzioni reali, esse infatti appartengono all'insieme dei numeri immaginari, sottoinsieme dell'insieme dei numeri complessi, indicato con formula_25, che vengono espressi come somma di un numero reale e un numero immaginario.
Ad esempio, l'equazione formula_26 avrà per soluzioni formula_27 e formula_28, dove formula_29 rappresenta l'unità immaginaria.
Quanto visto finora ci permette d'individuare che, ad esempio, la condizione di esistenza del radicale formula_30 è formula_31, dato che il radicando deve essere sempre positivo.
Ecco altri esempi di condizioni di esistenza:
Esistono delle proprietà fondamentali delle radici che vengono elencate di seguito:
Dalla definizione di radicale segue che: 
Si elevino all'ennesima potenza entrambi i membri dell'uguaglianza:
Poiché le formula_1-esime potenze dei due membri sono uguali (formula_59), sono uguali anche le basi.
Applicando la proprietà:
Allo stesso modo, con formula_61:
Si elevino all'ennesima potenza entrambi i membri dell'uguaglianza:
Poiché le formula_1-esime potenze dei due membri sono uguali formula_70, sono uguali anche le basi.
Applicando la proprietà:
Allo stesso modo, con formula_72:
Non è necessario dimostrare questa proprietà in quanto è una diretta conseguenza della seconda proprietà dei radicali con il radicando sempre positivo.
Applicando la proprietà:
Allo stesso modo, con formula_78
Si elevino all'formula_83-esima potenza entrambi i membri dell'uguaglianza:
Poiché le formula_83-esime potenze dei due membri sono uguali (formula_87), sono uguali anche le basi.
Applicando la proprietà:
Allo stesso modo, con formula_89, formula_90
Per il teorema del prodotto si ottiene:
Ma, per la seconda proprietà fondamentale dei radicale è formula_97, perciò:
Applicando la proprietà:
Allo stesso modo, con formula_100:
Il teorema presenta le seguenti varianti, facilmente verificabili:
Elevando tutto alla formula_1-esima potenza si ottiene:
Radicando ora il tutto sotto radice di indice formula_1 risulta:
Quindi:
Applicando la proprietà:
Allo stesso modo:
Il teorema presenta le seguenti varianti, facilmente verificabili:
Tenendo conto di quanto detto finora, si ha che per formula_4
Il primo enunciato si ottiene direttamente dalla definizione di radicale, il secondo applicando il teorema delle potenze ad esponente negativo.
dove formula_24, formula_65e formula_136.
Per ogni numero complesso formula_137, ci sono formula_1 diversi numeri complessi formula_10 tali che formula_7, quindi il simbolo formula_5 non può essere usato univocamente. Se formula_142, parliamo di radici n-esime dell'unità.
È importante ricordare che, in generale, è sempre (per formula_47, formula_54):
tenendo presente che l'uguaglianza si ha se e solo se almeno uno tra formula_8 e formula_10 è formula_148.
Quindi, affermare che formula_149 sarebbe un gravissimo errore.
Partendo dalla disequazione:
Elevando al quadrato si ottiene:
Poiché è formula_47 e formula_54 per ipotesi, è anche formula_156, quindi la tesi è vera.
Il teorema è facilmente estendibile alle radici di indice formula_1-esimo:
La somma di radicali è possibile solo se sono presenti radicali simili, cioè nel caso in cui:
Ad esempio:
Nel secondo esempio si tenga presente che vale formula_166.
La proprietà invariantiva dei radicali afferma che:
""Moltiplicando o dividendo sia l'indice di un radicale che l'esponente del suo radicando per un numero naturale diverso da 0, si ottiene un radicale equivalente a quello dato.""
In simboli:
Si elevi alla formula_170 potenza ciascuno dei due membri:
Si ottiene formula_173, e, poiché le formula_170-esime potenze dei due membri sono uguali, sono uguali anche le basi.
Utilizzando la proprietà invariantiva è possibile semplificare i radicali, dividendo sia indice che esponente del radicando per uno stesso numero:
Allo stesso modo:
Si noti che nell'espressione è stato inserito il valore assoluto: questo perché, mentre il primo radicale formula_177 esiste sempre, dato che ha il radicando elevato ad un indice pari, successivamente viene semplificato ed il suo radicando non è più elevato ad un esponente pari. Quindi è necessario inserire il valore assoluto, per fare in modo che l'uguaglianza si mantenga valida.
La radice formula_1-esima di formula_148 vale sempre formula_148, escludendo il caso in cui è formula_181, poiché la radice di indice formula_148 ha significato solo se il radicando è uguale ad formula_183, ossia nel caso:
Inoltre, è sempre:
Nelle elaborazioni di espressioni e formule algebriche, è spesso utile manipolare i radicali usando le relazioni scritte sopra, senza tentare di calcolare il valore di ogni singolo elemento. Ad esempio, se formula_8 e formula_10 sono due numeri positivi distinti:
L'ultima relazione può servire per razionalizzare il denominatore di un'espressione o di un'equazione.
Può capitare, spesso in analisi, di trovare radicali letterali, ossia radici quadrate con radicando letterale. In questo caso, dapprima bisogna trovare la condizione di esistenza (chiamata anche C.A. "Condizione di accettabilità", o C.R.R. "Condizione di Realtà del Radicando"), nel caso si lavori solo tra i numeri reali, per poi considerare sempre quando le lettere indicano numeri positivi o numeri negativi.
Un esempio di radicale letterale:
Le condizioni di esistenza si ricavano nel seguente modo:
Pertanto il campo di esistenza del radicale è: formula_200.
</text>
</doc>
<doc id="267624" url="https://it.wikipedia.org/wiki?curid=267624">
<title>Logaritmo naturale</title>
<text>
Il logaritmo naturale (o "logaritmo neperiano") è il logaritmo in base "e", dove formula_1 è uguale a formula_2 Il logaritmo naturale è definito per tutte le formula_3 reali e positive, ma anche per i numeri complessi diversi da zero.
Se la funzione esponenziale è stata definita usando una serie infinita, il logaritmo naturale può essere definito come la sua funzione inversa, intendendo che formula_4 è il numero per cui formula_5. Dal momento che il dominio della funzione esponenziale include tutti i numeri reali positivi e poiché la funzione esponenziale è strettamente crescente, questa è definita per tutte le formula_3 reali positive.
In alternativa è possibile definire il logaritmo come segue:
Questo definisce il logaritmo perché soddisfa la proprietà fondamentale dei logaritmi:
Questo può essere dimostrato definendo formula_8 e mediante la regola della sostituzione degli integrali, come segue:
Il numero formula_10 può essere definito come l'unico numero reale formula_11 tale che formula_12.
La funzione logaritmo è la funzione inversa della funzione esponenziale, quindi si ha che:
In altre parole, la funzione logaritmo è la corrispondenza biunivoca dall'insieme di numeri reali positivi all'insieme di tutti i numeri reali. Nello specifico, è un isomorfismo da un gruppo di numeri reali positivi sotto moltiplicazione al gruppo dei numeri reali sotto addizione.
I logaritmi possono essere definiti per una qualsiasi base reale strettamente positiva e diversa da formula_22, non solo formula_1, inoltre possono essere utili nella risoluzione di equazioni in cui l'incognita appare all'esponente di una qualsiasi quantità.
La derivata della funzione logaritmo naturale è data da
La serie di Taylor centrata in formula_22 del logaritmo naturale è:
Utilizzando l'identità
e sostituendo formula_28 nella serie di Taylor dell'arcotangente iperbolica si ottiene
Applicando la trasformazione binomiale alla serie di Taylor si ottiene la seguente serie, valida per ogni formula_3 con valore assoluto maggiore di formula_22:
Si noti inoltre che formula_33 è la sua stessa funzione inversa, quindi per ottenere il logaritmo naturale di un certo numero formula_34 è sufficiente sostituire formula_35 al posto di formula_3.
Una serie esotica dovuta a Bill Gosper è la seguente:
L'integrale della funzione logaritmo naturale si risolve per parti:
Il logaritmo naturale è fondamentale per rapide integrazioni di funzioni della forma formula_39 che si traducono nella scrittura formula_40: l'integrale di una derivata fratto la sua funzione è uguale al logaritmo naturale del valore assoluto di quella funzione. Si tratta della diretta conseguenza della regola di derivazione per le funzioni composte, ossia:
Cioè
e
Con quest'ultima regola, è possibile calcolare gli integrali della tangente e della cotangente sfruttando le loro definizioni:
Da cui ponendo formula_46 si ha che formula_47 e quindi:
dove formula_50 è la costante reale arbitraria degli integrali indefiniti.
Prima della diffusione delle calcolatrici, la formula del cambio di base logaritmica era necessaria per il calcolo dei logaritmi neperiani, riportandoli su base formula_13. È ancora utile per ottenere l'ordine di grandezza di un numero neperiano (che è appunto una potenza di formula_13):
che diventa:
Alla fine delle tavole dei logaritmi, la tabella di trasformazione riportava i valori di:
e
</text>
</doc>
<doc id="230403" url="https://it.wikipedia.org/wiki?curid=230403">
<title>Divisione per zero</title>
<text>
In matematica, una divisione per zero è una divisione della forma formula_1. Il risultato non esiste (cioè l'espressione non ha significato) in aritmetica e in algebra.
È piuttosto diffusa l'errata opinione per cui il valore di formula_1 sarebbe formula_3 (infinito). Questa affermazione fa riferimento, in modo non del tutto corretto, a un'interpretazione della divisione in termini della teoria dei limiti dell'analisi matematica.
Un primissimo riferimento registrato dell'impossibilità di assegnare un risultato alla divisione per zero si ha nella critica al calcolo infinitesimale contenuta in "The Analyst" di George Berkeley.
Esistono comunque particolari strutture matematiche all'interno delle quali la divisione per zero potrebbe essere definita in modo consistente (per esempio, la sfera di Riemann).
In informatica, e in particolare nell'implementazione elettronica dell'aritmetica nelle ALU dei processori, una divisione per zero causa un'eccezione (o "trap") hardware e di conseguenza (in genere) la terminazione del programma che ha tentato l'operazione. Nei linguaggi interpretati come Java, un tentativo di eseguire una divisione per zero viene generalmente intercettato dall'interprete, che segnala l'anomalia (per esempio attraverso una eccezione) senza tentare di eseguire l'operazione. In JavaScript, al contrario, il risultato è Infinity.
Il "brahmasphutasiddhanta" di Brahmagupta è il più antico testo conosciuto che tratta lo zero come un numero vero e proprio e che cerca di definire le operazioni che lo riguardano. L'autore tuttavia dà alla divisione per zero un significato che noi considereremmo scorretto; secondo Brahmagupta, infatti:
Nell'830, Mahavira tentò senza successo di correggere l'errore di Brahmagupta nel suo libro "Ganita Sara Samgraha":
Bhaskara II tentò di risolvere il problema definendo formula_4. Questa definizione non è priva di senso, ma può portare a paradossi se non viene trattata con attenzione. È difficile che Bhaskara II abbia compreso tutti i problemi connessi, quindi la sua soluzione non viene considerata corretta.
È generalmente stabilito fra i matematici che un modo naturale per interpretare la divisione per zero è prima definire la divisione in termini di altre operazioni aritmetiche. Stando alle normali regole per l'aritmetica su interi, numeri razionali, numeri reali e numeri complessi, il valore di una divisione per zero è "indefinito", così come in un qualunque campo. Il motivo è che la divisione è definita in modo da essere l'operazione inversa della moltiplicazione. Questo significa che il valore di formula_5 è la soluzione formula_6 dell'equazione
qualora un tale valore esista e sia unico. In caso contrario l'espressione formula_5 è indefinita. Per formula_9, l'equazione formula_10 può essere riscritta come formula_11 o semplicemente formula_12. Quindi, in questo caso, l'equazione formula_10 ha "nessuna soluzione" se formula_14 è diverso da formula_15, e ne ha "infinite" se formula_16 è uguale a formula_15. In entrambi i casi, formula_5 è indefinito. Al contrario, negli insiemi numerici menzionati sopra, l'espressione formula_5 è "sempre" definita se formula_20 non è uguale a zero.
È possibile nascondere una divisione per zero in una dimostrazione algebrica, portando ad un sofisma algebrico simile a come segue:
La fallacia è nell'assumere che la divisione per formula_24, dunque per zero, sia definita. In pratica, la divisione per un termine in una qualunque dimostrazione algebrica richiede o una esplicita assunzione che il termine non sia mai zero o una separata giustificazione che mostri che tale termine non possa mai essere zero.
Simili proposizioni sono vere in strutture algebriche più generali, come in un anello o in un campo. In un campo, ogni elemento non zero è invertibile sotto la moltiplicazione, così, come sopra, la divisione pone problemi solo durante la divisione per zero. In altri anelli, però, anche la divisione per elementi non zero può porre problemi. Consideriamo, per esempio, l'anello formula_30 degli interi modulo 6. Quale significato dobbiamo dare all'espressione formula_31.
Questa dovrebbe essere la soluzione formula_6 dell'equazione
Ma l'equazione ha due distinte soluzioni, e per cui l'espressione è indefinita. Il problema sorge poiché 2 non è invertibile rispetto alla moltiplicazione.
Ad un primo acchito, potrebbe sembrare possibile definire formula_34 considerando il limite di formula_5 con formula_36 che tende a formula_15.
Con formula_36 che tende a formula_15 da destra (positivo), per ogni formula_14 maggiore di zero (positivo), è noto che:
invece per ogni formula_14 minore di zero (negativo),
Studiando invece il limite con formula_36 che tende a formula_15 da sinistra (negativo), per formula_14 positivo
e per formula_14 negativo
Tuttavia usando l'equazione
si giunge al risultato errato formula_51 (che è scaturito dal non considerare la diversità del limite destro e sinistro in formula_15). Si potrebbe anche condurre uno studio considerando un "infinito senza segno", ma la definizione che ne risulterebbe non sarebbe utile in questo contesto poiché non sarebbe compatibile con la struttura dei numeri reali di campo ordinato.
L'equazione
ancora non possiede soluzione per ogni formula_14 finito. Inoltre, non vi è nessuna definizione ovvia di formula_55 che possa essere derivata considerando il limite di una divisione. Il limite
non esiste. Limiti nella forma
nei quali sia formula_58 e formula_59 tendono a formula_15 quando formula_6 tende a formula_15, possono convergere a qualunque valore o non convergere affatto. Vedere la regola di De L'Hôpital per discussioni ed esempi sui limiti di rapporti.
Nella teoria delle distribuzioni si può estendere la funzione formula_63 ad una distribuzione sullo spazio intero dei numeri reali (utilizzando il valore principale di Cauchy). Non ha comunque senso chiedere il 'valore' di questa distribuzione con formula_64; una risposta sofisticata si appoggia al supporto singolare della distribuzione.
Anche se la divisione per zero è indefinita coi numeri reali e gli interi è possibile definirla consistentemente in altre strutture matematiche, per esempio sulla sfera di Riemann (vedere anche i poli in analisi complessa). Nei numeri iperreali e nei numeri surreali la divisione per infinitesimi è possibile. Se un sistema numerico forma un anello commutativo, come gli interi, i numeri reali e i numeri complessi, per esempio, può essere esteso ad una ruota nella quale la divisione per zero è sempre possibile, anche se la divisione ha un significato leggermente diverso.
Nello standard IEEE 754 per la virgola mobile, supportato da praticamente tutti i moderni processori, viene specificato che ogni operazione aritmetica in virgola mobile, compresa la divisione per zero, ha un risultato ben definito. Nell'aritmetica IEEE 754, formula_34 è infinito positivo quando formula_14 è positivo, infinito negativo quando formula_14 è negativo, e NaN ("not a number") quando formula_68. Queste definizioni derivano dalle proprietà dei limiti di rapporti, come discusso sopra.
La divisione intera per zero è generalmente gestita differentemente poiché non vi è una rappresentazione intera per il risultato. La maggior parte dei processori genera una eccezione quando viene tentata la divisione intera per zero. Il risultato è tipicamente la terminazione del programma anche se in alcuni casi (specialmente quelli che impiegano l'aritmetica a virgola fissa nel caso in cui non sia disponibile hardware dedicato per la virgola mobile) viene impiegato un comportamento simile allo standard IEEE, utilizzando grandi numeri positivi e negativi per approssimare gli infiniti.
</text>
</doc>
<doc id="224802" url="https://it.wikipedia.org/wiki?curid=224802">
<title>Teorema dei seni</title>
<text>
In trigonometria, il teorema dei seni (noto anche come teorema di Eulero) esprime una relazione di proporzionalità diretta fra le lunghezze dei lati di un triangolo e i seni dei rispettivi angoli opposti.
Si consideri il triangolo generico "ABC" rappresentato nella figura a lato, in cui gli angoli sono indicati da lettere greche minuscole e i lati opposti agli angoli dalle corrispondenti lettere latine minuscole.
Vale quindi
dove "R" è il raggio del cerchio circoscritto al triangolo "ABC" e 
è l'area del triangolo ricavata dal semiperimetro "p" grazie alla formula di Erone.
La relazione di proporzionalità viene formulata a volte in questo modo:
Il teorema può essere adoperato
Per una superficie non euclidea dalla curvatura "K", il "raggio di curvatura" "ρ" è
Si definiscono quindi le dimensioni ridotte del triangolo:
Nel caso di un "triangolo sferico", "a", "b" e "c" corrispondono alle misure angolari dei segmenti degli archi grandi ["BC"], ["AC"] e ["AB"] (vedere figura).
In un triangolo sferico "ABC" tracciato sulla sfera di centro "O" e di raggio "ρ", il teorema del seno è espresso da
dove "V" è il volume del tetraedro "OABC".
In un triangolo iperbolico, il teorema dei seni si esprime con
Si consideri un tetraedro "A""A""A""A" nello spazio tridimensionale. La figura di lato mostra un tetraedro proiettato su un piano e indica le notazioni di vertici, facce e angoli del tetraedro:
Il seno dell'angolo triedro in corrispondenza del vertice "A" si definisce nel modo seguente:
E in modo analogo per gli altri angoli triedri.
Vale quindi
dove "V" è il volume del tetraedro.
</text>
</doc>
<doc id="308533" url="https://it.wikipedia.org/wiki?curid=308533">
<title>Equazione parametrica</title>
<text>
In matematica l'equazione parametrica o letterale è un'equazione matematica in cui le variabili (indipendente e dipendente) sono espresse a loro volta in funzione di uno o più parametri.
Un tipico parametro potrebbe essere il tempo ("t"): esso, in equazioni riguardanti la cinematica, è utilizzato per stabilire la velocità, l'accelerazione e altri aspetti del movimento. Il contrario di equazione parametrica è equazione numerica.
Una retta e una curva in genere possono essere sempre espressi parametricamente.
Da notare che, in genere, la parametrizzazione non è mai unica, infatti il parametro (o i parametri) può essere scelto in diversi modi a seconda del tipo di curva, di equazione o in modo da semplificare i calcoli. 
Genericamente un'equazione parametrica si può pensare come una relazione in forma di equazione espressa in funzione di "R" legata a un parametro e a una "rappresentazione parametrica".
Per esempio, una generica retta di equazione cartesiana
come equazione parametrica diventa:
e il parametro "t" è dato da:
formula_4 (formula_5 e formula_6)
L'equazione di una parabola, formula_7
può essere parametrizzata in funzione del parametro "t", ponendo
La parametrizzazione di una circonferenza di raggio "r" e centro nell'origine (formula_10) è:
Le equazioni parametriche dell'ellisse sono:
con formula_15 come limiti del parametro
Alcune forme geometriche sono difficili da descrivere come singole equazioni cartesiane, ma risultano evidenti in forma parametrica, ad es.:
descrive una curva tridimensionale, l'elica, con raggio "a" e passo 2π"b" unità per giro. (Le equazioni sono identiche nel piano a quelle della circonferenza.)
Tipiche espressioni parametriche sono:
mentre una generica curva parametrica (in funzione di t) si può scrivere
formula_20
mettendo in risalto le sue componenti parametriche di parametro "t".
Si ha: formula_21: formula_22
Con questa notazione è più agevole derivare la funzione che rappresenta la curva e calcolare anche integrali curvilinei e integrali di linea.
Poiché in questo modo si può integrare e differenziare queste curve con riguardo ai loro termini, si può, ad es., descrivere la velocità di una particella avendo riguardo alla parametrizzazione del percorso:
e l'accelerazione come:
Se in generale una curva parametrica (ivi compresa la retta) è una funzione di un parametro indipendente (in genere "t"), per parametrizzare superfici, per esempio nel calcolo vettoriale, si usano funzioni di due parametri, in genere notati con ("s", "t") o ("u","v"). In generale per parametrizzare una varietà di dimensione "n" occorrono "n" parametri liberi.
Un esempio di curva parametrica con due parametri è il cilindro con equazioni parametriche:
L'equazione deriva da quella della circonferenza nel piano, e rappresenta un cilindro in R. Il parametro "z" è fissato arbitrariamente.
Un'applicazione delle equazioni parametriche consiste nel dover determinare il valore di un parametro incognito all'interno di un'equazione in modo che le radici dell'equazione stessa soddisfino determinate condizioni.
Per esempio nell'equazione
si determini il valore di "k" affinché l'equazione risulti impossibile. La soluzione dell'equazione è
perciò affinché risulti impossibile deve essere "k" = 0.
Meno immediato è il procedimento per un'equazione di 2º grado, nella quale solitamente si deve determinare il valore del parametro note alcune relazioni tra le due radici dell'equazione (formula_28 e formula_29). Per fare ciò si utilizzano alcune proprietà delle equazioni di secondo grado, cioè, detta
l'equazione, e "x" e "x" le sue due soluzioni, vale che:
Nel caso stesse lavorando nel campo dei numeri reali, ci si deve ricordare che non è mai accettabile che il discriminante sia minore di zero.
Per esempio nell'equazione
determinare "k" in modo che:
1) le radici siano distinte
quindi per la seconda proprietà:
2) il prodotto delle radici sia −16
quindi per la quarta proprietà:
Quando le relazioni note tra le radici non sono del tipo
si deve tentare di portare le relazioni note sotto forma di somma o prodotto di radici. A questo scopo vengono spesso utilizzate le cosiddette Formule di Waring.
Per esempio, nel caso sapessimo che:
formula_42
la formula si può sostituire con
formula_43
è facile verificare che le due sono equivalenti, però nella seconda è possibile sostituire i coefficienti dell'equazione parametrica che stiamo risolvendo:
formula_44
per poi sostituire ad formula_45 , formula_46 e formula_47 i valori presenti nell'equazione.
Queste trasformazioni sono indispensabili per la risoluzione dell'equazione parametrica; eccone alcune, tre le più usate.
Nel caso non sia possibile applicare le formule di Waring o non sia intuitivo, si può ricorrere - senza formule difficili da ricordare e senza dover risolvere l'equazione - al metodo generale, che consiste nei seguenti passi:
</text>
</doc>
<doc id="231495" url="https://it.wikipedia.org/wiki?curid=231495">
<title>Funzione periodica</title>
<text>
In matematica, a livello intuitivo, per funzione periodica si intende una funzione che assume valori che si ripetono esattamente a "intervalli" regolari.
Una funzione formula_1 definita su un gruppo abeliano formula_2 è periodica di periodo formula_3, con formula_4, se formula_5 per ogni formula_6.
Le funzioni periodiche più note sono le funzioni reali di variabile reale.
Formalmente, una funzione reale formula_7 si dice periodica di periodo formula_3 se esiste un numero reale formula_9 tale che 
Se formula_13 è periodica di periodo formula_18 ed è periodica di periodo formula_19, allora è periodica di ogni periodo
L'insieme formula_21 dei periodi formula_3 di formula_13 è quindi uno formula_24-modulo.
La somma e il prodotto di due funzioni periodiche di periodo formula_3, aventi lo stesso dominio, sono funzioni periodiche di periodo formula_3.
Da ogni funzione a valori reali definita su un dominio limitato si può definire una funzione periodica, di periodo maggiore o uguale all'ampiezza del dominio.
Ad esempio, la funzione identità ristretta all'intervallo formula_43,
definisce una funzione periodica di periodo 1 definita su tutti i reali: la parte frazionaria
Una funzione può ammettere due o più periodi non "commensurabili" (la definizione dipende dalle caratteristiche che si richiedono al dominio).
Ad esempio, una funzione ellittica è una funzione doppiamente periodica:
</text>
</doc>
<doc id="3626" url="https://it.wikipedia.org/wiki?curid=3626">
<title>Radice quadrata</title>
<text>
In matematica, la radice quadrata o radice con indice 2 di un numero formula_1 è un numero formula_2 tale che il suo quadrato sia formula_1, ovvero tale che formula_4. Ogni numero reale non negativo ha un'unica radice quadrata non negativa, chiamata radice quadrata principale, che viene rappresentata simbolicamente come formula_5 o, nella notazione esponenziale, come formula_6. Ogni numero reale maggiore di zero ha due radici quadrate distinte, quella principale e il suo opposto, ovvero formula_5 e formula_8.
Il concetto di radice quadrata può essere esteso ai numeri negativi nell'ambito dei numeri complessi. Più generalmente, il concetto di radice quadrata può essere esteso in qualunque contesto in cui sia ben definita la nozione di quadrato di un elemento.
Quando si sono definiti i numeri reali si può definire radice quadrata principale di un numero reale non negativo formula_9 ogni numero reale non negativo formula_1 tale che
formula_11.
Questo numero formula_1, del quale si dimostrano l'esistenza e l'unicità, si indica con la scrittura formula_13. Si osserva poi che anche l'opposto formula_14 soddisfa la precedente equazione quadratica [*]; inoltre entrambe le soluzioni di tale equazione sono notevoli, in quanto danno i due zeri della parabola di equazione formula_15. È dunque opportuno definire radice quadrata di un numero reale positivo formula_9 quel numero reale positivo formula_1 che soddisfi la [*]. Lo zero reale possiede due radici quadrate coincidenti; ciò lo colloca sullo stesso piano dei numeri reali positivi, sebbene lo zero si possa considerare l'unico limite delle due radici quadrate formula_1 e formula_14 del numero formula_9 al tendere a 0 di questo reale. Perciò, volendo semplificare, si può dire che lo zero reale possiede come radice quadrata solo se stesso.
Restringendo la ricerca della radice quadrata al dominio dei numeri interi positivi, si trova che solo alcuni numeri, detti quadrati perfetti, ammettono per radice quadrata principale un numero intero. Sono quadrati perfetti ad esempio, formula_21 che ha per radice il numero formula_22, e formula_23 che ha per radice formula_24; viceversa molti altri interi positivi, a cominciare da formula_22 e formula_26, non ammettono una radice intera.
Se ampliamo il dominio di ricerca ad includere i numeri razionali positivi, si trova che solo i numeri razionali che sono quadrati perfetti, ovvero che sono dati da frazioni con numeratore e denominatore entrambi quadrati perfetti, ammettono per radice principale un numero razionale positivo: 4/9 ammette per radice 2/3, ma 1/2 o 25/39 non ammettono radici razionali.
Si è quindi trovato che l'insieme dei numeri razionali presenta una limitazione operativa e si è sentita la necessità di ampliare il campo dei razionali ad un campo numerico nel quale si possa trovare una radice quadrata per ogni numero positivo.
Questo ha condotto alla introduzione dei numeri reali: se allarghiamo il dominio a questi numeri, ogni numero reale positivo (che in questo contesto viene chiamato radicando) possiede una radice quadrata dello stesso genere. È possibile dimostrare che un numero che sia la radice quadrata di un numero che non è un quadrato perfetto o una frazione il cui numeratore e denominatore sono ambedue quadrati perfetti è un numero irrazionale, cioè un numero non esprimibile come frazione ma rappresentabile con una scrittura decimale infinita non periodica. Ad esempio è irrazionale la radice quadrata di formula_22; in più, l'insieme di tutti gli interi positivi non quadrati perfetti è un sottoinsieme numerabile dei numeri reali, come pure quello dei razionali, mentre l'insieme degli irrazionali è non numerabile.
Si osserva poi che nessun numero reale negativo possiede una radice quadrata reale e questo ha contribuito (vedi anche Rafael Bombelli) all'introduzione dei numeri complessi. Quando si estende a queste entità la ricerca di radici quadrate, si trova che ogni numero complesso ammette due radici quadrate complesse, l'una essendo il numero opposto dell'altra. Particolarmente importanti sono le radici quadrate di formula_28 indicate con formula_29, detta unità immaginaria, e con formula_30. In generale si trova che il numero espresso in forma polare come
possiede due radici complesse date da
Per esprimere questi numeri complessi può essere conveniente estendere agli argomenti complessi la nozione di radice quadrata principale e la relativa notazione
in modo da poter dire ancora che le radici del numero complesso formula_9 sono
formula_35.
Le radici del numero complesso formula_36 coincidono con lo stesso formula_36 e a tale radice si attribuisce molteplicità formula_22.
La funzione radice quadrata principale ha una grande utilità, perché pone in corrispondenza l'insieme dei numeri reali non negativi formula_39 con se stesso; si individua scrivendo formula_5 o anche formula_41. Più precisamente questa endofunzione entro formula_42 è una biiezione crescente e continua.
L'equazione formula_43 ha solo due soluzioni, formula_36 e formula_45. In altre parole la funzione radice quadrata principale è una permutazione (cioè una endofunzione biiettiva) di formula_46 avente formula_47 come insieme dei punti fissi.
Per ogni due numeri reali positivi formula_1 e formula_2 si trovano subito le identità
Queste uguaglianze sono in sintonia con il fatto che la funzione radice quadrata fa corrispondere all'area di un quadrato la lunghezza del suo lato. Esse inoltre per formula_51 diventano
Queste uguaglianze implicano che per tabulare nella notazione decimale i valori assunti dalla funzione radice quadrata principale è sufficiente conoscere i suoi valori nell'intervallo formula_53.
Per ogni numero reale formula_1 si trova che
Si supponga che formula_1 e formula_57 siano reali e che formula_58, e che si voglia ottenere la formula_1. Un errore comune consiste nell'estrarre la radice quadrata e dedurre che formula_60. Questo non è lecito, in quanto la radice quadrata principale di formula_61 non è formula_1, ma il valore assoluto formula_63, come dice l'uguaglianza precedente. Non commettendo questo errore si potrebbe concludere che formula_64, o equivalentemente formula_65.
L'uguaglianza che segue è utile in molti passi del calcolo infinitesimale, ad esempio per dimostrare che la funzione radice quadrata è continua e differenziabile, o per calcolare certi limiti:
valida per tutte le coppie di interi non negativi formula_1 e formula_2 che non sono entrambi zero.
La funzione formula_69 ha il seguente grafico, ottenibile da una metà di parabola avente come asse l'asse delle formula_1.
Questa funzione, continua per tutti gli formula_1 non negativi, è differenziabile per tutti gli formula_1 positivi, ma non è differenziabile per formula_73, poiché la pendenza della tangente nel corrispondente punto tende a formula_74).
La derivata della funzione è data da
La serie di Taylor di formula_76 in un intorno di formula_73 si può ottenere servendosi del teorema binomiale:
per formula_79.
Il problema dell'estrazione della radice quadrata può porsi anche in un generico anello e in altre strutture di genere algebrico nelle quali si definisce un prodotto.
In particolare si definisce radice quadrata di una matrice quadrata formula_80 su un campo ogni matrice formula_81 con lo stesso dominio tale che sia formula_82. Nel caso delle matrici formula_83 la ricerca della matrice radice quadrata si riconduce alla soluzione di un sistema di quattro equazioni di secondo grado in quattro incognite. Infatti l'equazione nelle incognite formula_84, formula_85, formula_1 e formula_2
equivale al sistema
In particolare una matrice diagonale formula_83 a valori reali
con formula_57 e formula_93 positivi possiede le quattro le radici quadrate date dall'espressione
Si può definire anche la radice quadrata di un linguaggio formale, in relazione al prodotto di giustapposizione, prodotto non commutativo e associativo. Ad esempio, se formula_57 è un carattere, il linguaggio formula_96 possiede come unica radice quadrata il linguaggio formula_97 e il linguaggio formula_98 possiede come unica radice quadrata il linguaggio formula_99; ogni linguaggio finito con più di una stringa non possiede invece alcuna radice quadrata.
</text>
</doc>
<doc id="16420" url="https://it.wikipedia.org/wiki?curid=16420">
<title>Insieme</title>
<text>
 matematica, un raggruppamento di oggetti rappresenta un insieme se esiste un criterio oggettivo che permette di decidere univocamente se un qualunque oggetto fa parte o no del raggruppamento. Si tratta di un concetto fondamentale della matematica moderna, a partire dal quale si è sviluppata la teoria degli insiemi.
Nell'uso informale gli oggetti della collezione possono essere qualunque cosa: numeri, lettere, persone, figure, ecc., anche non necessariamente omogenei; nelle formalizzazioni matematiche gli oggetti della collezione vanno invece ben definiti e determinati. Il concetto di insieme è considerato primitivo ed intuitivo: "primitivo" perché viene introdotto come nozione non derivabile da concetti più elementari; "intuitivo" perché viene introdotto come generalizzazione della nozione di insieme finito, che a sua volta è introdotta dall'analogia con l'esperienza sensibile di scatole che contengono oggetti materiali (tendenzialmente omogenei); questa impostazione si basa sulla convinzione che l'idea di insieme sia naturalmente presente nella mente umana.
Gli oggetti che compongono un insieme si dicono elementi di questo insieme; nel linguaggio matematico, detto "a" un elemento dell'insieme "A", si dice che "a appartiene ad A" o in simboli formula_1. Un insieme "A" è sottoinsieme di un altro insieme "B" quando tutti gli elementi di "A" appartengono anche a "B".
Ciò che caratterizza il concetto di insieme e lo differenzia da strutture matematiche simili sono essenzialmente le seguenti proprietà:
Gli insiemi, con le loro operazioni e relazioni, possono essere rappresentati graficamente con i diagrammi di Eulero-Venn.
Solitamente un insieme viene indicato con le lettere maiuscole dell'alfabeto: "A", "B", "E", "M", "S" ... e si chiede che sia univocamente determinato: se ad esempio diciamo che "M" è l"'insieme degli x tali che x è un mammifero marino", allora supponiamo che si sappia sempre decidere se un qualsiasi animale possibile ed immaginabile abbia o meno le caratteristiche necessarie per rientrare in "M." Se un oggetto "x" appartiene ad un insieme "F" viene detto "elemento" di "F" e la relazione si denota nella forma formula_2. Viceversa, la relazione di non appartenenza a un insieme si denota nella forma formula_3.
Un insieme può essere definito nei seguenti modi:
La cardinalità di un insieme è il numero che indica la quantità dei suoi elementi. Ad esempio, l'insieme formula_11 ha tre elementi (considerando distinte le tre lettere), quindi cardinalità 3; l'insieme dei numeri naturali formula_12 ha invece cardinalità formula_13, il primo cardinale infinito.
Un insieme si dice "finito" se ha un numero finito di elementi, "infinito" se contiene infiniti elementi.
Le principali operazioni tra insiemi sono:
Due insiemi "A" e "B" si dicono inoltre:
"B" è sottoinsieme di "A" se "A" contiene gli elementi di "B". Secondo la definizione ogni insieme è contenuto in sé stesso. Per esprimere questo si usa la notazione:
Se si vuole escludere che "B" coincida con "A", cioè prevedere che esistono elementi di "A" non contenuti in "B", si usa la notazione:
che si legge: ""B è un sottoinsieme proprio di A"" oppure ""B è incluso propriamente in A"" oppure ""B è contenuto propriamente in A"". Alcuni autori utilizzano solo la seconda notazione, indipendentemente dal tipo di inclusione.
La relazione binaria di inclusione tra insiemi rende una qualsiasi classe di insiemi un insieme parzialmente ordinato.
Insieme vuoto è l'insieme che non contiene "nessun" elemento. Si indica con i simboli formula_25, formula_26 o con due parentesi graffe, la prima aperta e l'altra chiusa formula_27.
L'insieme vuoto è sottoinsieme di qualsiasi altro insieme (incluso sé stesso).
Per qualunque insieme "A" si definisce insieme delle parti o "insieme potenza" di "A" e si indica con formula_28 o formula_29 l'insieme che ha come elementi tutti e soli i sottoinsiemi di "A". Ad esempio, se formula_30 allora il suo insieme delle parti è costituito da formula_31.
L'insieme delle parti ha cardinalità strettamente maggiore di quella dell'insieme di partenza. Se "A" è finito ed ha |"A"| elementi, il numero degli elementi di formula_28 è dato da formula_33 (in simboli, formula_34).
L'insieme delle parti di qualsiasi insieme, considerato congiuntamente all'operazione di differenza simmetrica, forma un gruppo abeliano. Se vengono considerate insieme unione, intersezione e complementazione la struttura generata è un'algebra di Boole.
Si chiama partizione dell'insieme A un insieme di sottoinsiemi di A che ha queste caratteristiche:
Dati gli insiemi A e B, con Bformula_35A, l'insieme complementare di B rispetto ad A è A-B. Lo indichiamo con formula_36(B).
Alcuni insiemi, detti "numerici", hanno un ruolo particolarmente importante e pervasivo in tutte le branche della matematica:
Questi insiemi si possono vedere intuitivamente come contenuti uno nell'altro:
Più propriamente si dovrebbe parlare di immersione di ogni insieme nel seguente, poiché secondo la corrente assiomatizzazione i vari insiemi sono definiti in modi radicalmente diversi l'uno dall'altro.
</text>
</doc>
<doc id="3558" url="https://it.wikipedia.org/wiki?curid=3558">
<title>Polinomio</title>
<text>
In matematica un polinomio è un'espressione composta da costanti e variabili combinate usando soltanto addizione, sottrazione e moltiplicazione. In altre parole, un polinomio tipico, cioè ridotto in forma normale, è la somma algebrica di alcuni monomi non simili tra loro, vale a dire con parti letterali diverse. Ad esempio:
è la somma di tre monomi. Ciascun monomio viene chiamato "termine" del polinomio.
Le costanti sono anche chiamate "coefficienti" e sono tutte elementi di uno stesso insieme numerico o di un anello.
Quando valutati in un opportuno dominio, i polinomi possono essere interpretati come funzioni. Ad esempio, il polinomio
definisce una funzione reale di variabile reale.
Quando questo ha senso, le radici del polinomio sono definite come l'insieme di quei valori che, sostituiti alle variabili, danno all'espressione polinomiale il valore nullo. Ad esempio, formula_3 ha come radici i valori formula_4 e formula_5, poiché sostituendoli nell'espressione del polinomio si ha 
I polinomi sono oggetti matematici di fondamentale importanza, alla base soprattutto dell'algebra, ma anche dell'analisi e della geometria analitica.
Un polinomio si dice:
Due polinomi sono considerati "uguali" se, dopo essere stati ridotti in forma normale, hanno gli stessi termini, a meno dell'ordine. Quindi i polinomi seguenti sono uguali:
Il "grado" di un polinomio non nullo e ridotto in forma normale è il massimo grado dei suoi monomi, mentre il "grado parziale" rispetto ad una variabile è il grado risultante vedendo tutte le altre variabili come coefficienti. Quindi
ha grado due, mentre ha gradi parziali uno rispetto sia a formula_16 che a formula_17.
Si dicono "coefficienti" di un polinomio i coefficienti dei suoi singoli termini. Quindi i coefficienti di formula_18 sono rispettivamente formula_5, formula_4 e formula_4: il coefficiente formula_4 in un monomio è solitamente sottinteso.
Il "termine noto" di un polinomio ridotto in forma normale è l'unico monomio (se esiste) di grado zero, cioè non contenente variabili. Se non esiste un tale monomio, il termine noto è considerato generalmente inesistente o uguale a zero, secondo il contesto. Ad esempio, in
il termine noto è l'ultimo monomio: formula_24.
Due polinomi possono essere sommati, sottratti, e moltiplicati usando le usuali proprietà commutativa, associativa e distributiva delle operazioni di somma e prodotto. Ad esempio, se
allora la somma ed il prodotto di "p" e "q" sono rispettivamente
Somme e prodotti di polinomi danno come risultato un nuovo polinomio.
Il grado ("degree") della somma (o differenza) di due polinomi è minore o uguale al polinomio di grado maggiore. È sempre uguale al massimo tra i due, quando i due polinomi hanno grado differente:
Esempi:
Il grado del prodotto di un polinomio per un numero scalare (diverso da zero) è uguale al grado del polinomio:
Esempio:
Si noti che questo non è sempre vero per i polinomi definiti su un anello che contiene un divisore di zero. Ad esempio, in formula_36, formula_37, ma formula_38. L'insieme dei polinomi aventi coefficienti da un dato campo F e grado minore o uguale a n, forma uno spazio vettoriale (questo insieme non è un anello, e non è chiuso, come mostrato in precedenza).
Il grado del prodotto di due polinomi definiti su un campo -(oggetto in cui sono definite le operazioni di somma e prodotto, con certe proprietà)- oppure su un dominio d'integrità, è pari alla somma dei gradi dei due polinomi:
Es.:
Si noti che ciò non è sempre vero per i polinomi definiti su un anello arbitrario. Ad esempio, in formula_36, formula_43, ma formula_44.
Il grado della composizione di due polinomi formula_45 e formula_46 a coefficienti non costanti è uguale al prodotto dei rispettivi gradi:
Es.:
Si noti che ciò non è sempre vero per i polinomi definiti su un anello arbitrario. Ad esempio, in formula_36, formula_52, ma formula_53.
Possiamo affermare correttamente sia che il grado del polinomio zero è indefinito, sia che il grado del polinomio zero può essere definito con un numero negativo (per convenzione −1 o −∞).
Come qualsiasi valore costante, il valore zero può essere considerato come un polinomio (costante), detto polinomio-zero. Questo polinomio non ha termini che non siano nulli, e perciò, propriamente non ha un grado, vale a dire che il suo grado è indefinito.
Le proposizioni precedenti sul grado della somma, prodotto e composizione di polinomi non si applicano se anche uno dei due è un polinomio-zero..
Le formule valgono se si introducono alcune opportune estensioni. È pertanto utile definire il grado di un polinomio-zero, pari a "meno infinito", −∞, e introdurre quindi queste regole aritmetiche
e
I seguenti esempi illustrano come questa estensione soddisfi quelle di somma, prodotto e composizione di due polinomi.:
In un polinomio, è spesso utile considerare alcune variabili come costanti. Ad esempio, il polinomio
può essere considerato anche come polinomio in formula_16 soltanto, dando a formula_17 il ruolo di un valore costante. Alternativamente, può essere visto come polinomio in formula_17 soltanto. Le proprietà dei polinomi che ne risultano possono essere molto diverse tra loro: qui ad esempio formula_68 ha grado formula_5 rispetto a formula_16, e solo formula_4 rispetto a formula_17. Ad esempio, il polinomio
è di grado formula_24, ma se visto soltanto nelle singole variabili formula_16 o formula_17 o formula_12 ha grado rispettivamente formula_5, formula_79 e formula_80.
Un polinomio generico con una sola variabile si può rappresentare con la seguente scrittura:
con formula_82 diverso da zero. Con questa scrittura, formula_83 è il termine noto e formula_84 è il grado. formula_82 si dice "coefficiente direttore".
Un tale polinomio è
Una "radice" di un polinomio formula_89 in una sola variabile è un numero formula_90 tale che
cioè tale che, sostituito a formula_16, rende nulla l'espressione. Quindi se
il numero formula_90 è radice se
Nel caso di polinomi a coefficienti reali l'insieme delle radici reali di un polinomio formula_68 si può visualizzare sul piano cartesiano come l'intersezione del grafico della funzione polinomiale formula_97 con l'asse delle ascisse.
In un dominio, un polinomio di grado formula_84 può avere al più formula_84 radici distinte. Esistono polinomi senza radici reali, come ad esempio
poiché formula_101 per ogni formula_90 reale. D'altra parte, per il teorema fondamentale dell'algebra ogni polinomio complesso ha esattamente formula_84 radici complesse, contate con molteplicità.
Nella scuola vengono insegnate formule per trovare le radici dei polinomi di primo e secondo grado. Esistono formule analoghe per esprimere la radici di un polinomio di terzo e quarto grado in termini dei coefficienti, utilizzando solamente le quattro operazioni ed estrazioni di radice (la cosiddetta "risoluzione per radicali"). È stato invece dimostrato nella teoria di Galois che non esiste una formula generale di questo tipo per polinomi dal quinto grado in su.
Sia formula_104 un anello. A un polinomio
a coefficienti in formula_104 si può associare una "funzione polinomiale", che è la funzione da formula_104 in sé definita da
per formula_109. Se formula_104 è finito, allora polinomi diversi possono dare luogo alla stessa funzione. Per esempio se formula_111 è il campo con un numero primo formula_68 di elementi, allora al polinomio nullo e al polinomio formula_113 è comunque associata, per il piccolo teorema di Fermat, la funzione che manda ogni elemento di formula_104 in zero. Lo stesso può valere se formula_104 è infinito, ma non è un dominio, per esempio se formula_104 è un'algebra esterna infinita, in cui vale formula_117 per ogni formula_118.
Se invece formula_104 è un dominio infinito, allora vale il seguente "principio d'identità dei polinomi", che afferma che a polinomi diversi sono associate funzioni polinomiali diverse (cioè la funzione sopra descritta che associa a un polinomio una funzione polinomiale è iniettiva):
Questo dipende dal fatto che in un dominio un polinomio non nullo ha solo un numero finito di radici.
Negli esempi che seguono, fissiamo formula_104 eguale al campo dei numeri reali. A seconda del grado,
Una funzione polinomiale a coefficienti reali
è derivabile e la sua derivata è ancora un polinomio,
Ragionando quindi induttivamente, si può quindi affermare che le funzioni polinomiali sono infinitamente derivabili (o "lisce") e che la derivata ("n"+1)-esima di un polinomio di grado formula_84 è la funzione nulla. In realtà esse sono anche funzioni analitiche.
Dato un anello formula_104, il simbolo
denota l'insieme di tutti i polinomi nelle variabili formula_135 con coefficienti in formula_104. Ad esempio, formula_104 può essere un campo come quello dei numeri reali o complessi.
L'insieme formula_134 risulta essere anch'esso un anello, l'anello dei polinomi in formula_84 variabili con coefficienti in formula_104. Lo studio delle proprietà di questo anello è una parte importante dell'algebra e della geometria algebrica.
Se formula_104 è un campo, l'anello dei polinomi è un'algebra su formula_104, e quando formula_143 è anche un anello euclideo, nel senso che i polinomi possono essere divisi con quoziente e resto come i numeri interi (se formula_144 questo non è vero poiché l'anello di polinomi non è un dominio ad ideali principali).
Il calcolo della derivata di un polinomio si estende come "definizione" di derivata (chiamata "derivata formale") nel caso in cui il polinomio abbia coefficienti in un anello formula_104, anche in assenza del calcolo infinitesimale. Molte delle proprietà della derivata si estendono anche alla derivata formale.
Siano formula_163 le n radici di un polinomio di grado formula_84, e sia formula_165.
Allora
Per le relazioni radici/coefficienti un polinomio di secondo grado si può scrivere nella forma
dove
Allora
Per le relazioni radici/coefficienti un polinomio di terzo grado si può scrivere nella forma
dove
Allora
</text>
</doc>
<doc id="11816" url="https://it.wikipedia.org/wiki?curid=11816">
<title>Grado d'arco</title>
<text>
Un grado d'arco o grado sessagesimale, normalmente indicato dal simbolo ° (in apice), è un'unità di misura dell'angolo piano, oppure di un angolo che individua un punto lungo la circonferenza maggiore di una sfera (ad esempio, per approssimazione, la Terra oppure la sfera celeste). 
Il grado d'arco rappresenta un angolo corrispondente a un arco che misura 1/360 della circonferenza di un cerchio o una sfera. Il grado ha dei sottomultipli: i minuti (1/60 di grado, angolo corrispondente a 1/21600 di circonferenza, indicato dal simbolo ′ ) e i secondi (1/60 di minuto, angolo corrispondente a 1/1296000 di circonferenza, indicato dal simbolo ″ ). Quindi, ad esempio, 40° 12′ 13″ indica 40 gradi, 12 minuti, 13 secondi, cioè all'incirca l'angolo corrispondente a 11/100 di circonferenza.
Nella figura qui sotto si può vedere una circonferenza suddivisa in spicchi di diverse grandezze. Dividendo la circonferenza in 360 spicchi uguali, un grado è la misura dell'angolo formato da ognuna di queste suddivisioni.
Esistono tuttavia altri tipi di grado:
La divisione in gradi e l'uso del sistema sessagesimale sono convenzioni introdotte dalle civiltà Sumerica e Babilonese migliaia di anni fa. 
Molti esperti ritengono che la convenzione di introdurre il sistema sessagesimale fu usata (e tuttora adottata) per la facilità di creare sottomultipli delle misure: infatti possiamo notare che il numero 60 è divisibile per numeri come 2, 3, 4, 5, 6. Ciò porta a creare molti sottomultipli che, proprio nel caso di unità di misura che usano il sistema sessagesimale come la misura degli angoli e l'avanzare del tempo, risultano molto utili per esprimere tali grandezze.
α° β' γ" = λ°<br/>
λ = α + β/60 + γ/3600<br/>
es. 20° 5' 46" = 20+5/60+46/3600 = 20,0961°
λ° = α° β' γ"<br/>
α = int(λ)<br/>
β = int((λ-α)*60)<br/>
γ = int(((λ-α)*60-β)*60)<br/>
es. λ = 20,0961<br/>
α = int(20,0961) = 20°<br/>
β = int((20,0961-20)*60) = int(5,766) = 5'<br/>
γ = int((5,766-5)*60) = 46"
In un contesto matematico, gli angoli vengono normalmente misurati in radianti invece che in gradi. Un grado è pari a π/180 radianti.
</text>
</doc>
<doc id="28421" url="https://it.wikipedia.org/wiki?curid=28421">
<title>Matrice triangolare</title>
<text>
La locuzione matrice triangolare, in matematica, indica matrici quadrate che hanno tutti gli elementi nulli sotto o sopra la diagonale principale. A seconda che gli elementi nulli siano sotto o sopra la diagonale la matrice viene chiamata rispettivamente matrice triangolare superiore o matrice triangolare alta e matrice triangolare inferiore o matrice triangolare bassa.
Le matrici triangolari inferiori sono matrici quadrate che hanno nulli tutti gli elementi al di sopra della diagonale principale, cioè della forma:
Se i numeri sulla diagonale di una tale formula_2 sono tutti uguali a 1 (elementi del tipo formula_3) la matrice è chiamata "matrice unità triangolare inferiore", "matrice triangolare inferiore unitaria" o "matrice triangolare inferiore normata".
Si dice invece matrice triangolare superiore una matrice quadrata con nulli gli elementi al di sotto della diagonale principale, cioè della forma:
Se tutte le entrate formula_5 sulla diagonale di formula_6 sono uguali ad 1 la matrice è chiamata "matrice unità triangolare superiore", "matrice triangolare superiore unitaria" o "matrice triangolare superiore normata".
Per maggiore chiarezza invece di matrice triangolare inferiore (superiore) si dovrebbe parlare di matrice triangolare inferiore/sinistra (superiore/destra), per distinguere queste dalle matrici triangolari definite a partire considerando la diagonale secondaria invece di quella principale.
Matrici che sono simili a matrici triangolari sono dette "triangolarizzabili".
Diverse operazioni preservano la forma triangolare:
Grazie a questi fatti l'insieme delle matrici triangolari superiori è una sottoalgebra dell'algebra associativa delle matrici quadrate di una data dimensione. Inoltre, segue anche che le matrici triangolari superiori possono essere trattate come una sottoalgebra di Lie dell'algebra di Lie delle matrici quadrate di una data dimensione, dove la parentesi di Lie formula_7 è data dal commutatore formula_8. Tali proprietà, esposte per una matrice triangolare superiore, sono valide in modo analogo per matrici triangolari inferiori.
Una matrice che è sia triangolare inferiore che triangolare superiore è una matrice diagonale. Più precisamente l'intersezione dell'insieme delle matrici triangolari inferiori con l'insieme delle matrici triangolari superiori coincide con l'insieme delle matrici diagonali. Più particolarmente l'intersezione dell'insieme delle matrici triangolari inferiori normate con l'insieme delle matrici triangolari superiori normate contiene solo la matrice identità.
Si osserva anche che per trasposizione si trasformano le matrici triangolari inferiori in matrici triangolari superiori e viceversa. In particolare la trasposizione trasforma le matrici triangolari inferiori normate in matrici triangolari superiori normate e viceversa. Quindi molte conclusioni ottenute esaminando le matrici singolari inferiori si possono trasformare piuttosto facilmente in conclusioni sulle matrici singolari superiori.
Il prodotto di due matrici triangolari inferiori è una matrice triangolare inferiore: quindi l'insieme delle matrici triangolari inferiori forma un'algebra.
Più in particolare il prodotto di due matrici triangolari inferiori normate è una matrice triangolare inferiore normata: quindi l'insieme delle matrici triangolari inferiori normate forma un'algebra che costituisce una sottoalgebra della precedente.
Per dualità le stesse conclusioni si traggono per le matrici triangolari superiori.
È particolarmente semplice e significativa l'algebra delle matrici triangolari superiori normate 2 x 2. Se formula_9 e formula_10 sono due reali si osserva che:
Si osserva che queste matrici esprimono le trasformazioni del piano che portano le rette orizzontali formula_12 in se stesse facendole slittare rigidamente in modo che il punto formula_13 vada nel punto formula_14.
Le algebre di matrici triangolari superiori hanno una generalizzazione naturale nell'analisi funzionale che conduce alle algebre nido.
Generalmente, le operazioni sulle matrici triangolari possono essere compiute in metà tempo delle corrispondenti su matrici generiche.
Il sistema di equazioni:
retto da una matrice triangolare superiore normata può essere risolto per via analoga. Poiché le matrici triangolari si calcolano facilmente, sono molto importanti in analisi numerica. La decomposizione LU fornisce un algoritmo per la decomposizione di ogni matrice invertibile formula_16 in una matrice triangolare inferiore normata formula_2 e una matrice triangolare superiore formula_18.
</text>
</doc>
<doc id="1823" url="https://it.wikipedia.org/wiki?curid=1823">
<title>Fattorizzazione</title>
<text>
In matematica la fattorizzazione è la riduzione in fattori: "fattorizzare" un numero intero positivo formula_1 significa trovare un insieme di numeri interi positivi formula_2 tali che il loro prodotto sia il numero originario (formula_3).
I numeri interi di poche cifre si possono fattorizzare velocemente a mano aiutandosi con i criteri di divisibilità o, per numeri di qualche decina di cifre, con un computer. Per numeri con più di un centinaio di cifre il problema di fattorizzazione rimane tuttavia un problema complesso nonché un argomento di ricerca fondamentale per la teoria dei numeri computazionale e la crittografia.
Innanzitutto bisogna tenere presente che un qualunque numero intero ha infinite fattorizzazioni: lo si può infatti moltiplicare quante volte si vuole per 1. In pratica, però, non si considerano i fattori 1 nella fattorizzazione: è questa tra l'altro la ragione per cui si preferisce non considerare 1 come un numero primo.
La maggior parte dei numeri ha comunque svariate fattorizzazioni possibili: ad esempio, formula_4. Per convenzione, ci si concentra su una sola tra tutte queste, che è anche la più importante: la fattorizzazione in numeri primi, che consiste nel cercare un insieme di fattori del numero che siano tutti primi (generalmente indicati con formula_5 per ricordare la loro primalità). Per il teorema fondamentale dell'aritmetica, ogni numero intero ha una ed una sola fattorizzazione in numeri primi. Ad esempio
Nel corso della storia sono stati ideati molti algoritmi per rendere la fattorizzazione un problema sempre più veloce. Il problema però rimane tuttora un problema complesso. Tra i metodi utilizzati vi sono i seguenti:
Nel 1994 Peter Shor ha sviluppato un algoritmo di fattorizzazione polinomiale (cubico, per la precisione). Questo algoritmo, noto come algoritmo di fattorizzazione di Shor, richiede tuttavia un computer quantistico.
Segue un'implementazione dell'algoritmo per la fattorizzazione in numeri primi.
</text>
</doc>
<doc id="1862" url="https://it.wikipedia.org/wiki?curid=1862">
<title>Funzione (matematica)</title>
<text>
In matematica, una funzione è una relazione tra due insiemi, chiamati dominio e codominio della funzione, che associa a ogni elemento del dominio uno e un solo elemento del codominio.
Se i due insiemi sono rispettivamente indicati con formula_1 e formula_2, la relazione è indicata con formula_3 e l'elemento associato a formula_4 tramite la funzione formula_5 viene abitualmente indicato con formula_6 (si pronuncia "effe di x").
La parola funzione quindi non si riferisce alla sola relazione, ma alla terna: relazione, domino e codominio.
Per esempio: la funzione che associa a un numero "naturale" la radice quadrata di quel numero è diversa dalla funzione che associa a un numero "intero" la radice quadrata di quel numero (a seconda di come è definito il codominio, la seconda potrebbe non essere neppure un'associazione corretta).
Si dice che formula_7 è l'argomento della funzione, oppure un valore della variabile indipendente, mentre formula_8 è un valore della variabile dipendente della funzione. 
Sinonimi del termine "funzione" sono "applicazione" e "mappa". Il termine "trasformazione" viene utilizzato spesso in ambito geometrico per indicare una funzione formula_9 invertibile e che conserva le proprietà geometriche di formula_1, mentre "operatore" è talvolta utilizzato nella trattazione di funzioni lineari tra spazi vettoriali. 
Le funzioni hanno un ruolo molto importante in tutte le scienze esatte. Il concetto di dipendenza funzionale tra due grandezze sostituisce infatti, all'interno delle teorie fisiche e matematiche, quello di causa-effetto, che, al contrario del precedente, non riguarda gli enti teorici ma direttamente gli elementi della realtà concreta. Se si afferma, ad esempio, che la pressione di una certa quantità di gas perfetto è funzione della sua temperatura e del suo volume si sta facendo un'affermazione interna a un modello termodinamico, mentre il rapporto di causa-effetto che viene individuato fra le tre grandezze dipende in modo sostanziale dalle possibilità di intervento concreto su di esse. Rimanendo a questo esempio, il valore della pressione viene visto più spesso come conseguenza del valore degli altri due parametri, poiché è generalmente molto più facile intervenire sul volume e sulla temperatura che direttamente sulla pressione.
Gli esempi più semplici di funzione sono quelli per cui sia il dominio che il codominio sono insiemi numerici. Per esempio, se a ogni numero naturale si associa il doppio di tale numero, si ha una funzione, il cui dominio è dato dai naturali e il cui codominio è costituito dai naturali pari.
Tuttavia si parla di funzione anche quando il dominio o il codominio, o entrambi, non sono insiemi numerici. Se, per esempio, a ogni triangolo del piano si associa il cerchio in esso inscritto, si ha ugualmente una funzione, in quanto per ogni triangolo esiste uno e un solo cerchio in esso inscritto.
Inoltre spesso si parla di funzioni con più argomenti, o con più valori: per esempio la funzione che alle coordinate formula_11 di un punto nello spazio fa corrispondere temperatura formula_12 e pressione formula_13 dell'aria. In tal caso, la funzione ha in realtà sempre un solo argomento, che è la terna formula_14 e ha sempre un solo valore, che è la coppia formula_15
Dati due insiemi non vuoti formula_1 e formula_2, si chiama funzione da formula_1 in formula_2 una relazione formula_5 tale che per ogni formula_21 esiste uno ed un solo elemento formula_22 tale che formula_23. Tale elemento tradizionalmente si denota con formula_6: in altre parole, invece di scrivere formula_23 si può usare la scrittura più tradizionale:
Il fatto che formula_5 sia una funzione da formula_1 in formula_2 che associa a formula_7 l'elemento formula_6 si può esprimere con la scrittura:
L'insieme formula_1 (da cui la funzione formula_5 "prende" i valori) è il dominio della funzione formula_5, mentre l'insieme formula_2 (in cui si trovano i valori "restituiti" dalla funzione formula_5) è il codominio della funzione formula_5.
Le espressioni "prendere un valore" e "restituire un valore" fanno riferimento a un modello meccanico delle funzioni, rappresentate come meccanismi che, fornito loro un elemento del dominio, lo "trasformano" nel corrispondente elemento del codominio. 
Data una funzione formula_5 di dominio formula_1 e codominio formula_41 comunque scelto un elemento formula_7 del dominio, si chiama "immagine" di formula_7 il corrispondente elemento del codominio, indicato con formula_44 Analogamente, se formula_45 è un elemento del codominio che sia immagine di un elemento formula_7 del dominio, cioè se formula_47, si dice che formula_7 è una controimmagine di formula_49 Mentre a ogni elemento del dominio di formula_5 è assegnata una e una sola immagine, è possibile che un elemento nel codominio possegga diverse controimmagini, o che non ne possieda affatto. Si definisce quindi "controimmagine" dell'elemento formula_51 l'insieme
Se formula_53 per ogni formula_54 si dice che formula_5 è "suriettiva", mentre se formula_56 contiene al più un elemento per ogni formula_45 si dice che formula_5 è "iniettiva". Se valgono entrambe le condizioni, formula_5 è detta "biiettiva" o "biunivoca".
L'insieme
degli elementi formula_45 del codominio per i quali esiste almeno un formula_7 nel dominio che ha formula_45 come immagine è detto "immagine" di formula_5 e si denota con formula_65 o con formula_66.
Per il valore di una funzione formula_67 corrispondente a un elemento formula_7, denotabile con la notazione tradizionale formula_69, vengono usate anche altre due scritture.
Per quella che chiamiamo notazione a funzione prefissa si pone
Per quella che chiamiamo notazione a funzione suffissale si pone
A volte al posto delle parentesi tonde si usano parentesi quadrate:
In questo modo si evitano confusioni con le parentesi che indicano l'ordine delle operazioni. Questa notazione è usata da alcuni programmi di calcolo simbolico.
Nelle funzioni di due variabili si usa talvolta la notazione infissa, ossia
ad esempio, nelle usuali operazioni di addizione e sottrazione si usa scrivere formula_74 e formula_75 invece di formula_76 e formula_77
Data una funzione formula_78 e un insieme formula_1 tale che formula_80, si dice che la funzione formula_81 è un'"estensione di f all'insieme formula_1" se
dove formula_84 è l'inclusione di formula_85 in formula_1, data da formula_87. Si dice viceversa che "formula_5 è la restrizione di "formula_89" all'insieme formula_85".
La restrizione di una funzione formula_5 a un insieme formula_85 contenuto nel suo dominio è abitualmente indicata con formula_93.
Quando il dominio di una funzione formula_5 è il prodotto cartesiano di due o più insiemi, e dunque la funzione agisce su formula_95-uple di elementi di insiemi, allora l'immagine del vettore di questi elementi formula_96 viene indicata con la notazione
In questo caso la funzione viene anche chiamata funzione di vettore. A tal proposito in fisica si parla di campo.
Per esempio, si consideri la funzione di moltiplicazione che associa un vettore di due numeri naturali formula_7 e formula_45 al loro prodotto: formula_100. Questa funzione può essere definita formalmente come avente per dominio formula_101, l'insieme di tutte le coppie di numeri naturali; si noti inoltre che in questo caso la funzione è simmetrica rispetto alle componenti del vettore: formula_102 e quindi si tratta di una funzione di un insieme formula_103 in cui non importa cioè l'ordine degli elementi. Sono inoltre possibili anche altri raggruppamenti delle variabili: per esempio risulta estremamente importante nello studio dei sistemi di equazioni differenziali la teoria della funzione di matrice:
Molte operazioni binarie dell'aritmetica, come l'addizione e la moltiplicazione, sono funzioni dal prodotto cartesiano formula_105 a valori in formula_106, e vengono descritte tramite la notazione infissa: si scrive cioè formula_107 (e non formula_108) per descrivere l'immagine della coppia formula_109 tramite l'operazione formula_110.
Questa notazione è stata generalizzata dall'algebra moderna, per definire strutture algebriche come ad esempio quella di gruppo, come un insieme formula_111 dotato di alcune operazioni binarie aventi determinate proprietà.
Se il "codominio" di una funzione formula_112 è il prodotto cartesiano di due o più insiemi, questa può essere indicata come funzione vettoriale. Tali variabili spesso vengono aggregate in un vettore; a tal proposito in fisica si parla di campo vettoriale.
Un esempio tipico è dato da una trasformazione lineare del piano, ad esempio:
Una funzione è invece detta polidroma nel caso in cui esista almeno un elemento del dominio cui corrisponde più di un elemento del codominio. In effetti tali funzioni non rientrano nella definizione data inizialmente, ma in alcuni campi (ad esempio in analisi complessa) essa viene estesa proprio in questo senso. Un esempio di funzione polidroma è la radice quadrata di un numero reale positivo, che può essere descritta come una funzione
che associa a ogni numero reale positivo l'insieme delle sue due radici quadrate. Un esempio analogo è il logaritmo definito sull'insieme dei numeri complessi.
Nella matematica e sostanzialmente in tutte le sue applicazioni si incontrano numerosi tipi di funzioni, che si presentano anche con caratteristiche molto diverse, e che vengono classificate seguendo diversi criteri.
Data una funzione formula_6 di variabile reale a valori reali e una costante formula_116, su di essa sono applicabili le operazioni aritmetiche elementari ovvero somma, sottrazione, moltiplicazione, divisione, elevamento a potenza, radice "n"-esima ovvero:
se formula_120 si ha anche
se formula_122 si ha anche
e se formula_124 intero maggiore di 1, e se formula_124 pari si deve avere anche formula_126, si ha anche
Date due funzioni formula_6 e formula_129 di variabile reale a valori reali sono applicabili le operazioni aritmetiche elementari di cui sopra ovvero:
se formula_133 si ha anche
se formula_122 (o formula_126 nel caso in cui formula_137) si ha anche 
Date due funzioni formula_5 : formula_1 → formula_2 e formula_142 : formula_2 → formula_144 si può definire la loro composizione: questa è definita applicando prima formula_5 ad formula_7 e quindi applicando formula_142 al risultato formula_6.
Questa nuova funzione viene denotata con formula_149 Riconducendoci alla notazione tradizionale con le due notazioni il risultato della precedente composizione applicato all'elemento x del dominio si può scrivere
Data una funzione formula_6 di variabile reale a valori reali e una costante formula_152:
Data una funzione formula_6 di variabile reale a valori reali:
</text>
</doc>
<doc id="23394" url="https://it.wikipedia.org/wiki?curid=23394">
<title>Serie</title>
<text>
In matematica, una serie è la somma degli elementi di una successione, appartenenti in generale ad uno spazio vettoriale topologico. Si tratta di una generalizzazione dell'operazione di addizione, che può essere in tal modo estesa al caso in cui partecipano infiniti termini.
Le serie si distinguono primariamente in base alla natura degli oggetti che vengono sommati, che possono essere ad esempio numeri (reali o complessi) o funzioni, ma si utilizzano anche serie formali di potenze, serie di vettori, di matrici e, più in astratto, di operatori. Nell'ambito della teoria dei linguaggi formali vi sono le serie di variabili non commutative, cioè serie di stringhe.
Tra le serie di particolare interesse vi è la serie aritmetica, caratterizzata dal fatto che la differenza tra ciascun termine e il suo precedente è una costante, e la serie geometrica, in cui il rapporto tra ciascun termine e il suo precedente è una funzione costante. Nel caso più generale in cui il rapporto fra termini successivi è una funzione razionale, la serie è detta ipergeometrica.
Di particolare importanza in analisi complessa sono le serie di funzioni che sono serie di potenze, come la serie geometrica e la serie di Taylor. Le serie di funzioni costituiscono inoltre efficaci strumenti per lo studio delle funzioni speciali e per la risoluzione di equazioni differenziali.
Si consideri una successione di elementi formula_1. Si definisce serie associata ad formula_1 la somma formale:
Per ogni indice formula_4 della successione si definisce "successione delle somme parziali (o ridotte)" formula_5 associata a formula_6 la somma dei termini della successione formula_6 da formula_8 a formula_9:
Si dice che la serie formula_11 tende o converge al limite formula_12 se la relativa successione delle somme parziali formula_13 converge a formula_12. Ovvero:
se e solo se:
Questo limite si dice "somma della serie".
Più in generale, sia formula_17 una funzione da un insieme di indici formula_18 ad un insieme formula_19. Allora la serie associata ad formula_20 è la somma formale:
Se formula_22, la funzione formula_23 è una successione, con formula_24. Nel caso in cui formula_19 è un semigruppo, la successione delle somme parziali formula_26 associata a formula_27 è definita per ogni formula_4 come la somma della successione formula_29 da formula_30 a formula_31:
Se inoltre il semigruppo formula_19 è uno spazio topologico, allora la serie formula_34 converge a formula_35 se e solo se la relativa successione delle somme parziali formula_5 converge a formula_12.
In simboli: formula_38
Nel caso in cui il termine generale è una funzione formula_39, si definisce "dominio di convergenza" della serie di funzioni l'insieme dei valori di formula_40 per cui la serie converge. Si nota che valutando la funzione formula_39 in un punto formula_42 la serie diventa una serie numerica.
Una serie formula_11 è una "serie convergente" al limite formula_44 se la relativa successione delle somme parziali converge a formula_12, ovvero si verifica:
Se il limite formula_12 è infinito la serie si dice "serie divergente", mentre se il limite non esiste la serie si dice "serie indeterminata" o "serie oscillante". Se inoltre la serie converge o diverge, essa è detta "serie regolare".
Per determinare il carattere di una serie sono stati sviluppati diversi criteri di convergenza che legano la convergenza della serie allo studio del limite di successioni associate alla serie. Una condizione necessaria ma non sufficiente affinché una serie converga è che:
Un controesempio alla sufficienza è dato dalla serie armonica. Per mostrare la precedente condizione, sia:
la somma parziale ennesima. La convergenza della serie significa che esiste il limite finito:
Poiché formula_51, si ha:
Nelle serie numeriche il termine generale della serie formula_53 è un numero, reale o complesso, che dipende solo da n e non da altre variabili.
Per la determinazione della convergenza o meno delle serie numeriche conviene individuarne tre tipi per i quali sono disponibili criteri di convergenza spesso semplici ed efficaci.
Una serie numerica converge se e solo se per ogni formula_54 esiste un formula_55 tale che per tutti gli formula_56 e per ogni formula_57 si verifica:
L'enunciato è sostanzialmente il criterio di convergenza di Cauchy applicato alla successione delle somme parziali.
Una serie si dice "a termini positivi" quando tutti i suoi termini sono reali positivi, cioè data la serie:
il numero formula_53 è reale positivo. Si noti che tali serie possono solo divergere o convergere, e le somme parziali sono monotone non decrescenti:
perciò per il teorema di esistenza del limite nel caso di successioni monotone, questo tipo di serie convergono, se le somme parziali n-esime sono limitate, o sono divergenti ma non possono essere indeterminate.
Il carattere di una serie a termini di segno costante si ottiene applicando vari metodi, quali il criterio del confronto asintotico, il criterio della radice, il criterio del rapporto e il criterio del confronto. Se la condizione necessaria di convergenza non è rispettata, allora per il teorema di regolarità della serie a termini di segno costante, la serie diverge sicuramente.
Si dicono inoltre "serie a termini di segno qualsiasi" le serie a termini reali le quali presentano sia infiniti termini positivi che infiniti termini negativi.
La somma di due serie è la serie:
Se le serie a e b sono convergenti anche la somma delle due serie sarà convergente. Se una delle due serie diverge anche la somma delle serie sarà divergente. Inoltre:
Si definisce prodotto di Cauchy di due serie la serie:
dove:
Se le due serie a termini positivi sono convergenti allora il prodotto è convergente e la sua somma vale il prodotto delle somme delle serie date.
Questo risultato si estende a serie di termini qualunque nell'ipotesi che almeno una delle serie sia assolutamente convergente. Se entrambe le serie convergono ma non assolutamente, la successione formula_66 potrebbe non essere infinitesima e il prodotto potrebbe non convergere, come avviene nel caso formula_67. In generale, invece:
La serie formula_69 a termini di segno qualunque si dice "assolutamente convergente" se la serie dei valori assoluti formula_70 è convergente. La convergenza assoluta implica la convergenza (ordinaria), detta anche "convergenza semplice". Occorre sottolineare che non tutte le serie che convergono semplicemente convergono anche assolutamente: se ciò non accade, si dice che la serie è "condizionatamente convergente". Ad esempio, la serie:
converge semplicemente (a formula_72), ma non converge assolutamente, dato che la serie ad essa associata è quella armonica.
Data una serie, si può pensare di cambiare l'ordine dei suoi addendi: mentre una somma finita gode della proprietà commutativa, questo non è vero in generale per una serie infinita di addendi. Per esempio, una serie i cui termini pari siano -1 e quelli dispari 1 è oscillante, ma se si disordinano gli addendi la serie risultante può essere divergente.
Data una qualunque funzione biunivoca formula_73, si definisce una "permutazione" (anche detta "riarrangiamento" o "permutata") della serie formula_74 ogni oggetto della forma formula_75. Ora, se la serie originaria converge, si dice che essa è "incondizionatamente convergente" se tutte le sue serie permutate convergono.
Un notevole teorema (dimostrato da Riemann) ci dice che:
Si definisce serie infinita a termini complessi una somma del tipo:
o più sinteticamente:
dove formula_82, e dunque si scrive:
Questa serie si dice "convergente" se la somma dei primi "n" termini:
tende ad un limite finito al tendere di formula_85. Si può dedurre che la serie è convergente ad formula_86 se sono convergenti le due serie parte reale e parte immaginaria rispettivamente ai punti formula_87 e formula_88, e in tal caso la serie generale converge al punto formula_89, che è detta "somma della serie".
Condizione necessaria per la convergenza della serie è che:
cioè i termini della serie sono infinitesimi. Se la serie complessa ottenuta prendendo i valori assoluti dei termini di una serie:
è convergente, allora anche la serie di partenza è convergente. Infatti, dalle disuguaglianze:
segue che entrambe le serie formula_6 e formula_94 convergono.
Una condizione necessaria e sufficiente per la convergenza è invece che per ogni formula_95 esista formula_96 tale che per formula_97 intero positivo qualsiasi si abbia:
In generale, per le serie numeriche complesse valgono tutte le proprietà delle serie numeriche reali.
Una serie di funzioni complesse:
è uniformemente convergente se esiste formula_96 tale che per ogni formula_101 si ha:
per ogni formula_54 e per ogni formula_104 e formula_97 intero positivo. Se i termini della serie sono funzioni continue in un dominio formula_87 e la serie è uniformemente convergente, allora anche la somma della serie è continua in formula_87.
Condizione necessaria e sufficiente per la convergenza assoluta e uniforme della serie è che per tutti i valori di formula_108 i termini della serie siano tutti limitati nel dominio formula_87.
Il primo teorema di Weierstrass stabilisce che se i termini di una serie sono funzioni analitiche in un dominio formula_87 semplicemente connesso, la sua somma formula_111 è una funzione analitica nello stesso dominio. Infatti, nelle ipotesi del teorema la funzione somma è sicuramente continua e si può scambiare la serie con l'integrale:
dove formula_113 è una qualsiasi curva chiusa appartenente al dominio formula_87. Ne segue che:
e per il teorema di Morera, formula_111 è analitica.
Il secondo teorema di Weierstrass afferma invece che se una serie di funzioni analitiche in un dominio connesso e chiuso formula_87 è uniformemente convergente, allora può essere derivata termine a termine "n" volte.
In matematica, soprattutto in analisi complessa, sono di particolare importanza le serie di potenze. Si tratta di particolari serie di funzioni della forma:
dove formula_119 è detto il "centro" della serie. Si può dimostrare che per ogni serie di potenze esiste un numero formula_120, con formula_121 tale che la serie converge quando formula_122 e diverge quando formula_123. Il numero formula_120 è il raggio di convergenza della serie di potenze. Esistono alcuni criteri che facilitano la ricerca del raggio di convergenza della serie.
Una serie complessa di potenze positive è del tipo:
Dai teoremi di Weierstrass e Abel discende che la somma di una serie di potenze intere nel suo cerchio di convergenza è una funzione analitica, e che ogni serie di potenze è una serie di Taylor della funzione somma. Il teorema di Abel fornisce una caratterizzazione della regione di convergenza, mentre la formula di Cauchy-Hadamard mostra come si possa stabilire con esattezza il valore del raggio di convergenza.
Se la serie di potenze positive converge in un punto formula_126 allora converge uniformemente in ogni punto:
cioè in ogni cerchio di raggio:
Infatti, secondo le ipotesi del teorema la serie converge in formula_126, e si vuole provare la sua convergenza in tutto un cerchio di raggio formula_130. Se si riscrive:
e questa serie converge in formula_126, allora si può maggiorare:
La convergenza è quindi assoluta e uniforme.
Il raggio di convergenza di una serie di potenze intere positive è uguale a:
oppure:
se tale limite esiste ed è finito. All'interno di questo raggio la serie è uniformemente e assolutamente convergente. Sulla circonferenza può convergere o meno e si valuta caso per caso e la serie diverge al di fuori di questo cerchio. Può capitare il caso in cui la serie converga in un solo punto, allora la serie è necessariamente composta di un solo termine.
La serie di Taylor è lo sviluppo di una funzione (nel suo cerchio di convergenza) in serie di potenze in un punto in cui la funzione è analitica. Tale sviluppo è unico ed ha la forma:
con:
Infatti, dalla rappresentazione di Cauchy si ha:
Sviluppando il denominatore nel seguente modo:
e integrando termine a termine questa serie, che è uniformemente convergente, si ottiene:
dove:
come si voleva mostrare.
La serie è convergente entro il cerchio di convergenza (fino alla più vicina singolarità isolata) ed entro il dominio di analiticità della funzione formula_142, e può essere derivata termine a termine. Si deduce che l'analicità di una funzione e la sviluppabilità in serie di Taylor sono concetti equivalenti.
La serie di potenze di Laurent considera anche le potenze negative:
con:
In generale formula_145 "non" è la derivata formula_146.
Supponendo che la funzione formula_142 sia olomorfa nella corona circolare di centro formula_148 formata dalle circonferenze formula_149 interna e formula_150 esterna e sulle circonferenze, per ogni punto z la formula integrale di Cauchy si scrive:
Integrando il primo integrale su formula_150 si ha: formula_153 e si può rappresentare il primo membro in serie di Taylor. Il secondo membro dà sempre formula_153 e si ha uno sviluppo:
cioè in serie di potenze negative di formula_156. Raggruppando le due serie si ottiene la serie di Laurent.
La serie di Laurent ha potenze positive e negative dunque il dominio di questa serie non comprende il punto formula_157 che annullerebbe le potenze negative e risulta che la regione di convergenza non è un cerchio ma una regione anulare, cioè una "corona circolare":
o ancora meglio:
Data una funzione formula_160, l'espressione formula_161 rappresenta la somma:
Essa definisce chiaramente una funzione formula_163 che associa ad ogni formula_164 il valore formula_165.
Dall'analisi degli algoritmi si utilizza sovente la valutazione di somme di questo tipo, ad esempio nello studio in un'istruzione del tipo 
per un comando C qualsiasi si ottiene la somma:
dove formula_167 è il tempo di calcolo del comando C quando la variabile formula_168 assume il valore formula_4. L'ordine di grandezza di una somma può essere dedotto dall'ordine di grandezza dei suoi addendi.
Siano formula_20 e formula_171 due funzioni definite su formula_172 a valori in formula_173 e siano formula_174 e formula_19 le loro funzioni somma, cioè:
Allora formula_177 implica formula_178.
In altre parole, si può ricondurre lo studio asintotico di formula_174 e formula_19 sapendo che la relazione esistente tra le loro funzioni formula_181 e formula_182 sono formula_177, allora si ottiene che formula_178. Da notare che il simbolo formula_185 viene usato per indicare che due funzioni hanno lo stesso ordine di grandezza a meno di costanti moltiplicative.
La proprietà è una semplice conseguenza della definizione di formula_185. Infatti per l'ipotesi esistono due costanti positive formula_119, formula_188 tali che formula_189 per ogni formula_4 abbastanza grande. Sostituendo questi valori nelle rispettive sommatorie si ottiene:
per due costanti formula_192, formula_193 fissate e ogni formula_194 sufficientemente grande.
Si vuole valutare l'ordine di grandezza della somma:
Poiché formula_196, applicando la proposizione precedente si ottiene:
È importante conoscere il carattere di alcune cosiddette "serie fondamentali", cioè serie specifiche che vengono utilizzate spesso nell'applicazione dei criteri di convergenza. Esse sono, ad esempio, la serie di Mengoli, la serie geometrica, la serie armonica o la serie resto.
Nel seguito alcuni esempi:
</text>
</doc>
<doc id="23403" url="https://it.wikipedia.org/wiki?curid=23403">
<title>Numero reale</title>
<text>
In matematica, i numeri reali possono essere descritti in maniera non formale come numeri ai quali è possibile attribuire uno sviluppo decimale finito o infinito, come formula_1 I numeri reali possono essere positivi, negativi o nulli e comprendono, come casi particolari, i numeri interi (come formula_2), i numeri razionali (come formula_3) e i numeri irrazionali algebrici (come formula_4) e trascendenti (come formula_5 ed formula_6). Un numero reale razionale presenta uno sviluppo decimale finito o periodico; ad esempio formula_7 è razionale. L'insieme dei numeri reali è generalmente indicato con la lettera R o formula_8.
I numeri reali possono essere messi in corrispondenza biunivoca con i punti di una retta, detta "retta numerica" o "retta reale".
La definizione formale dei numeri reali ha rappresentato uno degli sviluppi più significativi del XIX secolo. Tra le definizioni maggiormente adottate oggi figurano le classi di equivalenza di successioni di Cauchy di numeri razionali, le sezioni di Dedekind, una ridefinizione del termine "rappresentazione decimale" e una definizione assiomatica come unico "campo archimedeo completo ordinato".
I termini "reale" e "immaginario" sono stati introdotti ne "La Géometrie" di René Descartes (1637), relativamente allo studio delle radici delle equazioni. Per estensione diversi autori hanno cominciato a parlare di "numeri reali" e "numeri immaginari". Nel 1874 appare un articolo fondamentale di Georg Cantor nel quale l'autore prende in considerazione l'insieme dei numeri reali dimostrando che tale insieme non è numerabile.
I numeri reali possono rappresentare qualsiasi grandezza fisica, come il prezzo di un prodotto, la distanza temporale fra due eventi, l'altitudine (positiva o negativa) di un sito geografico, la massa di un atomo o la distanza fra galassie. Gran parte dei numeri reali è usata quotidianamente, ad esempio in economia, informatica, matematica, fisica o ingegneria.
Di fatto, la maggior parte delle volte sono usati solo alcuni sottoinsiemi:
Questi insiemi, benché infiniti, hanno tutti cardinalità numerabile e sono quindi un'infinitesima parte dell'insieme dei numeri reali.
Ogni numero reale può essere identificato dalla sua numerazione decimale, ovvero mediante l'elenco delle cifre decimali della sua parte intera e, separate da una virgola, l'elenco delle cifre della parte frazionaria. In generale il numero di cifre decimali della parte frazionaria può essere infinito. Per questo in pratica il numero reale viene espresso presentando solo le prime cifre decimali come ad esempio nella scrittura formula_9 dove i tre punti esprimono il fatto che ci sono altre, infinite, cifre. Con questo procedimento di approssimazione è possibile presentare un numero razionale arbitrariamente vicino al numero reale in questione. Più sono le cifre decimali, più il numero razionale è vicino al numero reale che si vuole rappresentare, e maggiore quindi è la precisione dell'approssimazione. Ad esempio, pi greco può essere approssimato come
La rappresentazione decimale, molto utile nelle scienze applicate, presenta molti difetti dal punto di vista matematico, ad esempio:
Per questo motivo i matematici preferiscono definire e trattare i numeri reali con altre notazioni più astratte.
Sui numeri reali è possibile fare tutte le operazioni definite per i razionali, quali somma, differenza, prodotto, divisione per un numero diverso da zero ed elevamento a potenza con base positiva. Tali operazioni possono essere definite tramite il calcolo infinitesimale oppure è possibile estendere ai numeri reali, mediante approssimazione, le definizioni delle medesime operazioni date sui numeri razionali.
Dal punto di vista fisico, ogni esperimento è soggetto in modo intrinseco a un errore e quindi questo tipo di rappresentazione approssimata dei numeri reali non causa ulteriori problemi.
In informatica, i computer possono solo approssimare i numeri reali con numeri razionali: queste approssimazioni sono realizzate ad esempio in modo efficiente tramite la scrittura in virgola mobile. Alcuni programmi riescono a trattare alcuni numeri non razionali in modo esatto: ad esempio alcuni numeri algebrici possono essere descritti utilizzando la loro descrizione algebrica (come per esempio formula_10) piuttosto che la loro approssimazione decimale.
Più in generale, l'informatica può trattare in modo preciso solo i numeri "calcolabili": un numero reale è calcolabile se esiste un algoritmo che produce le sue cifre. Poiché esiste un'infinità numerabile di algoritmi ma un'infinità non numerabile di numeri reali, "quasi tutti" i numeri reali non sono calcolabili.
In matematica, i numeri reali giocano un ruolo fondamentale, e vengono continuamente manipolati, nonostante gran parte di questi non siano calcolabili. Il costruttivismo è una corrente matematica che accetta l'esistenza solo dei reali calcolabili.
La necessità di dare un nome ad alcune grandezze misurabili data dell'antichità. La prima risposta, realizzata dai Sumeri e nell'antico Egitto, fu quella di costruire le frazioni (⁄). Questo strumento permise subito la misura di qualsiasi grandezza positiva con precisione arbitraria.
La prima formalizzazione matematica nota è quella di Euclide nel III secolo a.C. Negli Elementi di Euclide, la geometria è formalizzata con assiomi, teoremi e dimostrazioni. Qui i numeri sono messi in corrispondenza con le lunghezze dei segmenti.
L'approccio di Euclide mette in evidenza che i numeri dell'epoca (le frazioni, cioè i numeri razionali) non potevano svolgere direttamente il ruolo di rappresentare le lunghezze di segmenti.
Un caso particolare del teorema di Pitagora mostra infatti che la lunghezza formula_11 dell'ipotenusa di un triangolo rettangolo i cui cateti hanno lunghezza formula_12, è tale che
D'altra parte, è facile mostrare che una tale formula_11 non è esprimibile come frazione: un risultato che risale alla scuola pitagorica ed era ben noto a Euclide. Una dimostrazione del risultato pitagorico, citata da Paul Erdős come una delle più belle di tutta la matematica, è mostrata a destra.
Per risolvere l'apparente contraddizione Euclide, nel V libro degli "Elementi", sviluppa una raffinata teoria dei rapporti tra grandezze (anche tra loro incommensurabili). Occorreva per questo innanzitutto avere un criterio per giudicare l'eventuale uguaglianza di due rapporti tra incommensurabili. Euclide fornisce un tale criterio nelle definizioni 4-9 del V libro, che riportiamo in una forma leggermente modernizzata nelle notazioni:
Date quattro grandezze formula_15, si dice che formula_16 se e solo se per ogni coppia di naturali, formula_17, formula_18, si verifica sempre una delle seguenti tre possibilità:
Grazie alla definizione precedente di uguaglianza tra rapporti anche i rapporti tra incommensurabili divennero un legittimo oggetto di studio della matematica e la loro eventuale uguaglianza era decisa semplicemente confrontando multipli interi delle grandezze considerate. In altre parole ogni rapporto tra incommensurabili era caratterizzato dal suo comportamento rispetto a tutte le coppie di naturali.
Altri sviluppi della matematica ellenistica che anticiparono in parte la moderna teoria dei reali furono quelli presenti nel metodo che fu poi detto "di esaustione"; ricordiamo che anche il primo calcolo di somme di serie risale ad Archimede (che sommò la serie geometrica di ragione formula_25).
Con l'ausilio delle frazioni i greci potevano esprimere con precisione arbitraria qualsiasi numero reale. L'assenza di un sistema di numerazione adeguato rendeva però difficili le operazioni elementari fra queste quantità, quali ad esempio la somma o la divisione.
Si deve attendere fino al V secolo per vedere finalmente riconosciuto lo zero come numero dalla scuola indiana, e per lo sviluppo del sistema di numerazione decimale.
Con il sistema di numerazione decimale compare un nuovo problema. Con questo sistema, ogni frazione possiede uno sviluppo decimale "periodico" ovvero la successione di decimali reitera all'infinito la stessa sequenza di numeri. Che significato dare a un oggetto avente uno sviluppo non periodico? Un esempio è il seguente
Nella seconda metà del XVII secolo, si assiste a un interessamento straordinario da parte dei matematici al calcolo delle serie e successioni. Tra questi, Nicolaus Mercator, i Bernoulli, James Gregory, Gottfried Leibniz lavorano su delle serie che sembrano convergere a un limite non razionale, come ad esempio:
Inoltre Joseph Liouville mostra nel 1844 l'esistenza di numeri trascendenti, cioè di numeri che non sono radici di nessun polinomio a coefficienti interi. Non è quindi sufficiente aggiungere i numeri algebrici ai razionali per ottenere "tutti i numeri".
Durante la seconda parte del XVII secolo, Isaac Newton e Gottfried Leibniz inventano una nuova branca della matematica, chiamata adesso analisi matematica, e conosciuta all'epoca come calcolo infinitesimale. Questa raggiunge subito la massima notorietà perché alla base di una nuova teoria fisica universale: la meccanica classica e la teoria della gravitazione universale.
Il calcolo infinitesimale necessita di un insieme di numeri più grande dei razionali, che "comprenda tutti i buchi", in modo da stare tutti su una retta, detta "retta reale".
Nel linguaggio moderno, la proprietà necessaria al calcolo è la completezza, e può essere espressa nel modo seguente:
Tale nozione, introdotta successivamente proprio da Cauchy, è estremamente importante in tutti i settori della matematica, e sarà anche all'origine della topologia all'inizio del XX secolo.
Il calcolo infinitesimale permette un'intuizione sempre più precisa sulla topologia dei numeri. Sarà necessario un ulteriore secolo per formalizzare in modo preciso l'insieme dei numeri reali, cioè per "tappare i buchi" lasciati dai razionali.
Come spesso accade in matematica, quando il problema è maturo, la soluzione arriva contemporaneamente da due ricercatori.
Il primo ad affrontare con successo la costruzione dei numeri reali è Augustin-Louis Cauchy. Il suo approccio resta il più fruttuoso, perché si applica anche ad altri casi. La sua idea è la seguente: una successione "dovrebbe" convergere se gli elementi sono (dopo un certo punto) arbitrariamente vicini fra loro: una tale successione è oggi detta successione di Cauchy.
Questa idea si traduce in una definizione rigorosa dei numeri reali solo verso la fine del XIX secolo, grazie ai lavori di Cantor e Dedekind nel 1872. Quest'ultimo propone in "Was sind und was sollen die Zahlen" (cosa sono e cosa devono essere i numeri) un metodo che sfrutta la relazione d'ordine fra le frazioni. La sua idea consiste nell'introdurre i reali non razionali tramite sottoinsiemi di razionali, i cosiddetti "tagli di Dedekind": ad esempio, la radice di 2 è rappresentata dall'insieme di tutti i numeri razionali il cui quadrato è minore di formula_30. Vi è un evidente rapporto tra la definizione di Dedekind e l'antica definizione di Euclide, ma anche una profonda differenza: mentre per Euclide e per gli altri matematici greci l'oggetto privilegiato di studio erano le grandezze e solo considerando i loro rapporti si trovavano di fronte a qualcosa di parzialmente analogo ai nostri numeri reali, all'epoca di Dedekind le grandezze numeriche avevano assunto da tempo un ruolo di protagonisti autonomi.
Sia formula_8 l'insieme di tutti i numeri reali. Allora:
L'ultima proprietà è quella che differenzia i reali dai razionali.
Per esempio, l'insieme dei numeri razionali il cui quadrato è minore di formula_30 ha un maggiorante razionale (per esempio formula_56) ma l'estremo superiore, che è il minore dei maggioranti, non è razionale in quanto la radice quadrata di formula_30 non è razionale.
I numeri reali son definiti in modo univoco dalle proprietà precedenti.
Detto in modo più preciso, dati due campi ordinati completi formula_58 e formula_59, esiste un unico isomorfismo da formula_58 a formula_59. Questa proprietà permette di pensare a essi come a un unico oggetto matematico.
L"'insieme reale esteso" si ottiene ampliando l'insieme dei numeri reali con due elementi aggiuntivi, indicati con formula_62 e formula_63:
La relazione d'ordine si estende a questi nuovi punti ponendo:
Alcune delle normali operazioni di somma e prodotto possono essere estese all'insieme reale esteso, ma non tutte. In particolare tale insieme non è più un campo e neppure un gruppo.
L'insieme reale esteso è però dotato di una topologia che estende quella dei numeri reali: un intorno di formula_63 (risp. formula_62) è una semiretta destra (risp. sinistra). Questo insieme è quindi spesso usato per definire in modo più uniforme il concetto di limite, e considerare alla stessa stregua le successioni che convergono a un numero reale o all'infinito.
La ragione principale che ha portato all'introduzione dei reali è che essi costituiscono uno spazio "senza buchi". Più precisamente, i reali sono uno spazio metrico completo. La completezza può essere espressa in vari modi, tutti equivalenti all'assioma di Dedekind descritto sopra.
Nei numeri reali vale, per definizione di completezza, il fatto seguente:
Ricordiamo che:
In uno spazio metrico qualsiasi, ogni successione convergente è una successione di Cauchy. Quando è vero anche l'opposto (come nei numeri reali), lo spazio si dice "completo".
L'insieme dei razionali non è completo.
Per esempio, la successione delle prime formula_18 cifre della radice quadrata di formula_30, ossia
è di Cauchy ma non converge a un numero razionale.
La completezza dei numeri reali può essere espressa nel modo seguente: dati due sottoinsiemi formula_83 non vuoti di formula_8 tali che
esiste un numero reale formula_86 tale che
Per i numeri reali vale l'"assioma di Archimede": dati due numeri formula_88 reali positivi, con formula_89, esiste un numero naturale formula_18 tale che
Un campo ordinato in cui vale questo assioma è detto "archimedeo". David Hilbert definisce il campo dei numeri reali come il ""campo completo archimedeo"": con questa frase, Hilbert sottolinea il fatto che i numeri reali formano il "più grande" campo archimedeo, nel senso che ogni altro campo archimedeo è contenuto in formula_8. In questo senso, formula_8 è "completo" secondo Hilbert.
Questo significato di completezza è il più vicino alla costruzione dei numeri reali a partire dai numeri surreali, poiché la costruzione comincia con una classe che contiene ogni campo ordinato (i surreali) e seleziona da essa il più grande sottocampo archimedeo.
A differenza dei numeri razionali, i reali non formano un insieme numerabile; l'insieme dei numeri reali è "strettamente più grande" di quello dei numeri naturali (pur considerando che entrambi sono infiniti). Formalmente, questo equivale a dire che non esiste una corrispondenza biunivoca fra i numeri reali e i numeri naturali.
Questo fatto distingue i numeri reali dagli altri insiemi numerici normalmente utilizzati. Infatti gli insiemi dei numeri naturali, razionali, algebrici hanno tutti la stessa cardinalità (ovvero possono essere messi in corrispondenza biunivoca), mentre l'insieme dei reali ha una "cardinalità superiore": esiste una funzione iniettiva dai numeri razionali ai reali, ma non viceversa.
In altre parole, nel tappare tutti i buchi lasciati dai numeri razionali si deve aggiungere una "tale quantità" di numeri nuovi da farne crescere la cardinalità. Questo fatto può essere dimostrato con il procedimento diagonale di Cantor.
Effettivamente, l'insieme formula_8 ha cardinalità 2, la stessa dell'insieme delle parti di un insieme numerabile: ovvero, la stessa cardinalità dell'insieme di tutti i sottoinsiemi dei numeri naturali.
Poiché anche i numeri algebrici hanno cardinalità numerabile, "quasi tutti" i numeri reali sono trascendenti.
L'ipotesi del continuo sostiene la non esistenza di una cardinalità intermedia fra quella degli interi e quella dei reali. Nell'ambito della teoria degli insiemi di Zermelo - Fraenkel, che è quella comunemente usata, questa ipotesi non può essere né dimostrata né confutata, cioè è indipendente dai suoi assiomi.
L'insieme formula_95 dei numeri razionali è denso nell'insieme dei numeri reali.
Siano formula_96 con formula_97, allora formula_98
formula_99 e formula_100 sono discordi: formula_101formula_102.
formula_99 e formula_100 sono ambedue positivi: formula_105
Dato che formula_105 si ha che formula_107 e che anche formula_108 quindi per la proprietà archimedea dei numeri reali formula_109 quindi:
Sia formula_115, formula_116, quindi per la proprietà archimedea dei numeri reali formula_117 infatti formula_118. Per le proprietà di buon ordinamento dei numeri naturali formula_119 ammette minimo ovvero formula_120 quindi:
formula_99 e formula_100 sono ambedue negativi: formula_131
formula_132formula_133 come nel caso appena illustrato formula_134 moltiplicando per −1 si invertono i segni della disuguaglianza e si ha che formula_135, formula_136.
Definito l'insieme formula_137 dei numeri irrazionali si dimostra che anch'esso è denso in formula_8.
Siano formula_139 con formula_97, allora formula_141.
Per la proprietà di compatibilità della relazione d'ordine fissata su formula_142 rispetto all'operazione di somma algebrica se formula_97 allora formula_144formula_145 e formula_146 quindi per la proprietà di densità dei numeri razionali nell'insieme dei numeri reali formula_147, aggiungendo formula_4 a tutti i membri della disuguaglianza si ha che formula_149 e che quindi formula_150.
I numeri reali formano uno spazio metrico: la distanza tra formula_35 e formula_36 è definita come il valore assoluto formula_153. Come accennato sopra, formula_8 risulta essere uno spazio metrico completo.
La metrica appena definita induce su formula_8 una struttura di spazio topologico. Un sottoinsieme formula_119 di formula_8 è aperto se e solo se è unione di intervalli aperti formula_158, dove formula_99 e formula_100 possono essere anche formula_161 o formula_63.
Lo spazio formula_8 è connesso ma non compatto. Lo spazio
è comunque localmente compatto, ed è una varietà differenziale di dimensione 1. Risulta essere omeomorfo a un qualsiasi intervallo aperto formula_158.
Lo spazio formula_8 è contraibile, e quindi semplicemente connesso, con tutti i gruppi di omotopia banali.
I numeri reali sono il prototipo di spazio vettoriale reale di dimensione uno: la moltiplicazione per uno scalare non è altro che la moltiplicazione usuale. La struttura lineare è compatibile con la topologia sopra descritta, dunque formula_8 è uno spazio vettoriale topologico.
L'insieme formula_8 può anche essere pensato come uno spazio vettoriale sul campo formula_168 dei numeri razionali; in questo caso risulta avere una dimensione infinita (così come il campo dei numeri algebrici).
Inoltre, la moltiplicazione funge anche da prodotto scalare, rendendo formula_8 uno spazio di Hilbert e quindi uno spazio normato, in cui la norma non è altro che la funzione valore assoluto.
I numeri reali sono dotati di una misura canonica, la misura di Lebesgue. La misura dell'intervallo formula_158 si definisce come formula_171. Qualsiasi sottoinsieme numerabile (come ad esempio quello dei numeri razionali), ha misura nulla. Esistono anche sottoinsiemi di misura nulla non numerabili, come l'insieme di Cantor.
Ci sono in formula_8 anche insiemi non misurabili, ma la loro costruzione necessita dell'assioma della scelta: un esempio è l'insieme di Vitali.
La misura di Lebesgue è la misura di Haar della struttura di formula_8 come gruppo topologico, normalizzata in modo che l'intervallo [0,1] abbia misura 1.
Ogni numero reale non negativo ha la sua radice quadrata in formula_8, i reali negativi no. Questo mostra che l'ordinamento in formula_8 è determinato dalla sua struttura algebrica.
Ogni polinomio di grado dispari ha almeno una radice. Esistono comunque polinomi senza radici reali, e questo fa di formula_8 un campo non algebricamente chiuso.
La chiusura algebrica di formula_8 (ovvero il più piccolo campo algebricamente chiuso che lo contiene) è il campo dei numeri complessi.
L'assioma di Dedekind si riferisce a sottoinsiemi di reali e quindi è un predicato della logica del secondo ordine. In generale, non è possibile caratterizzare i reali usando solo la logica del primo ordine.
Per il teorema di Löwenheim-Skolem (debole), esiste un insieme denso numerabile di numeri reali che soddisfa gli stessi predicati nella logica del prim'ordine dei numeri reali.
L'insieme dei numeri iperreali è più grande di formula_8 ma soddisfa gli stessi predicati della logica del prim'ordine di formula_8. I campi ordinati che soddisfano gli stessi predicati della logica del prim'ordine di formula_8 sono chiamati modelli non standard di formula_8. Questo è ciò che permette all'analisi non standard di funzionare; dimostrando un predicato del prim'ordine in qualche modello non standard (che può essere più semplice che dimostrarlo in formula_8), se ne deduce che lo stesso predicato è vero anche per formula_8.
I numeri reali possono essere generalizzati ed estesi in numerose direzioni. Forse l'estensione più naturale è quella dei numeri complessi, che formano un campo algebricamente chiuso. Tuttavia, rispetto ai reali, essi perdono la struttura di ordinamento, ciò significa che i numeri complessi non sono un campo ordinato. I numeri complessi hanno innumereveli applicazioni in fisica: per esempio, in elettrotecnica e in elettronica sono alla base del metodo simbolico che semplifica enormemente lo studio dei circuiti elettrici in regime sinusoidale, così come sono fondamentali in meccanica quantistica, poiché questa teoria è sviluppata in uno spazio di Hilbert a dimensione infinita sul campo dei complessi e, inoltre, l'unità immaginaria compare nell'equazione di Schrödinger.
Il campo dei numeri complessi è l'algebra ottenuta dal campo dei numeri reali mediante la costruzione di Cayley-Dickson. Proseguendo con tale costruzione, si ottengono algebre successive sul campo dei numeri reali, ciascuna di dimensione via via doppia rispetto all'algebra precedente, al prezzo della progressiva perdita di alcune proprietà. Dopo i numeri complessi, si ottengono, in sequenza, i quaternioni, gli ottonioni e i sedenioni. Tutte queste algebre costituiscono la famiglia delle algebre di Cayley-Dickson, che è inclusa nell'insieme dei cosiddetti numeri ipercomplessi, il quale, tuttavia, include anche la famiglia delle algebre di Clifford.
Un'altra possibile estensione per i numeri reali è rappresentata dai numeri duali che, sotto alcuni aspetti, mostrano proprietà complementari rispetto a quelle dei numeri complessi e che, a differenza di questi ultimi, sono caratterizzati da un'unità immaginaria nilpotente. Inoltre, a differenza dei numeri complessi, i numeri duali non costituiscono un campo, ma costituiscono semplicemente un'algebra associativa e commutativa dotata di unità, introducendo le operazioni di somma e di prodotto. Anche i numeri duali hanno applicazioni in fisica, come un semplice esempio di superspazio, utilizzato da alcune teorie fisiche, quali la relatività generale e le teorie supersimmetriche, per descrivere la configurazione spaziale. 
Ancora un'altra possibile estensione per i numeri reali è rappresentata dai numeri complessi iperbolici, caratterizzati da un'unità immaginaria il cui quadrato è posto uguale a "1", invece che a "-1", come accade per gli ordinari numeri complessi. I numeri complessi iperbolici presentano diverse analogie con gli ordinari numeri complessi, tuttavia, a differenza di questi ultimi e come i numeri duali, non costituiscono un campo; essi costituiscono, infatti, solamente un anello. Anche i numeri complessi iperbolici trovano applicazioni in fisica: per esempio, nell'ambito della relatività ristretta, possono essere utilizzati per rappresentare le trasformazioni di Lorentz.
Esempi di campi ordinati che estendono i reali sono i numeri iperreali e i numeri surreali: entrambi contengono numeri infinitesimali e infinitamente grandi, ma non soddisfano l'assioma di Archimede descritto sopra.
Occasionalmente, come scritto sopra, gli elementi formali formula_63 e formula_62 sono aggiunti ai reali per formare la "retta numerica estesa", con una naturale topologia compatta. Questo insieme non è un campo ma mantiene molte delle proprietà dei numeri reali.
Le forme hermitiane su uno spazio di Hilbert (per esempio, le matrici quadrate complesse autoaggiunte) generalizzano i reali in molti aspetti: possono essere ordinate (non totalmente), sono complete, i loro autovalori sono reali e formano un'algebra associativa reale. Gli operatori definiti positivi corrispondono ai numeri reali positivi e gli operatori normali corrispondono ai numeri complessi.
</text>
</doc>
<doc id="23432" url="https://it.wikipedia.org/wiki?curid=23432">
<title>Numero complesso</title>
<text>
Un numero complesso è un numero formato da una parte reale e da una parte immaginaria sommate tra loro. Può essere rappresentato dalla somma di un numero reale e di un numero immaginario (un multiplo dell'unità immaginaria, indicata con la lettera formula_1). I numeri complessi sono usati in tutti i campi della matematica, in molti campi della fisica (notoriamente in meccanica quantistica), nonché in ingegneria, specialmente in elettronica/telecomunicazioni o elettrotecnica, per la loro utilità nel rappresentare onde elettromagnetiche e correnti elettriche ad andamento temporale sinusoidale.
In matematica i numeri complessi formano un campo (nonché un'algebra reale bidimensionale) e sono generalmente visualizzati come punti di un piano, detto piano complesso. La proprietà più importante dei numeri complessi è basata sul teorema fondamentale dell'algebra, secondo il quale qualunque equazione polinomiale di grado n ha n soluzioni complesse, non necessariamente distinte.
Nel corso dei secoli gli insiemi dei numeri sono andati man mano allargandosi, presumibilmente per rispondere all'esigenza di dare soluzione a equazioni e problemi sempre nuovi.
I numeri complessi sono un'estensione dei numeri reali, nata inizialmente per consentire di trovare tutte le soluzioni delle equazioni polinomiali. Ad esempio, l'equazione 
non ha soluzioni nell'insieme dei numeri reali, perché in questo insieme non esistono numeri il cui quadrato sia negativo.
Si definisce allora il valore formula_1, chiamato unità immaginaria, che gode della seguente proprietà:
I numeri complessi sono formati da due parti, una parte reale e una parte immaginaria, e sono rappresentati dalla seguente espressione:
dove formula_6 e formula_7 sono numeri reali, formula_1 è l'unità immaginaria.
Le leggi della somma algebrica e del prodotto nei numeri complessi si applicano facendo i conti nel modo usuale e sapendo che formula_9.
Come i numeri reali sono in corrispondenza biunivoca con i punti di una retta, quelli complessi sono in corrispondenza con i punti del piano, detto piano complesso (o di Argand-Gauss): al numero complesso formula_10 si associa il punto di coordinate cartesiane formula_11.
Usando la relazione formula_4 si possono risolvere tutte le equazioni di secondo grado
con formula_14, incluse quelle che non hanno soluzioni reali perché dotate di discriminante negativo:
Le soluzioni sono determinate dalla formula risolutiva dell'equazione
che nel caso in cui il discriminante sia negativo, si svolge nel modo seguente:
Ad esempio:
Più in generale è vero che se un numero complesso è soluzione di un'equazione a coefficienti reali, allora anche il suo complesso coniugato è soluzione della stessa equazione. Quindi nel caso di un'equazione di grado dispari, tra le soluzioni ci sarà sempre almeno un numero reale.
I numeri complessi hanno avuto una genesi dilatata nel tempo. Cominciarono a essere utilizzati formalmente nel XVI secolo nelle formule di risoluzione delle equazioni di terzo e quarto grado di Tartaglia. I primi che riuscirono ad attribuire soluzioni alle equazioni cubiche furono Scipione Dal Ferro, il Bombelli e anche Niccolò Tartaglia, quest'ultimo, dopo molte insistenze, passò i risultati a Girolamo Cardano con la promessa di non divulgarli. Cardano dopo aver verificato l'esattezza delle soluzioni di Tartaglia non rispettò la sua promessa e pubblicò i risultati, citandone l'autore però, nella sua nota "Ars Magna" del 1545. Tartaglia aveva molte amicizie tra gli inquisitori e in seguito Cardano ebbe problemi legati alla giustizia del tempo, molti dei quali provenienti da accuse di eresia. Attualmente la comparsa di radici di numeri negativi viene attribuita principalmente a Tartaglia mentre nelle meno numerose pagine dedicate a Cardano non vi è traccia del suo probabile importante contributo a tale rappresentazione numerica.
Inizialmente i numeri complessi non vennero considerati come "numeri" ma solo come artifici algebrici utili a risolvere equazioni. Erano infatti numeri "che non dovrebbero esistere": Cartesio nel XVII secolo li chiamò "numeri immaginari". Abraham de Moivre ed Eulero nel XVIII secolo incominciarono a fornire ai numeri complessi una base teorica, finché questi assunsero piena cittadinanza nel mondo matematico con i lavori di Gauss. Contemporaneamente si affermò l'interpretazione dei numeri complessi come punti del piano.
In matematica molti oggetti e teoremi dipendono dalla scelta di un insieme numerico di base: spesso la scelta è fra numeri reali e complessi. L'aggettivo "complesso" è in questo caso usato per specificare questo insieme di base. Per esempio, si definiscono le matrici complesse, i polinomi complessi, gli spazi vettoriali complessi e l'algebra di Lie complessa. Esistono anche il teorema di Sylvester complesso e il teorema spettrale complesso.
Formalmente un numero complesso si può definire come una coppia ordinata di numeri reali formula_19. Si definiscono quindi somma e prodotto di due numeri complessi nel modo seguente:
Con queste due operazioni, l'insieme dei numeri complessi risulta essere un campo, che viene indicato con formula_22.
Il numero complesso formula_23 viene identificato con il numero reale formula_24, mentre il numero formula_25 è chiamato unità immaginaria ed è descritto con la lettera formula_26. L'elemento 1 è l'elemento neutro per la moltiplicazione, mentre si verifica che:
Ogni numero complesso formula_28 si scrive facilmente come combinazione lineare nel modo seguente:
I numeri "a" e "b" sono rispettivamente la parte reale e la parte immaginaria di "z". Questa rappresentazione dei numeri complessi rende agevole lo svolgimento delle operazioni di somma e prodotto. Ad esempio:
Usando gli strumenti della teoria dei campi, il campo dei numeri complessi può essere definito come la chiusura algebrica del campo dei numeri reali. 
Usando gli strumenti della teoria degli anelli, può anche essere introdotto come l'anello quoziente dell'anello dei polinomi reali con una variabile tramite l'ideale generato dal polinomio formula_31:
Questo è effettivamente un campo perché formula_31 è irriducibile. La radice del polinomio formula_31 è l'unità immaginaria formula_26, quindi l'anello quoziente è isomorfo a formula_36.
Un numero complesso può essere visto come un punto del piano cartesiano, chiamato in questo caso piano di Gauss. Una rappresentazione di questo tipo si chiama diagramma di Argand-Gauss. Nella figura si vede che
essendo formula_38 e formula_39 funzioni trigonometriche.
Le formule inverse sono:
Usando la formula di Eulero, possiamo esprimere formula_45 come
tramite la funzione esponenziale. Qui formula_47 è il modulo (o valore assoluto o norma) e formula_48 (detta anomalia) è l'argomento di formula_45. L'argomento è determinato da formula_45 se è inteso nell'intervallo formula_51, altrimenti è definito solo a meno di somme con formula_52 per qualche intero formula_53.
Il valore assoluto (modulo) ha le seguenti proprietà:
valide per tutti i numeri complessi formula_45 e formula_60.
La prima proprietà è una versione della disuguaglianza triangolare.
La distanza fra due punti del piano complesso è data semplicemente da
Il complesso coniugato del numero complesso formula_62 è definito come 
A volte è anche indicato come formula_64. Nel piano complesso formula_65 è ottenuto da formula_45 per simmetria rispetto all'asse reale. Valgono le seguenti proprietà:
Conoscendo il valore assoluto e il coniugato di un numero complesso formula_74 è possibile calcolare il suo reciproco formula_75 attraverso la formula:
Ovvero, se formula_62 otteniamo
Valgono le relazioni
La somma di due numeri complessi equivale alla usuale somma fra vettori nel piano complesso.
Vale
In realtà il prodotto non è che il risultato di un normalissimo prodotto di binomi. Usando la rappresentazione 
e le proprietà della funzione esponenziale, il prodotto di due numeri complessi 
assume la forma più agevole
In altre parole, nel prodotto di due numeri complessi, si sommano gli argomenti e si moltiplicano i moduli.
Questa affermazione consente di dimostrare la regola dei segni del prodotto: formula_85. Difatti se si considera che l'argomento di un numero reale negativo è 180º, moltiplicando tra loro due di questi numeri si ottiene un numero con argomento 360° e quindi 0° che è l'argomento di un numero reale positivo.
Una moltiplicazione per un numero complesso può essere vista come una simultanea rotazione e omotetia. Moltiplicare un vettore o equivalentemente un numero complesso per l'elemento formula_26 produce una rotazione di 90°, in senso antiorario, del numero complesso di partenza. Ovviamente la moltiplicazione per formula_26 e poi ancora per formula_26 produce una rotazione di 180º; ciò è logico visto che formula_9.
Il rapporto fra due numeri complessi formula_90 e formula_91 è dato da:
Usando la rappresentazione 
il rapporto di due numeri complessi è
Rappresentando ogni numero complesso come
è facile descrivere la potenza formula_96-esima
per ogni formula_96 intero. Con una notazione lievemente differente:
Si ottiene la formula di De Moivre:
Inoltre, ogni numero complesso ha esattamente formula_101 radici formula_101-esime: in particolare non esiste un modo univoco di definire la radice quadrata di un numero complesso.
La funzione esponenziale complessa formula_103 è definita facendo uso delle serie e degli strumenti del calcolo infinitesimale, nel modo seguente:
In particolare, se formula_105 si ottiene
facendo uso della formula di Eulero.
Il logaritmo naturale formula_107 di un numero complesso formula_108 è per definizione un numero complesso formula_109 tale che
Se 
il logaritmo di formula_108 è un qualsiasi numero complesso formula_109 del tipo
dove formula_115 è un numero intero qualsiasi. Poiché il valore formula_115 è arbitrario, un numero complesso ha una infinità di logaritmi distinti, che differiscono per multipli interi di formula_117. 
Se formula_118 si può scrivere
In questo caso, se formula_45 è reale (cioè se formula_121) fra gli infiniti valori ce n'è uno reale, che corrisponde all'usuale logaritmo di un numero reale positivo.
Supponiamo di voler individuare i numeri complessi z tali che 
La prima possibilità è quella di porre formula_123 e di uguagliare la parte reale di formula_124 alla parte reale del coniugato di formula_125 e analogamente per le rispettive parti immaginarie. Seguendo questa strada si ottengono due equazioni:
da cui si ricavano 7 soluzioni:
In alternativa, si può usare la rappresentazione polare
e uguagliare le norme e gli argomenti di formula_124 e del coniugato di formula_125, ottenendo anche qui due equazioni: 
con formula_134. Ovviamente si ottengono le stesse soluzioni, per esempio 
Diversamente dai numeri reali, i numeri complessi non possono essere ordinati in modo compatibile con le operazioni aritmetiche. Non è cioè possibile definire un ordine tale che
come avviene con i numeri reali. Quindi non ha senso chiedere ad esempio se formula_1 è maggiore o minore di formula_139, né studiare disequazioni nel campo complesso. Infatti in ogni campo ordinato tutti i quadrati devono essere maggiori o uguali a zero: per costruzione dell'unità immaginaria, invece formula_4.
Ciò non deve essere confuso con il dire che l'insieme dei numeri complessi non può essere totalmente ben ordinato. Infatti i numeri complessi hanno, ad esempio, un ordinamento in termini di ordine lessicografico, e costituiscono quindi un insieme ordinabile (come ogni insieme in ZFC stante l'assioma della scelta), ma non formano un campo ordinato (per la ragione di cui sopra) né una struttura algebrica ordinabile rispetto alla metrica indotta da una norma. 
Quando si disegna una funzione nel piano cartesiano il cui codominio contiene numeri dell'insieme immaginario, tali numeri non possono essere rappresentati da una coppia di coordinate formula_141, poiché essendo formula_142 complesso non può avere ordinamento rispetto alla retta formula_142.
L'insieme formula_144 è contemporaneamente uno spazio vettoriale complesso a una dimensione (come tutti i campi), e uno spazio vettoriale reale a due dimensioni. In quanto spazio vettoriale reale a dimensione finita è inoltre uno spazio normato completo, cioè uno spazio di Banach, e più in particolare uno spazio di Hilbert.
Una "radice complessa" di un polinomio formula_145 a coefficienti reali è un numero complesso formula_108 tale che formula_147. Il teorema fondamentale dell'algebra asserisce che ogni polinomio di grado formula_101 ha esattamente formula_101 soluzioni complesse, contate con molteplicità. Questo risultato indica che i numeri complessi sono (a differenza dei reali) un campo algebricamente chiuso.
Lo studio delle funzioni con variabili complesse è detto analisi complessa e trova largo impiego nella matematica applicata e nella teoria dei numeri, oltre che in altre branche della matematica, della fisica e dell'ingegneria. Spesso, le dimostrazioni più semplici per gli enunciati dell'analisi reale o persino della teoria dei numeri impiegano tecniche di analisi complessa (vedi teorema dei numeri primi per un esempio). Diversamente dalle funzioni reali, che sono rappresentate comunemente come grafici bidimensionali, le funzioni complesse hanno grafici a quattro dimensioni e spesso vengono rappresentate come grafici colorati dove il colore sopperisce alla dimensione mancante (si veda, ad esempio, la voce Immagini conformi). Si possono anche usare delle animazioni per mostrare la trasformazione dinamica della funzione complessa del piano complesso.
I numeri complessi sono presenti in tutta la matematica, e sono protagonisti di interi settori, come l'analisi complessa o la geometria algebrica. Elenchiamo qui soltanto alcune applicazioni dei numeri complessi a settori della matematica in cui questi non hanno un ruolo dominante.
I numeri complessi sono utilizzati per la risoluzione delle equazioni differenziali associate al moto di tipo vibratorio dei sistemi meccanici. Sono molto usati anche nell'ingegneria elettrica, soprattutto per rappresentare lo sfasamento tra reattanza e resistenza.
I numeri complessi vengono utilizzati nell'analisi dei segnali e in tutti i campi dove si trattano segnali che variano sinusoidalmente nel tempo, o anche semplicemente periodici. Il valore assoluto di |"z"| è interpretato come l'ampiezza del segnale mentre l'argomento di "z" è interpretato come la fase. I numeri complessi rendono possibile anche l'analisi di Fourier, che rende possibile scomporre un generico segnale tempo-invariante in una somma di infinite sinusoidi: ogni sinusoide è scritta come un singolo numero complesso
dove ω è la pulsazione della sinusoide e "z" la sua ampiezza.
Nell'ingegneria elettrica ed elettronica vengono utilizzati per indicare la tensione e la corrente. L'analisi dei componenti resistivi, capacitivi e induttivi è stata unificata con l'introduzione dei numeri complessi, che riassumono tutte e tre queste componenti in una sola entità detta impedenza, semplificando notevolmente i calcoli. Possono esprimere delle relazioni che tengono conto delle frequenze e di come i componenti varino il loro comportamento al variare della frequenza. In questo tipo di calcoli si usa tradizionalmente la lettera "j" per indicare l'unità immaginaria, dato che la "i" è riservata alla corrente: i primi trattati di elettrotecnica, all'inizio del XX secolo, stabilivano "j" = "-i", cioè l'unità immaginaria nelle formule usate per l'elettrotecnica era il negativo di quella usata dai matematici. L'uso è stato mantenuto nel tempo, e questo dettaglio, sia pure ignoto ai più, è parzialmente vero anche oggi. Anche se, la stragrande maggioranza delle volte, nella letteratura tecnica con "j" oramai si intende l'unità immaginaria stessa, per cui "j"="i"
Il processo di estensione del campo R dei numeri reali al campo C dei numeri complessi è noto come costruzione di Cayley-Dickson. Esso può essere portato oltre a dimensioni più elevate, ottenendo i quaternioni H, gli ottetti (o ottonioni) O e i sedenioni, i quali costituiscono, rispettivamente, delle algebre a "4", "8", "16" dimensioni sul campo dei numeri reali.
In questo contesto, i numeri complessi sono stati chiamati binarioni.
Le algebre prodotte da questo processo sono note come algebre di Cayley-Dickson e, poiché estendono i numeri complessi, vanno a costituire una famiglia dell'insieme dei cosiddetti numeri ipercomplessi, il quale, tuttavia, include anche la famiglia delle algebre di Clifford.
</text>
</doc>
<doc id="15193" url="https://it.wikipedia.org/wiki?curid=15193">
<title>Moto circolare</title>
<text>
Il moto circolare è uno dei moti semplici studiati dalla fisica e dalla cinematica, e consiste in un moto di un punto materiale lungo una circonferenza.
Il moto circolare assume importanza per il fatto che la velocità e l'accelerazione variano in funzione del cambiamento di direzione del moto. Tale cambiamento si può misurare comodamente usando le misure angolari per cui le equazioni del moto, introdotte con il moto rettilineo, vanno riviste e rielaborate con misure angolari.
La retta passante per il centro della circonferenza e perpendicolare alla stessa prende il nome di asse di rotazione. Per semplificare l'analisi di questo tipo di moto, infatti, consideriamo che l'osservatore si ponga sull'asse di rotazione. Ciò è possibile per l'isotropia e l'omogeneità dello spazio.
Il sistema più comodo per analizzare un moto circolare fa uso delle coordinate polari. Infatti nel caso particolare di movimento che avviene su una circonferenza di raggio R, il moto in coordinate polari è determinato dalle coordinate:
mentre in coordinate cartesiane si ha:
che soddisfano la seguente identità (in ogni istante di tempo):
Nel moto circolare si possono definire due diverse tipologie di velocità: la velocità angolare e la velocità tangenziale.
Per descriverle consideriamo nello spazio tridimensionale, il vettore infinitesimo "spostamento angolare"
dove formula_5 è un versore disposto lungo l'asse di rotazione e formula_6 la variazione infinitesima della variabile angolare formula_7.
Sia ora formula_8 il vettore posizione del punto "P" ad ogni istante formula_9, allora lo "spostamento lineare" formula_10 (ovvero la variazione infinitesima di formula_8) del punto "P" sull'arco di circonferenza nell'intervallo di tempo (infinitesimo) formula_12 sarà legata allo spostamento angolare formula_13 dal prodotto vettoriale:
La direzione e il verso risultano corretti per la regola della mano destra, come si vede dalla figura a lato. Il modulo è dato da (si ricordi che l'angolo è infinitesimo):
che corrisponde, per definizione essendo formula_6 espresso in radianti, all'arco di circonferenza sottesa dall'angolo formula_17.
La velocità angolare è definita come la derivata, rispetto al tempo, del vettore spostamento angolare ed è comunemente indicata con la lettera greca formula_18 (omega):
(ricordando che formula_20 è costante) ed è una misura della velocità di variazione dell'angolo formato dal vettore posizione, si misura in radianti al secondo formula_21 ed ha la stessa direzione del vettore spostamento angolare.
La velocità lineare (o tangenziale) si ottiene derivando rispetto al tempo il vettore posizione formula_22:
ed è legata alla velocità angolare dalla seguente relazione (per approfondire si veda anche derivata di un vettore):
Si noti che la costanza della velocità angolare implica la costanza del modulo della velocità.
Se si esegue il prodotto scalare dei due vettori formula_25 e formula_26 si ottiene zero per ogni istante di tempo "t", e questo dimostra che la velocità tangenziale è sempre ortogonale al raggio vettore formula_25.
Oltre a queste, si può introdurre la velocità areolare, definita come la derivata, rispetto al tempo, dell'area spazzata dal raggio vettore formula_28:
formula_29
si misura in metri quadri al secondo formula_30 ed ha la stessa direzione e lo stesso verso della velocità angolare.
Derivando rispetto al tempo l'espressione del vettore velocità tangenziale otteniamo l'accelerazione; che ha una componente parallela alla velocità (responsabile della variazione del modulo di questa) e una normale (o radiale): si tratta rispettivamente dell'accelerazione tangenziale e dell'accelerazione centripeta:
La prima frazione si chiama accelerazione angolare di solito indicata con formula_32, formula_33 oppure formula_34.
Si misura in radianti su secondi quadri formula_35, fornisce la variazione della velocità angolare ed ha stessa direzione di questa.
Sviluppando la relazione precedente otteniamo (tralasciando le dipendenze dal tempo):
dove si vede chiaramente la componente tangenziale che rappresenta la variazione del modulo della velocità lineare e la componente normale o centripeta che rappresenta la variazione della direzione della velocità lineare, diretta sempre verso il centro della circonferenza.
Pertanto possiamo concludere che l'accelerazione ha un componente radiale di modulo:
e una tangenziale di modulo:
Può essere utile a questo punto introdurre la curvatura definita come formula_39, misurata in formula_40. Inserendola nelle formule dell'accelerazione si ha:
Da ciò si deduce che all'aumentare della curvatura, e conseguentemente al diminuire del raggio, prevale la componente normale dell'accelerazione, restringendo la traiettoria. Viceversa, al crescere del raggio, con conseguente riduzione della curvatura, prevale la componente tangenziale che conduce ad un allargamento della traiettoria. 
Per questa ragione il moto rettilineo può essere letto come un moto circolare con accelerazione normale nulla.
Omologamente, derivando la velocità areolare, si ottiene l'accelerazione areolare, misurata in metri quadri su secondi quadri formula_43:
formula_44
Se il moto circolare è uniforme significa che è costante il vettore velocità angolare, cioè si ha velocità lineare costante in modulo.
Integrando la formula_45 tra i due istanti, l'iniziale formula_46, e formula_9 corrispondenti ad un angolo iniziale formula_48 e un altro angolo formula_7:
essendo formula_18 la velocità angolare costante.
Ne consegue (dalle equazioni viste alla sezione precedente) che la velocità tangenziale ha modulo costante pari a:
e dal momento che essa vettorialmente varia solo in direzione, segue che formula_53, dunque l'accelerazione ha solo componente radiale, detta accelerazione centripeta:
Il moto circolare uniformemente accelerato è il moto più generale ad accelerazione costante in modulo e in inclinazione rispetto alla velocità.
In particolare ciò significa che l'accelerazione angolare è costante. Integrando l'accelerazione angolare formula_56 tra due istanti di tempo formula_46 e formula_9 corrispondenti alle due velocità angolari iniziale e finale formula_59 e formula_18:
formula_61
Integrando la relazione formula_62 tra due istanti di tempo iniziale e finale formula_46 e formula_9 e sostituendo a formula_65 il valore trovato sopra, possiamo ricavare lo spostamento angolare formula_66:
formula_67
Risulta costante anche l'accelerazione areolare:
formula_68
Per una rappresentazione vettoriale delle grandezze cinematiche relative al moto circolare, è opportuno introdurre i versori tangente e normale alla traiettoria, che sono definiti nel modo seguente (il versore normale punta verso l'interno):
Tenendo conto delle regole di derivazione, le derivate di questi versori rispetto al tempo sono date da
Possiamo quindi esprimere i vettori posizione, velocità e accelerazione usando i versori formula_73 e formula_74:
Nel moto circolare uniforme l'accelerazione tangenziale è nulla.
Infine si possono scrivere le componenti del vettore velocità in coordinate cartesiane:
Introdotto il "vettore velocità angolare", di modulo formula_87, con direzione perpendicolare al piano del moto e con verso tale da vedere ruotare il corpo in senso antiorario,
il vettore velocità può semplicemente essere scritto come:
</text>
</doc>
<doc id="15273" url="https://it.wikipedia.org/wiki?curid=15273">
<title>Monomio</title>
<text>
In matematica un monomio è un'espressione algebrica costituita da un coefficiente ed una parte letterale dove tra le lettere compaiono moltiplicazioni e elevamenti a potenza aventi esponente naturale. Questi sono tre esempi:
Nell'ultimo esempio, l'esponente "n" è un numero naturale non specificato. In alcuni casi si ammette la presenza nel monomio di esponenti negativi e si parla di monomi "frazionari" (o "fratti"): in questo caso, il monomio è in realtà una frazione algebrica:
Talvolta si ammette anche l'operazione di estrazione di radice. I monomi con esponenti esclusivamente interi positivi sono detti "interi" e in questa voce ci limitiamo a considerare questo tipo di monomi.
In un monomio non compaiono somme o sottrazioni; una espressione del tipo
dove compaiono anche delle somme algebriche è invece detta polinomio: un polinomio è quindi una somma algebrica di monomi.
Ogni monomio è diviso in due parti:
Ad esempio il monomio
ha coefficiente 6 e parte letterale formula_6. I monomi
hanno coefficiente formula_4 e formula_9 rispettivamente.
In alcuni contesti il coefficiente può contenere delle costanti non numeriche, indicate con delle lettere. Ad esempio l'espressione
può indicare un monomio avente coefficiente formula_11 e parte letterale formula_12. Di solito si intende distinguere fra quelle lettere come formula_13 che rappresentano delle costanti e altre lettere come formula_14 che rappresentano le variabili.
Un monomio senza parte letterale è detto "costante".
Il "grado" di un monomio è la somma algebrica degli esponenti della parte letterale. 
Ad esempio, il monomio
ha grado 2. Le variabili senza esponente hanno come di consueto esponente 1 anche se non indicato esplicitamente: quindi
ha grado 1+3 = 4: le variabili formula_14 e formula_18 hanno infatti esponente 1 e 3 rispettivamente.
I monomi costanti sono precisamente quelli con grado zero.
I monomi ridotti in forma normale aventi la stessa parte letterale, con gli stessi esponenti, si dicono "monomi simili". Ad esempio
Tra questi due monomi aventi il coefficiente con valore assoluto uguale e segno opposto si dicono "opposti", mentre due monomi aventi lo stesso coefficiente si diranno appunto uguali.
Lo 0 viene chiamato "monomio nullo".
Una somma algebrica di monomi viene chiamata polinomio.
La "somma algebrica" di due o più monomi simili è un monomio simile ad essi, in cui il coefficiente è la somma algebrica dei coefficienti dei singoli monomi. Quando i monomi non sono simili la somma non può essere applicata e si lascia l'espressione inalterata. Quando si ha un'espressione con più monomi si deve sempre cercare di sommare i termini simili fino ad arrivare ad una forma non più modificabile.
L'addizione algebrica tra monomi simili è una operazione "interna", ossia ha come risultato un monomio simile a quelli dati il cui coefficiente è la somma algebrica dei coefficienti. Operativamente, si raccoglie a fattor comune la parte letterale, applicando all'inverso la proprietà distributiva della moltiplicazione rispetto all'addizione e poi si esegue la somma dei coefficienti numerici. Dei semplici esempi sono dati dalle seguenti somme: 
Quando i monomi non sono simili l'addizione algebrica non porta semplificazioni, l'espressione rimane inalterata ed il risultato non è più un monomio, ma un polinomio:
La somma algebrica viene fatta solo tra monomi simili lasciando inalterati gli altri:
questo procedimento viene anche detto "riduzione dei termini simili".
Il prodotto di due o più monomi è il monomio che ha per coefficiente il prodotto dei coefficienti dei singoli monomi e come parte letterale il prodotto delle loro parti letterali. Ogni fattore letterale ha l'esponente uguale alla somma degli esponenti che esso ha nei singoli monomi.
Considerando ad esempio il prodotto tra formula_26 e formula_27, il prodotto dei coefficienti è:
mentre quello delle parti letterali è:
quindi il prodotto dei singoli monomi risulta essere
Altri esempi di moltiplicazione tra monomi:
La potenza di un monomio è il monomio che ha per coefficiente la potenza del coefficiente e per parte letterale la potenza di ciascun fattore letterale del monomio. Considerando il monomio formula_34 calcolare il suo cubo vuol dire moltiplicare 3 volte per se stesso il monomio:
che per le regole del prodotto viste sopra diventa:
Altre potenze di monomi sono:
In alcuni casi molto particolari, anche il quoziente di due monomi è un monomio:
Questo accade però solo in casi molto particolari, cioè quando il grado del monomio dividendo è maggiore o uguale del monomio divisore e quando le lettere che compaiono nel divisore si trovano, con grado maggiore o uguale, anche nel dividendo. In generale, un monomio che contiene delle lettere non ha un inverso (rispetto alla moltiplicazione). Ad esempio, dato il monomio 
non esiste nessun altro monomio che, moltiplicato per formula_40, dia come risultato 1. Questo perché la moltiplicazione fra monomi può solo incrementare il numero di lettere coinvolte, e non può eliminarle.
Minimo comune multiplo tra due monomi è definito come quel monomio di grado minimo che è divisibile per i due dati.
I minimi comuni multipli tra due monomi sono infiniti, essi infatti possono avere qualsiasi coefficiente.
Per determinare la parte letterale dell'm.c.m. tra due monomi si prendono tutte le lettere, comuni e non comuni, dei monomi con il loro massimo esponente.
Per quanto riguarda il coefficiente, per convenzione, si utilizza l'm.c.m. tra i coefficienti quando è possibile calcolarlo, altrimenti 1.
Esempio:
Massimo comune divisore tra due monomi è definito come quel monomio di grado massimo che divide i due dati.
I massimi comuni divisori tra due monomi sono infiniti, essi infatti possono avere qualsiasi coefficiente.
I monomi descritti sopra sono tutti in "forma normale", cioè espressi come un unico coefficiente numerico che moltiplica delle lettere, ciascuna delle quali compare una volta sola con un certo esponente. Lo stesso monomio può però essere espresso anche in altre forme, posizionando in modo diverso i suoi elementi. Ad esempio, le scritture
rappresentano tutte lo stesso monomio, scritto in modi diversi. Solo la prima di esse rappresenta il monomio in forma normale.
</text>
</doc>
<doc id="38045" url="https://it.wikipedia.org/wiki?curid=38045">
<title>Equazione differenziale</title>
<text>
In analisi matematica un'equazione differenziale è un'equazione che lega una funzione incognita alle sue derivate: se la funzione è di una sola variabile e l'equazione presenta soltanto derivate ordinarie viene detta equazione differenziale ordinaria; se invece la funzione è a più variabili e l'equazione contiene derivate parziali della funzione stessa è detta equazione alle derivate parziali.
Lo studio delle equazioni differenziali ha inizio in seguito all'introduzione del calcolo infinitesimale da parte di Newton e Leibniz nel diciassettesimo secolo. Nel secondo capitolo del suo testo del 1671 "Methodus fluxionum et Serierum Infinitarum", Isaac Newton focalizza il discorso su tre tipologie di equazioni differenziali di primo grado, di cui due ordinarie:
e una con derivate parziali:
Ne risolve inoltre un esempio per ognuna delle tipologie, esprimendo il termine non derivato come serie di potenze e ponendo che abbiano come soluzione delle serie infinite, di cui nota che i coefficienti possono essere scelti in maniera arbitraria producendo così un'infinità di soluzioni particolari.
Un importante contributo alle equazioni ordinarie fu dato dai fratelli Jacob e Johann Bernoulli. Nel 1695 Jacob Bernoulli si occupa dell'equazione oggi nota come equazione differenziale di Bernoulli:
per la quale Leibniz, l'anno successivo, ottiene delle soluzioni semplificandola ad un'equazione lineare. L'anno successivo il fratello Johann si occupa invece del problema della curva brachistocrona.
Un altro importante problema meccanico, quello della corda vibrante, viene inoltre incluso negli studi di Jean le Rond d'Alembert, Leonhard Euler, Daniel Bernoulli, e Joseph-Louis Lagrange. Nel 1746, d'Alembert affronta l'equazione delle onde monodimensionale:
e successivamente Eulero introduce il caso tridimensionale.
A partire dal 1750 fu poi sviluppata l'equazione di Eulero-Lagrange da parte di Eulero e Lagrange, che è alla base della meccanica lagrangiana.
Un altro importante testo è "Théorie analytique de la chaleur" del 1822, in cui Fourier espone l'equazione del calore.
Le equazioni differenziali sono tra le equazioni più studiate in matematica, avendo un ruolo fondamentale nella controparte matematica di moltissimi ambiti della scienza e dell'ingegneria. Possono descrivere, per esempio, una situazione generale in cui una certa quantità formula_5 varia rispetto al tempo in una maniera che dipende dal valore della quantità stessa in quel momento: ciò corrisponde al fatto che nell'equazione compare sia la funzione incognita formula_5 che la sua derivata rispetto al tempo formula_7. Nel caso più semplice compare solo la derivata:
e l'equazione viene risolta utilizzando il teorema fondamentale del calcolo integrale. Le sue soluzioni hanno cioè la forma:
dove formula_10 è costante e formula_11 è la primitiva di formula_12:
Si tratta tuttavia di relazioni di cui è raramente possibile avere una forma analitica della soluzione, o una sua espressione in termini di funzioni elementari, ma vengono piuttosto studiate l'esistenza e l'unicità delle soluzioni e il loro comportamento in contesti di particolare interesse, solitamente in relazione alla situazione di un sistema fisico descritto dall'equazione differenziale. L'insieme di tutte le soluzioni di un'equazione differenziale è detto "integrale generale" dell'equazione differenziale data.
Lo studio delle equazioni differenziali, come avviene spesso in matematica, è stato fortemente influenzato dall'esigenza di analizzare problemi concreti; coinvolge poi diversi ambiti, come l'algebra lineare, l'analisi numerica e l'analisi funzionale. 
Data una funzione formula_14 definita in un intervallo formula_15 dell'insieme dei numeri reali, l'equazione differenziale ad essa associata è un'equazione differenziale ordinaria (abbreviato con ODE, acronimo di "Ordinary Differential Equation") e si chiama "ordine" o "grado" dell'equazione il più alto ordine tra gli ordini delle derivate presenti nell'equazione. La scrittura generale di un'equazione differenziale ordinaria di ordine formula_16 per una funzione formula_17 può avere la forma:
dove formula_19 sono le derivate di formula_20 fino all'ordine formula_16. Se formula_5 è lineare, l'equazione è lineare. Per esempio, l'equazione differenziale di primo grado:
viene soddisfatta dalla funzione esponenziale formula_24, che è uguale alla propria derivata.
Nel caso in cui la funzione incognita formula_20 dipende da più variabili, le derivate sono derivate parziali e si ha un'equazione differenziale alle derivate parziali (abbreviato con PDE, da "Partial Differential Equation"). Una PDE di ordine formula_26 per la funzione formula_27 ha la forma:
dove formula_26 è un numero intero e la funzione formula_5 è data. Esempi particolarmente importanti di equazioni (lineari) alle derivate parziali sono l'equazione delle onde, l'equazione del calore, l'equazione di Poisson, l'equazione del trasporto, l'equazione di continuità, o l'equazione di Helmholtz.
Per esempio, l'equazione:
afferma che formula_32 è indipendente da formula_33, e non avendo nessuna informazione sulla dipendenza da formula_34 ha soluzione generale:
dove formula_5 è una funzione arbitraria di formula_34. L'equazione ordinaria:
ha invece soluzione formula_39 con formula_40 costante.
Le equazioni differenziali vengono analizzate conferendo un preciso valore ad alcune delle variabili in gioco, in particolare la funzione incognita e le sue derivate (fino all'ordine formula_41 per un'equazione in forma normale di ordine formula_42) in certi punti del dominio di definizione dell'equazione. Il problema differenziale che ne risulta è detto "problema di Cauchy"; consiste solitamente nel porre delle condizioni iniziali o delle condizioni al contorno per gli estremi del dominio in cui è definita l'equazione.
Nel caso l'equazione sia definita su una superficie, fornire le condizioni al contorno consiste nel dare il valore della funzione sulla frontiera o della sua derivata rispetto alla direzione normale alla frontiera. Tale assegnazione viene detta condizioni al contorno di Cauchy, e corrisponde ad imporre sia le condizioni al contorno di Dirichlet (i valori che la soluzione assume sul bordo della superficie) che le condizioni al contorno di Neumann (i valori della derivata della soluzione).
Per le equazioni ordinarie il teorema di esistenza e unicità per un problema di Cauchy stabilisce che per un problema:
esiste una sola funzione formula_44 che soddisfa tutte le relazioni se formula_45 è sufficientemente regolare, ad esempio se è differenziabile in un intorno di formula_46.
Per un'equazione alle derivate parziali di ordine formula_42 definita su formula_48 le condizioni iniziali sono date dal valore dell'incognita e delle sue derivate fino all'ordine formula_41 su una varietà liscia formula_50 di dimensione formula_51, detta talvolta "superficie di Cauchy". Il problema di Cauchy consiste, nello specifico, nel trovare la funzione formula_20 soluzione della PDE che soddisfa:
dove formula_55 sono funzioni date definite sulla superficie formula_56 e la derivata formula_57 è calcolata rispetto alla direzione formula_58 del versore normale a formula_56.
Il teorema di Cauchy-Kovalevskaya, che si applica sia per le equazioni alle derivate parziali che per quelle ordinarie, stabilisce che se l'incognita e le condizioni iniziali di un'equazione differenziale sono localmente funzioni analitiche allora una soluzione analitica esiste ed è unica.
Data una generica equazione ordinaria:
è lineare se:
Un'equazione ordinaria lineare si può scrivere come:
Se formula_63 sono fattori costanti l'equazione si risolve trovando una soluzione all'equazione omogenea associata:
alla quale si somma una soluzione particolare dell'equazione completa, ottenibile ad esempio con il metodo delle variazioni delle costanti. Per ogni equazione ordinaria lineare omogenea (anche a coefficienti non costanti) vale inoltre il principio di sovrapposizione: se formula_65 e formula_66 sono soluzioni, allora lo è anche ogni loro combinazione lineare formula_67, con formula_68 e formula_69 costanti.
Un'equazione differenziale alle derivate parziali può essere invece lineare, semilineare, quasilineare o totalmente non lineare. L'equazione si dice lineare se ha la forma:
per opportune funzioni formula_71 ed formula_72, dove formula_73 è la derivazione di ordine formula_74 rispetto ad una o più variabili. Se formula_75 l'equazione si dice "omogenea".
Si dice semilineare se ha la forma:
quasilineare se ha la forma:
e totalmente non lineare se dipende non-linearmente dal più alto grado di derivazione.
Le equazioni che non sono lineari sono spesso molto difficili da affrontare, ed in molti casi si cercano metodi per linearizzarle.
Una classe di equazioni alle derivate parziali di cui si trovano frequentemente soluzioni analitiche, e che sono ampiamente utilizzate in fisica ed ingegneria, sono le equazioni lineari del secondo ordine, ovvero del tipo:
Supponendo che formula_79, formula_80 e formula_81 non siano tutti nulli, i termini con le derivate seconde definiscono una forma quadratica nel punto formula_82:
alla quale si associa la matrice simmetrica:
L'equazione nel punto formula_82 si dice:
Le equazioni a coefficienti costanti sono iperboliche, ellittiche o paraboliche in tutti i punti del loro dominio, ed in tal caso si parla rispettivamente di "equazione iperbolica", "equazione ellittica" o "equazione parabolica". Ad esempio l'equazione di Poisson (e la sua versione omogenea, l'equazione di Laplace) è ellittica, l'equazione del calore è parabolica, e l'equazione delle onde è iperbolica.
Le equazioni a coefficienti non costanti possono tuttavia presentare un carattere "misto", cioè possono essere iperboliche in alcune regioni del dominio ed ellittiche o paraboliche in altre. Ad esempio l'equazione di Eulero-Tricomi:
è ellittica nella regione formula_93, iperbolica nella regione formula_94 e parabolica degenere sulla retta formula_95.
Un esempio elementare di come le equazioni differenziali possano emergere naturalmente nello studio dei sistemi è il seguente: si supponga di avere una popolazione di batteri composta inizialmente da formula_96 individui e sia formula_97 la popolazione al tempo formula_98. È ragionevole aspettarsi che, in media, in ogni istante formula_98 dopo un tempo relativamente piccolo formula_100 nasca una quantità di nuovi individui proporzionale alla popolazione e al tempo trascorso formula_100, cioè pari a formula_102, dove formula_16 è un numero (che si suppone costante) che individua il tasso di natalità. Analogamente è ragionevole aspettarsi che muoiano formula_104 individui nello stesso intervallo di tempo, essendo formula_105 il tasso (costante) di mortalità. La popolazione al tempo formula_106, quindi, sarà data dalla popolazione al tempo formula_98 a cui si aggiunge la popolazione appena nata e si sottrae quella morta, ovvero:
Quindi si ha che:
Si può riconoscere nell'espressione a primo membro il rapporto incrementale della funzione formula_97; se formula_100 è molto piccolo, tale rapporto verrà sostituito con la derivata formula_112 e si scriverà:
Questa è un'equazione differenziale ordinaria del primo ordine. Risolvere questa equazione significa determinare l'andamento nel tempo della popolazione, cioè la funzione formula_97. Si sta cercando quindi una funzione che sia dimensionalmente sommabile alla sua derivata prima, ovvero la funzione esponenziale (la cui derivate sono la funzione stessa per una costante):
dove formula_116 e formula_26 sono costanti. Imponendo di rispettare il vincolo formula_118 si ha:
Si tratta di una funzione esponenziale che cresce nel tempo se formula_120, cioè se la natalità è maggiore della mortalità, e decresce fino ad annullarsi velocemente se formula_121. Il modello che si è esaminato è particolarmente semplificato; in generale il tasso di crescita non è semplicemente proporzionale alla popolazione presente con una costante fissa di proporzionalità: è ragionevole aspettarsi ad esempio che le risorse a disposizione siano limitate ed insufficienti a soddisfare una popolazione arbitrariamente grande. Si possono considerare, inoltre, situazioni più complicate come quella in cui ci siano più popolazioni che interagiscono tra loro, come ad esempio prede e predatori nel modello di Volterra-Lotka.
Solitamente non è possibile trovare soluzioni esatte per le equazioni differenziali. Invece che trovare un'espressione analitica di una funzione che soddisfi l'equazione si è spesso limitati a studiarne l'esistenza e l'andamento qualitativo, oppure se ne determinano soluzioni approssimate servendosi di computer in grado di effettuare approssimazioni tramite metodi di calcolo numerici. Nel corso dei secoli sono tuttavia stati trovati diversi casi in cui è possibile ricavare l'espressione analitica di funzioni che sono soluzione di un'equazione differenziale, così come sono stati sviluppati molti strumenti di vario tipo per la ricerca di tali soluzioni: per affrontare le equazioni ordinarie si può ricorrere ad esempio all'utilizzo di un fattore di integrazione, del metodo delle differenze finite, del metodo delle variazioni delle costanti e diversi altri metodi di soluzione analitica e numerica.
Per quanto riguarda le equazioni alle derivate parziali, non vi è una teoria generale per analizzarle, ma vi sono casi in cui è possibile trovare una soluzione unica che dipende in modo continuo dai dati forniti dal problema. Tali soluzioni sono dette "classiche", e si distinguono da soluzioni deboli o generalizzate. Tra i molti metodi utilizzati per studiare le PDE vi è il metodo delle caratteristiche, l'utilizzo della funzione di Green, diverse trasformate integrali o il metodo di separazione delle variabili.
Accade inoltre spesso che si identificano classi di funzioni caratterizzate dal fatto che soddisfano alcune importanti equazioni differenziali, e per tale motivo godono di proprietà particolari che le rendono di notevole interesse. Ad esempio le onde, che soddisfanno l'equazione delle onde, le funzioni armoniche, che soddisfano l'equazione di Laplace, e le funzioni speciali, tra cui le funzioni ipergeometriche che soddisfano l'equazione ipergeometrica, oppure le funzioni di Struve, le funzioni di Anger e le funzioni di Weber che soddisfano le equazioni di Bessel.
Le soluzioni numeriche sono degli algoritmi che permettono di approssimare la soluzione del sistema di equazioni differenziali che costituiscono il "modello matematico" del sistema. Questi algoritmi sono alla base dei software di simulazione come MATLAB/Simulink ed in linea generale possono risolvere anche problemi che non ammettono soluzioni in forma chiusa.
</text>
</doc>
<doc id="12917" url="https://it.wikipedia.org/wiki?curid=12917">
<title>Teorema fondamentale dell'algebra</title>
<text>
Il teorema fondamentale dell'algebra asserisce che ogni polinomio di grado formula_1 (cioè non costante), su un campo algebricamente chiuso (come il campo dei numeri complessi), del tipo:
ammette esattamente formula_3 radici in quel campo, o zeri. Dal teorema segue che un polinomio complesso ammette esattamente formula_3 radici complesse (contate con la giusta molteplicità), mentre un polinomio reale ammette al più formula_3 radici reali.
Un'enunciazione del teorema in una pubblicazione fu opera del matematico di origine fiamminga Albert Girard nel 1629 nel libro "L'invention en algebre", per quanto anticipata da una formulazione debole da parte di Peter Roth, riportata nei suoi "Arithmetica Philosophica" (1608). Non vi era comunque alcuna dimostrazione. Nel 1702 Leibniz sostenne di aver trovato un controesempio con il polinomio formula_6. Nel 1742 Nicolas Bernoulli e Christian Goldbach in una lettera inviata allo stesso Leibniz dimostrarono l'esistenza di radici complesse del polinomio.
Il primo tentativo serio di dimostrazione del teorema fu operato da d'Alembert nel 1746, il quale però utilizzò un teorema non ancora dimostrato (la dimostrazione fu fatta da Puiseux nel 1751 utilizzando lo stesso teorema fondamentale dell'algebra). Altri tentativi di dimostrazione furono portati avanti nel 1749 da Eulero, Lagrange nel 1772, Laplace nel 1795.
Finalmente nel 1799 Gauss riuscì nell'intento sfruttando i tentativi dei suoi predecessori. Infine, nel 1814 Jean-Robert Argand, un libraio appassionato di matematica, pubblicò un'altra dimostrazione molto più semplice rispetto a quella di Gauss.
Un numero reale è un particolare numero complesso: il teorema è quindi valido per ogni polinomio a coefficienti reali. Ad esempio, si consideri il polinomio
Questo polinomio non ammette nessuna radice reale: i numeri reali non formano un campo algebricamente chiuso. Per il teorema fondamentale dell'algebra, il polinomio ha però almeno una radice complessa: questa è l'unità immaginaria formula_8. Infatti:
Questa non è però l'unica radice. Il polinomio ha grado due e ha due radici complesse formula_10 e formula_11.
Esistono numerose dimostrazioni del teorema fondamentale dell'algebra che coinvolgono settori molto diversi della matematica come la topologia, l'analisi complessa e l'algebra.
Sia formula_12 un polinomio a coefficienti complessi di grado formula_13. 
Abbiamo formula_14, quindi esiste formula_15 tale che 
formula_16 per ogni formula_17 tale che formula_18. 
Il disco chiuso formula_19 è compatto, dunque per il teorema di Weierstrass esiste un punto formula_20 in cui formula_21 assume il suo minimo valore assoluto in formula_22.
Proviamo che formula_23: sviluppando formula_12 in serie di Taylor intorno a formula_25 abbiamo
dove formula_27, formula_28 è intero e formula_29 con formula_30,
notare che la serie di Taylor è finita poiché formula_31 per ogni intero formula_32 essendo formula_12 un polinomio di grado formula_3. Quindi:
dove formula_36 per formula_37. 
Per ogni formula_38 possiamo scegliere formula_17 di modo che formula_40, in tal caso quando formula_41 allora formula_37 quindi per formula_43 sufficientemente piccolo avremo 
pertanto 
ovvero si è trovato un assurdo.
Sia formula_12 un polinomio complesso, tale che formula_47 per ogni formula_48 complesso. Allora la funzione 
è una funzione intera, cioè è una funzione olomorfa su tutto formula_50. D'altra parte
implica
e quindi la funzione formula_53 è limitata. Per il teorema di Liouville formula_53 è costante, da cui segue che anche formula_12 è costante.
Quindi gli unici polinomi senza zeri sono i polinomi costanti.
Consideriamo un polinomio a coefficienti complessi non costante
vogliamo dimostrare che esiste un punto formula_57 tale che formula_58. A tale scopo possiamo considerare il caso in cui formula_59.
Supponiamo per assurdo che formula_60 non ammetta radici, cioè che l'origine non sia nella sua immagine. Consideriamo sul piano complesso la circonferenza di centro l'origine e raggio formula_61 parametrizzata da
Il polinomio formula_60 rappresenta una funzione continua del piano complesso in sé stesso e come tale manderà la circonferenza formula_64 in una curva piana parametrizzata formula_65. La curva così ottenuta non passerà per l'origine dal momento che abbiamo assunto che 0 non è nell'immagine di formula_60, e questo qualunque sia il raggio formula_61. Quindi possiamo considerare l'indice di avvolgimento di formula_65 rispetto all'origine formula_69
Poniamo
Poiché l'indice di avvolgimento non varia per deformazioni della curva tali che questa non tocchi mai l'origine (è un invariante omotopico) la funzione formula_71 sarà continua e poiché l'indice assume solo valori interi dovrà anche essere una funzione costante.
Ora consideriamo il valore di formula_71 per due differenti valori di formula_61:
Il fatto che formula_71 assuma valori differenti per differenti raggi contraddice il fatto che deve essere una funzione costante, e siamo quindi giunti a un assurdo da cui concludiamo che l'ipotesi che formula_60 non avesse nessuna radice è impossibile.
Si dice che il campo complesso formula_50 è un campo algebricamente chiuso per indicare il fatto che 
ogni polinomio di grado maggiore o uguale a 1, a coefficienti complessi, ha almeno una radice in formula_50, come stabilisce il teorema qui esposto. Tale proprietà non è condivisa dai sottocampi formula_108 e formula_109 come si può vedere subito considerando i polinomi 
che non ha radici nel campo formula_108 dei razionali, e
che non ha radici nel campo formula_109 dei reali.
</text>
</doc>
<doc id="4403" url="https://it.wikipedia.org/wiki?curid=4403">
<title>Unione (insiemistica)</title>
<text>
In matematica, e in particolare in teoria degli insiemi, esiste un'operazione detta unione (simbolo formula_1) di insiemi. Dati due insiemi formula_2 e formula_3, la loro unione è un insieme formato da tutti e soli gli elementi
che appartengono:
L'unione è una operazione binaria. Nell'algebra booleana corrisponde all'operatore OR; in logica, corrisponde alla disgiunzione.
L'unione di due insiemi formula_2 e formula_3 si denota comunemente con formula_8. Si ha che formula_9 è un elemento di formula_8 se e solo se formula_9 è un elemento di almeno uno degli insiemi formula_2 e formula_3, in simboli:
L'unione di due o più insiemi è detta disgiunta se gli insiemi, presi a due a due, hanno intersezione vuota. In generale, data una arbitraria famiglia formula_15 di insiemi, l'unione è definita come l'insieme formula_16 a cui un elemento formula_9 appartiene se e solo se appartiene ad almeno uno degli formula_18.
Ad esempio si possono considerare due insiemi finiti, un insieme con un numero finito di elementi: formula_19 e formula_20. In questo caso si ottiene l'unione prendendo gli elementi che appartengono ad almeno uno dei due insiemi:
Un altro esempio è dato da due insiemi definiti mediante una proprietà dei loro elementi: Siano:
formula_26 è l'insieme dei numeri interi divisibili per formula_23 e/o per formula_25.
L'unione è un'operazione commutativa, in simboli:
Infatti
L'unione è un'operazione associativa:
Infatti
Per questo si può rinunciare alle parentesi quando si considera l'unione di più di due insiemi, scrivendo formula_34
Il simbolo ∪, così come ad esempio anche i simboli ∈, ∩, ⊂, venne introdotto per la prima volta da Giuseppe Peano nel Formulario mathematico, opera pubblicata nel 1895. 
</text>
</doc>
<doc id="1630" url="https://it.wikipedia.org/wiki?curid=1630">
<title>Derivata</title>
<text>
In matematica, la derivata è il tasso di cambiamento di una funzione rispetto a una variabile, vale a dire la misura di quanto la crescita di una funzione cambi al variare del suo argomento.
La derivata di una funzione è una grandezza puntuale, cioè si calcola punto per punto. Nel caso di funzioni a una variabile nel campo reale, essa è la pendenza della tangente al grafico della funzione in quel punto e ne rappresenta la migliore approssimazione lineare. Nel caso in cui la derivata esista (cioè la funzione sia derivabile) in ogni punto del dominio, la si può vedere a sua volta come una funzione che associa a ogni punto proprio la derivata in quel punto.
Il concetto di derivata è, insieme a quello di integrale, uno dei cardini dell'analisi matematica e del calcolo infinitesimale.
Il significato pratico di derivata è il "tasso" di variazione di una certa grandezza presa in considerazione. Un esempio molto noto di derivata è la variazione della posizione di un oggetto rispetto al tempo, chiamata velocità istantanea.
La derivata di una funzione formula_1 in un punto formula_2 è il valore del coefficiente angolare della retta tangente alla curva nel punto, cioè la tangente trigonometrica dell'angolo formato dalla tangente in un punto della curva di equazione formula_3 e l'asse delle ascisse. Se la derivata è uguale a zero la retta tangente alla curva di equazione formula_3 risulta parallela all'asse delle ascisse, mentre se la derivata tende a infinito la retta tangente alla curva di equazione formula_3 è parallela all'asse delle ordinate. La funzione derivata si ricava con una serie di operazioni algebriche note come regole di derivazione, applicabili universalmente a tutte le funzioni derivabili.
Nel caso di funzioni di più variabili la tangente in un punto alla curva della funzione non è unica, ma varia a seconda della direzione scelta. Non si può più quindi definire una sola funzione delle stesse variabili indipendenti che renda conto della pendenza del grafico della funzione in un punto: si ricorre allora alle derivate parziali della funzione, cioè ai coefficienti angolari di tangenti considerate lungo direzioni parallele agli assi che rappresentano le variabili indipendenti.
Le derivate parziali sono in numero pari alle variabili stesse, e una loro notevole proprietà è che se la funzione è sufficientemente "regolare" (cioè differenziabile) è possibile calcolarne la tangente lungo una direzione qualunque con una combinazione lineare delle derivate parziali stesse. Questo è possibile perché l'operatore di derivazione è un operatore lineare, e quindi la derivata di una combinazione lineare di funzioni derivabili è la combinazione lineare delle derivate delle singole funzioni, e la derivata del prodotto di uno scalare per una funzione è il prodotto dello scalare per la derivata della funzione.
La nozione di derivata si introduce, nel caso più semplice, considerando una funzione reale formula_6 di variabile reale formula_7 e un punto formula_2 del suo dominio. La derivata di formula_6 in formula_2 è definita come il numero formula_11 uguale al limite del rapporto incrementale al tendere a 0 dell'incremento, sotto l'ipotesi che tale limite esista e sia finito. In modo esplicito, detto formula_12 l'incremento, una funzione formula_1 definita in un intorno di formula_2 si dice derivabile nel punto formula_2 se esiste ed è finito il limite:
e il valore di questo limite è la derivata della funzione nel punto formula_2. Se la funzione formula_6 è derivabile in ogni punto di un dato intervallo formula_19, allora si dice che essa è "derivabile" in formula_19 e la funzione formula_21 che associa a ogni punto formula_7 la derivata formula_23 di formula_6 è la "funzione derivata" di formula_1.
Nonostante il caso più semplice sia quello delle funzioni reali, la definizione di derivata trova la sua collocazione più naturale nell'ambito dell'analisi complessa, dove, applicata alle funzioni di variabile complessa, prende il nome di derivata complessa. Detto formula_26 un sottoinsieme aperto del piano complesso, una funzione complessa formula_27 è "differenziabile in senso complesso" in un punto formula_28 se esiste il limite:
Tale limite va inteso in relazione alla topologia del piano. In altre parole, per ogni successione di numeri complessi che converge a formula_30, il rapporto incrementale deve tendere a un medesimo numero, indicato con formula_31. Se formula_32 è differenziabile in senso complesso in ogni punto formula_33, si dice che è una funzione olomorfa su formula_34.
La relazione tra la differenziabilità di funzioni reali e funzioni complesse è data dal fatto che se una funzione complessa:
è olomorfa allora formula_36 e formula_37 possiedono derivata parziale prima rispetto a formula_38 e formula_39 e soddisfano le equazioni di Cauchy-Riemann:
In modo equivalente, la derivata di Wirtinger formula_41 di formula_1 rispetto al complesso coniugato formula_43 di formula_44 è nulla.
La "derivata destra" di formula_1 in formula_2 è il numero:
Analogamente, la "derivata sinistra" di formula_1 in formula_2 è il numero:
Una funzione è derivabile in formula_2 se e solo se esistono finite e uguali le derivate destra e sinistra. Queste permettono inoltre di definire la derivabilità su un intervallo non aperto: se formula_1 è definita ad esempio nell'intervallo chiuso formula_53, si dice che formula_1 è derivabile in formula_53 se è derivabile in ogni punto interno formula_56 e se esistono le derivate destra e sinistra rispettivamente negli estremi formula_57 e formula_58.
La prima notazione di derivata nel punto "x" che compare storicamente è:
ancora oggi usata in fisica. In alternativa, secondo la notazione di Lagrange viene indicata con:
secondo la notazione di Cauchy - Eulero con:
secondo la notazione di Leibniz con:
e secondo la notazione di Newton con:
Nel caso di una funzione di più variabili, l'incremento della funzione rispetto a una sola variabile è la derivata parziale della funzione rispetto a tale variabile. Data una funzione vettoriale di più variabili formula_64 definita su un insieme aperto dello spazio euclideo formula_65, dette formula_66 e formula_67 le basi canoniche di formula_65 e formula_69 rispettivamente, la funzione può essere scritta nel seguente modo:
La componente i-esima della funzione è allora:
Si definisce derivata parziale di formula_72 rispetto alla variabile formula_73 il limite:
Tale limite è a volte chiamato limite del rapporto incrementale di formula_1 nel punto formula_76, e viene denotato anche con formula_77. La derivata parziale di una funzione, o nel caso di funzione vettoriale di una sua componente, si effettua quindi considerando le variabili diverse da quella rispetto a cui si vuole derivare come costanti e calcolandone il rapporto incrementale.
La derivata direzionale di una funzione scalare formula_78 lungo un vettore unitario formula_79 è la funzione definita dal limite:
Se la funzione formula_1 è differenziabile in formula_82, allora la derivata direzionale esiste lungo ogni vettore unitario formula_83 e si ha:
dove formula_85 al secondo membro rappresenta il gradiente di formula_1 e formula_87 il prodotto scalare euclideo. In formula_82 la derivata direzionale di formula_1 rappresenta la variazione di formula_1 lungo formula_91.
Una funzione differenziabile in un punto è una funzione che può essere approssimata da una trasformazione lineare nel punto. Affinché ciò si verifichi è necessario che tutte le derivate parziali calcolate nel punto esistano, ovvero esistono finiti i limiti dei rapporti incrementali direzionali (dunque, se una funzione è differenziabile in un punto allora è derivabile nel punto). La proprietà di differenziabilità di una funzione consente di generalizzare il concetto di funzione derivabile a funzioni vettoriali di variabile vettoriale, e permette di individuare per ogni punto del suo grafico un iperpiano tangente.
Una funzione formula_92 definita su un insieme aperto dello spazio euclideo formula_65 è detta differenziabile in un punto formula_94 del dominio se esiste una applicazione lineare formula_95 tale che valga l'approssimazione:
dove formula_97 si annulla all'annullarsi dell'incremento formula_98. Tale condizione si può scrivere in modo equivalente:
Se la funzione formula_100 è differenziabile in formula_94, l'applicazione formula_102 è rappresentata dalla matrice jacobiana formula_103.
Il vettore:
si chiama "differenziale" di formula_100 in formula_94 e formula_107 è la derivata totale della funzione formula_100.
La funzione formula_100 è infine differenziabile se lo è in ogni punto del dominio. In particolare, il teorema del differenziale totale afferma che una funzione è differenziabile in un punto se tutte le derivate parziali esistono in un intorno del punto per ogni componente della funzione e se sono inoltre funzioni continue. Se inoltre l'applicazione che associa formula_82 a formula_111 è continua, la funzione si dice "differenziabile con continuità".
Il teorema di continuità asserisce che se formula_6 è derivabile in formula_2 allora formula_6 è anche continua in formula_2.
L'inverso non è sempre vero: ad esempio, la funzione formula_116 è continua su tutto il dominio, ma non è derivabile nel punto formula_117, perché la derivata destra non coincide con la derivata sinistra. La continuità di una funzione è quindi condizione necessaria, ma non sufficiente, per determinarne la derivabilità. Una funzione può inoltre essere derivabile (e quindi continua) in un punto formula_118, ma essere discontinua in ogni punto intorno a formula_118. Questo accade per funzioni come:
essendo formula_121 l'insieme dei numeri razionali e formula_122 l'insieme dei numeri reali, mentre il simbolo "\" denota la differenza tra insiemi. La funzione in esame ammette derivata in 0 (vale 0 il limite del rapporto incrementale) ma non è continua in nessun punto eccetto lo 0. Notiamo che se invece una funzione è due volte derivabile in un punto, allora è continua in un intorno di quel punto.
Per mostrare che se formula_6 è derivabile in formula_2 allora è continua in formula_2, si considera l'uguaglianza precedente:
da cui:
Quindi la funzione è continua in formula_2. La stima lineare della funzione attorno a formula_2 costituisce una migliore approssimazione rispetto a:
garantita dalla sola continuità (qui formula_131). Se la funzione è derivabile in formula_2 si può "scomporre" l'infinitesimo formula_133 in un termine lineare e un infinitesimo di ordine superiore. Il teorema di Lagrange fornisce una diversa approssimazione (sempre lineare) nell'ipotesi che la funzione sia derivabile in un intorno di formula_2:
per tutti gli formula_7 in tale intorno, e con formula_137 un dato punto in formula_138 (o formula_139, se è un intorno sinistro). Benché ora l'approssimazione sia "esatta" (non ci sono termini infinitesimi che vengono trascurati), il teorema non è in grado di mostrare per quale formula_137 sia vera l'uguaglianza.
Una funzione continua può essere non derivabile. Ad esempio, una funzione continua può non essere derivabile in un punto isolato del dominio, in presenza di un punto angoloso, una cuspide o un flesso a tangente verticale. Esistono anche funzioni continue che presentano forme più complesse di non derivabilità, come ad esempio la funzione di Cantor. La funzione di Weierstrass è una funzione reale di variabile reale che ha la proprietà di essere continua in ogni punto ma di non essere derivabile in nessuno.
Vengono enunciati di seguito alcuni teoremi e risultati significativi.
Siano formula_6 e formula_142 funzioni reali di variabile reale formula_7 derivabili, e sia formula_144 l'operazione di derivazione rispetto a formula_7:
Sia formula_6 una funzione derivabile, e quindi continua, in un punto formula_2 interno al dominio. Se formula_2 è un punto di massimo o di minimo per la funzione formula_1 allora la derivata della funzione in formula_2 è nulla, cioè formula_159.
Non è indispensabile che formula_2 sia interno al dominio, essendo sufficiente che si tratti di un punto di accumulazione da destra e da sinistra per il dominio, mentre è essenziale porre che la funzione sia derivabile nel punto formula_2 in quanto non è possibile dedurne la derivabilità dalle altre ipotesi del teorema. Ogni punto in cui formula_23 si annulla (cioè è uguale a zero) è chiamato punto stazionario. I massimi e minimi relativi sono chiamati "punti stazionari" di formula_23.
Questo teorema è molto usato nello studio di funzione, in quanto definisce la possibilità di avere un punto di massimo o di minimo dove la funzione derivata si annulla.
Sia formula_6 una funzione continua nell'intervallo chiuso formula_53 e derivabile nell'intervallo aperto formula_166. Se formula_167 allora esiste almeno un punto formula_168 dove la derivata prima formula_23 si annulla.
Sia formula_6 una funzione continua in formula_53 e derivabile nell'intervallo aperto formula_166. Allora esiste almeno un punto formula_168 tale per cui:
Il teorema afferma che esiste almeno un punto formula_175 del grafico della funzione in cui la retta tangente ha coefficiente angolare uguale a quello della corda della retta passante per i punti formula_176 e formula_177. Si tratta di una generalizzazione del teorema di Rolle che analizza il caso in cui formula_178 è diverso da formula_179.
Siano formula_6 e formula_142 funzioni continue in formula_53 e derivabili in formula_166 con formula_184 diversa da 0 per ogni punto dell'intervallo. Allora esiste almeno un punto formula_168 tale per cui:
Considerando in particolare la funzione formula_187, si ottiene l'affermazione del teorema di Lagrange.
Con il teorema di Cauchy è inoltre possibile dimostrare la regola di de l'Hôpital.
Sia formula_6 continua in formula_53 e derivabile in formula_166. Allora:
La funzione può non essere "strettamente" crescente (o decrescente), e il teorema è direttamente ricavabile dall'enunciato di Lagrange.
Analogamente, valgono anche i fatti seguenti:
Una funzione strettamente crescente non ha necessariamente derivata ovunque positiva. Ad esempio, formula_203 è strettamente crescente, ma ha derivata nulla nell'origine, dove c'è un punto di flesso.
Il teorema della funzione costante afferma che una funzione è costante in un intervallo formula_53 se e solo se è derivabile e la derivata è ovunque nulla nell'intervallo. Mentre la condizione necessaria è conseguenza della definizione di derivata (la derivata di una costante è uguale a zero), la sufficienza segue dal teorema di Lagrange.
La derivata "n"-esima formula_205 di una funzione formula_1 è la funzione che si ottiene derivando successivamente formula_207 volte la funzione formula_1. Si definiscono così la derivata seconda, terza, e così via; e si usa generalmente una delle seguenti notazioni:
Una funzione derivabile non è necessariamente derivabile formula_207 volte. Ad esempio, la funzione formula_211 ha una derivata prima, ma non una seconda: infatti, la derivata di formula_1 è formula_213, che non è a sua volta derivabile nell'origine.
La classe delle funzioni derivabili formula_207 volte e la cui derivata formula_207-esima è continua si indica con formula_216.
Sia formula_217 derivabile. Allora formula_6 è convessa se e solo se formula_23 è crescente in formula_53. Se formula_1 possiede derivata seconda, allora la convessità della funzione è data dalla disequazione:
Il cambiamento di segno della derivata seconda determina quindi un cambiamento di convessità della funzione e un relativo punto di flesso.
Il valore della derivata di formula_6 calcolata in formula_2 ha un significato geometrico: è il coefficiente angolare della retta tangente alla curva rappresentata dal grafico di formula_6 nel punto di coordinate formula_175. In altre parole, la derivata è il valore della tangente trigonometrica dell'angolo (convesso) che la retta tangente in formula_2 al grafico della funzione forma con l'asse delle ascisse (a patto che tale angolo non sia retto).
L'equazione della retta tangente in formula_2 risulta:
Più precisamente, se formula_6 è derivabile nel punto formula_2, allora esiste una funzione formula_232 definita in un intorno di formula_2 tale che:
con:
e tale formula è l'espansione di Taylor di formula_6 troncata al termine di primo grado. Si dice che formula_232 è un infinitesimo di ordine superiore alla funzione formula_238, e con questo si vuole esprimere l'idea che il termine formula_232 fornisce un contributo che diventa trascurabile rispetto agli altri termini quando ci si avvicina a formula_2. Si può anche dire che una funzione derivabile in formula_2 è approssimabile linearmente intorno a formula_2 con la sua retta tangente in tale punto.
Se si definisce infatti formula_232, avente lo stesso dominio di formula_1, come:
si verifica che:
Ricordando che per formula_247 allora formula_248, e quindi formula_249. Sostituendo questa ultima uguaglianza con la precedente equazione si ha:
Una funzione espressa come serie di potenze formula_251 con raggio di convergenza formula_252 è continua e derivabile su tutto l'intervallo formula_253. La derivata può essere calcolata derivando termine a termine la serie nel modo seguente:
Tuttavia, in una serie di potenze si preferisce che formula_207 sia l'indice della potenza, quindi utilizzando uno shift diventa:
Questo tipo di derivata è importante per lo sviluppo di Taylor e Mc-Laurin.
</text>
</doc>
<doc id="1744" url="https://it.wikipedia.org/wiki?curid=1744">
<title>Ellisse</title>
<text>
In geometria, l'ellisse (dal greco "ἔλλειψις", col significato di "mancanza") è una curva piana ottenuta intersecando un cono con un piano in modo da produrre una curva chiusa.
Affinché la sezione conica produca una curva chiusa l'inclinazione del piano deve essere superiore a quella della generatrice del cono rispetto al suo asse. Per contro, le due sezioni coniche ottenute con piani aventi inclinazione uguale o inferiore a quella della retta generatrice rispetto all'asse del cono danno vita ad altri due tipi di curve che sono aperte e illimitate: la parabola e l'iperbole.
La circonferenza è un caso speciale di ellisse che si ottiene quando l'intersezione viene fatta con un piano ortogonale all'asse del cono.
Un'ellisse è anche il luogo geometrico dei punti del piano per i quali "la somma delle distanze da due punti fissi detti fuochi rimane costante".
L'ellisse può essere anche la proiezione verticale su un piano orizzontale di una circonferenza appartenente a un piano inclinato: se il piano inclinato forma un angolo formula_1 con il piano orizzontale, la proiezione verticale della circonferenza è un'ellisse di eccentricità formula_2.
Dopo la circonferenza, si tratta della più semplice tra le figure di Lissajous ottenuta dalla composizione dei due moti verticale e orizzontale di tipo sinusoidale della stessa frequenza.
In base alle leggi di Keplero, l'orbita di un pianeta è un'ellisse con il Sole che ne occupa uno dei due fuochi.
L'ellisse è una curva simile a un cerchio allungato in una direzione: è un esempio di sezione conica e può essere definita come il luogo dei punti del piano per cui la somma delle distanze da due punti fissi, detti fuochi, rimane costante. Se i due fuochi coincidono si ha una circonferenza, che quindi può essere considerata il caso particolare di ellisse a eccentricità nulla.
È una curva con due assi di simmetria e un centro di simmetria. La distanza tra i punti antipodali dell'ellisse, cioè tra punti simmetrici rispetto al suo centro, è massima lungo l'asse maggiore, che contiene anche i due fuochi, ed è minima lungo l'asse minore perpendicolare a quello maggiore. Il semiasse maggiore è una delle due metà dell'asse maggiore: parte dal centro, passa attraverso un fuoco e arriva all'ellisse. Analogamente il semiasse minore è metà dell'asse minore. I due assi sono per l'ellisse l'equivalente del diametro per la circonferenza, mentre i due semiassi sono l'equivalente del raggio.
La dimensione e la forma di un'ellisse sono determinate da due costanti reali positive, dette convenzionalmente formula_3 e formula_4. La costante maggiore è la lunghezza del semiasse maggiore mentre la costante minore quella del semiasse minore.
L'equazione dell'ellisse si trova uguagliando la somma delle distanze fra i due fuochi formula_5 e formula_6 e un punto generico formula_7 con il doppio del semiasse maggiore:
che equivale a:
Per trovare l"'equazione canonica o normale" dell'ellisse, con centro nell'origine e fuochi sull'asse delle formula_10 (cioè formula_11), si operino le sostituzioni formula_12, formula_13, formula_14, formula_15, formula_16. Dopo alcuni passaggi si ricava che l'ellisse centrata nell'origine di un sistema di assi cartesiani con l'asse maggiore posto lungo l'asse delle ascisse è definita dall'equazione:
La stessa ellisse è rappresentata anche dall'equazione parametrica:
che fa uso delle funzioni trigonometriche seno e coseno.
L'eccentricità formula_19 di un'ellisse è compresa tra formula_20 e formula_21 ed è il rapporto della distanza tra i due fuochi formula_22 ed formula_23 e la lunghezza dell'asse maggiore formula_24:
L'eccentricità rende conto della forma più o meno schiacciata dell'ellisse: quando è uguale a formula_20, i due fuochi coincidono e l'ellisse degenera in una circonferenza di raggio formula_3. Facendo tendere l'eccentricità a formula_21, l'ellisse si schiaccia sempre più e quando assume il valore unitario essa degenera in un segmento lungo formula_24 percorso due volte, quindi il perimetro dell'ellisse è uguale a formula_30.
Il semilato retto di un'ellisse, solitamente denotato dalla lettera formula_31, è la distanza tra ciascuno dei fuochi dell'ellisse e i punti sull'ellisse di cui i fuochi sono proiezione ortogonale sull'asse maggiore. È legato ad formula_3 e formula_4 dalla formula
Come per le altre coniche, anche per l'ellisse vale la proprietà seguente:
i punti medi di un fascio di corde parallele sono allineati.
Il segmento che congiunge i punti medi di un fascio di corde parallele prende il nome di diametro dell'ellisse. I punti medi delle corde parallele ad un diametro dell'ellisse costituiscono il diametro coniugato al diametro dato. Due diametri coniugati si intersecano nel centro dell'ellisse. Gli assi di simmetria dell'ellisse sono gli unici diametri coniugati perpendicolari tra loro. La retta tangente ad un'ellisse nell'estremo di un diametro è sempre parallela al diametro coniugato.
In coordinate polari, un'ellisse con un fuoco nell'origine e con la coordinata angolare formula_35 misurata a partire dall'asse maggiore è rappresentata dall'equazione:
dove formula_31 denota il semilato retto e la coordinata angolare formula_35 è l'angolo che la retta r passante per formula_39 forma con l'asse maggiore (vedere figura a lato).
Se si considera la retta formula_40 passante per il fuoco formula_41 e la coordinata angolare formula_35 è l'angolo che la retta formula_40 passante per formula_41 forma con l'asse maggiore, l'equazione diviene:
L'area racchiusa da un'ellisse è data da
[[File:Ellisse5.png|thumb|upright=1.3|Coefficiente angolare: formula_47Equazione: formula_48 e il punto formula_49, con formula_50 e formula_51. Sapendo che nella traslazione si conserva anche il parallelismo, i coefficienti angolari delle tangenti a formula_52 passanti per formula_53 sono uguali a quelli delle tangenti a formula_54 passanti per il punto formula_55. Si scrive il sistema di due equazioni con la prima relativa all'equazione dell'ellisse e la seconda relativa al fascio di rette passanti per il punto formula_55
Si impone la condizione di tangenza, ossia che il discriminante formula_61 sia nullo:
[[File:Tangent lines to an ellipse from a point outside.JPG|thumb|upright=1.8| Rette tangenti ad un'ellisse condotte da un punto esterno formula_53]]
È data un'ellisse di fuochi formula_39, formula_41 e asse maggiore formula_24, e un punto formula_53 esterno all'ellisse. Esistono due metodi per tracciare le rette tangenti all'ellisse condotte dal punto esterno formula_53.
Tracciare la circonferenza di centro formula_39 e raggio formula_24. Tracciare la circonferenza di centro formula_53 e raggio formula_77. Le due circonferenze si intersecano nei punti formula_78 e formula_79. Tracciare i segmenti formula_80 e formula_81. Fissare i punti formula_82 ed formula_83 di intersezione tra i due segmenti e l'ellisse. Le rette formula_84 e formula_85 sono le rette tangenti cercate.
Tracciare la circonferenza di centro formula_39 e raggio formula_24. Tracciare la circonferenza di centro formula_53 e raggio formula_77. Le due circonferenze si intersecano nei punti formula_78 e formula_79. Tracciare i segmenti formula_101 e formula_102. Condurre per formula_53 la retta formula_104 perpendicolare al segmento formula_101. Condurre per formula_53 la retta formula_107 perpendicolare al segmento formula_102. Le rette formula_104 ed formula_107 sono le rette tangenti cercate.
L'equazione generale dell'ellisse avente i fuochi formula_113 ed formula_114 posti in posizione generica sul piano cartesiano e avente il semiasse maggiore denotato con formula_3 è data da
dove i parametri formula_78, formula_79, formula_119, formula_120, formula_121 ed formula_122 sono uguali a
La [[Curva (matematica)#Lunghezza della curva|lunghezza]] dell'ellisse è:
in cui la funzione formula_121 è l'[[integrale ellittico completo di seconda specie]] ed formula_19 è l'eccentricità.
Sono state proposte numerose formule approssimate per il calcolo della lunghezza dell'ellisse, che differiscono molto per complessità e accuratezza.
Lo sviluppo in [[serie]] è:
Una semplice ma poco raffinata approssimazione per la lunghezza è
che fornisce il risultato esatto quando l'ellisse è una circonferenza, cioè per formula_134, mentre dà un risultato approssimato per eccesso negli altri casi. Nel caso limite in cui formula_135 la formula dà formula_136, mentre il valore esatto è formula_137. La formula è più precisa per ellissi con bassa eccentricità. Utilizzare questa formula equivale ad assumere che l'ellisse abbia la stessa lunghezza di una circonferenza che ha raggio uguale alla media quadratica dei semiassi dell'ellisse.
Un'approssimazione migliore si ottiene con uno sviluppo in serie nel modo seguente: posto formula_138 si ha
Anche in questo caso l'approssimazione è migliore per le ellissi di bassa eccentricità.
Due formule approssimate sono dovute a [[Ramanujan]]:
Entrambe le formule danno il risultato esatto per una circonferenza e, nel caso generale, l'errore delle due formule è dell'ordine di formula_142e di formula_143, rispettivamente. Nel caso di ellisse degenere in un segmento (formula_135, formula_145) la prima dà formula_146, mentre la seconda dà formula_147, quando il risultato esatto è formula_137.
Fissare i due fuochi formula_39 e formula_41 e l'asse maggiore di lunghezza formula_24 (con formula_152). Costruire una circonferenza di centro formula_39 e raggio formula_24. Fissare sulla circonferenza un punto generico formula_155. Tracciare il raggio formula_156. Tracciare il segmento formula_157 e l'asse di tale segmento (retta perpendicolare al segmento passante per il suo punto medio formula_158) che interseca formula_156 nel punto formula_53. Il punto formula_53 è equidistante da formula_41 e da formula_155 in quanto sta sull'asse del segmento formula_157. Dunque formula_165. D'altra parte formula_166 e quindi formula_167. Quindi formula_53 è un punto dell'ellisse.
Questo metodo viene detto della tangente in quanto la retta formula_169 è la tangente all'ellisse nel punto formula_53, infatti gode della proprietà tangenziale, precedentemente descritta.
In questo caso sono note le lunghezze dei lati del rettangolo circoscritto all'ellisse. La linea rossa nella sia la corda utilizzata dal "giardiniere" per tracciare l'ellisse. Nel film [[Agora (film)|Agorà]] del 2009 Ipazia, interpretata da [[Rachel Weisz]], studiando l'orbita della Terra attorno al Sole traccia sulla sabbia un'ellisse con il metodo del giardiniere. In alcuni momenti si vede anche un [[Sezione conica|cono di Apollonio]].
[[Categoria:Sezioni coniche]]
[[Categoria:Curve piane]]
</text>
</doc>
<doc id="2708" url="https://it.wikipedia.org/wiki?curid=2708">
<title>Matematica</title>
<text>
La matematica (dal greco μάθημα ("máthema"), traducibile con i termini "scienza", "conoscenza" o "apprendimento"; μαθηματικός ("mathematikós") significa "incline ad apprendere") è la disciplina che studia le quantità (i numeri), lo spazio, le strutture e i calcoli.
Per l'origine del termine occorre andare al vocabolo egizio "maat", nella cui composizione appare il simbolo del cubito, strumento di misura lineare, un primo accostamento al concetto matematico. Simbolo geometrico di questo ordine è un rettangolo, da cui sorge la testa piumata della dea egizia Maat, personificazione dei concetti di ordine, verità e giustizia. Figlia di Ra, unico Uno, creatore di ogni cosa, la sua potenza demiurgica è limitata e ordinata da leggi naturali e matematiche.
All'inizio del papiro di Rhind si trova questa affermazione: ""Il calcolo accurato è la porta d'accesso alla conoscenza di tutte le cose e agli oscuri misteri"". Il termine "maat" riappare in copto, in babilonese e in greco. In greco la radice "ma", "math", "met" entra nella composizione di vocaboli contenenti le idee di ragione, disciplina, scienza, istruzione, giusta misura, e in latino il termine "materia" indica ciò che può essere misurato.
Col termine matematica di solito si designa la disciplina (ed il relativo corpo di conoscenze) che studia problemi concernenti quantità, estensioni e figure spaziali, movimenti di corpi, e tutte le strutture che permettono di trattare questi aspetti in modo generale. La matematica fa largo uso degli strumenti della logica e sviluppa le proprie conoscenze nel quadro di sistemi ipotetico-deduttivi che, a partire da definizioni rigorose e da assiomi riguardanti proprietà degli oggetti definiti (risultati da un procedimento di astrazione, come triangoli, funzioni, vettori ecc.), raggiunge nuove certezze, per mezzo delle dimostrazioni, attorno a proprietà meno intuitive degli oggetti stessi (espresse dai teoremi).
La potenza e la generalità dei risultati della matematica le ha reso l'appellativo di "regina delle scienze": ogni disciplina scientifica o tecnica, dalla fisica all'ingegneria, dall'economia all'informatica, fa largo uso degli strumenti di analisi, di calcolo e di modellizzazione offerti dalla matematica.
La matematica ha una lunga tradizione presso tutti i popoli della storia antica e moderna; è stata la prima disciplina a dotarsi di metodi di elevato rigore e portata. Ha progressivamente ampliato gli argomenti della sua indagine e progressivamente ha esteso i settori ai quali può fornire aiuti computazionali e di modellizzazione. È significativo che, in talune lingue e in talune situazioni, al termine singolare si preferisca il plurale "matematiche".
Nel corso della sua lunga storia e nei diversi ambienti culturali si sono avuti periodi di grandi progressi e periodi di stagnazione degli studi. Questo in parte è dovuto a singoli personaggi, capaci di dare apporti profondamente innovativi e illuminanti e di stimolare all'indagine matematica grazie alle loro doti didattiche. Si sono avuti anche periodi di arretramento delle conoscenze e dei metodi, specie in relazione a eventi distruttivi o a periodi di decadenza complessiva della vita intellettuale e civile. Negli ultimi 500 anni, per il miglioramento dei mezzi di comunicazione, è prevalsa la crescita del patrimonio di risultati e di metodi,
dovuta alla natura stessa delle attività matematiche, tese alla esposizione precisa di problemi e soluzioni; ciò impone di comunicare col fine ultimo di chiarire ogni dettaglio delle costruzioni logiche e dei risultati (alcuni chiarimenti richiedono un impegno non trascurabile, talora molti decenni). Questo ha corrisposto alla definizione di un linguaggio, strumento esemplare per la trasmissione e la sistemazione delle conoscenze.
Del linguaggio matematico moderno, fatto di simboli riconosciuti in tutto il mondo, la maggior parte è stata introdotta dopo il XVI secolo. Prima di allora la matematica era scritta usando parole, un processo faticoso che rallentava le scoperte matematiche. Eulero (1707-1783) è stato il responsabile di molte delle notazioni oggi in uso. La notazione matematica moderna rende molto più facile il lavoro del matematico, ma i principianti lo trovano scoraggiante. È estremamente compressa: pochi simboli contengono una grande quantità di informazioni; come le note musicali, la notazione matematica moderna ha una sintassi rigorosa (che in misura limitata varia da autore ad autore, e da disciplina a disciplina) e codifica informazioni difficili da scrivere in qualsiasi altro modo.
Il linguaggio matematico può essere difficile per i principianti. Parole come "o" e "solo" hanno precisi significati, più che nella lingua corrente. Inoltre, parole come aperto e campo hanno specifici significati matematici. Il "gergo matematico" comprende moltissimi termini tecnici, come omeomorfismo e integrabile, perché la matematica richiede assai più precisione del linguaggio quotidiano.
Nelle dimostrazioni matematiche è fondamentale il rigore. Per rigore si intende un utilizzo preciso e logico di teoremi già dimostrati, in modo che, analizzando la dimostrazione in profondità attraverso un processo a ritroso, si arrivi ad assiomi e definizioni "universalmente accettati". Il livello di rigore richiesto in matematica è variato col tempo: i Greci richiedevano argomentazioni dettagliate, ma nel periodo di Isaac Newton il rigore utilizzato nelle dimostrazioni si era alleggerito. I problemi nati dalle definizioni usate da Newton hanno portato alla rinascita di una attenta analisi delle dimostrazioni nel corso del Diciannovesimo secolo. Il significato di rigore matematico non è sempre chiaro. Ad esempio i matematici continuano a discutere sull'opportunità di considerare valide le dimostrazioni effettuate attraverso computer: dato che lunghi calcoli sono difficili da verificare, tali dimostrazioni potrebbero essere considerate non sufficientemente rigorose.
Gli assiomi, nel pensiero tradizionale, erano considerati le "verità auto-evidenti", ma questa concezione comporta alcuni problemi. A livello formale, un assioma è solo una successione di simboli, che ha un significato intrinseco solo nel contesto di tutte le formule derivabili di un sistema assiomatico. L'obiettivo del programma di Hilbert è stato proprio quello di fornire l'intera matematica di una solida base assiomatica, ma secondo il teorema di incompletezza di Gödel una completa assiomatizzazione della matematica è impossibile. Nonostante ciò, la matematica è spesso immaginata consistere (per lo meno nel suo contenuto formale) nella teoria degli insiemi in una qualche assiomatizzazione, nel senso che ogni enunciato matematico, o dimostrazione, può essere scritto con formule esprimibili all'interno di tale teoria.
Le attività matematiche sono naturalmente interessate alle possibili generalizzazioni e astrazioni, in relazione alle economie di pensiero e ai miglioramenti degli strumenti (in particolare degli strumenti di calcolo) che esse sono portate a realizzare. Le generalizzazioni e le astrazioni quindi spesso conducono a visioni più approfondite dei problemi e stabiliscono rilevanti sinergie tra progetti di indagine inizialmente rivolti ad obiettivi non collegati.
Nel corso dello sviluppo della matematica si possono rilevare periodi ed ambienti nei quali prevalgono alternativamente atteggiamenti generali e valori riconducibili a "due" differenti generi di motivazioni e di approcci: le "motivazioni applicative", con la loro spinta a individuare procedimenti efficaci, e le esigenze di "sistemazione concettuale" con la loro sollecitazione verso generalizzazioni, astrazioni e panoramiche strutturali.
Si tratta di due generi di atteggiamenti tra i quali si costituisce una certa polarizzazione; questa talora può diventare contrapposizione, anche astiosa, ma in molte circostanze i due atteggiamenti stabiliscono rapporti di reciproco arricchimento e sviluppano sinergie. Nel lungo sviluppo della matematica si sono avuti periodi di prevalenza di uno o dell'altro dei due atteggiamenti e dei rispettivi sistemi di valori.
Del resto la stessa nascita della matematica può ragionevolmente ricondursi a due ordini di interessi: da un lato le esigenze applicative che fanno ricercare valutazioni praticabili; dall'altro la ricerca di verità tutt'altro che evidenti, forse tenute nascoste, che risponde ad esigenze immateriali, la cui natura può essere filosofica, religiosa o estetica.
Negli ultimi 30 o 40 anni tra i due atteggiamenti si riscontra un certo equilibrio non privo di tensioni riemergenti, ma con molteplici episodi di mutuo supporto. A questo stato di cose contribuisce non poco la crescita del mondo del computer, rispetto al quale il mondo della matematica presenta sia canali di collegamento (che è ormai assurdo cercare di interrompere) che differenze, ad esempio differenze dovute a diverse velocità di mutazione e a diversi stili comunicativi, che proiettano le due discipline agli antipodi.
Cerchiamo ora di segnalare a grandi linee i temi dell'indagine matematica, illustrando una sorta di itinerario per un progressivo accostamento dei problemi, delle argomentazioni e delle sistemazioni teoriche.
I primi problemi che inducono ad accostarsi alla matematica sono quelli che si possono affrontare con l'aritmetica elementare: i calcoli eseguibili con le quattro operazioni possono riguardare contabilità finanziarie, valutazioni di grandezze geometriche o meccaniche, calcoli relativi agli oggetti ed alle tecniche che si incontrano nella vita quotidiana.
I più semplici di questi calcoli possono effettuarsi servendosi solo di numeri interi naturali, ma presto i problemi di calcolo richiedono di saper trattare i numeri interi relativi e i numeri razionali.
I problemi aritmetici più semplici sono risolti mediante formule che forniscono risultati conseguenti. Ad esempio: l'area di un rettangolo con lati lunghi formula_1 e formula_2 è il loro prodotto formula_3. Complicando gli enunciati diventa necessario servirsi di equazioni. 
Ad esempio: per il teorema di Pitagora, se un triangolo rettangolo ha i lati più corti (cateti) di lunghezza formula_1 e formula_5, quello più lungo (ipotenusa) ha come lunghezza il numero positivo formula_6 che risolve l'equazione:
formula_7.
Le equazioni più semplici sono le equazioni lineari, sia perché rappresentano le questioni geometriche più semplici, sia perché sono risolvibili con procedimenti standard.
Nelle formule e nelle equazioni conviene far entrare parametri con valori indeterminati: in tal modo si viene a disporre di strumenti di portata più generale, che permettono di conseguire evidenti economie di pensiero. 
Ad esempio: in un triangolo rettangolo con cateti di
lunghezza formula_8 e formula_9, la lunghezza dell'ipotenusa è il numero positivo formula_6 tale che formula_11. 
Per meglio valutare le formule e per risolvere molti tipi di equazioni è necessario sviluppare un calcolo letterale che permetta di rimaneggiarle. Le regole di questo calcolo letterale costituiscono la cosiddetta algebra elementare.
L'algebra moderna si occupa anche dello studio delle relazioni fra insiemi e delle strutture algebriche, cioè strutture che caratterizzano insiemi concreti (come i numeri) o astratti sui quali è stata definita una o più operazioni.
Lo studio della geometria piana e spaziale riguarda inizialmente primitivi: il punto, la retta, il piano. Combinando questi elementi nel piano o nello spazio si ottengono altri oggetti quali segmenti, angoli, angoli solidi, poligoni e poliedri.
Punto, retta, piano e spazio hanno dimensione rispettivamente 0, 1, 2 e 3. Tramite il calcolo vettoriale si definiscono e studiano spazi a dimensione più alta (anche infinita). Gli analoghi "curvi" di questi spazi "piatti" sono le curve e le superfici, di dimensione rispettivamente 1 e 2.
Uno spazio curvo in dimensione arbitraria si chiama varietà. 
Dentro a questo spazio si possono spesso definire punti e rette (dette geodetiche), ma la geometria che ne consegue può non soddisfare gli assiomi di Euclide: una tale geometria è generalmente detta non euclidea. Un esempio è dato dalla superficie
terrestre, che contiene triangoli aventi tutti e tre gli angoli retti.
L'analisi riguarda principalmente il calcolo infinitesimale, introduce la fondamentale nozione di limite, e quindi di derivata e integrale. Con questi strumenti sono analizzati i comportamenti delle funzioni, che spesso non hanno una descrizione esplicita ma sono soluzioni di una equazione differenziale, derivante ad esempio da un problema fisico.
Come riportato sopra, le discipline principali sviluppate all'interno della matematica sono nate dalla necessità di eseguire calcoli nel commercio, di capire i rapporti fra i numeri, di misurare la terra e di predire eventi astronomici. Questi quattro bisogni possono essere collegati approssimativamente con la suddivisione della matematica nello studio sulla quantità, sulla struttura, sullo spazio e sul cambiamento (cioè, aritmetica, algebra, geometria e analisi matematica). Oltre a queste, vi sono altre suddivisioni come la logica, la teoria degli insiemi, la matematica empirica di varie scienze (matematica applicata) e più recentemente allo studio rigoroso dell'incertezza.
Lo studio sulle quantità inizia con i numeri, in primo luogo con i numeri naturali (numeri interi non negativi) e tramite operazione aritmetiche su di essi. Le proprietà più profonde dei numeri interi sono studiate nella teoria dei numeri, di cui un esempio famoso è l'ultimo teorema di Fermat. La teoria dei numeri inoltre presenta due problemi non risolti, largamente considerati e discussi: la Congettura dei numeri primi gemelli e la congettura di Goldbach.
I numeri interi sono riconosciuti come sottoinsieme dei numeri razionali ("frazioni"). Questi, a loro volta, sono contenuti all'interno dei numeri reali, usati per rappresentare quantità continue. I numeri reali sono generalizzati ulteriormente dai numeri complessi. Queste sono i primi punti di una gerarchia dei numeri che continua ad includere i quaternioni e gli ottonioni. L'analisi dei numeri naturali conduce inoltre ai numeri infiniti.
Fra gli strumenti informatici negli ultimi anni si sono resi disponibili vari generi di pacchetti software volti ad automatizzare l'esecuzione di calcoli numerici, le elaborazioni simboliche, la costruzione di grafici e di ambienti di visualizzazione e, di conseguenza, volti a facilitare lo studio della matematica e lo sviluppo delle applicazioni che possano essere effettivamente incisive.
Particolare importanza ed efficacia vanno assumendo quelli che vengono chiamati sistemi di algebra computazionale o addirittura con il termine inglese Computer algebra systems, abbreviato con CAS.
Segnaliamo alcuni programmi open source o comunque gratuitamente disponibili per lo studio della matematica:
Molti oggetti matematici, come gli insiemi di numeri e funzioni, mostrano la loro struttura interna e coerente. Le proprietà strutturali di questi oggetti sono investigate nello studio di gruppi, anelli, campi e altri sistemi astratti, i quali sono a loro volta oggetti. Questo è il campo dell'algebra astratta. In questo campo un concetto importante è rappresentato dai vettori, generalizzati nello spazio vettoriale, e studiati nell'algebra lineare. Lo studio di vettori combina tre tra le fondamentali aree della matematica: quantità, struttura, e spazio. Il calcolo vettoriale espande il campo in una quarta area fondamentale, quella delle variazioni.
Lo studio dello spazio inizia con la geometria, in particolare con la geometria euclidea. La Trigonometria poi combina simultaneamente spazio e numeri. Lo studio moderno dello spazio generalizza queste premesse includendo la Geometria non euclidea (che assume un ruolo centrale nella teoria della relatività generale) e la topologia. Quantità e spazio sono trattati contemporaneamente in geometria analitica, geometria differenziale, e geometria algebrica. Con la geometria algebrica si ha la descrizione di oggetti geometrici come insiemi di soluzioni di equazioni polinomiali combinando i concetti di quantità e spazio, e anche lo studio di gruppi topologici, i quali combinano a loro volte spazio e strutture. I gruppi di Lie sono usati per studiare lo spazio, le strutture e le variazioni. La Topologia in tutte le sue molte ramificazioni può essere considerata la zona di sviluppo più grande nella matematica del XX secolo ed include la congettura di Poincaré e il controverso teorema dei quattro colori, di cui l'unica prova, eseguita a computer, non è mai stata verificata da un essere umano.
Matematica discreta è il nome comune per i campi della matematica utilizzati nella maggior parte dei casi nell'informatica teorica. Questa include teoria della computazione, teoria della complessità computazionale, e informatica teorica. La teoria della computazione esamina le limitazioni dei vari modelli di computer, compresi i modelli più potenti conosciuti - la Macchina di Turing. La teoria della complessità è lo studio delle possibilità di trattazione da parte di un calcolatore; alcuni problemi, nonostante siano teoricamente risolvibili attraverso un calcolatore, sono troppo costosi in termini di tempo o spazio tanto che risolverli risulta praticamente impossibile, anche prevedendo una rapida crescita delle potenze di calcolo. Infine la teoria dell'informazione si interessa della quantità di dati che possono essere immagazzinati su un dato evento o mezzo e quindi di concetti come compressione dei dati e entropia.
Come campo relativamente nuovo, la matematica discreta possiede un numero elevato di problemi aperti. Il più famoso di questi è il problema " P=NP?" uno dei problemi per il millennio.
La matematica applicata considera l'utilizzo della matematica teorica come strumento utilizzato per la risoluzione di problemi concreti nelle scienze, negli affari e in molte altre aree. Un campo importante della matematica è la statistica, la quale utilizza la teoria della probabilità e permette la descrizione, l'analisi, e la previsione di fenomeni aleatori. La maggior parte degli esperimenti, delle indagini e degli studi d'osservazione richiedono l'utilizzo della statistica (molti statistici, tuttavia, non si considerano come veri e propri matematici, ma come parte di un gruppo collegato ad essi). L'analisi numerica investiga metodi computazionali per risolvere efficientemente una vasta gamma di problemi matematici che sono, in genere, troppo grandi per le capacità di calcolo umane; essa include lo studio di vari tipi di errore che generalmente si verificano nel calcolo.
</text>
</doc>
<doc id="38771" url="https://it.wikipedia.org/wiki?curid=38771">
<title>Equazione di secondo grado</title>
<text>
In matematica, un'equazione di secondo grado o quadratica ad un'incognita formula_1 è un'equazione algebrica in cui il grado massimo con cui compare l'incognita è 2, ed è sempre riconducibile alla forma:
Per il teorema fondamentale dell'algebra, le soluzioni (dette anche radici o zeri dell'equazione) delle equazioni di secondo grado nel campo complesso sono sempre due, se contate con la loro molteplicità. Nel campo reale invece le equazioni quadratiche possono ammettere due soluzioni, una soluzione doppia, oppure nessuna soluzione.
Sono poi particolarmente semplici da risolvere le cosiddette equazioni "incomplete", dove alcuni coefficienti sono uguali a zero.
Il grafico della funzione
nel piano cartesiano è una parabola, la cui concavità dipende dal segno di formula_4. Più precisamente: se formula_5 la parabola ha la concavità rivolta verso l'alto, se formula_6 la parabola ha la concavità rivolta verso il basso.
Gli antichi babilonesi lasciarono nelle tavolette di argilla le prime testimonianze della scoperta delle equazioni quadratiche e trovarono le prime tecniche per risolverle. In Mesopotamia spesso le equazioni erano introdotte da problemi di tipo geometrico: ad esempio si chiede di trovare il lato di un quadrato sapendo che l'area meno un lato è uguale a 870; problema che corrisponde alla nostra equazione formula_7 (ridotta in forma normale come formula_8). I Babilonesi non accettavano però le soluzioni negative e nulle delle equazioni e, non accettando il fatto che i coefficienti potessero assumere valori sia positivi che negativi, non veniva riconosciuta nemmeno una forma normale unica ma erano distinti tre casi con coefficienti positivi:
Espresse nella forma moderna la prima ha il termine noto negativo, la seconda il coefficiente di secondo grado negativo, e la terza entrambi i coefficienti minori di zero. L'equazione con tutti i termini positivi non era nemmeno presa in considerazione in quanto ammette solo soluzioni negative.
Nella forma normale babilonese il coefficiente di secondo grado è unitario ma non arrivavano a tale forma, come successivamente gli arabi dividendo tutti i membri per formula_4. Data, per esempio, l'equazione formula_13, entrambi i membri venivano infatti moltiplicati per formula_4: formula_15 e poi veniva effettuata la sostituzione formula_16 in modo da ottenere un'equazione in forma normale nella variabile formula_17; formula_18. Questo procedimento testimonia l'elevato grado di flessibilità raggiunto dall'algebra babilonese.
La soluzione era data tramite formule che ricordano molto quelle odierne. Per esempio la formula risolutiva per il primo caso era, espressa in notazione moderna, la seguente:
che può essere ridotta tramite semplici passaggi algebrici alla formula risolutiva moderna per questo caso
Il matematico indiano Baudhāyana, che scrisse un Shulba Sutras nell'antica India all'incirca nell'VIII secolo a.C., usò per primo equazioni quadratiche della forma formula_21 e formula_22, indicando i metodi per risolverle.
I matematici babilonesi (intorno al 400 a.C.) e cinesi utilizzarono il metodo del completamento del quadrato per risolvere varie equazioni quadratiche con radici positive, ma non ottennero una formula generale.
Euclide descrisse un metodo geometrico più astratto intorno al 300 a.C.; Diofanto di Alessandria si occupò della soluzione delle equazioni di secondo grado, tuttavia il suo lavoro non ebbe conseguenze significative poiché la matematica greca era in una fase di declino. Il "manoscritto di Bakshali", scritto in India fra il 200 a.C. e il 400 d.C., introdusse la formula risolutiva delle equazioni quadratiche.
Il primo matematico noto ad aver usato la formula algebrica generale, consentendo sia le soluzioni positive che quelle negative, fu Brahmagupta (India, VII secolo).
Al-Khwarizmi (BagdĀd, IX secolo d.C) sviluppò indipendentemente un insieme di formule che funzionava per le soluzioni positive. Nell"'al-Jabr", al-Khwarizmi distingue 5 tipi di equazione: i tre già noti ai babilonesi e in più l'equazione pura formula_23 e quella spuria formula_24. Anche qui si pone il coefficiente di secondo grado uguale a 1 ma ci si arriva tramite divisione. Le soluzioni negative non sono, nemmeno stavolta, accettate.
Il metodo usato da al-Khwarizmi è quello del "completamento del quadrato". L'equazione formula_25, per esempio, sarebbe stata risolta aggiungendo 16 a entrambi i termini in modo da "completare" il quadrato al primo membro: formula_26 ossia formula_27. Da questa si otteneva formula_28 e si trovava così a soluzione positiva formula_29.
Il matematico arabo proponeva anche una trasposizione grafica. Supponiamo di dover risolvere la stessa equazione formula_25. Il metodo usato dal persiano in questo caso avrebbe potuto essere "simile" al seguente: si tracci un quadrato che supponiamo avere lato formula_1 (quello blu in figura). Vi si affianchino due rettangoli di dimensioni formula_1 e 4 ossia formula_33 (quelli verdi in figura). L'area della figura verde e blu è formula_34. Poniamo ora questa area = 33. Aggiungiamo ora il quadratino rosso di lato 4, in modo da "completare" il quadrato grande. L'area totale sarà quindi formula_35 e il lato del quadrato grande è dunque 7. Poiché il lato grande è dato dal lato del quadrato blu (cioè formula_1) sommato al lato del rettangolo verde (cioè 4); formula_37.
Al-Khwaritzmi pone per la prima volta l'accento sul segno del discriminante, che deve essere positivo perché l'equazione sia risolubile.
La priorità della scoperta della formula generale per risolvere un'equazione quadratica è stata attribuita a Sridhara (870-930 circa), sebbene ai suoi tempi vi sia stata una disputa. La regola (come riportata da Bhaskara II) è:
Abraham bar Hiyya Ha-Nasi (conosciuto anche con il nome latino Savasorda) fu il primo a introdurre in Europa la soluzione completa con il suo "Liber embadorum".
Nell'epoca moderna, in Europa, si iniziò ad accettare le soluzioni negative e, successivamente, quelle complesse e a porre l'equazione in un'unica forma normale.
Cartesio introdusse nel XVII secolo la regola dei segni, secondo la quale un'equazione di secondo grado ha tante soluzioni positive quanti sono i cambi di segno fra due coefficienti consecutivi. L'equazione formula_38, per esempio, ammette una soluzione negativa, invece formula_39 ne ha due.
Viète introdusse per primo delle lettere per esprimere i coefficienti delle equazioni, ipotizzando per primo che potessero assumere anche valori negativi. Scoprì poi le formule che portano il suo nome e che mettono in relazione i coefficienti dell'equazione con le radici. In particolare per l'equazione di secondo grado si afferma che se il coefficiente di secondo grado formula_4 è 1, allora il prodotto delle radici dà il termine noto e l'opposto della loro somma il coefficiente di primo grado.
Si dice "spuria" un'equazione quadratica che manca del termine noto, ossia avente la forma:
Un'equazione di questo tipo si risolve facilmente tramite scomposizione:
Per la legge di annullamento del prodotto quest'equazione è equivalente alle due:
E in definitiva le sue soluzioni sono
Si dice "equazione quadratica pura" un'equazione polinomiale di secondo grado mancante del termine di primo grado:
Portando formula_46 al secondo membro e dividendo per formula_4 si ottiene:
Se formula_49, non ammette soluzioni nel campo reale, in quanto non esistono numeri reali che siano radici quadrate di un numero negativo (per esempio formula_50) bensì esistono due soluzioni nel campo dei numeri complessi.
Se formula_51, l'equazione è risolta da:
Si dice "equazione monomia" un'equazione quadratica nella quale formula_53 e formula_54, dunque nella forma formula_55. In questo caso l'equazione ammette come unica soluzione doppia, o di molteplicità due, formula_56
Un'equazione polinomiale di secondo grado viene detta "equazione quadratica completa" quando tutti i suoi coefficienti sono diversi da 0. Essa viene risolta con il cosiddetto metodo del completamento del quadrato, così chiamato perché si modifica l'equazione fino a ottenere al suo primo membro il quadrato di un binomio nella forma formula_57.
Anzitutto portiamo formula_46 al secondo membro:
Moltiplichiamo per formula_60 entrambi i membri, ottenendo:
Notiamo che
e che
dunque possiamo considerare il termine formula_64 come la formula_65 della formula del quadrato di binomio e formula_66 come il doppio prodotto formula_67 dove la formula_68 è uguale a formula_69, dunque, per fare in modo che al primo membro si abbia un quadrato di binomio, sommiamo ad ambo i membri dell'equazione formula_70:
ovvero:
Il secondo membro di quest'equazione è detto "discriminante" e in genere viene indicato con la lettera greca formula_73 (Delta). Se formula_74 è negativo non ci sono soluzioni reali, dal momento che il primo membro, essendo un quadrato, è sempre maggiore o uguale a formula_75. In caso contrario, possiamo scrivere:
che con semplici passaggi possiamo riscrivere come:
Quest'ultima è nota come "formula risolutiva delle equazioni di secondo grado".
Alla luce della dimostrazione precedente è chiaro che, nella risoluzione di un'equazione quadratica, è anzitutto necessario calcolare il discriminante formula_78.<br/>
Si distinguono tre casi:
Le radici dell'equazione quadratica
sono anche i punti in cui la funzione
assume valore nullo, dal momento che essi sono i valori di formula_1 per cui
Se formula_4, formula_69 e formula_46 sono numeri reali e il dominio di formula_86 è l'insieme dei numeri reali, allora gli zeri di formula_86 sono esattamente le ascisse dei punti dove il grafico di formula_86 tocca l'asse formula_1.
Dalle considerazioni precedenti si deduce che, se il discriminante è positivo, il grafico interseca l'asse delle ascisse in due punti; se è nullo, il grafico è tangente all'asse formula_1, ovvero lo interseca nel vertice della parabola; se è negativo, il grafico non tocca mai l'asse formula_1.
La formula risolutiva dell'equazione di secondo grado può essere "semplificata" moltiplicando per formula_92 il denominatore e il numeratore
e applicando la sostituzione formula_94
Questa formula può risultare comoda quando il coefficiente dell'incognita di primo grado dell'equazione, formula_69, è esattamente divisibile per due.
Nel caso in cui formula_97, allora la formula si semplifica in 
formula_98
Poniamo formula_99 uguale alla somma delle due soluzioni dell'equazione quadratica e formula_100 il loro prodotto, quindi formula_101 e formula_102. Sommando membro a membro le due soluzioni abbiamo:
Effettuando invece il prodotto membro a membro abbiamo:
Queste due relazioni consentono di determinare somma e prodotto delle radici senza risolvere l'equazione; esse sono un caso particolare delle formule di Viète. Inoltre, se riscriviamo la generica equazione di secondo grado nella cosiddetta "forma normale", cioè dividendo ambo i termini per formula_4:
con banali sostituzioni si ottiene la forma:
Meno usata ma altrettanto importante è la relazione:
dimostrabile attraverso semplici passaggi algebrici.
Consideriamo il polinomio completo di secondo grado:
e supponiamo anche che il discriminante, dell'equazione che si ottiene uguagliando a zero il polinomio, sia positivo (ipotesi non necessaria nel campo dei numeri complessi). Raccogliendo formula_4 si ottiene:
Abbiamo già trovato prima che formula_112 e formula_113. Dunque:
Pertanto è possibile scomporre un polinomio di secondo grado in due binomi di primo grado calcolando le soluzioni dell'equazione data dal polinomio eguagliato a zero:
Se il formula_116 l'equazione associata ha una sola soluzione reale di molteplicità due e quindi la scomposizione del trinomio di secondo grado può essere così riscritta:
La "regola dei segni" o "regola di Cartesio" consente di determinare il segno delle radici di un'equazione completa con discriminante non negativo. Consideriamo, nell'ordine, i segni di formula_4, formula_69 e formula_46. Possiamo assumere che sia formula_121, a meno di moltiplicare entrambi i termini per -1. Ci sono 4 possibili combinazioni:
Chiamando "permanenza" ogni successione di due segni uguali e "variazione" ogni successione di segni contrari, possiamo riassumere i risultati precedenti affermando che a ogni permanenza corrisponde una soluzione negativa e a ogni variazione una soluzione positiva; quando le radici sono discordi, in valore assoluto è maggiore quella positiva se la variazione precede la permanenza; quella negativa se la permanenza precede la variazione.
Sia
allora
da cui
A questo punto è possibile disegnare il grafico di "f(x)", traslando la parabola associata a formula_137 di -3/2 lungo l'asse "x" e di -1/4 lungo l'asse "y".
Ad alcune equazioni possono essere applicati metodi diversi per trovarne le radici. Si usa il teorema del resto di Ruffini, si controllano i divisori possibili del termine noto formula_46 e si prendono una volta formula_139 e una volta formula_140, poi considerando la formula del teorema del resto di Ruffini formula_141, possiamo subito sapere che formula_142, e che sostituendo la formula_17 nell'equazione al posto della formula_1 possiamo verificare il resto che ci darà il polinomio diviso con formula_141, se è formula_75 allora formula_141 sarà il nostro divisore, dove una soluzione dell'equazione è formula_17.
A questo punto si può trovare l'altra soluzione in due modi:
Ruffini:
Applichiamo la divisione di Ruffini o il metodo Canonico per avere un binomio dove il termine noto cambiato di segno sarà la nostra seconda soluzione.
Quindi: formula_150 formula_151
Metodo delle radici:
Sapendo che:
E che:
Avendo la prima soluzione trovata dal metodo visto sopra del teorema del resto di Ruffini, possiamo trovare la seconda soluzione in modo più breve senza applicare la divisione in questi due modi:
Vale anche viceversa.
Due casi particolari sono formula_156 oppure formula_157. Nel primo caso le soluzioni sono formula_158 e formula_159, mentre nel secondo caso le soluzioni sono formula_160 e formula_161
In certe situazioni è preferibile esprimere le radici in una forma alternativa.
Tuttavia, questa formula è corretta solo con la condizione aggiuntiva che formula_46 non sia nullo. Se formula_164, questa formula fornisce correttamente la soluzione formula_165, ma non consente di ottenere la radice diversa da zero (dal momento che si otterrebbe la divisione formula_166, che non è definita).
Naturalmente, i valori delle due radici risultano uguali indipendentemente che si usi la formula "classica" o quella alternativa, che è in effetti una semplice variante algebrica della prima:
Un'attenta implementazione su un calcolatore dotato di operazioni in virgola mobile differisce da entrambe le formule per garantire la robustezza del risultato. Assumendo che il discriminante sia positivo e "b" diverso da 0, si può usare codice come il seguente:
dove formula_169 denota la funzione segno, che vale formula_170 se formula_69 è positivo e formula_172 se formula_69 è negativo; questo accorgimento assicura di sommare due quantità dello stesso segno, evitando l'eventuale perdita di precisione. Il calcolo della seconda radice formula_174 sfrutta il fatto che il prodotto delle radici è uguale a formula_175.
</text>
</doc>
<doc id="31077" url="https://it.wikipedia.org/wiki?curid=31077">
<title>Matrice</title>
<text>
In matematica, in particolare in algebra lineare, una matrice è una tabella ordinata di elementi.
Ad esempio:
Le matrici sono ampiamente usate in matematica e in tutte le scienze per la loro capacità di rappresentare in maniera utile e concisa diversi oggetti matematici, come valori che dipendono da due parametri o anche sistemi lineari, cosa, quest'ultima, che le rende uno strumento centrale dell'analisi matematica.
Tracce dell'utilizzo di matrici risalgono fino ai primi secoli a.C. Nel corso della storia più volte è capitato che matematici vissuti in epoche e luoghi diversi, durante lo studio di sistemi lineari, abbiano disposto i coefficienti del sistema in forma tabellare, fatto che evidenzia come le matrici siano una struttura particolarmente intuitiva e conveniente per questi scopi. Interessanti reperti sono anche i quadrati latini e i quadrati magici. Fu solo a partire dal XVII secolo comunque che l'idea delle matrici fu ripresa e sviluppata, prima con risultati e idee ottenuti in contesti di studio specifici, poi con la loro generalizzazione. Lo sviluppo infine è continuato fino a dare alla teoria delle matrici la forma che oggi conosciamo.
I primi a sfruttare le matrici per agevolare i propri calcoli furono i matematici cinesi, proprio nell'affrontare i sistemi lineari.
Nel "Jiuzhang Suanshu" ("Nove capitoli sulle arti matematiche"), steso durante la dinastia Han, l'ottavo capitolo è interamente dedicato allo svolgimento di un problema matematico formulato sotto forma di sistema lineare. L'autore dispone ingegnosamente i coefficienti di ogni equazione parallelamente in senso verticale, in maniera quindi differente dalla notazione odierna, che li vuole disposti orizzontalmente, per righe: una semplice differenza di notazione.
Ai numeri così disposti si applicava una serie di operazioni portandoli in una forma tale da rendere evidente quale fosse la soluzione del sistema: è quello che oggi conosciamo come metodo di eliminazione gaussiana, scoperto in occidente solo agli inizi del XIX secolo con gli studi del matematico tedesco Carl Friedrich Gauss.
All'interno dello "Jiuzhang Suanshu" comparve anche il concetto di determinante, inteso come metodo per determinare se un sistema lineare ammette un'unica soluzione.
Un'idea più moderna di determinante fece la sua comparsa nel 1683, a distanza di poco tempo sia in Giappone, con Kōwa Seki ("Method of solving the dissimulated problems"), che in Europa, con Leibniz.
Nella prima metà del XVIII secolo, Maclaurin scrisse il "Treatise of Algebra" ("Trattato di algebra"), pubblicato postumo nel 1748, nella quale mostrava il calcolo dei determinanti per matrici quadrate di ordine 2 e 3. Cramer diede il suo contributo nel 1750 presentando l'algoritmo per il calcolo del determinante per matrici quadrate di ordine qualunque, usato nel metodo oggi noto come regola di Cramer ("Introduction à l'analyse des lignes courbes algébriques"). Ulteriori sviluppi sul concetto di determinante furono poi apportati da Bézout ("Sur le degré des équations résultantes de l'évanouissement des inconnues", 1764), Vandermonde ("Mémoire sur l'élimination", 1772), Laplace (1772), Lagrange (1773), Gauss (1801) che introdusse per la prima volta il termine "determinante", Cauchy (1812) che usò per la prima volta il determinante nella sua concezione moderna, ottenendo anche importanti risultati sui minori e le matrici aggiunte, e Jacobi. All'inizio del XIX secolo venne usato per la prima volta in occidente il metodo di eliminazione gaussiana da parte di Gauss, per lo studio dell'orbita dell'asteroide Pallas in base alle osservazioni ottenute fra il 1803 ed il 1809. Altri concetti ed idee fondamentali della teoria delle matrici furono poi studiati, sempre in contesti specifici, da Cauchy, Sturm, Jacobi, Kronecker, Weierstrass e Eisenstein.
Nel 1848 il matematico e avvocato inglese Sylvester introdusse per la prima volta il termine "matrice". Il suo collega avvocato Cayley introdusse nel 1853 l'inversa di una matrice., e nel 1858 fornì la prima definizione astratta di matrice, in "Memoir on the theory of matrices" ("Memorie sulla teoria delle matrici"), mostrando come tutti gli studi precedenti non fossero altro che casi specifici del suo concetto generale. All'interno del testo Cayley forniva inoltre un'algebra delle matrici, definendo le operazioni basilari di somma, moltiplicazione tra matrici, moltiplicazione per scalari e inversa di una matrice. Ancora ignaro di tali opere, nel 1878 Frobenius pubblicò "Ueber lineare Substitutionen und bilineare Formen" ("Sulle sostituzioni lineari e forme bilineari"), nel quale riportava importanti risultati sulle matrici, quale per esempio la definizione di rango. Nel 1888 il geodeta Jordan nella terza edizione del suo "Handbuch der Vermessungskunde" ("Manuale di geodesia") ampliò il metodo di eliminazione di Gauss in quello che oggi è noto come metodo di eliminazione di Gauss-Jordan. Altri contributi importanti furono dati da Bôcher nel 1907 con "Introduction to higher algebra"; altri testi di rilievo furono scritti da Turnbull ed Aitken negli anni trenta ("The Theory of Canonical Matrices" e "Determinants and Matrices") e da Mirsky nel 1955 ("An introduction to linear algebra").
A partire dalla seconda metà del XX secolo l'avvento dei computer ha dato un'impressionante accelerazione alla diffusione delle matrici e dei metodi matriciali. Grazie ai computer infatti è stato possibile applicare in maniera efficiente metodi iterativi precedentemente ritenuti troppo onerosi, portando di conseguenza allo sviluppo di nuove tecniche per la risoluzione di importanti problemi dell'algebra lineare, quali il calcolo degli autovettori e autovalori, il calcolo dell'inversa di una matrice e la risoluzione di sistemi lineari. Ciò a sua volta ha permesso l'introduzione delle matrici in altre discipline applicate, come per esempio la matematica economica e la probabilità, che grazie ad esse hanno potuto rappresentare concetti complessi in maniera più semplice. Altri campi relativamente più recenti, invece, come per esempio la ricerca operativa, hanno basato ampiamente la propria disciplina sull'utilizzo delle matrici.
Una matrice è una tabella rettangolare di numeri. Da un punto di vista formale, può essere definita come una funzione
dove formula_3 e formula_4 sono interi positivi fissati e formula_5 è un qualunque insieme fissato, come ad esempio quello dei numeri reali. Le righe orizzontali di una matrice sono chiamate "righe", mentre quelle verticali "colonne". Ad esempio, la matrice mostrata sopra ha due righe e tre colonne. Una matrice formula_6 generica è descritta come in figura sopra oppure anche nel modo seguente (che viene considerata più proficua come notazione per il fatto di non dover differenziare nelle operazioni l'elemento dalla matrice stessa):
indicando con formula_8 l'elemento posizionato alla riga formula_9-esima e nella colonna formula_10-esima.
La riga formula_9-esima viene indicata con formula_12, oppure più ambiguamente formula_13, mentre colonna formula_10-esima con formula_15, oppure più ambiguamente formula_16.
Gli elementi formula_17 costituiscono la diagonale principale della matrice.
I vettori possono essere considerati matrici aventi una sola riga o una sola colonna. Una matrice con una sola riga, di dimensione formula_18, è detta "matrice riga" o "vettore riga", mentre una matrice con una sola colonna, di dimensione formula_19, è detta "matrice colonna" o "vettore colonna".
Di seguito sono mostrati in ordine una matrice formula_20, una matrice colonna ed una matrice riga;
Come mostrato negli esempi, i valori presenti nella matrice possono essere di vario tipo: interi, reali o anche complessi. In molti casi si suppone che i valori siano elementi di un campo formula_5 fissato.
Sulle matrici si possono definire numerose operazioni che spesso dipendono anche dall'insieme in cui sono scelti i valori delle matrici. Nel resto del paragrafo supponiamo che le matrici abbiano tutte valori in uno stesso campo formula_5 fissato.
Due matrici formula_24 e formula_25, entrambe di tipo formula_26, possono essere sommate. La loro somma formula_27 è definita come la matrice formula_28 i cui elementi sono ottenuti sommando i corrispettivi elementi di formula_24 e formula_25. Formalmente:
Per esempio:
La moltiplicazione per uno scalare è un'operazione che, data una matrice formula_24 ed un numero formula_34 (detto "scalare"), costruisce una nuova matrice formula_35, il cui elemento è ottenuto moltiplicando l'elemento corrispondente di formula_24 per formula_34; gli elementi della matrice e lo scalare in questione devono appartenere allo stesso campo. Formalmente:
Per esempio:
La moltiplicazione tra due matrici formula_40 e formula_41 è un'operazione più complicata delle precedenti. A differenza della somma, non avviene moltiplicando semplicemente gli elementi aventi lo stesso posto. La definizione di moltiplicazione che segue è motivata dal fatto che una matrice modellizza un'applicazione lineare, e il prodotto di matrici corrisponde alla composizione di applicazioni lineari.
La moltiplicazione è quindi definita soltanto se le matrici formula_40 e formula_41 sono rispettivamente di tipo formula_44 e formula_45: in altre parole, il numero formula_46 di colonne di formula_40 deve coincidere con il numero formula_46 di righe di formula_41. Il risultato è una matrice formula_50 di tipo formula_51.
Ad esempio, siano formula_40 e formula_41 due matrici rispettivamente formula_54 e formula_55: tra queste si può effettuare la moltiplicazione formula_56 ed ottenere una matrice formula_57. Le stesse matrici, però, non possono essere moltiplicate nel modo formula_58, poiché le colonne di formula_41 non sono tante quante le righe di formula_40.
Il prodotto di formula_40 di formula_62 righe e formula_46 colonne e formula_41 di formula_46 righe e formula_66 colonne è la matrice formula_67 di dimensione formula_68, il cui elemento di posizione formula_69 è dato dalla somma:
Quest'ultimo viene detto "prodotto riga per colonna". Ad esempio:
formula_71formula_72
formula_73
Si osserva che moltiplicando una matrice formula_75 per una formula_76 si ottiene una matrice formula_75.
Prima riga:
Seconda riga:
A differenza dell'usuale moltiplicazione fra numeri, questa non è un'operazione commutativa, cioè formula_85 è in generale diverso da formula_86, quando si possono effettuare entrambi questi prodotti.
Un caso particolare, ampiamente usato in algebra lineare per rappresentare le trasformazioni lineari (come rotazioni e riflessioni) è il prodotto tra una matrice formula_6 ed un vettore colonna formula_88, che viene chiamato anche "prodotto matrice-vettore".
Le operazioni di somma e prodotto di matrici soddisfano tutte le proprietà usuali della somma e del prodotto di numeri, ad eccezione, nel caso del prodotto di matrici, della proprietà commutativa.
Sia formula_89 la matrice nulla, fatta di soli zeri (e della stessa taglia di formula_24). Sia inoltre formula_91 la matrice ottenuta moltiplicando formula_24 per lo scalare formula_93. Valgono le relazioni seguenti, per ogni formula_94 matrici formula_6 e, per ogni formula_96 numeri reali.
Le prime 4 proprietà affermano che le matrici formula_26 formano un gruppo abeliano rispetto all'operazione di somma. Come mostrato sopra, il prodotto non è commutativo in generale.
Sulle matrici sono definite numerose altre operazioni. Tra queste:
Le matrici permettono di rappresentare le trasformazioni lineari fra spazi vettoriali. Ogni operatore lineare formula_108 da uno spazio vettoriale formula_109 di dimensione formula_110 a uno spazio vettoriale formula_111 di dimensione formula_4, e per ogni possibile scelta di una coppia di basi formula_113 e formula_114, si associa a formula_115 la matrice formula_40 tale che:
Questa matrice rappresenta l'applicazione formula_115: questa rappresentazione dipende però dalle basi scelte. Molte operazioni fra matrici si traducono in operazioni fra applicazioni lineari:
Le matrici sono utili soprattutto a rappresentare sistemi di equazioni lineari. Il sistema:
può essere rappresentato con il suo equivalente matriciale, tramite il prodotto matrice-vettore:
Fra le matrici, occupano un posto di rilievo le matrici quadrate, cioè le matrici formula_122, che hanno lo stesso numero formula_4 di righe e di colonne. Una matrice quadrata ha una diagonale principale, quella formata da tutti gli elementi formula_124 con indici uguali. La somma di questi elementi è chiamata traccia. L'operazione di trasposizione trasforma una matrice quadrata formula_24 nella matrice formula_126 ottenuta scambiando ogni formula_127 con formula_128, in altre parole ribaltando la matrice intorno alla sua diagonale principale.
Una matrice tale che formula_129 è una matrice simmetrica. In altre parole, formula_24 è simmetrica se formula_131. Se tutti gli elementi che non stanno nella diagonale principale sono nulli, la matrice è detta diagonale.
Tra le più importanti matrici formula_132 vi è la matrice identità formula_133: si tratta di una matrice avente 1 su ogni elemento della diagonale e 0 altrove. La matrice è importante perché rappresenta l'elemento neutro rispetto al prodotto: infatti le matrici formula_134 possono essere moltiplicate fra loro, e vale (oltre a quelle scritte sopra) la proprietà seguente per ogni formula_24:
ovvero è l'elemento neutro del prodotto. Nello spazio delle matrici formula_132 sono quindi definiti una somma ed un prodotto, e le proprietà elencate fin qui asseriscono che l'insieme è un anello, simile all'anello dei numeri interi, con l'unica differenza che il prodotto di matrici non è commutativo.
Un'importante quantità definita a partire da una matrice quadrata formula_24 è il suo determinante. Indicato con formula_139, questo numero fornisce molte informazioni essenziali sulla matrice. Ad esempio, determina se la matrice è invertibile, cioè se esiste una matrice formula_25 tale che:
Il determinante è l'ingrediente fondamentale della regola di Cramer, utile a risolvere alcuni sistemi lineari.
La traccia ed il determinante possono essere racchiuse in un oggetto ancora più raffinato, di fondamentale importanza nello studio delle trasformazioni lineari: il polinomio caratteristico, un polinomio le cui radici sono gli autovalori della matrice. La conoscenza di autovalori e autovettori consente ad esempio di studiare la similitudine fra matrici, in particolare la similitudine ad una matrice diagonale.
Oltre alle matrici diagonali e simmetriche già introdotte, vi sono altre categorie di matrici importanti.
Lo spazio di tutte le matrici formula_26 a valori in un fissato campo formula_146 è indicato generalmente con formula_147 o formula_148. Per quanto già visto, questo spazio è un gruppo abeliano con la somma. Considerato anche con la moltiplicazione per scalare, l'insieme ha una struttura di spazio vettoriale su formula_146.
Questo spazio ha una base canonica, composta da tutte le matrici formula_150 aventi valore 1 sulla casella di posto formula_151 e zero in tutte le altre. La base consta di formula_26 elementi, e quindi lo spazio formula_153 ha dimensione formula_26.
Nel caso formula_155 delle matrici quadrate, è definito anche il prodotto. Con questa ulteriore operazione, lo spazio formula_156, indicato anche con formula_157, eredita una struttura di anello con unità. Tale struttura è compatibile con quella di spazio vettoriale definita sopra, e fornisce quindi un esempio basilare di algebra su campo.
Una "matrice infinita" può essere definita come una successione di elementi formula_127, indicizzati da coppie di numeri naturali formula_151, senza nessun limite superiore per entrambi.
Più in generale, una generalizzazione del concetto di matrice è costruita prendendo due insiemi di indici formula_160 qualsiasi (parametrizzanti le "righe" e le "colonne") e definendo una matrice come un'applicazione:
a valori in un altro dato insieme formula_109. La matrice usuale formula_28 corrisponde al caso in cui formula_164 e formula_165, e formula_109 è ad esempio l'insieme dei numeri reali o complessi.
Questa definizione generale si serve solo di nozioni insiemistiche e non ricorre a nozioni visive e intuitive come quella di schieramento rettangolare. Consente di trattare casi molto generali: ad esempio matrici le cui righe e colonne sono etichettate da indici in un qualunque sottoinsieme formula_167 degli interi formula_168, matrici etichettate da coppie o in generale da formula_4-uple di interi come quelle che si incontrano nella meccanica quantistica o nella chimica molecolare, matrici infinite etichettate con gli insiemi formula_170 e formula_168 come quelle che permettono di rappresentare successioni polinomiali o serie formali con due variabili.
Per poter definire somma, prodotto e altre operazioni sulle matrici, è opportuno che l'insieme formula_109 sia dotato di tali operazioni, ad esempio che sia un anello.
La teoria delle funzioni di matrice è di grande interesse per lo studio dei sistemi differenziali: in generale la funzione di una matrice non coincide con la matrice delle funzioni dei suoi elementi, ma si dimostra sfruttando il teorema di Hamilton-Cayley che ciascun suo elemento è una combinazione lineare di queste ultime.
</text>
</doc>
<doc id="2401" url="https://it.wikipedia.org/wiki?curid=2401">
<title>Insieme vuoto</title>
<text>
Nella teoria degli insiemi si indica con insieme vuoto quel particolare insieme che non contiene alcun elemento.
Nella teoria assiomatica degli insiemi l'assioma dell'insieme vuoto ne postula l'esistenza. Partendo da questo sono costruiti tutti gli insiemi finiti. L'insieme vuoto è chiamato talvolta anche "insieme nullo", ma ciò può creare confusione con il concetto esposto nella voce insieme nullo, argomento studiato in teoria della misura.
Diverse proprietà insiemistiche sono banalmente vere per l'insieme vuoto.
Solitamente l'insieme vuoto è indicato col simbolo formula_1, formula_2, ∅ oppure formula_3, usato per la prima volta dal gruppo di matematici, principalmente francesi, dell'inizio del XX secolo che scrivevano sotto lo pseudonimo collettivo di Nicolas Bourbaki (in particolare, fu il matematico André Weil ad introdurlo nel 1939). Non deve essere confuso con la lettera greca Φ (phi) o con la vocale scandinava Øø (sebbene Weil si sia ispirato proprio a questa).
Si noti che la notazione {formula_2} indica l'insieme che contiene l'insieme vuoto e va pertanto non confusa con il semplice insieme vuoto formula_2.
Per notare meglio le differenze tra i vari simboli, li si osservi uno a fianco all'altro: ∅ Øø Φ - il simbolo di insieme vuoto è basato su un cerchio, mentre la lettera scandinava è più simile a un ovale, come la lettera O; infine la barra della Φ è verticale e non obliqua.
Poiché l'insieme vuoto è unico si parla "dell"'insieme vuoto e non "di un" insieme vuoto. Nella teoria degli insiemi, infatti, due insiemi sono uguali se hanno gli stessi elementi, quindi ci può essere un solo insieme senza elementi.
Considerato come sottoinsieme della retta reale (o, più in generale, di un qualsiasi spazio topologico), l'insieme vuoto è sia chiuso che aperto. Tutti i suoi punti frontiera (cioè nessun punto) appartengono all'insieme vuoto, che perciò è chiuso; ma anche tutti i suoi punti interni (ancora una volta nessun punto) appartengono all'insieme vuoto, che dunque è anche aperto. Inoltre l'insieme vuoto è un insieme compatto per il fatto che ogni insieme finito è compatto.
La chiusura dell'insieme vuoto è vuota. Questo fatto è noto come "conservazione dell'unione nulla".
Il concetto di insieme vuoto non è la stessa cosa che il concetto di "niente". È un insieme che non contiene niente al suo "interno", ma un insieme è "qualcosa". Questo fatto spesso causa difficoltà a chi lo incontra per la prima volta. Può essere d'aiuto immaginare un insieme come un contenitore di oggetti: un contenitore vuoto è vuoto, eppure certamente esiste.
L'insieme vuoto è un sottoinsieme di un qualunque insieme "A". Per definizione di sottoinsieme, si ha che per "ogni" elemento "x" di {}, "x" appartiene ad "A". Se non fosse vero che ogni elemento di {} si trova in "A", allora dovrebbe esistere almeno un elemento di {} che non è presente in "A". Ma dal momento che "non" ci sono elementi in {}, allora non esiste alcun elemento di {} che non sta in "A", e dunque si può concludere che ogni elemento di {} si trova in "A" e quindi {} è un sottoinsieme di "A". Questo concetto è spesso parafrasato con "tutto è vero per gli elementi dell'insieme vuoto" e può essere visto come una applicazione della regola logica "ex falso quodlibet".
Nella teoria assiomatica degli insiemi nota come teoria degli insiemi di Zermelo-Fraenkel, l'esistenza dell'insieme vuoto è assicurata dall'assioma dell'insieme vuoto. L'unicità dell'insieme vuoto segue dall'assioma di estensionalità.
Ogni assioma che stabilisce l'esistenza di un qualunque insieme implica l'assioma dell'insieme vuoto, utilizzando lo schema di assiomi di specificazione. Per esempio, se formula_15 è un insieme, allora lo schema di assiomi di separazione permette la costruzione dell'insieme formula_16, che può essere definito come l'insieme vuoto.
Sebbene l'insieme vuoto sia un concetto standard ed universalmente accettato in matematica, esistono persone che ancora manifestano qualche dubbio.
Jonathan Lowe ha affermato che anche se l'idea "è stata certamente una pietra miliare nella storia della matematica, … non dobbiamo assumere che la sua utilità nei calcoli sia dipendente dal suo denotare effettivamente un qualche oggetto". Non è chiaro se tale idea abbia un senso. "Tutto ciò di cui siamo a conoscenza riguardo all'insieme vuoto è che (1) è un insieme, (2) non ha elementi, e (3) è unico tra tutti gli insiemi che non hanno elementi. Però esistono molte cose che "non hanno elementi" nel senso della teoria degli insiemi — e cioè tutti i non-insiemi. È chiaro il motivo per cui questi oggetti non hanno elementi: perché non sono insiemi. Ciò che non è chiaro è come possa esistere, in modo univoco tra gli insiemi, un "insieme" che non ha elementi. Non possiamo evocare una tale entità semplicemente per accordo".
In seguito George Boolos, in "To be is to be the value of a variable…", Journal of Philosophy, 1984 (ristampato nel suo libro "Logic, Logic and Logic"), ha detto che si può fare molta strada utilizzando semplicemente la quantificazione multipla sugli oggetti, senza reificare gli insiemi come singole entità che hanno altre entità come membri.
In un libro recente Tom McKay ha espresso un'opinione negativa riguardo all'assunzione "singolarista" che le espressioni naturali che usano il plurale possano essere analizzate utilizzando surrogati del plurale, come i simboli per gli insiemi. Egli appoggia una teoria anti-singolarista che differisce dalla teoria degli insiemi nel fatto che non esiste l'analogo dell'insieme vuoto, ed esiste una sola relazione, "fra" ("among" in inglese), che è analoga sia al concetto di appartenenza che a quello di inclusione.
Le operazioni sull'insieme vuoto (inteso come insieme di oggetti sui quali si effettua l'operazione) possono creare confusione. Per esempio, la somma degli elementi dell'insieme vuoto è zero, ma la moltiplicazione degli elementi dell'insieme vuoto è uno (è il prodotto vuoto). Questo fatto può sembrare sbagliato, dato che non ci sono elementi nell'insieme vuoto, e quindi sembra che non possa fare differenza se essi sono sommati o moltiplicati (dato che “essi” non esistono). In effetti, i risultati di queste operazioni rivelano di più sulle operazioni stesse di quanto non facciano sull'insieme vuoto. Per esempio, si noti che lo zero è l'elemento neutro per l'addizione, mentre l'uno è l'elemento neutro per la moltiplicazione.
Dato che l'insieme vuoto non ha elementi, quando viene considerato come sottoinsieme di un qualunque insieme ordinato risulta che ogni elemento di quell'insieme è sia un maggiorante che un minorante per l'insieme vuoto. Per esempio, quando l'insieme vuoto viene considerato un sottoinsieme dei numeri reali, con l'ordinamento usuale, risulta che ogni numero reale è sia maggiorante che minorante per esso. Quando viene considerato come sottoinsieme dei numeri reali estesi (formati aggiungendo i due "numeri" (o punti) "meno infinito", indicato con formula_17 e "più infinito", indicato con formula_18 ai numeri reali, definiti in modo tale che formula_17 è minore di qualunque numero reale e formula_18 è maggiore di qualunque numero reale) si ha che: 
e
E cioè, il più piccolo maggiorante (sup o estremo superiore) è formula_23, mentre il più grande minorante (inf o estremo inferiore) è formula_24.
In precedenza si è affermato che l'insieme vuoto ha zero elementi, o che la sua cardinalità è zero. La connessione tra questi due concetti va oltre: nella definizione astratta di numero naturale lo zero è "per definizione" associato all'insieme vuoto, l'uno all'insieme con unico elemento l'insieme vuoto, e così via, in questo modo:
Se "A" è un insieme, allora esiste esattamente una funzione "f" da {} a "A", la funzione vuota. Di conseguenza, l'insieme vuoto è l'unico oggetto iniziale della categoria degli insiemi e delle funzioni.
L'insieme vuoto può essere considerato uno spazio topologico in un unico modo (definendolo aperto); questo spazio topologico vuoto è l'unico oggetto iniziale nella categoria degli spazi topologici con funzioni continue.
</text>
</doc>
<doc id="8662" url="https://it.wikipedia.org/wiki?curid=8662">
<title>Sistema numerico binario</title>
<text>
Il sistema numerico binario è un sistema numerico posizionale in base 2. Esso utilizza solo due simboli, di solito indicati con 0 e 1, invece delle dieci cifre utilizzate dal sistema numerico decimale. Ciascuno dei numeri espressi nel sistema numerico binario è definito "numero binario".
In informatica il sistema binario è utilizzato per la rappresentazione interna dell'informazione dalla quasi totalità degli elaboratori elettronici, in quanto le caratteristiche fisiche dei circuiti digitali rendono molto conveniente la gestione di due soli valori, rappresentati fisicamente da due diversi livelli di tensione elettrica. Tali valori assumono convenzionalmente il significato numerico di 0 e 1 o quelli di vero e falso della logica booleana.
Un numero binario è una sequenza di cifre binarie (dette bit). Ogni cifra in posizione formula_1 (contate da destra verso sinistra iniziando da 0) si considera moltiplicata per formula_2, anziché per formula_3, come avviene nella numerazione decimale.
Nella seguente tabella vengono confrontate le rappresentazioni binarie, esadecimale e decimale dei numeri compresi tra 0 e 15:
La formula per convertire un numero da binario a decimale (dove con formula_4 si indica la cifra di posizione formula_1 all'interno del numero, contate da destra verso sinistra iniziando da 0) è
Ad esempio
L'utilizzo dei numeri binari non è ristretto esclusivamente alla rappresentazione dei numeri interi positivi. Adottando alcune convenzioni, è possibile rappresentare numeri interi relativi in binario. Oltre al segno è possibile esprimere in binario i numeri razionali utilizzando, ad esempio, lo standard IEEE 754.
I numeri binari possono essere messi in relazione tramite operazioni aritmetiche, con regole simili a quelle del sistema decimale. Le quattro operazioni aritmetiche eseguibili sono: addizione, sottrazione, moltiplicazione e divisione. Per rendere possibili le operazioni aritmetiche e l'elaborazione dei segnali digitali è necessario esprimere i numeri binari tenendo conto del loro segno.
L'addizione fra due o più numeri binari è analoga a quella riferita ai numeri decimali. La regola applicata in questo caso è la seguente:
Quando si ha un riporto si aggiunge 1 sulla colonna di sinistra (quella più significativa) e si procede rispettando la regola della somma.
La sottrazione nel sistema binario si svolge nel modo seguente:
Quando si ha un prestito si sottrae 1 dalla colonna di sinistra (quella più significativa) e si procede rispettando la regola della differenza. Se sulla colonna di sinistra non si può concedere il prestito perché la cifra è 0, esso si trascina alla colonna successiva verso sinistra finché non si restituisce il prestito.
La regola del prodotto binario segue esattamente quella della moltiplicazione di due numeri decimali. Infatti si ha:
L'operazione della divisione rispetta la regola: 
dove formula_9 è il dividendo, formula_10 il divisore, formula_11 è il quoziente ed formula_12 il resto.
Per eseguire una divisione si può procedere tramite il metodo tradizionale in cui si procede per sottrazioni successive tra il dividendo e il divisore d fino a che il risultato diventi inferiore al divisore. Il risultato finale rappresenta il resto mentre il numero di sottrazioni rappresenta il quoziente.
Il sistema numerico binario ha molti "padri". Il primo a proporne l'uso fu Juan Caramuel con la pubblicazione del volume "Mathesis biceps. Vetus, et noua" pubblicato a Campagna nel 1669. Se ne trova traccia anche nelle opere di Nepero. Successivamente, il matematico tedesco Gottfried Wilhelm von Leibniz ne studiò per primo l'aritmetica. Questa è la ragione per cui questo sistema di numerazione è considerato tra le sue più grandi invenzioni. Però non ebbe un seguito immediato. L'aritmetica binaria venne ben presto dimenticata e riscoperta solo nel 1847 grazie al matematico inglese George Boole che aprirà l'orizzonte alle grandi scuole di logica matematica del Novecento e soprattutto alla nascita del calcolatore elettronico.
</text>
</doc>
<doc id="23233" url="https://it.wikipedia.org/wiki?curid=23233">
<title>Numero naturale</title>
<text>
In matematica i numeri naturali sono quei numeri usati per contare e ordinare. Nel linguaggio comune i "numeri cardinali" sono quelli usati per contare e i "numeri ordinali" sono quelli usati per ordinare.
I numeri naturali corrispondono all'insieme {0, 1, 2, 3, 4, ...}.
Essi vengono fatti corrispondere biunivocamente all'insieme dei numeri interi non negativi {0, +1, +2, +3, +4, ...}.
Talvolta vengono usati anche per indicare l'insieme dei numeri interi positivi {1, 2, 3, 4, ...}.
I numeri naturali sono i numeri più "intuitivi" che esistono. L'operazione di distinguere tra nessuno, uno e molti risale all'uomo primitivo. Ma la comprensione che, ad esempio, una pecora e un albero hanno in comune il fatto di essere "uno", cioè la nozione astratta di numero, fu un processo graduale (probabilmente non legato a una singola cultura o popolazione) che da vari studi viene fatto risalire circa al 30.000 a.C. Col tempo furono introdotti diversi simboli e parole per indicare i numeri naturali e in diversi casi anche alcuni tipi di frazioni. Esistono simboli risalenti agli antichi Egizi che indicano frazioni unitarie, cioè con numeratore uguale a uno. Se ne possono trovare ad esempio nel papiro di Rhind risalente circa al 2000 a.C. Tuttavia il numero zero dovette aspettare più tempo per venire considerato un numero al pari degli altri.Le origini dell'idea di numero naturale astratto vengono fatte risalire ai Babilonesi nel 2000 a.C., come testimoniato dalla tavoletta "Plimpton 322", "sussidiario di matematica" per gli studenti dell'epoca, che contiene problemi matematici che a un'attenta analisi sembrano essere qualcosa di più di semplici esercizi con fini utilitaristici. Il superamento dei numeri naturali in favore dei numeri razionali positivi è attribuito ai pitagorici che sembra furono i primi a considerare la frazione non più come entità unica ma come rapporto tra numeri naturali.
Importanti risultati riguardanti i numeri naturali sono contenuti negli Elementi di Euclide, successivamente Diofanto di Alessandria si pose il problema della ricerca di soluzioni intere positive di equazioni date.
L'introduzione dei numeri interi relativi, in particolare dei numeri negativi dovette aspettare ulteriormente. Risultati e spunti fondamentali sono dovuti a Pierre de Fermat. Lo studio dei numeri interi, noto oggi come teoria dei numeri, viene ripreso nel XIX secolo da matematici del livello di Carl Friedrich Gauss e Carl Jacobi e da allora viene considerato un capitolo primario della matematica (vedi anche: Ultimo teorema di Fermat).
In matematica si usa il simbolo formula_1 (o N) per indicare l'insieme dei numeri naturali. Nella maggior parte della letteratura matematica contemporanea, nelle voci qui presenti e nello standard ISO 31-11 sui simboli matematici, si assume che l'insieme dei numeri naturali contenga anche lo zero; per evitare ogni ambiguità è spesso usata la dizione "interi non negativi". Per mettere in evidenza che l'insieme non contiene lo formula_2 si usa la scrittura formula_3, quindi
Per indicare l'insieme dei naturali senza lo zero si possono usare anche le scritture N, N, N, ℕ, ℕ, formula_5. Talvolta con la notazione formula_5 si indica invece l'insieme dei naturali con lo zero incluso.
Nella teoria degli insiemi, l'insieme dei numeri naturali in quanto insieme bene ordinato viene denotato con formula_7, e rappresenta il più piccolo numero ordinale infinito. Quando è usata questa notazione, lo zero è incluso.
Nonostante la sua intuitività, quello di numero naturale non è, in matematica, un concetto primitivo: è infatti possibile darne una definizione basandosi unicamente sulla teoria degli insiemi. La definizione è utile perché permette anche di estendere il concetto di numero a oggetti più generali: i numeri transfiniti.
Storicamente, la precisa definizione matematica dei numeri naturali ha incontrato alcune difficoltà. Gli assiomi di Peano definiscono le condizioni che ogni definizione matematica precisa deve soddisfare. Alcune costruzioni mostrano che dall'interno di una teoria degli insiemi è possibile costruire un modello degli assiomi di Peano.
Bisogna notare che lo "0", nella definizione sopra descritta, non deve necessariamente corrispondere con quello che si considera normalmente il numero zero. "0" significa semplicemente un oggetto che, quando combinato con una funzione successiva appropriata, soddisfa gli assiomi di Peano. Ci sono molti sistemi che soddisfano questi assiomi, inclusi i numeri naturali (sia che partano da zero o da uno).
Un numero naturale si può definire come una classe di insiemi aventi uguale cardinalità finita. In sostanza, si parte dalla proprietà (intuitiva) che tra due insiemi qualsiasi aventi lo stesso numero di elementi si può stabilire una corrispondenza biunivoca e la si riformula come definizione: tutti gli insiemi tra i quali si può stabilire una corrispondenza biunivoca vengono accomunati in una classe, che è come assegnare loro un'"etichetta", a questa etichetta viene dato il nome di numero naturale. La classe corrispondente all'insieme vuoto viene indicata con 0.
La seguente è una costruzione standard nella teoria degli insiemi per definire i numeri naturali:
</text>
</doc>
<doc id="23289" url="https://it.wikipedia.org/wiki?curid=23289">
<title>Numero intero</title>
<text>
I numeri interi (o numeri interi relativi o, semplicemente, numeri relativi) corrispondono all'insieme ottenuto unendo i numeri naturali (0, 1, 2, ...) e i numeri interi negativi (−1, −2, −3...), cioè quelli ottenuti ponendo un segno “−” davanti ai naturali. Questo insieme in matematica viene indicato con Z o formula_1, perché è la lettera iniziale di “"Zahl"” che in tedesco significa numero (originariamente "far di conto", infatti l'espressione implica l'utilizzo dei numeri negativi). 
Gli interi vengono quindi definiti esattamente come l'insieme dei numeri che sono il risultato tra sottrazioni di numeri naturali. I numeri interi possono essere sommati, sottratti e moltiplicati e il risultato rimane un numero intero. L'inverso di un numero intero non è però un intero in generale, ma un numero razionale; formalmente questo fatto si esprime dicendo che formula_1 è un anello commutativo, ma non un campo.
Come i numeri naturali, formula_1 è "chiuso" rispetto alle operazioni di addizione e di moltiplicazione, cioè la somma o il prodotto di due interi è un intero. Inoltre, con l'inclusione dei numeri naturali negativi e dello zero, formula_1 (a differenza dei numeri naturali) è chiuso anche rispetto all'operazione di sottrazione: se formula_5 e formula_6 sono interi, anche formula_7 lo è. Tuttavia, formula_1 non è chiuso sotto l'operazione di divisione, poiché il quoziente di due interi (per esempio formula_9) non è necessariamente un numero intero.
La tabella seguente elenca alcune delle proprietà di base dell'addizione e della moltiplicazione per ogni intero formula_5, formula_6 e formula_12.
Nel linguaggio dell'algebra astratta, le prime cinque proprietà elencate sopra per l'addizione dicono che formula_1 è un gruppo abeliano con l'operazione "somma". In particolare, formula_1 è un gruppo ciclico, poiché ogni intero non nullo può essere scritto sommando un certo numero di volte formula_15 oppure formula_16. Il gruppo formula_1 è l"'unico" gruppo ciclico infinito, nel senso che ogni altro gruppo ciclico infinito è isomorfo a formula_1.
Le prime quattro proprietà elencate sopra per la moltiplicazione dicono che formula_1 con l'operazione "prodotto" forma un monoide commutativo. Tuttavia, si nota che non tutti gli interi hanno in inverso rispetto alla moltiplicazione; per esempio non esiste un intero formula_20 tale che formula_21. Quindi formula_1 non è un gruppo se considerato con l'operazione "prodotto".
Tutte le proprietà dalla tabella prese insieme dicono che formula_1 con l'addizione e la moltiplicazione è un anello commutativo con unità. In effetti formula_1 è la motivazione principale per la definizione di tale struttura. La mancanza dell'inverso rispetto alla moltiplicazione è tradotta nel fatto che formula_1 non è un campo. 
L'anello formula_1 è inoltre un dominio d'integrità, perché non contiene divisori dello zero. Ogni dominio di integrità è contenuto in un campo, e il più piccolo campo contenente gli interi è il campo formula_27 dei numeri razionali.
Anche se la divisione ordinaria non è definita su formula_1, è possibile usare l'algoritmo di Euclide per effettuare una divisione con resto: dati due interi formula_5 e formula_6 con formula_31, esistono e sono unici due interi formula_32 e formula_33 tali che 
dove formula_35 è il valore assoluto di formula_6. L'intero formula_32 è chiamato il "quoziente" e formula_33 è chiamato il "resto", risultanti dalla divisione di formula_5 con formula_6. 
L'algoritmo di Euclide mostra come due numeri interi abbiano sempre un massimo comune divisore ed un minimo comune multiplo. Inoltre, per il teorema fondamentale dell'aritmetica ogni numero intero ha un'unica decomposizione come prodotto di numeri primi. L'esistenza dell'algoritmo di Euclide fa di formula_1 un anello euclideo.
La cardinalità di un insieme di interi è equivalente a formula_42 (aleph-zero). Ciò è dimostrabile tramite la costruzione di una corrispondenza biunivoca (ovvero una funzione sia iniettiva che suriettiva) fra formula_43 e formula_44.
Considerando formula_45, tale corrispondenza è la funzione formula_46 tale che:
Mentre considerando formula_48 è la funzione formula_49 tale che:
Ogni membro di formula_1 avrà uno ed un solo membro corrispondente in formula_52 (o formula_53), pertanto i due insiemi hanno la stessa cardinalità.
L'insieme formula_1 è un insieme totalmente ordinato senza estremo superiore o inferiore. L'ordine di formula_1 è dato da
Un numero intero è positivo se è maggiore dello zero e negativo se minore di zero; zero non è considerato un numero positivo né negativo.
L'ordine seguente è compatibile con le regole dell'algebra:
Più semplicemente: se formula_60 e formula_61 sono due qualsiasi numeri relativi si dice che formula_60 è maggiore di formula_61, e si scrive formula_64, se esiste un numero naturale formula_65 tale che formula_66. L'insieme formula_1 può essere definito a partire dall'insieme formula_52 dei numeri naturali tramite il concetto di insieme quoziente. Si consideri il prodotto cartesiano formula_69, ovvero l'insieme di tutte le coppie ordinate di numeri naturali formula_70. Si consideri la seguente relazione formula_71
Questa è una relazione di equivalenza, infatti è:
Si definisce formula_1 come l'insieme quoziente di formula_87 con la relazione formula_71:
A questo punto è facile dimostrare che ogni classe di equivalenza formula_90 contiene uno e un solo elemento nella forma formula_91 con formula_92 oppure formula_93. In questo modo possiamo introdurre la notazione più familiare per i numeri interi nel modo seguente:
Si dimostra facilmente che esiste un isomorfismo tra l'insieme dei numeri naturali e il sottoinsieme di formula_1 costituito dagli elementi del tipo formula_98. In questo senso si può dire che i numeri naturali sono un sottoinsieme dei numeri interi.
Le operazioni di somma e prodotto possono essere definite nel modo seguente:
Si verifica che le operazioni sono compatibili con la relazione d'equivalenza, e che si traducono nelle normali operazioni di somma e prodotto degli interi tramite la notazione appena introdotta. Ad esempio:
Si può anche dimostrare direttamente che l'insieme formula_1 con queste operazioni è un anello commutativo.
</text>
</doc>
<doc id="23290" url="https://it.wikipedia.org/wiki?curid=23290">
<title>Numero razionale</title>
<text>
In matematica, un numero razionale è un numero ottenibile come rapporto tra due numeri interi primi fra loro, il secondo dei quali diverso da 0. Ogni numero razionale quindi può essere espresso mediante una frazione "a/b", di cui a è detto il numeratore e b il denominatore. Sono ad esempio numeri razionali i seguenti:
I numeri razionali formano un campo, indicato con il simbolo formula_4, che sta per quoziente. In gran parte dell'analisi matematica i numeri razionali sono visti come particolari numeri reali, nel senso che esiste un isomorfismo tra i numeri reali dotati di parte decimale finita o periodica e i numeri razionali, il quale preserva la struttura di formula_4 come (sotto)-campo di formula_6; i numeri reali che non sono razionali sono detti "irrazionali". Ad esempio, sono irrazionali i seguenti:
Nessuno di questi numeri può infatti essere descritto come rapporto di due numeri interi. I numeri formula_8 e formula_9 indicano rispettivamente la costante di Nepero e pi greco.
Mentre oggi spesso l'insieme dei numeri razionali è visto come sottoinsieme di quello dei numeri reali, storicamente e naturalmente i razionali sono stati introdotti prima dei reali, per permettere l'operazione di divisione fra numeri interi. I numeri reali si possono introdurre servendosi dei numeri razionali in vari modi: mediante le sezioni di Dedekind, con una costruzione tramite successioni di Cauchy, con serie convergenti di numeri razionali.
In fisica, il risultato di una misurazione è solitamente esprimibile come numero razionale, dipendente dalla precisione dello strumento.
I numeri razionali (positivi) furono il primo tipo di numeri, dopo i naturali (ossia gli interi positivi) ad essere riconosciuti come numeri e ad essere comunemente usati in matematica.
Gli antichi Egizi li usavano scomponendoli come somme di frazioni dal numeratore unitario (ancora oggi chiamate frazioni egiziane), rappresentandoli ponendo un simbolo sopra la rappresentazione dell'intero corrispondente; i Babilonesi usavano invece una scrittura posizionale (come per gli interi) a base sessagesimale.
Pitagora e i pitagorici basavano la loro concezione del mondo sui rapporti tra numeri interi, ovvero sui numeri razionali, e pensavano che ogni cosa esistente al mondo potesse essere ridotta a tali numeri: la loro scoperta dell'irrazionalità della radice quadrata di due distrusse questa concezione. Lo stesso concetto di "rapporto" non è del tutto chiaro nemmeno negli "Elementi" di Euclide, dove l'intero quinto libro è dedicato alla teoria delle proporzioni. Secondo le sue definizioni, un rapporto è un "tipo di relazione dimensionale tra due grandezze dello stesso tipo", mentre due grandezze possono essere poste in rapporto se "esiste un multiplo intero della prima che supera l'altro" (definizione dovuta probabilmente a Eudosso, che ricalca quello che viene oggi chiamato assioma di Archimede). L'uguaglianza di rapporti implica un'altra definizione complicata: in notazione moderna, equivale a dire che formula_12 se e solo se, dati due numeri "m" ed "n", si ha che
La definizione di "grandezze commensurabili" è invece la prima del libro X, e stabilisce che queste sono le grandezze che hanno una misura comune, ovvero sono multipli interi dello stesso numero.
La notazione decimale dei numeri fu introdotta da Stevino verso la fine del XVI secolo, sebbene lui non accettasse sviluppo decimali che non si concludessero, lasciando fuori così un gran numero di razionali. Più tardi Clavius e Nepero eliminarono questa limitazione.
Il termine "razionale" deriva dal latino "ratio", nel suo significato di "rapporto".
Molte entità e strutture matematiche, come i polinomi o gli spazi vettoriali, nella loro definizione fanno riferimento ad un campo; l'aggettivo "razionale" attribuito ad una di queste entità è spesso usato per specificare che il campo scelto è quello dei numeri razionali. Per esempio si dice polinomio razionale ogni polinomio i cui coefficienti sono solo numeri razionali.
Va rilevato che sono dette "operazioni razionali" le quattro operazioni di addizione, sottrazione, moltiplicazione e divisione definite su strutture algebriche come i campi o gli anelli. Ne consegue che in vari casi l'aggettivo "razionale" riguarda entità ottenibili servendosi delle quattro operazioni razionali a partire da certi oggetti di base. Ad esempio si dicono funzioni razionali (in una o più variabili) le funzioni ottenibili componendo con operazioni razionali la variabile o le variabili e gli elementi di un campo.
Da un punto di vista formale, non è possibile definire i numeri razionali semplicemente come coppie di numeri interi (cioè come l'insieme delle frazioni del tipo formula_13), perché in questo caso, ad esempio, le coppie (3,2) e (6,4) sarebbero numeri diversi, mentre tra i razionali vale l'uguaglianza
È necessario quindi introdurre le nozioni di relazione e classe d'equivalenza, nel modo seguente.
Ogni numero razionale è una classe di equivalenza di coppie ordinate di numeri interi
formula_15, con formula_16 diverso da zero. La relazione di equivalenza è la seguente
L'addizione e la moltiplicazione di numeri razionali sono definite come
Si verifica che entrambe le operazioni così definite sono compatibili con la relazione di equivalenza: il loro risultato, infatti, non dipende dalle particolari coppie ordinate scelte per indicare i numeri razionali da sommare o moltiplicare. L'insieme quoziente di questa relazione è quindi Q.
Si noti che le operazioni ora definite non sono altro che la formalizzazione delle consuete operazioni tra frazioni:
Con le operazioni di cui sopra, formula_20 risulta un campo, ove la classe di formula_21 gioca il ruolo dello zero, e la classe di formula_22 quello di uno. L'opposto della classe di formula_23 è la classe di formula_24. Inoltre, se formula_25, ovvero la classe di formula_23 è diversa da zero, allora la classe di formula_23 è invertibile, ed ha per inverso la classe di formula_28.
La classe di equivalenza corrisponde all'esistenza di più rappresentazioni come frazione dello stesso numero razionale:
per ogni "k" intero non nullo.
Possiamo definire anche un ordine totale su Q nel modo seguente: 
Come tutti i numeri reali, i numeri razionali possono essere rappresentati tramite il sistema numerico decimale. Lo sviluppo decimale dei numeri razionali ha la particolarità di essere "periodico": un numero reale è razionale se e solo se nella sua scrittura esiste una sequenza finita di cifre (detta "periodo") che si ripete all'infinito, da un certo punto in poi dopo la virgola.
Si può facilmente dimostrare che nessun numero razionale, nel suo sviluppo decimale in base 10, può ammettere periodo 9.
Ad esempio:
Un numero razionale può essere descritto quindi "soprallineando" il periodo, come in questi esempi.
Questa equivalenza tra razionali e numeri periodici implica che nessun numero razionale è normale in una qualunque base. Può essere usata anche per dimostrare l'irrazionalità di molti numeri: ad esempio
dove ogni 1 è separato da una sequenza di zeri di lunghezza crescente, è irrazionale in qualsiasi base, in quanto, se fosse razionale, il suo periodo conterrebbe una sequenza finita di zeri separati da 1. Tuttavia nell'espansione possono essere trovati gruppi di zeri di qualsiasi lunghezza, e quindi un periodo di tal genere non può esistere. Con metodi simili si può dimostrare che la costante di Copeland-Erdős
formula_36
formata, in base dieci, dalla giustapposizione dei numeri primi, è irrazionale.
Questa tecnica è tuttavia inutile per provare l'irrazionalità di numeri non definiti in base alla loro espansione decimale, come formula_37 e pi greco.
I numeri razionali hanno una rappresentazione in frazione continua semplice finita, e sono gli unici a possedere questa proprietà. Inoltre sono gli unici in cui la rappresentazione non è unica, ma doppia: ad esempio
Munito di addizione, moltiplicazione e relazione d'ordine, l'insieme formula_39 ha la struttura algebrica di un campo ordinato archimedeo, e tuttavia non è un campo completo (si può dimostrare che il sottoinsieme formula_40 ha come estremante superiore il valore formula_37, che non è un numero razionale).
L'unico sottocampo del campo dei numeri razionali è se stesso. Gli elementi neutri per la somma ed il prodotto sono rispettivamente 0 ed 1. La caratteristica del campo è 0; si può dimostrare inoltre che ogni campo con caratteristica 0 contiene un sottocampo isomorfo ai numeri razionali, e quindi che ogni campo di questo tipo può essere considerato come un'estensione dei razionali. In particolare, i razionali ne formano il sottocampo fondamentale.
La chiusura algebrica dei numeri razionali non è formata dai numeri reali, ma dai numeri algebrici, i quali formano uno spazio vettoriale di dimensione infinita sui razionali.
Il campo dei numeri razionali è inoltre il campo dei quozienti dell'insieme formula_42 dei numeri interi.
Per il teorema di Ostrowski, i razionali sono uno spazio metrico rispetto solo a due tipi di valore assoluto: l'usuale modulo
e il valore assoluto "p"-adico
dove "p" è un qualsiasi numero primo e "n" è tale che formula_45 e "a", "b" e "p" sono a due a due coprimi. Le norme riferiti a questi due valori assoluti sono rispettivamente
e
I razionali non sono completi rispetto a nessuna di queste due norme: i completamenti sono rispettivamente i numeri reali e i numeri "p"-adici. Quest'ultimo è particolarmente usato in teoria dei numeri, mentre l'introduzione dei numeri reali è necessaria per poter stabilire alcuni teoremi fondamentali dell'analisi, tra cui il teorema degli zeri e il teorema di Weierstrass.
formula_4 è numerabile, cioè esiste una corrispondenza biunivoca tra i razionali e i numeri naturali. Questo risultato, apparentemente paradossale (è naturale, infatti, pensare che le frazioni siano "molte di più" degli interi), è stato dimostrato da Georg Cantor. Il suo ragionamento si basa sul diagramma a fianco: possiamo infatti ordinare i razionali positivi, seguendo le frecce, in modo che ad ognuno di essi sia assegnato un numero naturale; anzi, ogni numero sarà contato infinite volte (perché ognuno ha un'infinità di rappresentazioni diverse), ma questo non può rendere l'insieme formula_4 più grande. Lo stesso argomento può essere usato per dimostrare che i razionali negativi sono numerabili. Poiché l'unione di due insiemi numerabili è ancora numerabile, formula_4 risulta essere numerabile.
Al contrario, l'insieme dei numeri reali non è numerabile, e quindi "quasi tutti" i numeri reali sono irrazionali. Questo implica che, sebbene formula_4 sia denso in formula_6, abbia misura di Lebesgue nulla.
L'anello dei polinomi a coefficienti razionali si indica con formula_53. Al contrario dei polinomi a coefficienti reali o complessi, non esiste un criterio semplice per individuare l'eventuale irriducibilità di un polinomio a coefficienti razionali.
La maggior parte dei criteri usati si basano sul lemma di Gauss, il quale afferma che un polinomio a coefficienti interi è
riducibile nell'anello formula_53 se e solo se è riducibile in fattori di grado maggiore di 0 nell'anello formula_55 dei polinomi a coefficienti interi. Poiché ogni polinomio a coefficienti razionali può essere trasformato in uno a coefficienti interi moltiplicando per il massimo comun divisore dei denominatori senza cambiare la sua irriducibilità, questo lemma permette di applicare ai polinomi a coefficienti razionali alcuni criteri, come il criterio di Eisenstein, che si applicano sui polinomi a coefficienti interi.
In particolare, questo criterio permette di costruire polinomi irriducibili di qualunque grado: ad esempio
è irriducibile. Questo non avviene negli anelli di polinomi a coefficienti reali o complessi: nel primo caso i polinomi irriducibili possono essere solamente di primo o di secondo grado, mentre nel caso complesso, in conseguenza del teorema fondamentale dell'algebra, ogni polinomio si scompone in fattori di primo grado.
Al contrario di quanto avviene con le radici reali (o complesse), esiste un algoritmo molto veloce per stabilire quali siano (se esistono) gli zeri razionali di un polinomio (a coefficienti interi, forma a cui può essere ridotto ogni polinomio a coefficienti razionali). Il "teorema delle radici razionali" afferma infatti che, se
con gli formula_58 interi, allora, nelle eventuali radici razionali "p"/"q", "p" è un divisore di formula_59 e "q" di formula_60. Poiché i divisori di questi due numeri sono in numero finito, sarà sufficiente, per il teorema del resto, controllare se per ogni coppia di divisori si ha P("p"/"q")=0 (nel qual caso "p"/"q" è una radice) oppure no.
I "razionali complessi", o "razionali gaussiani" per analogia con gli interi gaussiani, sono quei numeri complessi nella forma "a+ib", dove "a" e "b" sono razionali e "i" rappresenta l'unità immaginaria. L'insieme dei razionali gaussiani forma un campo, che è il campo dei quozienti dell'anello degli interi gaussiani.
Tale insieme si denota generalmente con formula_61, cioè il più piccolo campo contenente i razionali e l'unità immaginaria "i".
Poiché i razionali sono densi in formula_6, possono essere usati per approssimare i numeri reali. Il primo risultato ad essere dimostrato è che per ogni irrazionale formula_63 esistono infiniti razionali "p/q" tali che
Un risultato importante è il teorema di Liouville, dimostrato nel 1844 da Joseph Liouville: esso asserisce che se formula_63 è un numero algebrico di grado "n", allora esiste una costante "c"&gt;0 tale che
per ogni razionale "p"/"q". Da questo Liouville riuscì a costruire i primi esempi di numeri trascendenti (detti oggi numeri di Liouville), mostrando che per questi esistevano delle successioni di razionali che rendevano impossibile l'esistenza di un tale "c".
Nel 1955 Klaus Roth dimostrò che per ogni algebrico formula_63 e per ogni formula_68 la disuguaglianza
può avere solamente un numero finito di soluzioni in cui "p" e "q" sono interi coprimi. Tale risultato migliorava quelli ottenuti in precedenza da Axel Thue e Carl Ludwig Siegel.
</text>
</doc>
<doc id="1419" url="https://it.wikipedia.org/wiki?curid=1419">
<title>Coefficiente binomiale</title>
<text>
In matematica, il coefficiente binomiale formula_1 (che si legge "formula_2 su formula_3") è un numero intero non negativo definito dalla seguente formula
e può essere calcolato anche facendo ricorso al triangolo di Tartaglia. Alla voce Combinazione è dimostrato che esso fornisce il numero delle combinazioni semplici di formula_2 elementi di classe formula_3.
Per esempio:
è il numero di combinazioni di formula_10 elementi presi formula_11 alla volta.
Il coefficiente binomiale ha le seguenti proprietà:
formula_62. Esempio:
formula_63
Si può estendere il coefficiente binomiale al caso che formula_3 sia negativo, oppure maggiore di formula_2, ponendo:
Si può anche estendere il coefficiente ai numeri reali. A tale scopo, può convenire iniziare con l'osservazione che il coefficiente binomiale è anche il rapporto tra il numero delle funzioni iniettive da un insieme di cardinalità formula_3 in uno di cardinalità formula_2 (ovvero il numero delle disposizioni semplici di formula_2 oggetti di classe formula_3) ed il numero delle permutazioni di formula_3 oggetti:
Si può porre:
ad esempio,
Con tale convenzione, si ha:
ad esempio:
Si può notare che per formula_78 il coefficiente binomiale equivale alla somma dei primi formula_79 numeri naturali:
</text>
</doc>
<doc id="38526" url="https://it.wikipedia.org/wiki?curid=38526">
<title>Equazione algebrica</title>
<text>
In matematica si chiamano equazioni algebriche o polinomiali quelle equazioni equivalenti (o riconducibili tramite opportune trasformazioni) ad un polinomio uguagliato a zero. Il grado di tale polinomio è anche il grado dell'equazione.
Un'equazione polinomiale di grado formula_1 in una incognita si può esprimere nella forma:
dove gli formula_3 sono numeri reali (o in generale complessi) e formula_4 è l'incognita da determinare. Il tipo più semplice di equazioni algebriche sono le equazioni lineari, cioè di primo grado.
In virtù del teorema fondamentale dell'algebra ogni equazione di grado formula_1 ammette esattamente formula_1 soluzioni nel campo complesso.
Il criterio di Cartesio stabilisce il numero massimo di soluzioni nel campo reale per un'equazione di grado formula_1: il massimo numero di soluzioni reali positive è dato dal numero di variazioni di segno fra coefficienti consecutivi formula_8, trascurando eventuali coefficienti nulli.
Le equazioni di secondo grado sono chiamate quadratiche; seguono le cubiche e le quartiche. Per il teorema di Abel-Ruffini le equazioni di grado superiore al quarto non sono generalmente risolvibili per radicali. 
Tra le equazioni particolari di grado superiore al terzo, si ricordano: 
</text>
</doc>
<doc id="38596" url="https://it.wikipedia.org/wiki?curid=38596">
<title>Equazione lineare</title>
<text>
Un'equazione lineare, o equazione di primo grado, è un'equazione algebrica in cui il grado massimo delle incognite è uguale a uno.
Quelle a una sola incognita sono riconducibili (tramite le usuali regole dell'algebra elementare) alla cosiddetta "forma normale" (o canonica):
dove formula_2 e formula_3 sono numeri reali o complessi.
Se formula_4 allora trasportando formula_3 al secondo membro e dividendo per formula_2 si ottiene:
L'equazione di primo grado ammette dunque una e una sola soluzione, pari a formula_8.
Se invece formula_9 allora l'equazione può essere impossibile o indeterminata:
Più in generale, un'equazione lineare in formula_17 incognite formula_18 è riconducibile alla forma:
In geometria analitica, un'equazione lineare a due incognite (scritta in genere nella forma formula_20 oppure formula_21) rappresenta una retta nel piano cartesiano. Nello spazio a tre dimensioni, un'equazione in tre incognite della forma formula_22 rappresenta un piano. In generale, nello spazio euclideo formula_17-dimensionale, l'insieme delle soluzioni di un'equazione lineare in formula_17 incognite rappresenta un iperpiano, cioè uno spazio ad formula_25 dimensioni. Allo stesso modo un'equazione lineare a una sola incognita rappresenta un semplice punto.
</text>
</doc>
<doc id="37548" url="https://it.wikipedia.org/wiki?curid=37548">
<title>Iperbole (geometria)</title>
<text>
n matematica, e in particolare in geometria, l'iperbole () è una delle sezioni coniche. 
tale che formula_8, dove tutti i coefficienti sono reali, e dove esiste più di una soluzione che definisce una coppia formula_9 di punti dell'iperbole.
L'equazione generale dell'iperbole si specializza e si semplifica in alcuni casi particolari.
Se l'iperbole soddisfa le seguenti condizioni:
allora la sua equazione sarà del tipo:
se invece l'iperbole soddisfa le prime due condizioni sopracitate, ma interseca l'asse delle ordinate, avrà un'equazione del tipo:
In entrambi i casi gli asintoti dell'iperbole hanno equazione formula_12.
Se gli asintoti sono perpendicolari (e quindi, nel caso dell'iperbole avente gli assi coincidenti con gli assi cartesiani, se formula_13), l'iperbole si dice iperbole equilatera. Se l'iperbole ha asintoti perpendicolari, ma non coincidenti con gli assi, allora essa sarà definita da una funzione omografica. Data un'iperbole equilatera, di asintoti formula_14 ed formula_15, il limite della sua funzione per formula_16 che tende ad formula_17 ed formula_18 che tende a formula_19, sarà infinito, graficamente cioè, l'iperbole non ha nessun punto di intersezione con i suoi asintoti, se non all'infinito.
Se un'iperbole equilatera viene riferita ai propri asintoti (e cioè se gli asintoti dell'iperbole coincidono con gli assi cartesiani), allora la sua equazione assume una forma molto semplice:
Se formula_21 è diverso da zero, a tale curva è associata la "funzione di proporzionalità inversa" formula_22. 
Se formula_23 la curva degenera nell'insieme formato dai due assi cartesiani, individuati dall'equazione formula_24.
I vari elementi associati ad un'iperbole sono:
L'iperbole che interseca l'asse delle formula_16 e avente centro nel punto formula_26, (quindi traslata) ha equazione
Se si applica una rotazione degli assi di 90 gradi, si ottiene l'equazione:
In entrambe le formule formula_17 è detto semiasse trasverso o semiasse maggiore; è la metà della distanza tra i due rami; formula_19 è chiamato semiasse non trasverso o semiasse minore. Si noti che, qualora si faccia uso dei secondi nomi, formula_19 può essere maggiore di formula_17; questa incongruenza viene risolta da alcuni testi invertendo le costanti formula_17 e formula_19. In questo caso l'equazione dell'iperbole che interseca l'asse delle formula_18 viene scritta come:
La distanza tra i due fuochi è pari a formula_37 dove:
L'eccentricità dell'iperbole può essere definita da:
I coefficienti angolari delle tangenti a un'iperbole formula_40: formula_41 condotte da un punto formula_42 ad essa esterno si ricavano dalla risoluzione della seguente equazione di secondo grado:
con formula_44 e formula_45.
L'iperbole equilatera con centro in formula_46 ha equazione formula_47. Il caso generale, di un'iperbole equilatera traslata, è descritta da un caso particolare della cosiddetta funzione omografica di equazione formula_48. essa ha il centro in formula_49 (centro della funzione omografica). inoltre gli asintoti di tale curva hanno equazione formula_50 (per quanto riguarda l'asintoto verticale) e formula_51 per l'asintoto orizzontale.
Questa prima parametrizzazione può essere ricavata geometricamente nel seguente modo: si sceglie uno dei due asintoti dell'iperbole (per esempio formula_57) e consideriamo tutte le rette parallele ad esso (cioè un fascio di rette parallele improprio). Ogni retta di questo fascio intersecherà l'altro asintoto in un punto generico di coordinate formula_58. Quindi il fascio di rette improprio avrà equazione formula_59 Ora si interseca esso con l'iperbole canonica formula_10 ottenendo il punto formula_61.
Ponendo formula_62 otteniamo formula_63 e applicando la sostituzione inversa formula_64.
Come l'Ellisse anche l'Iperbole ha funzioni Parametriche Trigonometriche. Per un Punto P(x;y) dell'Iperbole esse sono:
formula_67 
quadrando e sommando: 
formula_68 
Dove l'ultima espressione è l'Equazione conica dell'Iperbole. 
Gli angoli dell'equazione conica e quella parametrica hanno legame: 
formula_69
L'equazione generale delle iperboli con semiasse maggiore formula_17 dove i fuochi sono posti in posizione generica nel piano e siano formula_71 ed formula_72 è rappresentata dalla seguente equazione delle coniche:
I parametri sono dati dai seguenti valori:
</text>
</doc>
<doc id="37746" url="https://it.wikipedia.org/wiki?curid=37746">
<title>Equazione</title>
<text>
Un'equazione (dal latino "aequatio") è una uguaglianza matematica tra due espressioni contenenti una o più variabili, dette "incognite". L'uso del termine risale a Leonardo Fibonacci.
Se un'equazione ha formula_1 incognite, allora ogni formula_1-upla (ordinata) di elementi che sostituiti alle corrispondenti incognite rendono vera l'uguaglianza è una "soluzione" dell'equazione. Risolvere un'equazione significa individuare l'insieme di tutte le sue soluzioni.
Il "dominio" (o "insieme di definizione") delle variabili incognite è l'insieme degli elementi per cui le espressioni ad ambo i membri dell'equazione sono definite, ovvero quell'insieme di numeri per cui l'equazione esiste. L'insieme delle soluzioni è condizionato dal dominio: per esempio l'equazione
non ammette soluzioni se il dominio è l'insieme dei numeri razionali, mentre ammette due soluzioni nei numeri reali, che possono essere scritte come formula_4. Analogamente, l'equazione
non possiede soluzioni reali ma è risolvibile se il dominio è il campo dei numeri complessi.
Due equazioni si dicono "equivalenti" se i rispettivi insiemi delle soluzioni coincidono. Vi sono due principi che consentono di manipolare le equazioni per trovare l'insieme delle soluzioni; essi sono una conseguenza diretta delle proprietà delle uguaglianze:
Tipicamente in un'equazione compaiono, oltre alle incognite, dei coefficienti noti che moltiplicano le incognite stesse e dei termini noti che sono ad esse applicati tramite somma algebrica: questi elementi, se non sono esplicitati nel loro valore numerico, sono indicati in genere con le lettere formula_13, formula_14, formula_15... mentre alle variabili incognite sono convenzionalmente attribuite le ultime lettere dell'alfabeto (formula_16, formula_17, formula_18...).
Le soluzioni di un'equazione vengono generalmente indicate esplicitando le incognite delle espressioni che contengano le costanti ed eventuali parametri arbitrari. Ad esempio, la soluzione dell'equazione
dove formula_13 è un parametro non nullo, e il dominio è l'insieme dei numeri reali, si scrive come
Un'equazione si dice:
Dal teorema fondamentale dell'algebra, segue immediatamente che un'equazione polinomiale (cioè formata da un polinomio uguagliato a zero, in una variabile) di grado formula_1 ammette sempre formula_1 soluzioni nel campo complesso, di cui alcune possono essere multiple. In altre parole, un'equazione di grado formula_1 ammette almeno formula_25 soluzione e al massimo formula_1 soluzioni complesse differenti.
Per il teorema di Abel-Ruffini, non esiste una formula generale per esprimere le radici delle equazioni polinomiali di grado formula_27 o superiore tramite una formula per radicali. Viceversa le equazioni di primo grado, secondo grado, terzo grado e quarto grado ammettono una formula risolutiva generica. Casi particolari di equazioni di grado superiore al quarto possono comunque essere risolti tramite radicali.
Il metodo delle tangenti di Newton, sotto determinate ipotesi, fornisce un algoritmo per la risoluzione numerica delle equazioni. Un altro algoritmo con ipotesi più generali è il metodo di bisezione. Le soluzioni trovate mediante metodi numerici vengono chiamate "approssimate" in contrapposizione alle soluzioni date da formule chiuse che vengono chiamate "esatte". La nomenclatura è in parte fuorviante perché i metodi approssimati possono, in alcuni casi, determinare in maniera più precisa e più veloce le soluzioni numeriche di una equazione rispetto alle formule chiuse.
Una prima classificazione delle equazioni può avvenire in questo modo:
Le equazioni algebriche possono essere divise in vari gruppi in base alle loro caratteristiche; è necessario ricordare che un'equazione deve appartenere ad almeno e solo una delle categorie per ogni gruppo.
In base al grado del polinomio:
Possono inoltre essere divise in base alla presenza di incognite al radicando di radici:
Si definisce "equazione omogenea", un'equazione algebrica in più variabili i cui termini hanno tutti lo stesso grado. Un'equazione omogenea ammette sempre la soluzione banale con tutte le variabili uguali a formula_28 e, su un campo algebricamente chiuso, ammette sempre infinite soluzioni, infatti da ogni soluzione se ne ottengono infinite altre alterandole per un fattore di proporzionalità. Ad esempio:
ha per soluzioni, sul campo dei numeri complessi la coppia formula_30 e formula_31 e la coppia formula_32 e formula_33 con formula_34 e formula_35 numeri complessi qualsiasi.
Non è vero su un campo non algebricamente chiuso, infatti l'equazione omogenea
ammette come unica soluzione, sul campo dei numeri reali, la coppia formula_37 e formula_38.
Le equazioni trascendenti coinvolgono almeno un'incognita come argomento di una funzione non polinomiale. Le più comuni categorie di equazioni trascendenti sono:
Le equazioni con valori assoluti contemplano oltre le incognite la presenza del valore assoluto di espressioni algebriche o trascendenti. Possiamo aver quindi:
Le equazioni funzionali hanno almeno un'incognita che è una funzione. Le più comuni categorie di equazioni funzionali sono:
In base alla presenza di altre espressioni letterali tutte le equazioni possono essere divise in:
</text>
</doc>
<doc id="37747" url="https://it.wikipedia.org/wiki?curid=37747">
<title>Identità (matematica)</title>
<text>
Si dice identità, in matematica, un'uguaglianza tra due espressioni nelle quali intervengono una o più variabili, la quale è vera per tutti i valori che si possono attribuire alle variabili stesse, con il solo vincolo di rendere sensate le espressioni.
Viene enunciata mediante identità una gran parte dei risultati della matematica (teoremi, lemmi, corollari), delle ipotesi, delle condizioni, dei vincoli e delle affermazioni che sono solo allo stadio delle congetture.
è un'identità dell'algebra elementare.
è un'identità trigonometrica.
è un'identità concernente il polinomio di Legendre di grado "n"
</text>
</doc>
<doc id="2707085" url="https://it.wikipedia.org/wiki?curid=2707085">
<title>Logaritmo</title>
<text>
In matematica, il logaritmo di un numero in una data base è l'esponente al quale la base deve essere elevata per ottenere il numero stesso.
Per esempio, il logaritmo in base formula_1 di formula_2 è formula_3, poiché bisogna elevare formula_1 alla terza potenza per ottenere formula_2, ovvero formula_6. Più in generale, se formula_7, allora formula_8 è il logaritmo in base formula_9 di formula_10, cioè, scritto in notazione matematica,
I logaritmi furono introdotti da Nepero all'inizio del 1600, e trovarono subito applicazione nelle scienze e nell'ingegneria, soprattutto come strumento per semplificare calcoli con numeri molto grandi, grazie all'introduzione di "tavole di logaritmi".
La funzione formula_12 (logaritmo in base formula_9 di formula_14) è la funzione inversa della funzione esponenziale in base formula_9 data da formula_16
È di importanza fondamentale il logaritmo naturale, cioè il logaritmo che ha come base il numero di Nepero, indicato con formula_17 Il logaritmo naturale è l'inverso della funzione esponenziale formula_18
Dati due numeri reali positivi formula_9 e formula_10, con formula_21, si definisce "logaritmo" in base formula_9 di formula_10 l'esponente a cui elevare formula_9 per ottenere formula_25 Il numero formula_10 viene chiamato "argomento" del logaritmo. In altre parole, se formula_27
si scrive che
e si legge: formula_8 è il logaritmo in base formula_9 di formula_25
Le ipotesi su formula_9 e formula_10 sono necessarie per avere l'esistenza e l'unicità di formula_34 infatti:
Per esempio, formula_55 perché formula_56
I logaritmi possono anche essere negativi (a differenza della base e dell'argomento). Infatti
poiché formula_58
Dalle relazioni formula_59 e formula_60, che valgono qualsiasi sia la base formula_9, derivano le proprietà di base:
Inoltre, dalla definizione segue che:
Una delle più importanti proprietà dei logaritmi è che il logaritmo del prodotto di due numeri è la somma dei logaritmi dei due numeri stessi. Allo stesso modo, il logaritmo del quoziente di due numeri non è altro che la differenza tra i logaritmi degli stessi. In altre parole valgono
Applicando il logaritmo ad ambo i membri:
Il suo valore è ovviamente l'esponente stesso:
Inoltre, il logaritmo di un numero elevato a una certa potenza formula_72 è uguale a formula_72 moltiplicato per il logaritmo del numero stesso. Da questo discende che il logaritmo della radice formula_72-esima di un numero è uguale all'inverso di formula_72 per il logaritmo del numero, e che il logaritmo dell'inverso di un numero è l'opposto del logaritmo del numero stesso. In altre parole valgono le formule:
Noto il valore di un logaritmo in una base, è semplice calcolarne il valore in un'altra base (spesso le calcolatrici danno il logaritmo solo in basi formula_1 ed formula_80).
Se formula_10, formula_14, e formula_72 sono tutti numeri reali positivi (con formula_43 e formula_85):
dove "k" è una base qualsiasi. La formula può essere scritta nel modo seguente
e segue dalla relazione
Dalla formula del cambiamento di base, ponendo formula_89, si ricava la relazione seguente:
Anche se in linea di principio i logaritmi possono essere calcolati in qualunque base positiva e diversa da formula_45, quelle più utilizzate sono tre:
Il metodo dei logaritmi fu proposto dallo scozzese Nepero nel 1614, in un libro intitolato "Mirifici Logarithmorum Canonis Descriptio". Joost Bürgi inventò indipendentemente i logaritmi, ma pubblicò i suoi risultati sei anni dopo Nepero.
Per sottrazioni successive, Nepero calcolò formula_92 per formula_93 da formula_45 a formula_95; il risultato per formula_96 è approssimativamente formula_97, ovvero formula_98. Nepero poi calcolò il prodotto di questi numeri per formula_99, con formula_93 da formula_45 a formula_102. Questi calcoli, che occuparono 20 anni, gli permisero di trovare, per ogni numero intero formula_103 da 5 a 10 milioni, il numero formula_93 che risolve l'equazione
Nepero chiamò inizialmente questo valore un "numero artificiale", ma successivamente introdusse il nome "logaritmo", dalle parole del greco "logos", proporzione, e "arithmos", numero. Usando una notazione moderna, i calcoli di Nepero gli permisero di calcolare
dove l'approssimazione compiuta corrisponde alla seguente:
L'invenzione di Nepero fu subito largamente acclamata: i lavori di Bonaventura Cavalieri (Italia), Edmund Wingate (Francia), Xue Fengzuo (Cina) e Giovanni Keplero (Germania) permisero di diffondere velocemente l'idea.
Nel 1647, il fiammingo Gregorio di San Vincenzo collegò i logaritmi alla quadratura dell'iperbole, dimostrando che l'area formula_108 sottesa da formula_45 a formula_110 soddisfa
Il logaritmo naturale fu per la prima volta descritto da Nicolaus Mercator nel suo scritto "Logarithmotechnia" pubblicato nel 1668, anche se precedentemente l'insegnante di matematica John Speidell aveva compilato una tavola di logaritmi naturali nel 1619.
Intorno al 1730, Eulero definì la funzione esponenziale e la funzione logaritmo come
Eulero inoltre dimostrò che queste due funzioni erano una l'inversa dell'altra.
Semplificando calcoli complessi, i logaritmi contribuirono ampiamente all'avanzamento della scienza, e in particolare dell'astronomia. Lo strumento che ne permise l'uso pratico furono le tavole dei logaritmi. La prima di esse fu completata da Henry Briggs nel 1617, subito dopo l'invenzione di Nepero. Successivamente, furono scritte altre tavole con diversi scopi e precisione. In esse veniva elencato il valore di formula_114 e di formula_115 per ogni numero formula_14 in un certo intervallo, con una precisione fissata e con una base formula_10 scelta (solitamente formula_118). Per esempio, la tavola di Briggs conteneva il logaritmo in base formula_1 di tutti i numeri da formula_45 a formula_2, con una precisione di otto cifre decimali. La funzione formula_115, poiché inversa del logaritmo, venne chiamata "antilogaritmo".
Il prodotto e il quoziente di due numeri formula_8 e formula_124 venivano così calcolati con rispettivamente la somma e la differenza dei loro logaritmi. Il prodotto formula_125 è l'antilogaritmo della somma dei logaritmi di formula_8 e formula_124:
Il quoziente formula_129 è l'antilogaritmo della differenza dei logaritmi di formula_8 e formula_124:
Per compiere calcoli complessi con una buona precisione queste formule erano molto più veloci del calcolo diretto oppure dell'utilizzo di metodi precedenti, come quello di prostaferesi.
Anche il calcolo di potenze e di radici veniva semplificato, riducendosi a moltiplicazione e divisione di logaritmi:
e
Operando sui numeri reali, la "funzione logaritmo" è la funzione formula_135 definita da
La funzione ha come dominio l'intervallo formula_137 In figura sono disegnati tre esempi della funzione logaritmo con diversi valori per la base formula_10. La curva rossa è per la funzione con base formula_139 costante di Nepero (valore approssimato: formula_140).
Come si può notare dal grafico, il dominio della funzione logaritmo (l'insieme entro cui variano i valori delle formula_14), è l'intervallo formula_142; mentre il codominio, insieme in cui variano i valori delle formula_143, è formula_144.
La funzione logaritmo è derivabile e la sua derivata è la seguente:
dove "ln" è il logaritmo naturale, cioè con base formula_80. In particolare, la relazione seguente è fondamentale nel calcolo infinitesimale:
L'eguaglianza è dimostrabile usando la regola della funzione inversa:
La funzione inversa del logaritmo è la funzione esponenziale, la cui derivata coincide con se stessa:
Ne segue:
Si può utilizzare direttamente la definizione di derivata:
e, ricordando il limite notevole del logaritmo, si ottiene:
La derivata seconda della funzione logaritmo è
Se formula_156, questo valore è sempre negativo e la funzione è quindi funzione concava. Se formula_157 è invece sempre positivo e la funzione è convessa.
La funzione logaritmo è continua e quindi integrabile.
La funzione integrale del logaritmo, con base generica formula_10, è (applicando l'integrazione per parti):
dove formula_160 è la costante di integrazione, cioè una costante reale arbitraria.
La funzione logaritmo è analitica. Non è possibile però descrivere la funzione su tutto il suo dominio con una sola serie di potenze (come avviene ad esempio per la funzione esponenziale): lo sviluppo centrato in un punto formula_161 ha infatti raggio di convergenza formula_162 ed è quindi convergente solo nell'intervallo formula_163. Ad esempio, lo sviluppo in formula_164 è il seguente:
Per lo studio di funzioni esponenziali in cui è necessario estrapolare dati o parametri in modo semplice è possibile sfruttare la funzione logaritmo per ricavare una relazione implicita della funzione originale avente il vantaggio di essere lineare. Ad esempio, per una funzione descrivibile comeformula_166con "a" e "b" costanti è possibile pervenire alla relazione:formula_167che sul piano semi-logaritmico rappresenta una retta che interseca l'asse delle ordinate in "ln(a), "con derivata prima "b" e angolo di inclinazione pari ad "arctan(b)": in questo modo l'estrapolazione dei dati per la nuova funzione è più semplice ed accessibile.
La funzione logaritmo può essere estesa ai numeri complessi diversi da zero. Nel caso in cui si tratti di un logaritmo naturale con argomento complesso vale la formula seguente:
con formula_169 l'unità immaginaria e formula_170 l'argomento di formula_171. Il logaritmo complesso è in realtà una funzione a più valori, determinati dal parametro intero formula_72.
</text>
</doc>
<doc id="2614275" url="https://it.wikipedia.org/wiki?curid=2614275">
<title>Settore circolare</title>
<text>
Il settore circolare è la parte di un cerchio racchiusa fra un arco e i raggi che hanno un estremo negli estremi dell'arco della circonferenza.
La sua area può essere calcolata come sotto descritto. Inoltre, se θ si riferisce all'angolo al centro espresso in gradi, si può utilizzare questa formula simile. L'area totale del cerchio corrisponde alla nota formula π "r". Se θ è l'angolo al centro del settore circolare, espresso in radianti, e "r" è il raggio, l'area del settore circolare può essere ottenuta moltiplicando l'area del cerchio per il rapporto dell'angolo θ con 2π (poiché l'area del settore è proporzionale all'angolo θ e 2π è l'angolo dell'intero cerchio)
Inoltre, se θ si riferisce all'angolo al centro espresso in gradi, si può utilizzare questa formula simile.
Un altro modo di trovare la formula di cui sopra è partire da: 
dove C è la misura dell'arco che racchiude il settore circolare, ed r è il raggio del cerchio. Questa formula viene dalla visione dell'area del settore circolare come un triangolo che ha per base l'arco e per altezza il raggio. Per la definizione di angolo radiante
da cui si ricava la formula precedente.
I settori possono godere di speciali relazioni, tra le quali quelle tra i quadranti e gli ottanti.
</text>
</doc>
<doc id="2668967" url="https://it.wikipedia.org/wiki?curid=2668967">
<title>Glossario della simbologia matematica</title>
<text>
Questo è un glossario della simbologia matematica costituito da tabelle dedicate ai simboli utilizzati in matematica.
Questa tabella contiene i simboli matematici veri e propri, compresi quelli costituiti da una lettera greca rovesciata (come formula_1). Non potendo seguire un ordinamento alfabetico, i simboli sono ordinati per "affinità" (con tutta la soggettività che la parola implica).
Questa tabella contiene i simboli costruiti con caratteri (latini) alfanumerici. I simboli sono in ordine alfabetico,
Questa tabella contiene le abbreviazioni e gli acronimi utilizzati come simboli matematici (tipicamente nomi di funzioni). I simboli sono in ordine alfabetico,
</text>
</doc>
<doc id="2776743" url="https://it.wikipedia.org/wiki?curid=2776743">
<title>Divisione dei polinomi</title>
<text>
In matematica, la divisione dei polinomi è un algoritmo che permette di trovare il quoziente tra due polinomi, di cui il secondo di grado non superiore al grado del primo. È un'operazione che si può svolgere a mano, poiché spezza il problema in varie divisioni tra monomi, facilmente calcolabili.
Ricordiamo che, se i polinomi sono a coefficienti reali (o più in generale in un campo) per ogni coppia di polinomi formula_1 e formula_2 esistono "unici" altri due polinomi formula_3 e formula_4 tali che:
posto che il grado di formula_4 sia minore di quello di formula_2. Questo fatto è proprio degli anelli euclidei, come sono gli anelli di polinomi costruiti su un campo.
Il grado di formula_8 sarà equivalente invece alla differenza tra il grado di formula_9 e quello di formula_10.
Nel caso in cui formula_11, formula_1 sarebbe divisibile per formula_10.
L'algoritmo comporta l'esecuzione dei seguenti passi:
Per comprendere meglio l'algoritmo di divisione dei polinomi, in seguito viene svolto un esercizio a titolo d'esempio.
Dividiamo il polinomio
per il polinomio
Scriviamo i due polinomi formula_1 e formula_2 come nel modo illustrato più sopra. Così che ognuno dei due polinomi sia ordinato per grado (in modo decrescente) e siano esplicitati anche i monomi con coefficiente 0.
Dividiamo il termine di grado massimo di formula_1, che risulta essere formula_19, per il termine di grado massimo di formula_2, che è formula_21 e scriviamo il risultato sotto formula_2.
Ora scriviamo, sotto formula_1, il polinomio ricavato moltiplicando il risultato della divisione dei termini di grado massimo, per il polinomio formula_2. Bisogna tenere conto dei termini con coefficiente nullo.
Si può notare che, come già detto nel caso generale, i termini di grado maggiore di formula_1 e del polinomio scritto sotto formula_1, sono uguali.
Ora sottraiamo formula_1 con il polinomio scritto al di sotto per ottenere il polinomio formula_28.
Il grado di formula_29 è maggiore di quello di formula_2, dunque iteriamo il procedimento.
Dividiamo il termine di grado massimo di formula_31 che risulta essere formula_32 per il termine di grado massimo di formula_2 e scriviamo il risultato accanto a quello ottenuto precedentemente.
Ora, come nel passo 3, moltiplichiamo il risultato della divisione appena eseguita che, nel nostro esempio risulta essere formula_34, per il polinomio formula_2 e scriviamo il risultato della moltiplicazione sotto formula_28.
Eseguiamo la sottrazione tra il polinomio formula_28 e il polinomio scritto sotto per ottenere formula_38.
Dato che il grado di formula_38 non è inferiore a quello di formula_2 dobbiamo iterare ancora un'altra volta il procedimento.
Dividiamo il termine di grado superiore di formula_38 per il termine di grado superiore di formula_2.
Moltiplichiamo formula_2 per il risultato della divisione appena eseguita e scriviamo il risultato della moltiplicazione sotto formula_38.
Eseguiamo la sottrazione tra formula_38 e il polinomio scritto sotto per ottenere il polinomio formula_46.
Siamo giunti a formula_47, che ha grado strettamente minore di formula_15, dunque il resto è
e il quoziente della nostra divisione è
possiamo quindi scrivere
Una versione più sintetica di questo procedimento è attuabile quando il divisore "B" è della forma formula_52 o formula_53, un binomio di primo grado. Tale regola è stata esposta da Paolo Ruffini per la prima volta nel 1810.
</text>
</doc>
<doc id="1885173" url="https://it.wikipedia.org/wiki?curid=1885173">
<title>Somma vuota</title>
<text>
In matematica si usa l'espressione somma vuota (o somma nullaria) quando in una addizione non ci sono addendi. Una tale situazione capita ad esempio quando l'indice inferiore di una sommatoria è più grande dell'indice superiore, come in
In questo caso infatti non c'è alcun indice formula_2 che soddisfi la condizione richiesta (essere contemporaneamente minore o uguale di 1 e maggiore o uguale di 2), dunque non ci sono addendi da poter sommare.
Si potrebbe cadere nell'errore di credere che una somma vuota non sia definibile, o che ricada in qualche errore concettuale. Invece il risultato di una somma vuota è semplicemente zero. Lo si può comprendere pensando che lo zero è l'elemento neutro dell'addizione e quindi il risultato che si ottiene quando "non si modifica nulla". Altrimenti, lo si può visualizzare semplicemente facendo un conto veloce: prendiamo ad esempio la somma formula_3 e sottraiamo uno dopo l'altro tutti gli addendi; togliendo formula_4 rimane la somma dei primi tre addendi,
e così via:
per definizione di elemento opposto. Dunque togliendo tutti gli addendi, cioè rimanendo con la somma vuota, il risultato ottenuto è proprio zero.
</text>
</doc>
<doc id="2059793" url="https://it.wikipedia.org/wiki?curid=2059793">
<title>Funzione continua</title>
<text>
In matematica, una funzione continua è una funzione che, intuitivamente, fa corrispondere ad elementi sufficientemente vicini del dominio elementi arbitrariamente vicini del codominio.
Esistono diverse definizioni di continuità, corrispondenti ai contesti matematici in cui vengono utilizzate: la continuità di una funzione è uno dei concetti di base della topologia e dell'analisi matematica.
La continuità di una funzione può essere definita anche in modo locale: in questo caso si parla di "continuità in un punto" del dominio. Una funzione continua è, per definizione, continua in ogni punto del proprio dominio.
Una funzione che non è continua è detta "discontinua", e i punti del dominio in cui non è continua sono detti "punti di discontinuità".
Per esempio, la funzione "h(t)" che descrive l'altezza di un uomo rispetto alla sua età può essere vista come una funzione continua: in periodi brevi l'uomo cresce di poco. Al contrario, la funzione "g(t)" che rappresenta la quantità di denaro presente in un conto corrente nel tempo è una funzione discontinua, poiché prelievi e depositi le fanno fare salti da un valore all'altro.
La continuità di una funzione è un concetto topologico, e quindi la definizione generale di funzione continua si sviluppa con funzioni tra spazi topologici. Lo stesso concetto è però usato in ambiti meno generali, soprattutto per quanto riguarda il suo utilizzo in analisi matematica: è spesso presentata la definizione di continuità solo per funzioni tra spazi metrici, o ancora, solo per funzioni di una variabile reale.
Nel caso di funzioni di una variabile reale, spesso la continuità è presentata come una proprietà del grafico: la funzione è continua se il suo grafico è formato da un'unica curva che non compia mai salti. Sebbene questa nozione possa essere usata nei casi più semplici per distinguere funzioni continue da funzioni discontinue, non è formalmente corretta, e può portare ad ambiguità o errori.
Una funzione formula_1 si definisce continua nel punto formula_2 del suo dominio se il suo limite per formula_3 tendente a formula_2 coincide con la valutazione della funzione in formula_2, ovvero con formula_6. In simboli:
Tale definizione è usata maggiormente per funzioni definite su un intervallo della retta reale: infatti, essa ha senso solo se formula_2 è un punto di accumulazione per il dominio di formula_1. Essa è comunque estendibile anche nel caso di domini più complicati, che comprendono punti isolati: in essi, formula_1 risulta continua per una "verità vuota" (dall'inglese "vacuous truth").
La funzione si dice continua se è continua in ogni punto formula_2 del dominio.
Una funzione formula_12 definita su un sottoinsieme formula_13 dei numeri reali a valori reali si dice continua in un punto formula_14 se per ogni numero formula_15, arbitrariamente piccolo, esiste un secondo numero formula_16 tale che, formula_17, la funzione formula_18 dista da formula_6 per meno di formula_20, ovvero:
In linguaggio simbolico, una funzione è continua in un punto formula_2 se:
Se questa proprietà vale per ogni punto nel dominio di definizione della funzione, allora si dice che la funzione è continua.
In questo caso si dice che formula_24, che è l'insieme delle funzioni continue a valori reali e variabili in formula_13.
Più intuitivamente, se si vuole che la funzione formula_18 disti di un valore piccolo da formula_6 ci basta restringerci ad un intorno abbastanza piccolo del punto formula_2. Se questo è possibile qualunque sia la distanza scelta (a meno di restringere ulteriormente l'intorno di formula_2), allora la funzione è continua in formula_2.
Questa definizione è equivalente a quella data in precedenza: essa è costruita dalla prima semplicemente esplicitando la definizione di limite di una funzione. È stata usata per la prima volta da Cauchy.
La definizione di continuità data nel caso di funzioni reali può essere generalizzata in contesti più ampi, come quello degli spazi topologici.
Siano due spazi topologici formula_31, formula_32 e sia formula_33 un'applicazione. Allora:
formula_1 si dice continua in formula_35 se formula_36 intorno di formula_37 formula_38 intorno di formula_39 tale che formula_40;
formula_1 si dice continua se formula_42 aperto in formula_43 formula_44 è un insieme aperto in formula_45.
Osserviamo che altre definizioni equivalenti di funzione continua sono:
La definizione di continuità è strettamente legata alla topologia scelta nel dominio e nel codominio: funzioni continue con alcune scelte di topologia possono non esserlo con altre. Per esempio, la funzione identità è continua se lo spazio di arrivo ha la stessa topologia dello spazio di partenza, oppure se ne ha una meno fine, ovvero con meno aperti. Se invece lo spazio di arrivo ha una topologia più fine, con più aperti, la funzione identità non risulta continua.
Gli spazi metrici sono spazi topologici nei quali la topologia è generata da una base di intorni circolari. Sia formula_1 una funzione tra due spazi metrici formula_69 e formula_70. La funzione f si dice continua in un punto formula_2 se, per ogni scelta di formula_72, esiste un formula_73, tale che, per ogni punto formula_74 che dista meno di formula_75 da p, ovvero che:
si ha che formula_18 dista per meno di formula_20 da formula_6, ovvero:
La definizione può essere scritta servendosi della nozione di intorno sferico formula_81 centrato in formula_2, di raggio formula_75: in questo caso, la funzione è continua se formula_84 implica che formula_85 o, simbolicamente:
dove formula_87 è l'insieme di definizione di formula_1.
Nel caso di funzioni reali, le definizioni coincidono se le due distanze su dominio e codominio non sono altro che il modulo della differenza tra due valori in formula_89.
Inoltre, questa definizione è valida per funzioni definite e a valori in tutti gli spazi vettoriali normati, dove la distanza sia la norma della differenza tra due punti. In particolare, è valida in formula_90 con la norma euclidea, ed estende quindi la definizione di continuità a funzioni di più variabili.
Sono esempi di funzioni continue:
Sono esempi di funzioni "non" continue:
Sia formula_110 una funzione continua a valori reali definita su un intervallo formula_111. Valgono:
Se formula_1 è una funzione continua biiettiva a valori reali definita su un intervallo, allora formula_1 è strettamente monotona e la funzione inversa formula_137 è continua e strettamente monotona. L'implicazione non vale in generale per le funzioni il cui dominio non è un intervallo.
Sia formula_138 una funzione tra spazi metrici. Valgono:
Sia formula_154 una funzione continua tra spazi topologici. Valgono:
La composizione di funzioni continue è una funzione continua, ovvero se formula_1 e formula_156 sono due funzioni continue, allora anche:
è una funzione continua.
Come conseguenza di questa proprietà si hanno le seguenti:
In generale, l'inverso non è vero: ad esempio, se una funzione continua è somma di due funzioni, non è detto che entrambi gli addendi siano a loro volta funzioni continue. Ad esempio se
allora formula_1 e formula_156 non sono continue, ma
sono entrambe continue su tutto formula_166. Analogamente se
allora formula_1 e formula_156 non sono continue, ma
è continua su tutto formula_166.
Data una successione di funzioni continue formula_172 tali che il limite:
esiste finito per ogni formula_174 (convergenza puntuale), allora non è vero che formula_18 è una funzione continua. Se però la successione converge uniformemente, allora il limite puntuale formula_18 è continuo.
Una funzione derivabile (o più in generale una funzione differenziabile) in un punto formula_2 è sempre continua in quel punto. Non è vero l'inverso: esistono funzioni continue non derivabili, come ad esempio la funzione valore assoluto, continua in 0 ma non derivabile nello stesso punto. Esistono anche funzioni a variabile reale continue in tutti i punti del dominio e non derivabili in nessuno di essi, come la funzione di Weierstrass.
Una funzione continua formula_178 è sempre integrabile secondo Riemann (e quindi anche secondo Lebesgue). Inoltre, formula_1 ammette sempre primitive e ogni sua primitiva è continua. Viceversa, non tutte le funzioni integrabili sono continue: per esempio, sono integrabili tutte le funzioni costanti a tratti.
Una funzione formula_1 a valori reali è "continua per successioni" in formula_39 se, per ogni successione formula_182 a valori nel dominio della funzione e convergente a formula_39, la successione formula_184 converge a formula_37.
Questa formulazione di continuità è dovuta ad Eduard Heine.
Una funzione continua è sempre continua per successioni, mentre, al contrario è possibile dare esempi di funzioni continue per successioni, ma non continue. L'inverso vale solo se il dominio formula_186 è uno spazio sequenziale, come lo sono gli spazi primo-numerabili e dunque in particolare gli spazi metrici: in questo caso, quindi, le due definizioni si possono considerare equivalenti.
Una funzione reale formula_1 si dice "continua a destra" in formula_39 se:
dove il limite è inteso solo come limite destro.
Una funzione formula_1 si dice "continua a sinistra" in formula_39 se:
Una funzione è continua in un punto se e solo se è ivi continua a destra e a sinistra.
Queste proprietà non sono estendibili a funzioni a più di una variabile, in quanto nel piano, nello spazio, e generalmente in formula_90 quando formula_194 non esiste relazione d'ordine, ovvero non è possibile definire una "destra" o una "sinistra".
Una funzione formula_1 definita su uno spazio topologico formula_45 a valori reali si dice "semicontinua inferiormente" in formula_35 se per ogni formula_72 esiste un intorno formula_199 di formula_39 tale che per ogni formula_201, si ha:
Se invece vale, per ogni formula_203:
la funzione viene detta "semicontinua superiormente" in formula_39.
Se la prima (o rispettivamente la seconda) proprietà vale in ogni punto del dominio, si dice che la funzione è semicontinua inferiormente (o rispettivamente semicontinua superiormente).
La semicontinuità (sia inferiore che superiore), è una proprietà più debole della continuità: esistono funzioni semicontinue ma non continue. Viceversa, una funzione è continua se e solo se è sia semicontinua inferiormente che semicontinua superiormente.
Nel caso di funzioni di più variabili, è possibile definire una condizione più debole di continuità, detta "continuità separata": una funzione formula_1 è continua separatamente in un punto formula_2 rispetto a una delle variabili formula_208 se è continua la funzione di una variabile dipendente solo dal parametro formula_208, lasciando le restanti variabili fissate al valore assunto nel punto in esame.
Una condizione più forte (e globale) di continuità è quella di "continuità uniforme": una funzione continua tra due spazi metrici si dice uniformemente continua se il parametro formula_75 della definizione non dipende dal punto formula_2 considerato, ovvero se è possibile scegliere un formula_75 che soddisfi la definizione per tutti i punti del dominio.
Più precisamente, una funzione formula_1 è uniformemente continua se, per ogni formula_72 esiste un formula_73 tale che, comunque presi due punti formula_2 e formula_132 nel dominio di formula_1 che distano per meno di formula_75, allora le loro immagini formula_6 e formula_221 distano per meno di formula_20.
Quando gli elementi di un insieme di funzioni continue hanno il medesimo modulo di continuità, si parla di "insieme equicontinuo". Nello specifico, Siano formula_45 e formula_43 due spazi metrici e formula_225 una famiglia di funzioni definite da formula_45 in formula_43. La famiglia formula_225 è equicontinua nel punto formula_229 se per ogni formula_230 esiste formula_73 tale che formula_232 per tutte le formula_233 e per ogni formula_3 tali che formula_235. La famiglia formula_225 è equicontinua (in tutto formula_45) se è equicontinua in ogni suo punto. La famiglia formula_225 è uniformemente equicontinua se per ogni formula_230 esiste formula_73 tale che formula_241 per tutte le formula_233 e per ogni coppia di punti formula_243 e formula_244 in formula_45 tali che formula_246.
Più in generale, quando formula_45 è uno spazio topologico, un insieme formula_225 di funzioni da formula_45 in formula_43 è equicontinuo nel punto formula_74 se per ogni formula_230 il punto formula_3 possiede un intorno formula_254 tale che:
Tale definizione è sapesso utilizzata nell'ambito degli spazi vettoriali topologici.
L'insieme di tutte le funzioni continue su un dominio fissato formula_13 e a valori reali:
può essere dotato di una struttura di spazio vettoriale ponendo per formula_1 e formula_156 in tale insieme:
e per formula_261 numero reale:
Lo spazio vettoriale così definito è detto "spazio delle funzioni continue su formula_13".
Se il dominio formula_13 è compatto (e quindi per tutte le funzioni in formula_265 vale il teorema di Weierstrass) nello spazio formula_265 può essere definita una norma ponendo:
detta "norma uniforme" o "norma del sup".
La coppia costituita dallo spazio formula_265 e dalla norma uniforme individua uno spazio di Banach.
</text>
</doc>
<doc id="1135815" url="https://it.wikipedia.org/wiki?curid=1135815">
<title>Teorema delle radici razionali</title>
<text>
In algebra, il teorema delle radici razionali afferma che ogni soluzione razionale di un'equazione polinomiale a coefficienti interi:
è della forma formula_2, dove:
Il teorema non dà alcuna informazione su eventuali radici irrazionali o complesse.
Ad esempio, se abbiamo un'equazione della forma
allora le eventuali radici razionali sono contenute in quest'insieme:
Se il polinomio è monico, cioè è formula_9, evidentemente la formula si semplifica restringendo le opzioni tra i soli divisori del termine noto. La verifica di ogni singola possibile radice si può ad esempio attuare con il teorema del resto (oppure con la regola di Ruffini se si vuole avere direttamente anche direttamente il quoziente). Se nessun valore soddisfa le richieste, allora tutte le sue radici (che esistono per il teorema fondamentale dell'algebra) sono irrazionali o complesse. Al contrario, se sono state trovate formula_10 radici razionali, allora il polinomio è completamente fattorizzabile in polinomi lineari con coefficienti razionali.
Il teorema delle radici razionali è una diretta conseguenza del lemma di Gauss, il quale afferma che se un polinomio (a coefficienti interi) è fattorizzabile sui razionali, allora lo è anche sugli interi.
Quindi se esiste una radice razionale formula_2, questo significa che potremo scrivere il nostro polinomio iniziale come formula_12 con tutti i formula_13 interi. Facendo il prodotto (i coefficienti intermedi non ci interessano) e sfruttando il fatto che due polinomi sono uguali se e solo se coincidono tutti i coefficienti, avremo formula_14 e formula_15, da cui il teorema.
In altro modo, supponiamo che la frazione formula_16 sia una radice del polinomio. Possiamo supporre che la frazione sia ridotta ai minimi termini, ovvero che gli interi formula_3 e formula_5 siano primi fra loro. Sostituendo si ottiene 
da cui, moltiplicando per formula_20,
Ora formula_3 divide i primi formula_10 termini, dunque deve dividere anche l'ultimo termine formula_24. Dato che formula_3 e formula_5 sono primi fra loro, formula_3 deve dividere formula_4. Con un ragionamento analogo si vede che formula_5 divide formula_6.
</text>
</doc>
<doc id="1177064" url="https://it.wikipedia.org/wiki?curid=1177064">
<title>Infinito (matematica)</title>
<text>
l concetto di infinito (simbolo formula_1) ha molti significati, in correlazione con la nozione di limite, sia in analisi classica sia in analisi non standard. Nozioni di infinito sono usate in teoria degli insiemi e in geometria proiettiva.
Nella teoria degli insiemi un insieme formula_2 si dice infinito se ogni suo sottoinsieme finito è un sottoinsieme proprio. Una definizione alternativa è la seguente: un insieme formula_2 è infinito se esiste un'applicazione biunivoca di formula_2 in un suo sottoinsieme proprio formula_5. In altre parole, formula_2 è infinito se e solo se è equipotente a un suo sottoinsieme proprio. Per dimostrare l'equivalenza delle due definizioni è indispensabile l'assioma della scelta.
È possibile fare una distinzione tra differenti gradi di infinità dal momento che possono essere individuati insiemi infiniti che hanno una cardinalità più grande degli altri. Georg Cantor sviluppò la teoria dei numeri cardinali transfiniti, in cui il primo numero transfinito è aleph-zero formula_7, che corrisponde alla cardinalità dell'insieme dei numeri naturali. Il successivo grado di infinito noto è formula_8. L'infinito corrispondente alla cardinalità dei numeri reali viene generalmente indicato con formula_9. Il problema se formula_10, vale a dire dell'esistenza o meno di una cardinalità intermedia tra queste due, è la cosiddetta ipotesi del continuo. Nel 1940 Kurt Gödel dimostrò che tale ipotesi è coerente con gli assiomi di Zermelo - Fraenkel (con o senza l'assioma della scelta); nel 1963 Paul Cohen ha poi dimostrato che anche la negazione di tale ipotesi è coerente con quegli assiomi. Di conseguenza l'ipotesi del continuo, nell'ambito degli assiomi di Zermelo - Fraenkel, non è né dimostrabile né refutabile.
Cantor sviluppò anche la teoria dei numeri ordinali transfiniti, che generalizzano agli insiemi infiniti la nozione di ordinamento e di posizione di un elemento all'interno di un ordinamento.
Un esempio è il teorema di Goodstein, che può essere risolto solamente mediante le proprietà degli ordinali transfiniti, mentre non è dimostrabile con i soli assiomi di Peano.
Nello studio dei limiti si usa il simbolo formula_1, che talvolta è indicato pure col termine lemniscata.
Risulta utile servirsi di due entità collegate con l'infinito: l'insieme reale esteso è l'unione dei numeri reali con due punti, indicati con formula_12 e formula_13. In simboli:
La relazione d'ordine dei reali si estende a questi nuovi punti ponendo: 
si hanno invece limitazioni a estendere le operazioni aritmetiche a tali entità (formula_17).
Da un punto di vista topologico si tratta di una compattificazione della retta reale mediante l'aggiunta di due punti.
Una menzione a parte merita l'analisi non standard, introdotta da Abraham Robinson nel 1966: al contrario dell'analisi matematica comune, in essa gli infiniti (indicati con "Ω") e infinitesimi ("ε") hanno piena cittadinanza tra i numeri, e assieme ai reali formano i numeri iperreali. Ad esempio "1" e "1+ε" sono numeri iperreali distinti. Al contrario dei numeri complessi, è possibile un ordinamento dei numeri iperreali grazie al concetto di ultrafiltro. L'analisi non standard è perfettamente coerente, e anzi semplifica le dimostrazioni di molti teoremi, sia nel calcolo infinitesimale sia nella teoria dei numeri.
In geometria proiettiva invece risulta naturale completare le rette con il loro, unico, punto all'infinito, oggetto chiamato punto improprio o direzione della retta; tale nozione, in particolare, permette di dire che anche due rette parallele hanno un punto in comune, il loro punto all'infinito. Inoltre nel piano proiettivo si colloca anche la retta impropria, insieme dei punti impropri (all'infinito) delle varie rette; per non dire dello spazio proiettivo, "avvolto" nel suo piano improprio.
</text>
</doc>
<doc id="1178819" url="https://it.wikipedia.org/wiki?curid=1178819">
<title>Crescita esponenziale</title>
<text>
La crescita esponenziale (che comprende il decadimento esponenziale) si verifica quando il tasso di crescita di una funzione matematica è proporzionale al valore attuale della funzione. Nel caso di un dominio di definizione discreto con intervalli uguali è chiamata anche crescita geometrica o decadimento geometrico (i valori della funzione formano una progressione geometrica).
Il modello di crescita esponenziale è noto anche come Modello di Malthus.
Una quantità formula_1 dipende esponenzialmente dal tempo formula_2 se:
dove la costante formula_4 è il valore iniziale di formula_1, cioè formula_6, la costante formula_7 è un fattore di crescita positivo, e formula_8 è il tempo richiesto perché formula_1 aumenti di un fattore di formula_7:
Se formula_12 e formula_13, allora formula_1 ha crescita esponenziale. Se formula_15 e formula_13 o formula_12 e 0, allora formula_1 ha decadimento esponenziale.
Ad esempio, se una specie di batteri raddoppia ogni dieci minuti, cominciando con un batterio, per sapere quanti batteri sarebbero presenti dopo un'ora si procede ponendo formula_19, formula_20 e formula_8 pari a 10 minuti:
Ci sono quindi sessantaquattro batteri.
Per rappresentare un tasso di crescita si possono utilizzare diverse forme diverse ma matematicamente equivalenti, usando una base diversa. Le forme più comuni sono le seguenti:
Vi sono quindi molte possibilità di scegliere una coppia formula_25 di un numero non negativo adimensionale formula_7 e di un ammontare di tempo formula_8. Nello specifico, i parametri che compaiono (negativi nel caso del decadimento esponenziale) sono:
Le quantità "k", formula_8 e "T", e per un dato "p" anche "r", hanno un rapporto di uno a uno dato dalla seguente equazione (che può essere derivata prendendo il logaritmo naturale di cui sopra):
dove "k" = 0 corrisponde a "r" = 0 e a formula_8 e "T" che sono infiniti.
Se "p" è l'unità di tempo, il quoziente "t/p" è semplicemente il numero di unità di tempo. Usando la notazione "t" per il numero (adimensionale) di unità di tempo piuttosto che per il tempo stesso, "t/p" può essere sostituito da "t", ma per uniformità questo è stato qui evitato. In questo caso la divisione per "p" nell'ultima formula non è neanche una divisione numerica, ma converte un numero adimensionale nella corretta quantità contenente l'unità.
Un popolare metodo approssimato per calcolare il tempo di raddoppio dal tasso di crescita è la regola del 70,
cioè formula_38 (o meglio: formula_39).
La funzione esponenziale formula_40 soddisfa l'equazione differenziale lineare:
che stabilisce che il tasso di crescita di formula_1 al tempo formula_2 è proporzionale al valore di formula_44, con valore iniziale formula_6. Per formula_46 l'equazione differenziale è risolta dal metodo di separazione delle variabili:
da cui segue:
Incorporando il valore iniziale si ha:
ovvero:
La soluzione si applica per formula_51 dove il logaritmo non è definito.
Per una variazione non lineare di questo modello di crescita si veda funzione logistica.
Nel lungo periodo, qualsiasi tipo di crescita esponenziale sorpasserà qualsiasi tipo di crescita lineare (la base della catastrofe malthusiana nonché qualsiasi crescita polinomiale, cioè, per α qualunque:
C'è un'intera gerarchia di tassi di crescita concepibili che sono più lenti di quello esponenziale e più veloci di quello lineare (nel lungo periodo). Si veda grado di un polinomio.
I tassi di crescita possono essere anche più veloci di quello esponenziale.
Nell'equazione differenziale di sopra, se "k" &lt; 0, allora la quantità sperimenta il decadimento esponenziale.
I modelli di crescita esponenziale dei fenomeni naturali si applicano soltanto entro regioni limitate, in quanto la crescita illimitata non è fisicamente realistica. Anche se la crescita può essere inizialmente esponenziale, i fenomeni modellizzati alla fine entreranno in una regione nella quale fattori di retroazione negativa precedentemente ignorati diventeranno significativi (portando a un modello di crescita logistica) oppure altre assunzioni sottostanti del modello di crescita esponenziale, come la continuità o la retroazione istantanea, vengono meno.
Secondo la leggenda, un cortigiano si presentò al re di Persia con una bella scacchiera. Il re chiese che cosa gli sarebbe piaciuto in cambio del suo regalo e il cortigiano sorprese il re chiedendo un chicco di riso sulla prima casella, due chicchi sulla seconda, quattro chicchi sulla terza, ecc. Il re acconsentì prontamente e chiese che fosse portato il riso. Tutto andò bene all'inizio, ma la richiesta di formula_53 chicchi sull"'n"-esima esigeva oltre un milione di chicchi sulla 21ª casella, più di un milione di milioni (o, come si dice, di un trilione) sulla 41ª e non c'era semplicemente abbastanza riso nell'intero mondo per le caselle finali.
Per varianti di questa storia si veda la seconda metà della scacchiera, in riferimento al punto in cui un fattore esponenzialmente crescente comincia ad avere un impatto economico significativo sulla complessiva strategia d'affari di un'organizzazione.
Ai bambini francesi si racconta una storia in cui immaginano di avere uno stagno con foglie di ninfea che galleggiano sulla superficie. La popolazione di ninfee raddoppia di dimensione ogni giorno e se lasciata incontrollata soffocherà lo stagno in 30 giorni, uccidendo tutte le altre cose viventi nell'acqua. Giorno dopo giorno la pianta sembra piccola e così si decide di lasciarla crescere fino a quando non ricopra metà dello stagno, prima di tagliarla nuovamente. È stato poi chiesto loro in che giorno ciò avverrà. Questo si rivela essere il 29º giorno, dopo di che ci sarà appena un giorno per salvare lo stagno.
</text>
</doc>
<doc id="638972" url="https://it.wikipedia.org/wiki?curid=638972">
<title>Tavola trigonometrica</title>
<text>
In matematica la tavola trigonometrica è una tabella che elenca il valore di una funzione trigonometrica per un certo numero di possibili valori angolari. Una tavola trigonometrica è principalmente caratterizzata da due aspetti:
Il dettaglio può non essere costante per meglio mappare funzioni che abbiano asintoti verticali o orizzontali, che implicano significative variazioni del valore delle funzione in prossimità di alcuni valori angolari.
La prima tavola trigonometrica di cui si ha testimonianza risale al II secolo a.C. ed è quella predisposta da Ipparco di Nicea allo scopo di risolvere, ovvero individuare tutti gli angoli e le lunghezze dei lati nota una parte di essi, ogni possibile triangolo.
Secondo alcune interpretazioni della tavoletta d'argilla di origine babilonese nota con il nome di "Plimpton 322" (risalente al 1800 a.C.) potrebbe essere una rudimentale tavola trigonometrica.
Claudio Tolomeo, attorno al 150, nella sua opera "Almagesto" spiega come costruire una "tavola delle corde" utile per i calcoli astronomici.
È curioso notare come le prime tavole trigonometriche non mappassero le funzioni seno e coseno, come normalmente accade oggi, quanto le funzioni seno e senoverso.
Prima dell'introduzione delle calcolatrici tascabili, le tavole trigonometiche erano essenziali per i calcoli richiesti in numerose aree della scienza e delle applicazioni pratiche, si veda ad esempio l'importanza dell'algoritmo di prostaferesi per la navigazione e l'astronomia.
Il calcolo delle tabelle matematiche era un'area di studio molto importante che portò all'invenzione dei primi dispositivi automatici di calcolo.
Tra i matematici che si ricordano per aver contribuito a definire tavole sempre più accurate, vanno citati:
Oggi ogni calcolatrice è in grado di fornire i valori delle funzioni trigonometriche per qualsiasi angolo. I metodi usati dalle calcolatrici per ottenere questo risultato comprendono:
L'interpolazione, anche di tabelle non molto dettagliate, trova ancora oggi impiego nella computer grafica, dove la precisione di calcolo non deve essere necessariamente elevata volendo prediligere la velocità di calcolo.
Un'altra importante applicazione delle tabelle trigonometriche è negli algoritmi per il calcolo della trasformata di Fourier veloce (FFT), dove gli stessi valori delle funzioni trigonometriche (i cosiddetti fattori "twiddle" o "di rotazione") devono essere calcolati più volte all'interno della stessa trasformata, soprattutto nel caso non raro in cui si eseguono più trasformazioni dello stesso ordine. In questo caso non sarebbe accettabile chiamare ripetutamente la routine generica di calcolo poiché il tempo complessivo sarebbe eccessivo. Una possibilità è quella di chiamare la routine un'unica volta per ciascun valore twiddle e storicizzarne il risultato in una tabella da consultare durante l'algoritmo. Seppure efficace, la soluzione non risulta efficiente in termine di quantità di memoria usata.
Storicamente, il metodo più antico, e probabilmente quello più comunemente usato fino all'avvento dei calcolatori elettronici, per ottenere e ampliare con sistematicità una tavola trigonometrica è stato quello di applicare ripetutamente le identità trigonometriche notevoli note come formule di bisezione e formule di addizione e sottrazione partendo da valori noti quali formula_1 oppure formula_2:
L'impiego più remoto, di cui ci sia pervenuta una testimonianza scritta, risale a Tolomeo.
È un metodo rapido, ma impreciso, per ottenere una tabella di dimensione N nell'intervallo formula_7
Posto formula_8 e formula_9, si usano le seguenti formule:
per "n" = 0...,"N"-1, dove "d" = 2π/"N".
Queste formule si ottengono applicando il metodo di Eulero per l'integrazione del seguente sistema di equazioni differenziali ordinarie:
con le condizioni al contorno "s"(0) = 0 e "c"(0) = 1, la cui soluzione analitica è formula_12 e formula_13.
L'errore che si commette con l'uso di questo algoritmo è inversamente proporzionale ad N. In particolare per il seno il massimo dell'approssimazione è circa 15/N, mentre per il coseno è 20/N.
Ad esempio, per "N" = 256 l'errore massimo che si commette nel calcolo del seno è circa ~0.061 ("s" = -1.0368 invece di -0.9757). Per "N" = 1024, l'errore massimo che si commette nel calcolo del seno è circa ~0.015 ("s" = -0.99321 invece di -0.97832), all'incirca 4 (=1024/256) volte inferiore.
L'andamento dell'ampiezza dell'errore non è lineare ma oscillante e divergente, per cui se si volesse tracciare la curva definita anche da coppie (s,c) ottenute per valori superiori a N, si otterrebbe una spirale logaritmica anziché un cerchio.
Una semplice formula ricorsiva per la generazione delle tabelle trigonometriche è basata sulla Formula di Eulero e sulla seguente relazione:
Possiamo così calcolare i valori trigonometrici "s" e "c" come segue:
per "n" = 0, ..., "N - 1", dove formula_15 e formula_16. I due valori iniziali sono generalmente calcolati avvalendosi di funzioni di libreria esistenti (ad esempio utilizzando semplicemente con il metodo di Newton nel piano complesso per il calcolo delle radici dell'unità di "z" − 1).
Questo metodo in teoria (cioè in aritmetica esatta) dovrebbe fornire una tavola di valori "esatta", tuttavia gli errori dovuti all'aritmetica finita (virgola mobile) fanno sì che l'errore commesso sia un O(ε "N") (nel peggiore dei casi ma anche mediamente), dove ε è la precisione della rappresentazione in virgola mobile.
Un significativo miglioramento si ottiene utilizzando la seguente modifica, uno stratagemma (ideato da Richard C. Singleton) sovente utilizzato per ottenere i valori trigonometrici nelle implementazioni della FFT:
dove α = 2 sin(π/"N") e β = sin(2π/"N"). Gli errori di questo metodo sono molto più piccoli, O(ε √"N") in media e O(ε "N") nel peggiore dei casi, che è comunque in grado di inficiare significativamente la precisione per FFT di grandi dimensioni.
</text>
</doc>
<doc id="720811" url="https://it.wikipedia.org/wiki?curid=720811">
<title>Criterio di Cartesio</title>
<text>
Il criterio di Cartesio, descritto nel suo libro "La Géométrie", è una regola algebrica che determina il numero "massimo" di radici reali positive e negative di un polinomio a coefficienti reali.
Sia dato un polinomio a coefficienti "reali":
con coefficienti formula_2 reali e non tutti nulli. La regola di Cartesio stabilisce che:
Il massimo numero di radici reali positive di un polinomio è dato dal numero di variazioni di segno fra coefficienti consecutivi, trascurando eventuali coefficienti nulli. Inoltre, le radici sono ordinate in modulo decrescente da quella corrispondente alla coppia di coefficienti ai gradi massimo e subito precedente fino a quella corrispondente alla coppia di coefficienti lineare e di grado nullo.
L'informazione relativa al numero di radici negative si deduce applicando la stessa regola al polinomio trasformato a radici opposte, ossia al polinomio formula_3. Esso ha infatti radici opposte a quelle di formula_4.
Perciò: le variazioni relative ai coefficienti del polinomio formula_5 danno informazioni circa le sue radici positive e, di conseguenza, circa le radici negative di formula_4.
Se il polinomio ha tutte le radici non immaginarie, il numero di radici positive è quello massimo. Il criterio di Routh-Hurwitz raffina determinando il numero effettivo delle radici a parte reale positiva e negativa.
Il polinomio
formula_7
presenta due variazioni di segno fra coefficienti di grado 5 e 3 e tra i coefficienti di grado 3 e 1. Questo indica che ci possono essere 2 o nessuna radice positiva.
Il polinomio trasformato a radici opposte è:
formula_8
che presenta due variazioni di segno. Questo indica che il polinomio P(-x) può avere due o nessuna radice positiva e quindi il polinomio iniziale può avere due o nessuna radice negativa.
Le radici del polinomio formula_4 sono 0, -2,+2,-3,+3.
Si noti come l'assenza di permanenze di segno nel polinomio iniziale (0 permanenze) non fornisce assolutamente alcuna informazione circa il numero di radici negative (che infatti risulta essere pari a 2).
</text>
</doc>
<doc id="630336" url="https://it.wikipedia.org/wiki?curid=630336">
<title>Numero negativo</title>
<text>
Un numero negativo è un numero minore (più piccolo) di zero, come ad esempio formula_1.
Nell'immagine seguente si vede, ad esempio, la retta dei numeri reali, su cui sono stati segnati gli interi: in rosso sono evidenziati i numeri negativi.
Se il numero è, in particolare, intero o razionale, si parla, più specificatamente, di "numero intero negativo" o "numero razionale negativo".
I numeri negativi vengono descritti nella notazione decimale con un segno meno davanti alle cifre. 
I numeri negativi sono utili a descrivere molte quantità di uso comune come temperatura, debito, carica elettrica.
La notazione con il segno meno è coerente con il fatto che la somma di un primo numero con un secondo numero negativo è equivalente alla differenza tra il primo numero ed il valore assoluto del secondo:
formula_2
"Nelle immagini seguenti, esplicative dei segni risultati dalle operazioni, i cerchi più grandi significano banalmente che l'operando in questione (o il risultato) è più grande."
La somma di due numeri negativi dà come risultato un numero negativo:
Questa somma è equivalente all'opposto della somma dei due numeri presi entrambi in valore assoluto:
"Questa operazione si può descrivere come la somma di due debiti, che quindi risulta essere un debito maggiore."
La somma di un numero positivo con un numero negativo, come già detto, può essere trasformata nella differenza tra il numero positivo e quello negativo in valore assoluto:
L'operazione di moltiplicazione tra un numero positivo ed uno negativo risulta essere un numero negativo, se invece viene effettuata tra due numeri negativi darà come risultato un numero positivo.
La differenza tra due numeri positivi può restituire come risultato un numero negativo:
"Quindi ad esempio se io possiedo 2 cose ma ne devo dare 3 a te, ti comincio a dare le 2 che ho e rimango con un debito di 1 verso di te."
La sottrazione di un numero positivo equivale alla somma del suo opposto:
In egual modo la sottrazione di un numero negativo equivale alla somma del suo opposto (ossia dello stesso in valore assoluto). In altre parole la "perdita" di un debito equivale al "guadagno" di un credito.
Tenendo in considerazione quanto detto precedentemente per l'operazione di moltiplicazione, se si deve sottrarre ad un primo numero un secondo numero negativo, ciò equivale alla somma del primo numero con il secondo moltiplicato per −1, ossia cambiato di segno (perché il prodotto dei valori assoluti è uno e ci sono 2 numeri negativi da moltiplicare).
Per quanto riguarda il rapporto di un primo numero per altri "n" numeri (positivi e negativi), vale la seguente regola (simile a quella della moltiplicazione): il quoziente di questa operazione è un numero che ha come valore assoluto il rapporto ordinato dei valori assoluti, e segno positivo se vi è un numero pari di numeri negativi (tra dividendo e divisori), negativo se dispari.
Nel campo formula_12 dei numeri reali la radice quadrata di un numero negativo non esiste. Infatti la radice quadrata di un numero negativo formula_13 è un numero formula_14 per cui formula_15. Non esistono però numeri reali formula_14 che soddisfano questa proprietà: il numero formula_17 è infatti sempre non negativo (anche se formula_14 è negativo), e quindi non può essere uguale ad formula_13.
Questa mancanza ha contribuito all'introduzione dei numeri complessi, in cui è possibile estrarre la radice quadrata di un numero negativo.
Nel campo formula_12 dei numeri reali il logaritmo di un numero negativo non è definito.
Nel campo formula_21 dei numeri complessi è possibile calcolare il logaritmo complesso di un numero negativo, ma non univocamente.
</text>
</doc>
<doc id="696565" url="https://it.wikipedia.org/wiki?curid=696565">
<title>Segno (matematica)</title>
<text>
In algebra il segno è una proprietà che esprime l'ordine di un numero reale rispetto allo zero.
Di un numero reale "x" si dice che esso ha segno più o che è "positivo" se vale la relazione "x" &gt; 0; si dice invece che "x" ha segno meno o che è "negativo" quando vale "x" &lt; 0.
Nel caso di "x" = 0 si dice che "x" è "neutro": allora il segno non è definito.
Per la notazione matematica del segno si usano i simboli + ("più") e - ("meno"): il segno è seguito immediatamente dal valore assoluto del numero.
Se il segno non è espresso davanti a un numero diverso da zero, il numero s'intende positivo.
Va rilevato che i simboli + e - sono usati in matematica anche con altri significati, per esempio per notare le operazioni di addizione e sottrazione
oppure per distinguere i limiti destro e sinistro di una funzione in un punto di accumulazione
In questi casi l'uso dei simboli matematici non è legato al segno di un numero. Si parla qui di "segno" nel senso comune di "simbolo" o "carattere tipografico".
Il simbolo ± ("più o meno") nelle espressioni matematiche indica che due valori di segno opposto sono entrambi validi.
Il concetto di segno si estende naturalmente all'ambito delle funzioni. Il segno di una funzione "f" in un intervallo "I" è per definizione quello comune ad ogni valore che la funzione assume nell'intervallo.
Nell'analisi matematica, lo studio del segno di una funzione è particolarmente utile per tracciarne il grafico.
Se "f"("x") è una funzione continua in un dato intervallo, i valori di "x" per cui "f"("x") cambia di segno sono le ascisse dei punti in cui il grafico della funzione interseca l'asse delle "x", e dunque soluzioni dell'equazione "f"("x") = 0.
Ulteriori indicazioni sull'andamento di una funzione continua si possono ricavare studiando il segno della sua derivata, quando questa esiste. Dalla definizione matematica di monotonia si evince che una funzione continua e derivabile in un intervallo è strettamente crescente solo se la sua derivata è positiva; la funzione è invece strettamente decrescente negli intervalli dove la sua derivata è negativa.
Per alcune applicazioni pratiche legate alla fisica è interessante conoscere il segno di una grandezza anche quando il valore esatto non è noto: considerando per esempio che le cariche elettriche di segno uguale si respingono e le cariche di segno opposto si attraggono si può in certi casi prevedere il comportamento di un sistema senza misurarne le caratteristiche rilevanti.
</text>
</doc>
<doc id="91262" url="https://it.wikipedia.org/wiki?curid=91262">
<title>Geometria analitica</title>
<text>
La geometria analitica, chiamata anche geometria cartesiana, è lo studio delle figure geometriche attraverso il sistema di coordinate oggi dette cartesiane, ma già studiate nel Medioevo da Nicola d'Oresme.
Ogni punto del piano cartesiano è individuato dalle sue coordinate su due assi: ascisse (x) e ordinate (y), nello spazio è individuato da 3 coordinate (x,y,z). Le coordinate determinano un vettore rispettivamente del tipo formula_1 oppure formula_2. Gli enti geometrici come rette, curve, poligoni sono definiti tramite equazioni, disequazioni o insiemi di queste, detti sistemi.
Le proprietà di questi oggetti, come le condizioni di incidenza, parallelismo e perpendicolarità, vengono anch'esse tradotte in equazioni e quindi studiate con gli strumenti dell'algebra e dell'analisi matematica.
Il termine "geometria analitica" è stato usato anche da alcuni matematici moderni come Jean-Pierre Serre per definire una branca della geometria algebrica che studia le varietà complesse determinate da funzioni analitiche.
Le formule della geometria analitica possono essere agevolmente estese nello spazio a tre dimensioni. La geometria strutturale studia le proprietà delle figure geometriche in uno spazio a quattro o più dimensioni, e il loro rapporto con le figure in tre dimensioni.
La geometria descrittiva è in parte attinente poiché rappresenta su uno o più piani, oggetti bidimensionali e tridimensionali. Giuseppe Veronese tentò una descrizione a quattro o più dimensioni, priva di rigore formale logico, e fortemente criticata da Giuseppe Peano.
René Descartes introdusse le basi della geometria analitica nel 1637 nel saggio intitolato "Geometria" incluso nel suo libro "Discorso sul metodo per ben condurre la propria ragione e cercare la verità nelle scienze più la Diottrica, le Meteore e la Geometria che sono saggi di questo metodo" (la cui prefazione è il famoso "Discorso sul metodo"). 
Questo lavoro scritto in francese e i suoi principi filosofici, fornirono le fondamenta per il calcolo differenziale, che sarà successivamente introdotto da Isaac Newton e Gottfried Wilhelm Leibniz, in maniera autonoma fra loro.
I temi più importanti della geometria analitica sono:
Molti di questi problemi comprendono l'algebra lineare.
</text>
</doc>
<doc id="40041" url="https://it.wikipedia.org/wiki?curid=40041">
<title>Aritmetica</title>
<text>
L'aritmetica (dal greco ἀριθμός = numero) è la più antica branca della matematica, quella che studia le proprietà elementari delle "operazioni aritmetiche" sui numeri, specialmente i numeri interi.
È praticata quotidianamente da tutti per scopi molto semplici, come contare oggetti, valutare costi, stabilire distanze; viene utilizzata anche per scopi avanzati, ad esempio in complessi calcoli finanziari o nella tecnologia delle comunicazioni (crittografia).
I matematici talvolta usano il termine aritmetica per indicare la teoria dei numeri; questa disciplina però tratta problemi più avanzati e specifici rispetto all'aritmetica elementare e non viene presa in considerazione nel presente articolo.
La preistoria dell'aritmetica si limita ad un piccolo numero di piccoli artefatti che testimoniano una chiara concezione dell'addizione e della sottrazione; la prima fonte è l'osso d'Ishango rinvenuto nell'Africa centrale, datata circa tra il 20,000 e il 18,000 a.C.
È chiaro che i babilonesi avevano una solida conoscenza di quasi tutti gli aspetti dell'aritmetica elementare già intorno al 1800 a.C., anche se gli storici possono solo avanzare congetture sopra i metodi utilizzati per ottenere i risultati delle operazioni aritmetiche; questo è il caso dei risultati presentati nella tavoletta d'argilla Plimpton 322, che si presenta come una lista di terne pitagoriche; non si ha invece la possibilità di mostrare in qual modo questa lista sia stata originariamente prodotta. Similmente gli egizi, attraverso il Papiro di Rhind (che risale al 1650 a.C. circa, ma che, con ogni evidenza, è una copia di un vecchio testo del 1850 a.C. circa) testimoniano che le operazioni di addizione, sottrazione, moltiplicazione e divisione venivano effettuate all'interno di un sistema basato su frazioni unitarie.
Nicomaco di Gerasa, nella sua opera "Introduzione all'aritmetica", ha presentato concisamente l'approccio filosofico pitagorico ai numeri, e alle loro mutue relazioni. In quei tempi le operazioni aritmetiche di base costituivano attività molto complesse ed impegnative; è stato il metodo noto come il «Metodo degli indiani» (in latino "Modus Indorum"), che ha condotto all'aritmetica che conosciamo oggi. L'aritmetica indiana è stata molto più semplice di quella greca, a causa della semplicità del sistema di numerazione indiano, che per primo è riuscito a servirsi del numero zero e di una notazione posizionale. Nel VII secolo il vescovo siriaco Severo Sebokht menzionava questo metodo con ammirazione, precisando tuttavia che il metodo degli indiani si presentava senza alcuna descrizione.
Gli arabi appresero questo nuovo metodo e lo chiamarono "hesab". Leonardo Fibonacci (noto anche come Leonardo di Pisa), ha introdotto in Europa il metodo degli indiani nel 1202. Nel suo "Liber Abaci", Fibonacci afferma che, rispetto a questo nuovo metodo, tutti gli altri metodi sono stati sbagliati. Nel Medioevo, l'aritmetica era una delle sette Arti Liberali e veniva insegnata nelle università. Francesco Pellos ne compendiò lo stato nel XV secolo.
I moderni algoritmi dell'aritmetica (utilizzati sia per calcoli manuali che per calcoli automatici) sono stati resi possibili dalla introduzione dei numerali arabi e della loro notazione numerica posizionale e decimale. L'aritmetica araba basata sui numerali era stata sviluppata dai grandi matematici indiani Aryabhatta, Brahmagupta e Bhāskara I. Aryabhatta ha tentato di usare diverse notazioni posizionali e Brahmagupta ha arricchito dello zero il sistema numerico indiano. Brahmagupta ha messo a punto i procedimenti moderni per moltiplicazione, divisione, l'addizione e sottrazione basate sulle cifre decimali. Sebbene oggi siano ormai considerati elementari, questi procedimenti per la loro semplicità costituiscono il punto di arrivo di migliaia di anni di sviluppo della matematica. Per contrasto l'antico matematico Archimede ha dedicato un'intera opera, l"'Arenario ", a mettere a punto una notazione per un certo numero intero molto grande. Alla fioritura dell'algebra nel mondo islamico medievale e nell'Europa Rinascimentale ha contribuito in misura significativa anche l'enorme semplificazione dei calcoli numerici consentita dalla notazione decimale.
Il sistema numerico decimale è un sistema di numerazione posizionale che rappresenta i numeri a partire dalle cifre base da 0 a 9. Un numero decimale consiste in una sequenza di queste cifre base. Il valore di ciascuna cifra che compone il numero dipende dalla posizione che la cifra occupa nella notazione. Una parte essenziale di questo sistema (e un grande traguardo concettuale) è stato pensare allo 0 come ad un numero comparabile alle altre cifre base.
Le operazioni aritmetiche tradizionali sono addizione, sottrazione, moltiplicazione e divisione, sebbene vengano a volte incluse nella materia anche operazioni più avanzate come l'elevamento a potenza, l'estrazione di radice, i logaritmi e l'uso delle percentuali. I calcoli aritmetici vengono effettuati rispettando l'ordine delle operazioni. Qualunque insieme su cui possono essere effettuate tutte e quattro le operazioni aritmetiche (eccetto la divisione per 0), e in cui le quattro operazioni godono delle comuni proprietà, è chiamato campo.
Il termine "aritmetica" è usato anche per riferirsi alla teoria dei numeri. Quest'ultima studia le proprietà degli interi collegate ai numeri primi, la divisibilità, e le soluzioni intere delle equazioni, che sono argomenti in rapida crescita nella matematica moderna. È in questo contesto che si incontrano il teorema fondamentale dell'aritmetica e le funzioni aritmetiche.
In generale nella scuola elementare vengono insegnati gli algoritmi di calcolo manuali per eseguire le quattro operazioni nell'insieme dei numeri naturali e dei numeri razionali positivi nella forma decimale; nelle medie inferiori gli algoritmi di queste stesse operazioni eseguite su frazioni e vengono introdotti l'insieme dei numeri razionali e quello dei numeri reali.
Sempre alle medie inferiori si studiano le operazioni di elevamento a potenza, di estrazione di radice (in particolare la radice quadrata), di massimo comune divisore e di minimo comune multiplo. Tradizionalmente si insegnava l'algoritmo manuale di estrazione di radice quadrata, ma ciò ora non è più sempre vero, visto che molti insegnanti preferiscono insegnare l'uso delle tavole numeriche o quello del calcolatore tascabile. Viene inoltre appreso il metodo di approssimazione dei risultati. Vengono introdotte le proporzioni e la proporzionalità e le sue applicazioni alla risoluzione di problemi del tre semplice e del tre composto, di ripartizione e di applicazioni di matematica finanziaria.
Nelle scuole medie superiori si studiano i logaritmi; anche qui si è passati dall'apprendimento dell'uso del regolo calcolatore a quello del calcolatore o del computer. Entrambi questi strumenti sono ampiamente utilizzati terminati gli studi per eseguire calcoli numerici.
</text>
</doc>
<doc id="95417" url="https://it.wikipedia.org/wiki?curid=95417">
<title>Dimostrazione matematica</title>
<text>
Una dimostrazione matematica è un processo di deduzione che, partendo da premesse assunte come valide ("ipotesi") o da proposizioni dimostrate in virtù di queste premesse, determina la necessaria validità di una nuova proposizione in virtù della (sola) correttezza formale del ragionamento.
Il termine "dimostrare" deriva dal latino "demonstrare", composto dalla radice "de-" (di valore intensivo) e da "monstrare" ("mostrare", "far vedere"), da cui il significato di "mostrare a tutti" quella che viene considerata una verità. In matematica, però, il concetto 
viene appunto specializzato, e una dimostrazione ha una formulazione molto precisa: per dimostrare un'affermazione (la tesi), occorre partire da 
una o più affermazioni considerate vere (le "ipotesi"), usando un insieme ben definito di derivazioni logiche formali. In pratica, la catena di passaggi formali viene spesso in larga parte sottintesa, in modo da ridurre l'estensione della dimostrazione scritta ed evitare di appesantirla con puntualizzazioni considerate evidenti e immediate; tuttavia, in linea teorica, questo processo deduttivo può sempre essere applicato nelle dimostrazioni di natura matematica.
La dimostrazione matematica è generalmente deduttiva; da ipotesi generali si giunge a una tesi particolare. Esiste anche la dimostrazione 
induttiva; a differenza dell'uso comune del termine, che fa giungere ad una verità generale partendo da elementi particolari, la dimostrazione 
matematica induttiva deve essere presa come assioma, ad esempio nella formulazione di Peano.
Un'altra caratterizzazione delle dimostrazioni matematiche distingue una dimostrazione diretta, nella quale viene effettivamente dimostrata la 
tesi, dalla dimostrazione indiretta nella quale la tesi si suppone vera e si deve giungere alla ipotesi tramite passaggi logici o per assurdo, nella quale si suppone che la tesi non sia vera e si giunge a una contraddizione. Questo secondo tipo di dimostrazione, che si appoggia al 
principio del terzo escluso e sul quale si basano un gran numero di teoremi matematici, non è però considerato valido dalla scuola intuizionista fondata da Brouwer.
</text>
</doc>
<doc id="56184" url="https://it.wikipedia.org/wiki?curid=56184">
<title>Determinante (algebra)</title>
<text>
In algebra lineare, il determinante di una matrice quadrata formula_1 è un numero che descrive alcune proprietà algebriche e geometriche della matrice.
Esso viene generalmente indicato con formula_2 e, a volte, con formula_3. Quest'ultima notazione è più compatta, ma anche più ambigua, in quanto utilizzata talvolta per descrivere una norma della matrice.
Il determinante è un potente strumento usato in vari settori della matematica: innanzitutto nello studio dei sistemi di equazioni lineari, quindi nel calcolo infinitesimale a più dimensioni (ad esempio nello Jacobiano), nel calcolo tensoriale, nella geometria differenziale, nella teoria combinatoria, ecc.
Il significato geometrico principale del determinante si ottiene interpretando la matrice quadrata formula_1 di ordine formula_5 come trasformazione lineare di uno spazio vettoriale a formula_5 dimensioni: con questa interpretazione, il valore assoluto di formula_7 è il fattore con cui vengono modificati i volumi degli oggetti contenuti nello spazio (anche se ciò è improprio senza considerare il significato di misura). Se è diverso da zero, il segno del determinante indica inoltre se la trasformazione formula_8 preserva o cambia l'orientazione dello spazio rispetto agli assi di riferimento.
Il determinante di una matrice 2 × 2 è pari a:
Per definire il determinante di una generica matrice quadrata formula_10 si possono seguire due approcci: quello assiomatico, che definisce il determinante come l'unica quantità che soddisfa alcuni assiomi, e quello costruttivo tramite una formula esplicita. Esistono poi vari metodi di calcolo che risultano più agevoli a seconda del contesto.
Sia formula_11 lo spazio vettoriale delle matrici quadrate formula_12 a valori nel campo formula_13 (ad esempio, il campo dei numeri reali o complessi).
Il determinante è l'unica funzione formula_14 avente le proprietà seguenti: 
Le proprietà elencate hanno un significato geometrico: sono le proprietà che deve verificare una funzione il cui valore assoluto è il volume del poliedro individuato dai vettori riga della matrice formula_17 e il cui segno è positivo se e solo se tali vettori sono equiorientati alla base canonica.
Il determinante di una matrice formula_28 può essere definito in un modo più costruttivo, tramite la "formula di Leibniz":
Nella formula, formula_30 è l'insieme di tutte le permutazioni formula_31 dell'insieme numerico formula_32 e formula_33 denota il segno della permutazione (formula_34 se formula_31 è una permutazione pari, −1 se è dispari).
Da questa formula si vede che il numero di elementi della sommatoria è uguale a formula_36 (la cardinalità di formula_37).
Per esempio, il determinante di una matrice () è
In particolare:
Quest'ultima formula può essere memorizzata tramite la regola di Sarrus (che non è però estendibile ai casi formula_46).
La complessità della definizione costruttiva (comprese la generazione delle permutazioni) è elevata:
La definizione costruttiva del determinante è spesso complicata da usare per un calcolo concreto, perché si basa su una somma di ben formula_36 addendi. Esistono altri algoritmi che consentono di calcolare il determinante più facilmente. Ciascun metodo ha una efficienza variabile, dipendente dalla grandezza della matrice e dalla presenza di zeri.
Il determinante di una matrice 2 × 2 è pari a:
Il valore assoluto di questa espressione è pari all'area del parallelogramma con vertici in formula_50 e formula_51. Il segno del determinante (se questo è diverso da zero) dipende invece dall'ordine ciclico con cui compaiono i vertici del parallelogramma (il segno è negativo se il parallelogramma è stato "ribaltato", e positivo altrimenti).
Come spiegato più sotto, questa proprietà geometrica si estende anche in dimensioni maggiori di 2: il determinante di una matrice formula_52 è ad esempio il volume del poliedro i cui vertici si ricavano dalle colonne della matrice con lo stesso procedimento visto.
Il determinante di una matrice 3 × 3 è pari a:
Un metodo mnemonico per ricordare questa formula, espresso dalla regola di Sarrus (questo metodo non si estende a matrici più grandi), prevede di calcolare i prodotti dei termini sulle diagonali "continue". Ripetendo a destra della matrice le sue prime due colonne:
i prodotti delle componenti sulle 3 "diagonali" che partono dall'alto a sinistra (diagonali principali) sono formula_55, formula_56 e formula_57, mentre sulle 3 "diagonali" che partono dal basso a sinistra (diagonali secondarie) si trovano formula_58, formula_59, formula_60. Il determinante della matrice è esattamente la differenza tra la somma dei primi tre termini formula_61 e la somma degli ultimi tre formula_62.
Notare che il valore del determinante equivale in questo caso al prodotto misto tra i vettori:
e il suo valore assoluto equivale al volume del parallelepipedo che ha i tre vettori come spigoli.
Lo sviluppo di Laplace è un metodo di calcolo del determinante, che risulta efficiente solo per matrici molto piccole o contenenti un gran numero di zeri. Si procede scegliendo una riga, la formula_64-esima, tramite la formula:
dove formula_66 è il complemento algebrico della coppia formula_67, cioè formula_66 è data da formula_69 per il determinante (minore) di ordine formula_70 ottenuto dalla matrice formula_8 eliminando la riga formula_64-esima e la colonna formula_73-esima.
Esiste uno sviluppo analogo anche lungo la formula_73-esima colonna.
La definizione assiomatica fornisce un altro utile strumento di calcolo del determinante, che si basa su questi due principi:
Supponiamo di voler calcolare il determinante di:
Si può procedere direttamente tramite la definizione costruttiva:
Alternativamente si può utilizzare lo sviluppo di Laplace secondo una riga o una colonna. Conviene scegliere una riga o una colonna con molti zeri, in modo da ridurre gli addendi dello sviluppo; nel nostro caso sviluppiamo secondo la seconda colonna:
Lo sviluppo di Laplace può essere combinato con alcune mosse di Gauss. Ad esempio qui risulta particolarmente vantaggioso sommare la seconda colonna alla prima:
Questa mossa non cambia il determinante. Sviluppando lungo la prima colonna si ottiene quindi ancora:
Dalle proprietà elencate nella definizione assiomatica, è facile dedurre che:
Il determinante è una funzione moltiplicativa, nel senso che vale il teorema di Binet:
Una matrice quadrata formula_1 con valori in un campo formula_13 è invertibile se e solo se formula_93. In caso affermativo vale l'uguaglianza:
Le proprietà appena elencate mostrano che l'applicazione:
dal gruppo generale lineare negli elementi non nulli di formula_13 è un omomorfismo di gruppi.
Come conseguenza del teorema di Binet, se formula_16 è la matrice identità di tipo formula_98 e formula_99 uno scalare, è facile verificare che formula_100. Infatti:
Una matrice e la sua trasposta hanno lo stesso determinante:
Se formula_8 e formula_104 sono simili (cioè esiste una matrice invertibile formula_105 tale che formula_8 = formula_107) allora per il teorema di Binet formula_108
Questo significa che il determinante è un invariante per similitudine. Da questo segue che il determinante di una trasformazione lineare formula_109 è ben definito (non dipende dalla scelta di una base per lo spazio vettoriale formula_110).
D'altra parte, esistono matrici con lo stesso determinante che non sono simili.
Nel campo dei numeri reali, il segno del determinante è anche invariante per congruenza.
Il determinante di una matrice triangolare è il prodotto degli elementi nella diagonale.
Se formula_8 è di tipo formula_112 con valori reali o complessi e ha tutti gli autovalori formula_113 nel campo (contati con molteplicità), allora:
Questa uguaglianza segue dal fatto che formula_8 è sempre simile alla sua forma normale di Jordan, che è una matrice triangolare superiore con gli autovalori sulla diagonale principale.
Dal collegamento fra determinante e autovalori si può derivare una relazione fra la funzione traccia, la funzione esponenziale e il determinante:
Il determinante può considerarsi una funzione polinomiale:
quindi essa è differenziabile rispetto a ogni variabile corrispondente al valore che può assumere in una casella e per qualunque suo valore. Il suo differenziale può essere espresso mediante la formula di Jacobi:
dove formula_119 denota la trasposta della matrice dei cofattori (detta anche "dei complementi algebrici") di formula_8, mentre formula_121 ne denota la traccia. In particolare, se formula_8 è invertibile si ha:
o, più colloquialmente, se i valori della matrice formula_105 sono sufficientemente piccoli:
Il caso particolare di formula_8 coincidente con la matrice identità formula_127 comporta:
Il determinante è utile a calcolare il rango di una matrice e quindi a determinare se un sistema di equazioni lineari ha soluzione, tramite il teorema di Rouché-Capelli. Quando il sistema ha una sola soluzione, questa può essere esplicitata usando il determinante, mediante la regola di Cramer.
Una matrice è detta "singolare" se ha determinante nullo. Una matrice singolare non è mai invertibile, e se è definita su un campo vale anche l'inverso: una matrice non singolare è sempre invertibile.
Una trasformazione lineare del piano, dello spazio, o più in generale di uno spazio euclideo o vettoriale (di dimensione finita) formula_129 è rappresentata (dopo aver scelto una base) da una matrice quadrata formula_1. Il determinante è una quantità che non dipende dalla base scelta, e quindi solo dalla funzione formula_131: si può quindi parlare di "determinante di formula_131", che si indica con formula_133.
Molte affermazioni su formula_131 sono equivalenti:
Quindi ciascuna di queste affermazioni equivalenti è vera se e solo se il determinante non è zero.
Il determinante consente di trovare gli autovalori di una matrice formula_8 mediante il suo polinomio caratteristico:
dove formula_16 è la matrice identità avente stesso numero di righe di formula_1.
Dati formula_5 vettori nello spazio euclideo formula_149, sia formula_1 la matrice avente come colonne questi vettori. Le seguenti affermazioni sono equivalenti:
Se gli formula_5 vettori formano una base, allora il segno di formula_157 determina l'orientazione della base: se positivo, la base forma un "sistema di riferimento destrorso", mentre se è negativo si parla di "sistema di riferimento sinistrorso" (in analogia con la regola della mano destra).
Il valore assoluto formula_158 del determinante è uguale al volume del parallelepipedo sotteso dai vettori dati dalle colonne di formula_1 (il parallelepipedo è in realtà un parallelogramma se formula_42, e un solido di dimensione formula_161 in generale). Più in generale, data una trasformazione lineare:
rappresentata da una matrice formula_8, e un qualsiasi sottoinsieme formula_164 di formula_165 misurabile secondo Lebesgue, il volume dell'immagine formula_166 è dato da:
Ancora più in generale, se la trasformazione lineare formula_168 è rappresentata da una matrice formula_8 di tipo formula_170 e formula_171 è un sottoinsieme di formula_172 misurabile secondo Lebesgue, allora il volume di formula_166 è dato da:
Lo pfaffiano è un analogo del determinante per matrici antisimmetriche di tipo formula_175 . Si tratta di un polinomio di grado formula_176 il cui quadrato è uguale al determinante della matrice.
Per gli spazi a infinite dimensioni non si trova alcuna generalizzazione dei determinanti e della nozione di volume. Sono possibili svariati approcci, inclusa la utilizzazione dell'estensione della traccia di una matrice.
Se formula_177 è uno spazio vettoriale di dimensione finita formula_176 sul campo formula_179 allora è possibile definire il determinante di un endomorfismo formula_180 direttamente, senza fare ricorso a una base di formula_177. Sia formula_182 lo spazio vettoriale degli formula_183vettori di formula_177. Consideriamo l'endomorfismo formula_185 di formula_182 definito di modo che:
per ogni formula_188, ed esteso per linearità a tutto formula_182. Poiché formula_182 ha dimensione uguale a 1 risulta che formula_185 altro non è che la moltiplicazione per uno scalare. Quindi possiamo definire il determinante di formula_192 attraverso l'equazione:
per ogni formula_188. A questo punto seguono tutte le proprietà del determinante, in particolare è immediato che formula_195 dove formula_196 è l'endomorfismo identità di formula_177. Se formula_198 è un altro endomorfismo di formula_177 allora:
da cui formula_202. Se formula_192 non è un isomorfismo allora l'immagine di formula_192 ha dimensione strettamente minore di formula_176 e quindi formula_206 sono sicuramente linearmente dipendenti, essendo che formula_207 è una forma multilineare alternante segue che formula_208 e quindi formula_209. Si verifica che fissata una base su formula_177 il determinante della matrice associata a formula_192 rispetto a tale base coincide con il determinante di formula_192.
</text>
</doc>
<doc id="97243" url="https://it.wikipedia.org/wiki?curid=97243">
<title>Sistema di equazioni</title>
<text>
In matematica, un sistema di equazioni è un insieme di due o più equazioni che ammettono le stesse soluzioni.
Ad esempio:
è un sistema con due equazioni e due incognite che descrive l'intersezione di una circonferenza e una retta nel piano cartesiano.
La scrittura generica di un sistema di formula_2 equazioni in formula_3 incognite è la seguente:
dove formula_5 esprimono le funzioni delle incognite.
L"'insieme ambiente" formula_6 è l'insieme dei valori che possono assumere le variabili, ed è specificato "a priori". Generalmente, si assume che le variabili siano reali, e che le funzioni abbiano senso per ogni valore dell'insieme ambiente. Spesso l'insieme ambiente viene determinato "a posteriori" valutando per quali valori reali il sistema ha senso (valutando quindi il suo insieme di definizione). Ad esempio, il sistema
ha senso per ogni coppia di numeri reali formula_8 con formula_9.
Formalmente, l'insieme ambiente è quindi un sottoinsieme dello spazio euclideo formula_10, dove formula_3 è il numero di incognite.
In generale, i sistemi possono essere studiati anche con variabili non reali: possono essere ad esempio complesse, o più generalmente appartenere a qualche anello o campo.
"Risolvere un sistema" significa determinare l'insieme formula_12 dei valori che, sostituiti alle variabili, verificano tutte le equazioni. L'insieme formula_12 è un sottoinsieme dell'insieme ambiente, e prende il nome di "insieme delle soluzioni"; ciascuno dei suoi elementi è una "soluzione" del sistema.
Se formula_14 è l'insieme delle soluzioni della formula_15-esima equazione, abbiamo
I metodi di risoluzione più elementari si basano su operazioni che trasformano il sistema in un altro equivalente, ma più semplice. Negli esempi successivi si prendono in considerazione solo sistemi lineari per la loro facilità di risoluzione, ma questi metodi possono essere usati anche in altri casi.
Si esplicita un'incognita esprimendola in funzione delle altre (per esempio formula_17 diventa formula_18) in una delle equazioni del sistema e si sostituisce l'espressione così ottenuta nelle altre equazioni in luogo dell'incognita corrispondente. In questo modo l'incognita sparisce da tutte le equazioni eccetto la prima. Si applica iterativamente il metodo fino a giungere a un'equazione con una sola incognita; si calcola il valore di quest'ultima e si risale fino alla prima esplicitando via via i valori delle incognite calcolate.
Si esplicita, in due delle equazioni, una delle variabili (o in generale, una stessa quantità), ottenendo così di poter eguagliare i secondi membri (che risulteranno indipendenti dalla variabile esplicitata) per la proprietà transitiva dell'uguaglianza. L'equazione così composta potrà essere riscritta al posto di una delle due precedenti, ottenendo un sistema equivalente.
Dato un sistema lineare nella forma
dove formula_20 è il vettore colonna delle incognite, formula_21 è il vettore colonna dei termini noti e formula_6 è la matrice dei coefficienti ed è quadrata e invertibile, la soluzione è unica ed è pari al prodotto:
dove formula_24 è l'inversa di formula_6. Il calcolo della matrice inversa è spesso complicato e oneroso dal punto di vista computazionale, ragion per cui un sistema lineare normalmente non viene risolto calcolando direttamente la matrice inversa.
Di grande importanza teorica per i sistemi lineari, ma non utilizzata in pratica per motivi simili, è la regola di Cramer.
Di uso generale per sistemi con migliaia di equazioni è invece il metodo di eliminazione di Gauss, che si basa sul metodo di riduzione.
Il metodo di riduzione è specifico per i sistemi lineari. Il procedimento consiste nel sostituire una delle equazioni del sistema con una opportuna combinazione lineare di due equazioni del sistema stesso, ottenendo un sistema equivalente a quello dato. Più precisamente, se due righe sono espresse come prodotto tra opportune sottomatrici dei coefficienti e il vettore x delle soluzioni, ovvero 
allora è possibile sostituire una delle due con l'equazione 
dove formula_2 e formula_3 sono due numeri scalari qualsiasi, entrambi diversi da zero.
Lo studio dei sistemi non polinomiali è spesso molto difficile, e nella maggior parte dei casi non esistono formule o algoritmi che permettano di descrivere precisamente l'insieme delle soluzioni. Anche i sistemi polinomiali di grado basso sono spesso non risolvibili.
Spesso si ovvia a questo problema "linearizzando il sistema", studiando cioè le soluzioni di un sistema lineare che approssimi il sistema dato. In questo modo è spesso possibile ottenere una descrizione qualitativa o approssimativa delle soluzioni.
Si vuole utilizzare il metodo risolutivo per sostituzione. Esplicitiamo formula_31 nella prima equazione e sostituiamolo dove compare nelle altre:
Ora calcoliamo formula_33 nella seconda in funzione di formula_34:
In questo modo la terza equazione adesso contiene solo formula_34: risolvendola viene
Quindi ora calcolando la formula_33 nella seconda viene la soluzione
Si vuole utilizzare il metodo risolutivo per confronto. Isoliamo la variabile z nella prima e seconda equazione:
Confrontiamo le due espressioni risultanti:
Da cui risulta:
E risolvendo per sostituzione tra le prime due equazioni:
Dunque:
</text>
</doc>
<doc id="41866" url="https://it.wikipedia.org/wiki?curid=41866">
<title>Notazione scientifica</title>
<text>
La notazione scientifica (indicata anche come notazione esponenziale) è un modo conciso di esprimere numeri reali con molte cifre che altrimenti sarebbe poco conveniente rappresentare in notazione decimale. Per ottenere questo risultato si utilizzano potenze intere della base utilizzata per la notazione posizionale in uso. 
Utilizzando la notazione scientifica, per esempio, tutti i numeri in base dieci vengono rappresentati nella forma: 
"m" × 10 
("m" volte "dieci" elevato alla potenza di "n") dove l'esponente "n" è un numero intero e il coefficiente "m" è un qualsiasi numero reale. L'esponente "n" viene chiamato "ordine di grandezza" e il coefficiente "m" viene chiamato "mantissa". Il termine "mantissa" potrebbe tuttavia causare confusione, dal momento che è anche il nome della parte decimale di un logaritmo. 
La maggior parte delle calcolatrici e dei programmi per computer presentano i numeri molto grandi e molto piccoli usando la notazione scientifica. Il 10 è normalmente omesso e la lettera E è usata per indicare l'esponente: per esempio, 1,56234 E29. Da notare che questa E non ha relazioni con la costante matematica e.
In termini più generali possiamo dire che un numero "reale" formula_1 può essere rappresentato in una base formula_2 in questo modo: formula_3
Il "." è detto punto radice, mentre le prime cifre della mantissa (formula_4), sono dette cifre significative (o essenziali)
La notazione scientifica è molto utile per esprimere grandezze fisiche, dato che è possibile scrivere solo le cifre significative, rendendo leggibili testi che trattano quantità molto grandi o molto piccole in semplicità e chiarezza senza alterare il valore iniziale.
Qualsiasi numero intero può essere scritto nel formato "m" × 10 in molti modi: ad esempio, 350 può essere scritto come 3,5 × 10 o 35 × 10 o 350 × 10.
Nella notazione normalizzata, l'esponente "n" è scelto in modo che il valore assoluto di "m" rimanga almeno uno ma inferiore a dieci (1 ≤ |"m"| &lt; 10). Quindi 350 è scritto come 3,5 × 10. Questa forma consente un facile confronto dei numeri, in quanto l'esponente "n" fornisce l'ordine di grandezza del numero. Nella notazione normalizzata, l'esponente "n" è negativo per un numero con valore assoluto compreso tra 0 e 1 (ad es. 0,5 è scritto come 5 × 10). Il 10 e l'esponente sono spesso omessi quando l'esponente è 0.
La forma scientifica normalizzata è la tipica forma di espressione di grandi numeri in molti campi, a meno che non si desideri una forma non normalizzata, come la notazione ingegneristica. La notazione scientifica normalizzata viene spesso chiamata notazione esponenziale, anche se quest'ultimo è più generale e si applica anche quando "m" non è limitato all'intervallo da 1 a 10 (come nella notazione di ingegneria per esempio) e a basi diverse da 10 (come in 3,15 × 2).
La notazione ingegneristica (spesso indicata come modalità di visualizzazione "ENG" sui calcolatori scientifici) differisce dalla notazione scientifica normalizzata in quanto l'esponente "n" è limitato a multipli di 3. Di conseguenza, il valore assoluto di "m" è compreso nell'intervallo 1 ≤ |"m"| &lt; 1000, anziché 1 ≤ |"m"| &lt; 10. Sebbene simile nel concetto, notazione ingegneristica è raramente chiamata notazione scientifica. La notazione ingegneristica consente ai numeri di corrispondere esplicitamente ai prefissi SI corrispondenti, il che facilita la lettura e la comunicazione orale. Ad esempio, 12,5 × 10 m possono essere letti come "dodici virgola cinque nanometri" e scritti come 12,5 nm, mentre la sua notazione scientifica equivalente a 1,25 × 10 m verrebbe probabilmente letta come "uno virgola venticinque per dieci alla meno otto metri".
Una cifra significativa è una cifra in un numero che contribuisce a migliorarne la precisione. Sono definiti significativi tutti i numeri diversi da zero, gli zeri posizionati in mezzo a cifre significative e gli zeri esplicitamente indicati come significativi. Gli zeri all'inizio ed alla fine di un numero non sono significativi perché servono unicamente a definirne l'ordine di grandezza. Quindi, 1 230 400 normalmente presenta cinque cifre significative: 1, 2, 3, 0, e 4; gli ultimi due zeri non contribuiscono ad aumentare la precisione del valore.
Quando un numero viene convertito in notazione normalizzata in base dieci, la mantissa mantiene tutte le cifre significative, mentre gli zeri all'inizio e alla fine del numero vengono scartati. Dunque 1 230 400 diventa 1,2304 × 10. Di conseguenza questa notazione rende il numero di cifre significative facilmente identificabile.
</text>
</doc>
<doc id="41930" url="https://it.wikipedia.org/wiki?curid=41930">
<title>Espressione matematica</title>
<text>
Un'espressione matematica è un insieme di numeri legati da segni di operazioni matematiche, detti operatori matematici.
Le espressioni e la loro valutazione furono formalizzate da Alonzo Church e Stephen Kleene negli anni 1930 nel loro lambda calcolo. Il calcolo lambda ha avuto importanti implicazioni nello sviluppo della matematica moderna e dei linguaggi di programmazione per computer.
Le espressioni possono essere valutate a valori, e si può dire che rappresentano quei valori. La determinazione del valore di un'espressione dipende dalla definizione degli operatori matematici e del sistema di valori che forma il suo contesto.
Le espressioni possono avere "variabili libere" che non sono definite nell'espressione, ma si ricavano dal contesto. Due espressioni si dicono equivalenti se, valutate, determinano lo stesso valore.
Uno dei risultati più interessanti del calcolo lambda è che l'equivalenza di due espressioni è in alcuni casi indecidibile. Ciò è vero anche per espressioni in qualunque sistema che ha potenza equivalente al calcolo lambda.
Nelle espressioni valgono alcune convenzioni da rispettare riguardo l'ordine delle operazioni:
</text>
</doc>
<doc id="56928" url="https://it.wikipedia.org/wiki?curid=56928">
<title>Sistema di equazioni lineari</title>
<text>
In matematica, e in particolare in algebra lineare, un sistema di equazioni lineari, anche detto sistema lineare, è un sistema composto da più equazioni lineari che devono essere verificate tutte contemporaneamente. Una soluzione del sistema è un vettore i cui elementi sono le soluzioni delle equazioni che compongono il sistema, ovvero tali che se sostituiti alle incognite rendono le equazioni delle identità.
Un sistema di equazioni lineari è un insieme di formula_1 equazioni lineari in formula_2 incognite, che può essere scritto nel modo seguente:
Il numero formula_2 delle incognite è detto anche ordine del sistema.
Se termini noti formula_5 sono tutti nulli il sistema è detto "omogeneo".
Una formula_2-upla formula_7 di elementi nel campo è una "soluzione" del sistema se soddisfa tutte le formula_1 equazioni.
Due sistemi si dicono "equivalenti" se hanno lo stesso insieme di soluzioni. In particolare, due sistemi lineari sono equivalenti se ogni equazione di uno è combinazione lineare delle equazioni dell'altro.
In notazione indiciale il sistema si scrive:
Definendo i vettori dei coefficienti:
e il vettore degli formula_1 termini noti:
il sistema è equivalente alla combinazione lineare:
Definendo formula_14 il vettore delle formula_2 incognite:
ciascuna equazione è equivalente ad un prodotto scalare standard:
Se il sistema è omogeneo il vettore delle incognite è quindi ortogonale ai vettori dei coefficienti.
Usando le matrici ed il prodotto scalare fra matrici (prodotto riga per colonna) si possono separare i coefficienti, le incognite ed i termini noti del sistema, scrivendolo nel modo seguente:
Ora seformula_21 è la matrice formula_22 dei coefficienti:
di cui in effetti formula_24 sono le colonne, con le definizioni del vettore delle incognite e di quello dei termini noti il sistema si scrive finalmente in forma matriciale:
Il sistema può essere descritto usando la matrice completa:
detta matrice associata al sistema. Essa è ottenuta dalla giustapposizione della matrice dei coefficienti e del vettore dei termini noti.
Le matrici formula_27 e formula_28 sono dette rispettivamente matrice "incompleta" (o matrice dei coefficienti) e "completa" (o "orlata"). I numeri formula_29 sono le incognite, i numeri formula_30 sono i coefficienti ed i numeri formula_5 i termini noti. Coefficienti e termini noti sono elementi di un campo, ad esempio quello formato dai numeri reali o complessi.
Il grado di un sistema di equazioni polinomiali è definito come il prodotto dei gradi delle equazioni che lo compongono. Quindi un sistema lineare è un sistema polinomiale di primo grado.
In generale, un sistema lineare può essere:
Se il campo formula_32 di appartenenza di coefficienti e termini noti di un sistema di ordine formula_2 è infinito, ci sono tre possibilità: esiste una sola soluzione, non ci sono soluzioni oppure ce ne sono infinite. Il teorema che asserisce questo fatto e che permette di stabilire se e quante soluzioni esistono senza risolvere il sistema è il teorema di Rouché-Capelli. Nel caso in cui esistano soluzioni, queste formano un sottospazio affine di formula_34.
Si consideri l'operazione lineare:
Il nucleo di formula_36 è lo spazio delle soluzioni del sistema omogeneo associato, mentre l'immagine è lo spazio generato dalle colonne formula_37. Per il teorema del rango segue che la dimensione dello spazio delle soluzioni più il rango per colonne di formula_21 è pari ad formula_2.
Essendo il vettore delle incognite ortogonale ai vettori riga della matrice dei coefficienti, lo spazio delle soluzioni è il complemento ortogonale del sottospazio generato dalle righe di formula_21. La somma delle rispettive dimensioni deve pertanto essere pari ad formula_2.
Dalle due affermazioni precedenti si conclude che il rango formula_42 per righe è pari al rango per colonne, e che lo spazio delle soluzioni ha dimensione formula_43. Lo spazio delle soluzioni è dunque un sottospazio vettoriale di dimensione formula_44.
Il sistema ammette soluzione se e solo se il vettore formula_45 è l'immagine del vettore formula_46 ottenuta mediante l'applicazione lineare formula_47 definita nel seguente modo:
L'immagine di formula_49 è generata dai vettori dati dalle colonne di formula_50, e quindi formula_45 è nell'immagine se e solo se lo span delle colonne di formula_21 contiene formula_45, cioè se e solo se lo spazio generato dalle colonne di formula_50 è uguale allo spazio generato dalle colonne di formula_55. In modo equivalente il sistema ammette soluzione se e solo se le due matrici abbiano lo stesso rango, come stabilisce il teorema di Rouché-Capelli.
Se esiste una soluzione formula_56, ogni altra soluzione si scrive come formula_57, dove formula_58 è una soluzione del sistema lineare omogeneo associato:
Infatti:
Lo spazio delle soluzioni, ottenuto traslando il nucleo con il vettore formula_61, è quindi il sottospazio affine dato da:
La dimensione dello spazio delle soluzioni del sistema completo è uguale alla dimensione dello spazio delle soluzioni del sistema omogeneo associato. Per il teorema di Rouché-Capelli tale soluzione è unica se e solo se il rango della matrice formula_21 è formula_2. Altrimenti se il campo formula_32 è infinito esistono infinite soluzioni, e queste formano un sottospazio vettoriale di formula_66, avente come dimensione la nullità formula_67 della matrice.
Dato un sistema lineare nella forma
dove formula_69 è il vettore colonna delle incognite, formula_70 è il vettore colonna dei termini noti e formula_50 è la matrice dei coefficienti ed è quadrata e invertibile, la soluzione è unica ed è pari al prodotto:
dove formula_73 è l'inversa di formula_50. Il calcolo della matrice inversa è spesso complicato e oneroso dal punto di vista computazionale, ragion per cui un sistema lineare normalmente non viene risolto calcolando direttamente la matrice inversa.
Di grande importanza teorica per i sistemi lineari, ma non utilizzata in pratica per motivi simili, è la regola di Cramer.
Di uso generale per sistemi con migliaia di equazioni è invece il metodo di eliminazione di Gauss, che si basa sul metodo di riduzione.
Il metodo di riduzione è specifico per i sistemi lineari. Il procedimento consiste nel sostituire una delle equazioni del sistema con una opportuna combinazione lineare di due equazioni del sistema stesso, ottenendo un sistema equivalente a quello dato. Più precisamente, se due righe sono espresse come prodotto tra opportune sottomatrici dei coefficienti e il vettore x delle soluzioni, ovvero 
allora è possibile sostituire una delle due con l'equazione 
dove formula_77 e formula_78 sono due numeri scalari qualsiasi, entrambi diversi da zero.
</text>
</doc>
<doc id="56968" url="https://it.wikipedia.org/wiki?curid=56968">
<title>Numeri pari e dispari</title>
<text>
In matematica, ogni numero intero è pari oppure dispari: un numero è pari se è multiplo di 2, altrimenti è dispari. Esempi di numero pari sono: −56, 0, 12, 28, 56, 388. Esempi di numero dispari: −7, 19, 83, 95, 463, 1005, 32721.
L'insieme dei numeri pari può essere scritto nel seguente modo:
L'insieme dei numeri dispari può essere scritto nel seguente modo:
La caratterizzazione di un intero relativa all'essere pari o dispari si dice parità. Essa equivale alla appartenenza ad una delle due classe di resti modulo 2: [0] per gli interi pari, [1] per i dispari.
Un numero espresso con il sistema di numerazione decimale è pari o dispari a seconda che la sua ultima cifra sia pari o dispari. Ovvero, se l'ultima cifra è 1, 3, 5, 7, o 9, è dispari, altrimenti è pari. La stessa idea è valida se si usa una qualsiasi base pari. In particolare, un numero espresso nel sistema di numerazione binario è dispari se l'ultima cifra è 1 e pari se l'ultima cifra è 0; un intero espresso nella base 4 è pari se la sua ultima cifra è 0 o 2, è dispari in caso contrario, cioè se la sua ultima cifra è 1 o 3. In sistemi di numerazione a base dispari, il numero è pari o dispari a seconda della parità delle somma delle sue cifre, ovvero a seconda della sua radice fondamentale.
I numeri pari formano un ideale nell'anello degli interi, i numeri dispari invece no. Un intero è pari se è congruente a 0 modulo l'ideale, in altre parole se è congruente a 0 modulo 2, e dispari se è congruente a 1 modulo 2.
Tutti i numeri primi sono dispari con una eccezione: il numero primo 2. Tutti i numeri perfetti conosciuti sono pari; non si sa se esistano dei numeri perfetti dispari.
La congettura di Goldbach asserisce che qualsiasi numero pari maggiore di 2 può essere rappresentato come una somma di due numeri primi. I calcoli eseguiti con i moderni computer hanno mostrato che questa congettura è vera per interi fino ad almeno 4 × 10, ma non è ancora stata trovata una dimostrazione matematica generale.
Le leggi seguenti possono essere verificate usando le proprietà di divisibilità, e il fatto che 2 è un numero primo:
La divisione di due numeri interi non dà necessariamente come risultato un numero intero. Ad esempio, 1 diviso 4 è uguale a 1/4, che non è né pari né dispari, in quanto il concetto di pari o dispari si applica solo agli interi.
Ma quando il risultato è un intero:
</text>
</doc>
<doc id="39507" url="https://it.wikipedia.org/wiki?curid=39507">
<title>Matrice invertibile</title>
<text>
In matematica, in particolare in algebra lineare, una matrice quadrata è detta invertibile, o regolare, se esiste un'altra matrice tale che il prodotto matriciale tra le due restituisce la matrice identità.
L'insieme delle matrici invertibili di dimensioni formula_1 è un gruppo moltiplicativo rispetto all'ordinaria operazione di prodotto matriciale; tale struttura algebrica è detta Gruppo generale lineare ed è indicata con il simbolo formula_2.
Una matrice quadrata formula_3 è detta invertibile se esiste una matrice formula_4 tale che:
dove formula_6 denota la matrice identità formula_7 e la moltiplicazione usata è l'ordinaria moltiplicazione di matrici.
Se è questo il caso, allora la matrice formula_8 è univocamente determinata da formula_9 ed è chiamata l"'inversa" di formula_9, indicata con formula_11.
Nella definizione, le matrici formula_12 e formula_13 hanno valori in un anello con unità.
Una matrice formula_12 è "singolare" se ha determinante uguale a zero. Tra le affermazioni elencate sotto, la più importante dice che se formula_12 ha valori in un campo, come ad esempio quello dei numeri reali o complessi, la matrice è invertibile se e solo se non è singolare.
Sia formula_9 una matrice quadrata formula_7 con valori in un campo formula_18 (ad esempio, il campo dei numeri reali o complessi).
Le seguenti affermazioni sono equivalenti e caratterizzano una matrice formula_12 invertibile:
Come conseguenza delle proprietà precedenti, l'insieme delle matrici invertibili formula_1 costituisce un gruppo con la moltiplicazione, noto come il gruppo generale lineare formula_2.
Poiché le matrici invertibili formano un gruppo, possono in molti casi essere manipolate come se fossero dei numeri reali. Ad esempio:
Sul campo dei numeri reali l'insieme di tutte le matrici formula_62 è uno spazio vettoriale isomorfo a formula_63, e il sottoinsieme delle matrici non invertibili è un insieme nullo, cioè ha misura di Lebesgue zero, essendo l'insieme degli zeri della funzione determinante, che è un polinomio. Intuitivamente, questo vuol dire che la probabilità che una matrice quadrata casuale a valori reali sia non-invertibile è zero. Parlando in modo approssimativo, si dice che "quasi tutte" le matrici sono invertibili.
Il teorema della matrice invertibile generalmente non vale in un anello commutativo. In questo caso, la matrice è invertibile se e solo se il suo determinante è una unità, ossia è invertibile, in questo anello.
Se formula_12 è invertibile, l'equazione formula_65 ha una sola soluzione, data da formula_66. Analogamente formula_67 ha come unica soluzione formula_68.
Nel caso particolare in cui formula_69 e formula_13 abbiano dimensioni formula_71, ovvero siano vettori colonna, l'equazione formula_65 rappresenta un sistema lineare, dove formula_12 è la matrice dei coefficienti.
formula_12 è invertibile se il sistema ha una soluzione unica o, in modo equivalente, se il sistema omogeneo associato ha come unica soluzione il vettore nullo.
Esistono vari metodi per il calcolo dell'inversa di una matrice quadrata invertibile formula_9.
La matrice inversa di una matrice 2 per 2 invertibile:
è la seguente:
Il metodo della matrice dei cofattori risulta particolarmente rapido quando non interessa calcolare tutti gli elementi della matrice inversa, e quando la matrice è di dimensione contenuta. Inoltre, la presenza di variabili letterali tra gli elementi non aumenta di molto la complessità del calcolo.
Data una matrice formula_12 quadrata e invertibile:
la sua inversa formula_80 è la seguente:
dove formula_82 è il determinante di formula_12, la matrice formula_84 è la matrice dei cofattori (o "dei complementi algebrici") e l'esponente formula_85 indica l'operazione di trasposizione di matrici.
Uno schema mnemonico per la variazione del segno formula_86 è il seguente:
L'algoritmo di Gauss-Jordan, può essere usato per trovare (quando esiste) l'inversa di una matrice. Funziona nel modo seguente: sia formula_12 una matrice invertibile. Si costruisce la matrice formula_89 con formula_90 righe e formula_91 colonne affiancando formula_12 e la matrice identità formula_93. A questo punto si applica l'algoritmo di Gauss-Jordan alla nuova formula_13. Questo algoritmo trasforma la matrice formula_13 in una matrice a scalini, che sarà del tipo formula_96. La matrice formula_97 così trovata è proprio l'inversa di formula_12.
L'esempio seguente mostra che l'inversa di:
è la matrice:
Infatti:
Nel primo passaggio si è moltiplicata la prima riga per formula_103, nel secondo si è sommata alla seconda riga la prima, nel terzo si è moltiplicata la seconda riga per formula_104, nel quarto passaggio si è sommata alla prima riga la seconda e infine nell'ultimo passaggio si è divisa la prima riga per formula_103 e la seconda per formula_106. In questo modo si è partiti da una matrice di formula_107 e si è arrivati a formula_108. Si ha che formula_97 è l'inversa di formula_12.
Data una matrice partizionata a blocchi:
in cui le sottomatrici sulla diagonale formula_112 e formula_113 sono quadrate e non singolari, si può dimostrare che l'inversa di formula_12 risulta uguale a:
dove formula_93 è una matrice identità di ordine appropriato e:
ovvero:
con:
</text>
</doc>
<doc id="71811" url="https://it.wikipedia.org/wiki?curid=71811">
<title>Potenza (matematica)</title>
<text>
In matematica, la potenza è un'operazione che associa a una coppia di numeri formula_1 e formula_2 detti rispettivamente base ed esponente, il numero dato dal prodotto di formula_3 fattori uguali ad formula_1:
in questo contesto formula_1 può essere un numero intero, razionale o reale mentre formula_3 è un numero intero positivo. Con opportune ipotesi su formula_1 è possibile considerare anche altri valori numerici per gli esponenti, ad esempio esponenti interi (anche non positivi), razionali o reali.
Le potenze scritte nella forma formula_9 si leggono come "formula_1 elevato alla formula_3" o più semplicemente "formula_1 alla formula_3". L'esponente è usualmente rappresentato come apice immediatamente a destra della base.
Alcuni esponenti hanno un loro nome. L'esponente due è spesso indicato come "al quadrato" (un numero alla seconda rappresenta l'area di un quadrato che abbia per lato quel valore) e l'esponente formula_14 come "al cubo" (un numero alla terza rappresenta il volume di un cubo che abbia per spigolo quel valore).
Esempi:
L'operazione si estende ad formula_19 ponendo per ogni formula_20
e ad formula_3 negativi ponendo
Ad esempio,
Le seguenti proprietà sono di immediata verifica nel caso in cui gli esponenti siano numeri interi positivi:
Estraendo fino ad avere formula_27 otteniamo il seguente risultato:
formula_28
Notiamo che la definizione formula_36 risulta ora più comprensibile poiché è consistente con le proprietà appena viste, infatti:
Si noti che formula_38 è un prodotto vuoto e pertanto è uguale a formula_39
E lo stesso vale per la definizione di formula_40, infatti:
Dato un numero reale non negativo formula_1 e un numero intero positivo formula_2 si chiama radice formula_3-esima di formula_1 quel numero reale non negativo formula_46 tale che formula_47, tale numero si indica con formula_48.
Da questa definizione si ha subito che
per ogni numero reale non negativo formula_1. Quindi è ragionevole (in virtù delle proprietà delle potenze) porre
In questo modo le proprietà delle potenze sono ancora rispettate, infatti
come avviene per la radice formula_3-esima.
Più in generale la definizione di potenza può essere estesa ulteriormente, con alcune restrizioni, consentendo all'esponente di essere un numero razionale formula_54, con formula_55 e formula_56 interi primi tra loro e formula_57, se si pone:
In questo caso:
Trascurando tali restrizioni e l'ipotesi formula_55 e formula_56 primi tra loro si cade in assurdi quali:
Il passaggio errato è il terzo, in quanto formula_69 non è definito in formula_70.
È possibile estendere la definizione dell'operazione di elevamento a potenza anche ai casi in cui base ed esponente sono dei generici numeri reali (con la base però sempre positiva) facendo in modo che si conservino le regole di operazione tra potenze e che la funzione potenza risultante sia una funzione continua, e questa estensione è unica. Si può in tal modo dare senso a espressioni come formula_71 o e.
Definiamo inizialmente formula_72 con la base formula_73 e l'esponente formula_74, entrambi numeri reali.
Possiamo scrivere formula_46 nella sua rappresentazione in base formula_76 con la scrittura:
La successione formula_78 dei numeri
è una successione di numeri razionali crescente che tende a formula_46.
La potenza formula_85 ha esponente razionale, quindi è stata definita.
La successione di numeri reali
è una successione anch'essa crescente (poiché formula_73), risulta quindi naturale definire il valore di formula_72 come l'estremo superiore di tale successione:
Nel caso in cui la base fosse un numero compreso tra formula_93 e formula_94 si può definire:
poiché formula_96 in questo caso è maggiore di formula_94 e quindi il secondo membro è definito.
Difatti, essendo formula_98, si ha la seguente successione di numeri reali (considerando formula_78 come prima):
che è una successione decrescente e quindi si può porre, in questo caso, formula_104.
</text>
</doc>
<doc id="66003" url="https://it.wikipedia.org/wiki?curid=66003">
<title>Prodotto vettoriale</title>
<text>
In matematica, in particolare nel calcolo vettoriale, il prodotto vettoriale è un'operazione binaria interna tra due vettori in uno spazio euclideo tridimensionale che restituisce un altro vettore che è normale al piano formato dai vettori di partenza.
Il prodotto vettoriale è indicato con il simbolo formula_1 o con il simbolo formula_2. Il secondo simbolo è però anche usato per indicare il "prodotto esterno" (o "prodotto wedge") nell'algebra di Grassmann, di Clifford e nelle forme differenziali. Storicamente, il prodotto esterno è stato definito da Grassmann circa trent'anni prima che Gibbs e Heaviside definissero il prodotto vettoriale.
Il prodotto vettoriale tra due vettori formula_3 e formula_4 in formula_5 è definito come il vettore a loro perpendicolare:
dove formula_7 è l'angolo tra formula_3 e formula_4 e formula_10 è un versore normale al piano formato da formula_3 e formula_4, che fornisce la direzione del prodotto vettoriale. Si nota che il modulo formula_13 del prodotto vettoriale è l'area del parallelogramma individuato dai due vettori formula_3 e formula_4.
Esplicitamente, detti formula_16, formula_17 e formula_18 i vettori di una base ortonormale di formula_5, il prodotto di formula_20 e formula_21 può essere scritto in tale base come il determinante di una matrice (con un abuso di notazione):
Poiché il prodotto vettoriale tra due vettori non appartiene allo spazio di partenza, ci si riferisce ad esso come a uno pseudovettore. Sono ad esempio degli pseudovettori (detti anche vettori assiali) il momento angolare, la velocità angolare, il campo magnetico.
Dal momento che vi sono due versori formula_10 e formula_24 perpendicolari sia ad formula_3 che a formula_4, convenzionalmente si sceglie formula_27 in modo tale che i vettori formula_3, formula_4 ed formula_30 siano orientati secondo un sistema destrogiro se il sistema di assi coordinati formula_31 è destrogiro, oppure sinistrogiro se il sistema di assi è sinistrogiro. L'orientazione del versore formula_27 dipende quindi dall'orientazione dei vettori nello spazio, ovvero dalla chiralità del sistema di coordinate ortonormali.
Un modo semplice per determinare il verso del prodotto vettore è la «regola della mano destra». In un sistema destrogiro si punta il pollice nella direzione del primo vettore, l'indice in quella del secondo, il medio dà la direzione del prodotto vettore. In un sistema di riferimento sinistrogiro (terna sinistrosa) basta invertire il verso del prodotto vettore, ovvero usare la mano sinistra.
Un altro semplice metodo è quello della "vite destrorsa". In un sistema destrogiro si simula il movimento di avvitatura o di svitatura di una vite destrorsa; guardato dall'alto, se ruotando il primo vettore verso il secondo la rotazione è oraria, la vite verrà avvitata e quindi il verso del vettore sarà rivolto verso il basso; viceversa, se si compie una rotazione antioraria, la vite sarà svitata ed il verso del vettore sarà rivolto verso l'alto.
Il prodotto vettoriale formula_33 può essere definito in termini del tensore di Levi-Civita formula_34 come:
dove gli indici formula_36 sono le componenti ortogonali del vettore, usando la notazione di Einstein.
Il prodotto triplo di tre vettori è definito come:
Si tratta del volume con segno del parallelepipedo con lati formula_57, formula_58 e formula_59, e tali vettori possono essere interscambiati:
Un altro prodotto a tre vettori, detto "doppio prodotto vettoriale", è legato al prodotto scalare dalla formula:
Come caso speciale si ha:
Si tratta di una relazione particolarmente utile nel calcolo differenziale, in quanto riguarda l'equivalenza tra il rotore doppio e la differenza fra il gradiente della divergenza formula_63 e il laplaciano formula_64.
Un'altra relazione che lega il prodotto vettoriale con il prodotto triplo è:
Mentre per:
e più in generale:
Un'utile identità è:
che può essere confrontata con l'identità di Lagrange espressa come:
in cui formula_3 e formula_4 sono vettori "n"-dimensionali. Questo mostra che la forma di volume Riemanniana per le superfici è esattamente l'elemento di superficie del calcolo vettoriale. Nel caso tridimensionale, combinando le due precedenti relazioni si ottiene il modulo del prodotto vettoriale scritto attraverso le componenti:
Si tratta di un caso speciale delle moltiplicazione formula_73 della norma nell'algebra dei quaternioni.
La regola di Leibniz si applica anche al prodotto vettoriale:
come si può dimostrare utilizzando la rappresentazione tramite moltiplicazione tra matrici.
Il prodotto vettoriale è largamente adoperato anche in fisica e in ingegneria, oltre che in geometria e in algebra.
Si riporta un elenco - non esaustivo - di alcune applicazioni.
Il momento angolare formula_75 di un corpo è definito come:
dove formula_77 è il vettore quantità di moto, mentre formula_78 è il vettore-posizione del corpo rispetto al polo di riferimento. 
Analogamente, il momento di una forza
è definito come:
dove formula_80 è la forza applicata al punto individuato dal raggio vettore formula_78.
Poiché posizione formula_78, quantità di moto formula_77
e forza formula_80 sono tutti "vettori polari", 
sia il momento angolare formula_75
sia il momento meccanico formula_86
sono "pseudo-vettori" o "vettori assiali"
Il prodotto vettoriale compare anche nella descrizione dei moti di rotazione.
Ad esempio, per due punti "P" e "Q" su un corpo rigido vale la seguente "legge di trasporto"
delle velocità:
dove formula_78 è la posizione di un punto, formula_89 la sua velocità
e formula_90 la velocità angolare del corpo rigido.
Poiché posizione formula_78 e velocità formula_89
sono "vettori polari", la velocità angolare formula_90 è uno "pseudo-vettore".
Data una particella puntiforme, la forza elettromagnetica esercitata su di essa è pari a:
dove:
Si noti che la componente magnetica della forza è proporzionale al prodotto vettoriale tra 
formula_89 e formula_101 , pertanto
essa risulta sempre perpendicolare alla velocità formula_89
e non compie lavoro.
Poiché velocità formula_89, forza formula_80
e campo elettrico formula_105 sono tutti "vettori polari", 
il campo magnetico formula_101 è uno "pseudo-vettore".
Il prodotto esterno (prodotto wedge) di due vettori è un bivettore, cioè un elemento di piano orientato (analogamente ad un vettore che può essere visto come un elemento di linea orientato). Dati due vettori formula_107 e formula_108, il bivettore formula_109 è il parallelogramma orientato formato dai due vettori formula_107 e formula_108. Il prodotto vettoriale si ottiene considerando il duale di Hodge del bivettore formula_109:
che mappa bivettori in vettori. Si può pensare a tale prodotto come un elemento multidimensionale, che in tre dimensioni è un vettore, che è "perpendicolare" al bivettore.
Non esiste un analogo del prodotto vettoriale in spazi di dimensione maggiore che restituisca un vettore. Il prodotto esterno, tuttavia, gode di proprietà molto simili, anche se produce un bivettore e non un vettore. Il duale di Hodge del prodotto wedge produce un vettore di formula_114 componenti che è una naturale generalizzazione del prodotto vettoriale in dimensione arbitraria.
Il prodotto vettoriale può essere visto come uno dei più semplici prodotti di Lie, ed è pertanto generalizzato dalle algebre di Lie, che sono assiomatizzate come prodotti binari soddisfacenti gli assiomi di multilinearità, antisimmetria e l'identità di Jacobi. Ad esempio, l'algebra di Heisenberg fornisce un'altra struttura di algebra di Lie su formula_5. Nella base formula_116 il prodotto è:
Un prodotto esterno per vettori 7-dimensionali può essere ottenuto similmente utilizzando gli ottonioni invece dei quaternioni. Invece non possono esistere altre estensioni del prodotto vettoriale che restituiscano un vettore
e ciò è collegato al fatto che le sole algebre di divisione normate sono quelle con dimensioni 1,2,4 e 8.
Se però si considera il risultato dell'operazione non più come un vettore o pseudovettore ma come una matrice, allora è possibile estendere l'idea di prodotto vettoriale in qualsiasi numero di dimensioni
In meccanica, ad esempio, la velocità angolare può essere interpretata sia come uno pseudo-vettore formula_118 sia come una matrice antisimmetrica formula_119. In quest'ultimo caso
la "legge di trasporto" delle velocità per un corpo rigido sarà:
dove formula_119 è definita formalmente a partire dalla matrice di rotazione formula_122 del corpo rigido:
formula_123
In ambito quantistico anche il momento angolare formula_124 viene spesso rappresentato con una matrice antisimmetrica
risultato di un prodotto tra posizione formula_125 e
quantità di moto formula_77:
Dato che formula_125 e formula_77 possono avere un numero arbitrario formula_130 di componenti, questa forma di prodotto "vettoriale" (che pure non produce un vettore) si può generalizzare in qualsiasi dimensione, pur conservando l'interpretazione "fisica" dell'operazione stessa.
Nel contesto dell'algebra multilineare il prodotto vettoriale può essere visto come un tensore (misto) di ordine (1,2), nello specifico una mappa bilineare, ottenuto da una forma di volume tridimensionale per innalzamento degli indici.
Il prodotto vettoriale × è rappresentato come:
</text>
</doc>
<doc id="66621" url="https://it.wikipedia.org/wiki?curid=66621">
<title>Campo vettoriale</title>
<text>
In matematica, un campo vettoriale su uno spazio euclideo è una costruzione del calcolo vettoriale che associa a ogni punto di una regione di uno spazio euclideo un vettore dello spazio stesso. 
Un campo vettoriale tangente su una varietà differenziabile è una funzione che associa ad ogni punto della varietà un vettore dello spazio tangente in quel punto alla varietà.
Il teorema di Helmholtz è fondamentale per questi oggetti, in quanto afferma che la conoscenza della divergenza e del rotore sono necessari e sufficienti alla conoscenza del campo stesso.
Un campo vettoriale sul piano si può rappresentare visivamente pensando ad una distribuzione nel piano di vettori bidimensionali, in modo che il vettore immagine del punto formula_1 abbia l'origine in formula_1 stesso (eventualmente riscalato per una migliore resa visiva come in figura). In modo analogo, si possono visualizzare campi vettoriali su superfici o nello spazio tridimensionale.
Dato un insieme aperto e connesso formula_3 contenuto in formula_4, un "campo vettoriale" è una funzione:
alla quale si richiede generalmente di essere continua o differenziabile per un certo numero di volte. In quanto funzione, ad esempio, nel caso di superficie, si può scrivere:
mentre nel caso di spazio:
Le "linee di flusso" sono intuitivamente delle curve che seguono in ogni loro punto le direzioni individuate dal campo vettoriale con velocità data dall'ampiezza dei vettori del campo.
Interpretando il campo vettoriale come un campo di velocità di un fluido, queste linee rappresentano le traiettorie di ogni singola particella. Formalmente, una linea di flusso passante per un punto formula_8 è una curva differenziabile:
definita per qualche formula_10 positivo, tale che 
Se il campo è sufficientemente "regolare" (ad esempio, se formula_14 è differenziabile o almeno lipschitziano), per ogni punto passa esattamente una linea di flusso. Questo perché una linea di flusso è soluzione di un problema di Cauchy, la cui esistenza e unicità è garantita dal teorema di esistenza e unicità per un problema di Cauchy.
Una linea di flusso è definita su un intervallo aperto formula_15, ma non è necessariamente definita su tutta la retta reale. In un campo vettoriale tale che tutte le linee di flusso sono definite sull'intera retta reale, è possibile definire una applicazione
che associa ad una coppia formula_17 - data da un "tempo" e un punto dello spazio delle fasi - il punto di arrivo che si ottiene seguendo la linea di flusso che parte da formula_1 per un tempo formula_19 a partire dal punto formula_1. Tale applicazione è il flusso associato al campo vettoriale e si è soliti scrivere formula_21 per indicare l'immagine della coppia formula_17.
Formalmente il flusso associato al campo vettoriale formula_14 è individuato da una famiglia di applicazioni
che verificano le condizioni:
per ogni formula_1 e per ogni tempo formula_27.
Il flusso è un "gruppo ad un parametro": è un gruppo perché soddisfa le proprietà seguenti
e definisce un'azione di formula_30 su formula_3. Le proprietà di gruppo ci dicono informalmente che l'evoluzione dello spazio indotta dal flusso per un tempo formula_32 corrisponde a lasciare tutto com'è, e che l'evoluzione per un tempo formula_33 equivale all'applicazione successiva di due evoluzioni per tempi formula_34 e formula_19.
Un punto critico o "punto singolare" per un campo vettoriale è un punto in cui il campo si annulla, o in cui non è definito perché tende a infinito (similmente a quanto accade per i poli). Generalmente, si suppone che il campo sia sufficientemente regolare, così che i punti critici sono isolati.
I punti critici devono il proprio nome al ruolo "speciale" che rivestono all'interno del campo vettoriale. Nell'intorno di qualsiasi punto "non" critico la struttura topologica del campo vettoriale è sempre la stessa: il campo ristretto ad un intorno piccolo del punto è diffeomorfo ad un campo costante (che associa ad ogni punto lo stesso vettore non nullo), le cui linee di flusso sono quindi rette parallele, come stabilito dal teorema della scatola di flusso. I punti critici invece hanno una casistica molto più ricca.
Le possibili strutture topologiche del campo in un intorno del punto critico isolato si possono classificare associando ai punti critici un numero intero chiamato indice.
Il tipo ed il numero complessivo dei punti critici di un campo vettoriale sono anche legati alla struttura topologica globale del dominio su cui il campo è definito. Questo legame è stabilito dal teorema di Poincaré-Hopf il quale afferma che se il campo vettoriale è definito su una varietà differenziabile compatta allora la somma degli "indici" dei suoi punti critici è uguale alla caratteristica di Eulero della varietà.
Una varietà differenziale è "pettinabile" se ammette un campo vettoriale (sufficientemente regolare) mai nullo. Per il teorema di Poincaré-Hopf menzionato sopra, una varietà compatta pettinabile deve avere caratteristica di Eulero zero. Per questo motivo, la sfera non è pettinabile: questo enunciato è il teorema della sfera pelosa.
D'altro canto, il toro e la bottiglia di Klein sono superfici con caratteristica di Eulero zero, e sono pettinabili.
I campi vettoriali si incontrano sia nella matematica pura che in quella applicata:
Data una funzione differenziabile su formula_36
il "campo gradiente" di formula_38 è il campo vettoriale che associa ad ogni formula_39 in formula_40 il vettore 
dato dal gradiente di formula_38 in formula_1.
Un campo gradiente è conservativo, cioè il rotore è ovunque nullo.
Le linee di flusso di un campo gradiente associato ad una funzione scalare formula_38 sono ovunque ortogonali alle superfici di livello di formula_38, cioè alle ipersuperfici date dall'equazione cartesiana formula_46 al variare di formula_47 in formula_48.
Data una funzione differenziabile su un aperto formula_49 del piano cartesiano formula_50:
il "campo Hamiltoniano" di formula_52 è il campo vettoriale che associa ad un punto formula_53 in formula_40 il vettore 
dove formula_56 e formula_57 denotano le derivate parziali di formula_58.
La funzione scalare formula_52 che genera il campo Hamiltoniano si chiama "Hamiltoniana".
</text>
</doc>
<doc id="77615" url="https://it.wikipedia.org/wiki?curid=77615">
<title>Principio d'induzione</title>
<text>
Il principio d'induzione (da non confondersi con il metodo di induzione) è un enunciato sui numeri naturali che in matematica trova un ampio impiego nelle dimostrazioni, per provare che una certa proprietà è valida per tutti i numeri interi. L'idea intuitiva alla sua base è l'"effetto domino": affinché le tessere da domino disposte lungo una fila cadano tutte sono sufficienti due condizioni:
Il principio d'induzione estende quest'idea al caso in cui la fila sia composta da infinite tessere.
Il principio d'induzione asserisce che se formula_1 è una proprietà che vale per il numero 0, e se formula_2 per ogni formula_3, allora formula_4 vale per ogni formula_3.
In simboli:
dove formula_7 e formula_3 sono numeri naturali.
La prima attestazione specifica del principio è del 1861, a opera di Robert Grassmann. Il suo primo utilizzo in una dimostrazione risale al 1575 da parte dell'italiano Francesco Maurolico. Nel XVII secolo Pierre de Fermat ne raffinò l'utilizzo formulandolo come "principio della discesa infinita", e la nozione compare anche chiaramente nei lavori più tardi di Blaise Pascal (1653). L'espressione "induzione matematica" apparentemente sembra essere stata coniata dal logico e matematico A. De Morgan nei primi del XIX secolo. 
La sua formulazione completa, usata ancora oggi, è essenzialmente quella data da Giuseppe Peano nei suoi "Arithmetices Principia", pubblicati nel 1889. 
Il principio d'induzione deriva direttamente dal quinto assioma di Peano, ed è ad esso equivalente: assumendolo cioè come assioma, ne deriva il quinto assioma di Peano.
Il "principio d'induzione" offre un importante strumento per le dimostrazioni.
Per dimostrare che un certo asserto formula_4 in cui compare un numero naturale formula_3 vale per qualunque formula_11 si può sfruttare il principio d'induzione nel seguente modo:
Si pone formula_12,
e quindi si conclude che l'insieme formula_17 dei numeri per cui vale formula_4 coincide con l'insieme dei numeri naturali.
Il punto 1 è generalmente chiamato "base dell'induzione", il punto 2 "passo induttivo".
Un modo intuitivo con cui si può guardare a questo tipo di dimostrazioni è il seguente: se disponiamo di una dimostrazione della "base" 
e del "passo induttivo" 
allora chiaramente possiamo sfruttare queste dimostrazioni per dimostrare formula_13 usando la regola logica modus ponens su formula_14 (la base) e formula_29 (che è un caso particolare del passo induttivo per formula_30), poi possiamo dimostrare formula_31 poiché adesso usiamo il modus ponens su formula_13 e formula_33, così per formula_34, formula_35, eccetera... è chiaro a questo punto che possiamo produrre una dimostrazione in un numero finito di passi (eventualmente lunghissimo) di formula_4 per qualunque numero naturale formula_3, da cui deduciamo che formula_4 è vero per ogni formula_11.
Dimostriamo che vale l'asserto: per ogni formula_40 risulta:
Abbiamo in questo caso definito formula_42.
la dimostrazione di questa affermazione diventa più semplice dopo aver premesso che sommare i primi "n+1" numeri interi equivale ad aggiungere "n+1" alla somma dei primi n numeri interi, cioè che:
quindi la dimostrazione che cerchiamo si ottiene aggiungendo "n+1" a formula_4 e verificando che il risultato coincida con formula_21
i passaggi algebrici sono dunque:
Questo conclude la dimostrazione del "passo induttivo".
Avendo dunque verificato la validità sia della base dell'induzione che del passo induttivo, possiamo concludere che la formula
è vera per ogni formula_54.
Il principio d'induzione forte deriva da una versione con ipotesi apparentemente più restrittive del quinto assioma di Peano, ma equivalente: se formula_55 è un sottoinsieme dell'insieme formula_56 dei numeri naturali tale che 
allora formula_63
La parola "forte" è legata al fatto che questa formulazione richiede delle ipotesi apparentemente più forti e stringenti per inferire che l'insieme formula_17 coincide con formula_56: per ammettere un numero nell'insieme è richiesto infatti che "tutti" i precedenti ne facciano già parte (e non solo "il" numero precedente). 
In pratica, la proprietà formula_66 deve valere non solo per "n", ma per ogni formula_67. Non è difficile dimostrare la sua "equivalenza logica" con il principio d'induzione, ragionando così: se vale per 1 (o 0), vale anche per il suo successore, e per il successore di quest'ultimo, etc., fino a "n". Inoltre, se la proprietà vale per ogni numero minore di n, vale anche per 1 (o 0), e se vale per b, vale anche per b+1 ≤ n, ed è perciò equivalente al principio d'induzione.
Questa formulazione, talvolta, rende più agevoli le dimostrazioni per induzione, data la possibilità di disporre di una platea più ampia di ipotesi (tutti i numeri minori di formula_3) per la dimostrazione del successivo "passo induttivo". Questo si verifica, ad esempio, nella dimostrazione della fattorizzabilità dei numeri interi (v. teorema fondamentale dell'aritmetica): laddove, nella dimostrazione, si voglia usare il principio d'induzione, la regressione da un numero "n" a fattori più piccoli non porta al numero precedente "m" ma a numeri più piccoli. In tal caso il principio di induzione debole non sarebbe utile. La stessa situazione si incontra nella fattorizzazione dei polinomi.
In totale le forme del principio d'induzione sono 4:
Queste forme sono equivalenti nel senso che assumendone vera una si possono dimostrare le altre tre.
Generalmente, il principio d'induzione è indicato come assioma dei numeri naturali: a volte viene considerato al posto del quinto assioma di Peano, ottenendo un modello equivalente, in quanto, come detto in precedenza, i due sono equivalenti. La teoria che si ottiene considerando gli assiomi classici di Peano formalizzati (cioè gli assiomi dell'aritmetica di Peano) eccettuato il principio d'induzione viene chiamata aritmetica di Robinson ed ammette modelli alternativi in cui il principio d'induzione è falso.
Esistono però alcuni sistemi logici in cui esso può essere dimostrato: ad esempio, quando viene usato l'assioma
ovvero
noto anche come "principio del buon ordinamento per i numeri naturali".
In realtà, questo assioma aggiuntivo è una "formulazione alternativa" del principio d'induzione matematica: i due assiomi sono infatti equivalenti.
Infatti se un insieme non vuoto non ha minimo lo "0" non gli appartiene. Assumendo poi che numeri inferiori a "m" numeri non gli appartengono, allora anche m non gli appartiene (altrimenti sarebbe il minimo. Applicando l'induzione forte si ottiene che nessun numero gli appartiene.
Viceversa, dato un insieme goda delle due proprietà enunciate dal principio d'induzione senza coprire tutto formula_56. Esisterà, per il buon ordinamento, il minimo numero che non gli appartiene e sia questo "m". Allora "m" non può essere "0". Il suo precedente "m-1" non rispetta l'ipotesi induttiva.
Tuttavia, in alcuni casi il principio d'induzione non è considerato assioma, ciò dipende da come è definito l'insieme dei numeri naturali.
Se definisco l'insieme formula_56 per via assiomatica (con gli assiomi di Peano) avrò che il principio d'induzione è un assioma, come precedentemente detto, viceversa se definisco l'insieme formula_56 come il più piccolo insieme induttivo contenuto in formula_72, e più precisamente come l'intersezione di tutti gli insiemi induttivi contenuti in formula_72, avrò che il principio d'induzione non si potrà considerare un assioma non essendo l'insieme formula_56 definito per via assiomatica, ma sarà una conseguenza del fatto che formula_56 è il più piccolo insieme induttivo.
Una prima generalizzazione molto elementare consiste nel far partire l'induzione da un numero naturale "k" diverso da zero. Ovvero: se vogliamo dimostrare che un enunciato formula_4 vale per ogni numero naturale formula_3 maggiore o uguale di un numero prefissato formula_7 applichiamo la tecnica di dimostrazione per induzione considerando come "base" dell'induzione formula_79 anziché formula_14.
Il principio d'induzione transfinita generalizza il "principio d'induzione" alla classe degli ordinali transfiniti "On" (di cui i numeri naturali sono un sottoinsieme).
Esso afferma che se formula_17 è un sottoinsieme della classe formula_82 di tutti gli ordinali che verifica le seguenti due proprietà:
allora formula_17 coincide con l'intera classe degli ordinali formula_82.
Il "principio d'induzione transfinita", a differenza del "principio d'induzione forte", è un principio strettamente più potente del "principio d'induzione", infatti esistono teoremi come il Teorema di Goodstein che possono essere dimostrati per induzione transfinita ma non possono essere dimostrati mediante l'induzione semplice.
Una classica applicazione sbagliata del principio d'induzione è la seguente "dimostrazione" che porta a concludere che
Ragioniamo per induzione sulla grandezza dei possibili insiemi di cavalli: dimostriamo che per ogni formula_40 vale formula_4="un insieme di formula_3 cavalli contiene tutti cavalli dello stesso colore":
Segue dal principio d'induzione che qualunque sia il numero di cavalli presenti al mondo, questi hanno tutti lo stesso colore.
La dimostrazione del passo induttivo precedente è solo apparente: infatti per "n"=1 i due insiemi di "n" elementi hanno in comune "n"-1 = 0 elementi e non si può quindi dedurre che "n"+1 = 2 cavalli abbiano lo stesso colore.
</text>
</doc>
<doc id="43634" url="https://it.wikipedia.org/wiki?curid=43634">
<title>Insieme aperto</title>
<text>
l concetto di insieme aperto si trova in matematica in molti ambiti e con diversi gradi di generalità. Intuitivamente, un insieme è aperto se è possibile spostarsi sufficientemente poco in ogni direzione a partire da ogni punto dell'insieme senza uscire dall'insieme stesso. In realtà, seguendo le definizioni generali ci si può allontanare abbastanza da questa idea intuitiva; attraverso la definizione di insieme aperto si possono definire concetti come "vicino", "lontano", "attaccato", "separato"; definizioni non intuitive di insiemi aperti corrisponderanno a situazioni matematiche in cui questi concetti vengono utilizzati in modo non intuitivo.
La topologia è l'ambito più generale in cui si incontrano gli insiemi aperti; in questo contesto il concetto di insieme aperto viene considerato fondamentale; preso un insieme X, se una qualunque collezione T di sottoinsiemi di X soddisfa le proprietà riportate sotto, X diventa uno spazio topologico, T viene chiamata topologia di X e gli insiemi di T, per definizione, i suoi aperti.
Perché la collezione T sia una topologia deve valere:
Lo spazio topologico viene indicato specificando la coppia (X,T). È da notare che se si considera uno stesso insieme X con due diverse topologie T e T', si hanno due spazi topologici diversi; tuttavia in molti casi, in cui la struttura topologica emerge in modo "naturale", indicare l'insieme è sufficiente per individuare lo spazio topologico.
In uno spazio metrico formula_1, un sottoinsieme formula_2 di formula_3 si dice aperto se, per ogni formula_4, esiste un numero reale formula_5 tale che i punti che distano da formula_6 per meno di formula_7 appartengono ancora a formula_2. Formalmente: se formula_9, allora formula_10. Gli aperti metrici così definiti costituiscono una topologia di formula_3 secondo la definizione precedente: in questo modo ogni spazio metrico è dotato in modo naturale di una struttura di spazio topologico, e tutti gli aperti metrici possono essere considerati aperti topologici ("ma non viceversa").
Lo spazio euclideo formula_12 è un particolare spazio metrico. Un insieme aperto formula_2 dello spazio euclideo è un insieme tale che per ogni formula_6 di formula_2 esiste una palla di raggio formula_16 centrata in formula_6, interamente contenuta in formula_2.
In particolare, un intervallo in formula_19 è aperto se è del tipo formula_20, dove formula_21 e formula_22 possono anche essere rispettivamente formula_23 e formula_24.
A ogni definizione di insieme aperto corrisponde una definizione di insieme chiuso. In generale, un insieme è chiuso se e solo se è il complementare di un insieme aperto; nell'ambito degli spazi topologici questa è esattamente la proprietà definitoria, negli altri ambiti si danno definizioni a parte e questa proprietà viene provata come un teorema.
</text>
</doc>
<doc id="43859" url="https://it.wikipedia.org/wiki?curid=43859">
<title>E (costante matematica)</title>
<text>
In matematica il numero formula_1 è una costante matematica il cui valore approssimato a 12 cifre decimali è formula_2. È la base della funzione esponenziale formula_3 e del logaritmo naturale. Può essere definita in vari modi, il più comune tra i quali è come il limite della successione formula_4 al tendere di formula_5 all'infinito. Insieme a π è la costante matematica più importante, per via della sua presenza in molte formule apparentemente non correlate.
È un numero trascendente, e dunque irrazionale, e tramite la formula di Eulero è legato alle funzioni trigonometriche. In ambito internazionale il numero formula_1 è chiamato numero di Eulero, mentre in Italia viene anche detto numero di Nepero.
Il numero formula_1 può essere definito in uno dei seguenti modi:
dove formula_10 è il fattoriale del numero naturale formula_5.
Una dimostrazione dell'equivalenza di queste definizioni è data di seguito. Le definizioni sono usate in modo analogo nella definizione della funzione esponenziale.
Un modo alternativo (non standard) di definire formula_1 coinvolge le equazioni differenziali: il numero di Nepero si può definire come il valore in formula_13 della funzione formula_14 soluzione unica del problema di Cauchy dato dall'equazione differenziale formula_15 con condizioni iniziali formula_16.
Il numero formula_1 è irrazionale, più precisamente un numero trascendente, ossia non esiste un'equazione algebrica a coefficienti razionali che lo ammetta come soluzione.
Questo è stato il primo numero che si è dimostrato essere trascendente senza essere stato costruito per essere collocato nell'insieme dei numeri reali non algebrici, come era accaduto in precedenza 
per la costante di Liouville.
Una dimostrazione della irrazionalità di e è stata data da Charles Hermite nel 1873. 
Si presume che esso sia un numero normale.
La costante formula_1 compare nella formula di Eulero, una delle
più importanti identità della matematica:
dove formula_20 indica l'unità immaginaria.
Il caso particolare con formula_21 è noto come identità di Eulero:
questa uguaglianza è stata chiamata da Richard Feynman "gioiello di Eulero".
Lo sviluppo di formula_1 come frazione continua infinita è espresso 
dalla seguente interessante configurazione:
Troncando la frazione continua si ottengono le approssimazioni razionali di formula_1, di cui le prime (non intere) sono formula_26.
Il numero formula_1 è il punto centrale della commutazione dell'elevamento a potenza. Siano date tutte le coppie formula_28 per le quali formula_29. Oltre al caso banale formula_30, l'unica coppia intera (e razionale) per cui vale la proprietà è formata dai numeri 2 e 4, ma vale anche per infinite coppie irrazionali distribuite lungo una curva nel primo quadrante, asintotica alle rette formula_13 e formula_32. Tale curva e la retta formula_33 si intersecano nel punto formula_34.
Sempre in merito a funzioni esponenziali, la radice formula_35-esima di formula_35, ovvero formula_37, ha massimo per formula_38 e l'esponenziale formula_35-esimo di formula_35, ovvero formula_41, ha minimo per formula_42.
Il primo riferimento ad formula_1 in letteratura risale al 1618 ed è contenuto
nella tavola di un'appendice di un lavoro sui logaritmi di John Napier. Nella tavola non appare la costante, bensì un elenco di logaritmi naturali calcolabili a partire dalla costante. Sembra che la tavola sia stata scritta da William Oughtred. La prima espressione di formula_1 come una costante è stata trovata da Jakob Bernoulli:
Da questa espressione è difficile ricavare un buon valore numerico per la costante.
La sua prima citazione, rappresentata con la lettera formula_46 compare in due lettere di Gottfried Leibniz a Christiaan Huygens, del 1690 e del 1691. Leonhard Euler ha iniziato ad usare la lettera formula_1 per la costante nel 1727 e il primo uso di formula_1 compare nella "Mechanica" di Eulero (1736). Negli anni seguenti alcuni ricercatori hanno usato la lettera formula_49, poi l'uso di formula_1 si è fatto più comune. Oggi è usato come simbolo definitivo.
Non sono noti i motivi che condussero a scegliere la lettera formula_1, si può supporre che formula_1 fu scelto perché iniziale della parola "esponenziale". Un altro motivo sta nel fatto che formula_53, formula_46, formula_49, e formula_56 venivano frequentemente usate per altri oggetti matematici ed formula_1 era la prima lettera dell'alfabeto latino non utilizzata. È improbabile che Eulero abbia scelto la lettera in quanto iniziale del proprio nome, poiché il numero non era una sua scoperta, era già noto ai matematici dell'epoca.
La seguente dimostrazione prova l'equivalenza dello sviluppo in serie infinita presentato in precedenza e l'espressione del limite studiata da Bernoulli.
Definiamo
Dal teorema binomiale,
tale che
Qui deve essere usato il limite superiore o limsup, poiché non è ancora noto che formula_63 converge effettivamente.
Ora, per l'altra direzione, si nota che dall'espressione sopra di formula_63, se formula_65, abbiamo
Fissato formula_67 si fa tendere formula_5 all'infinito. Otteniamo
(di nuovo, dobbiamo usare il limite inferiore o liminf poiché non è ancora garantito che formula_63 converge).
Ora,
considerando la disuguaglianza precedente, formula_67 si avvicina all'infinito, e colloca quest'ultima assieme all'altra disuguaglianza; da cui:
Questo completa la dimostrazione.
Oltre alle rappresentazioni analitiche esatte per calcolare formula_1, esistono metodi stocastici per stimarlo. 
Uno di questi parte da una successione infinita di variabili casuali indipendenti formula_74 distribuite uniformemente 
nell'intervallo formula_75. Sia formula_76 il numero di somme parziali di variabili formula_77 che siano strettamente minori di formula_78, ponendo:
dove formula_80 allora il valore atteso formula_81 è proprio la costante formula_1.
</text>
</doc>
<doc id="53609" url="https://it.wikipedia.org/wiki?curid=53609">
<title>Polo (analisi complessa)</title>
<text>
In matematica, e in particolare in analisi complessa, per polo di una funzione olomorfa formula_1, si intende una singolarità isolata formula_2 della funzione per cui
Il polo si distingue dalla "singolarità eliminabile" e dalla "singolarità essenziale", per le quali tale limite rispettivamente è finito e non esiste.
La conoscenza delle caratteristiche dei poli di una funzione olomorfa consente di determinare molte delle sue caratteristiche; inoltre lo studio dei poli è fondamentale nel calcolo dei residui.
Una definizione equivalente può essere data tramite serie di Laurent. Una singolarità isolata formula_4 è un polo se e solo se lo sviluppo locale in serie di Laurent è del tipo
con formula_6, per qualche formula_7.
In altre parole, una singolarità isolata è un polo se e solo se la parte principale della serie di Laurent in un intorno bucato della singolarità è costituita da un numero finito di termini, cioè se i coefficienti con apice formula_8 negativo sono un numero finito formula_9 diverso da zero:
L'ordine del polo è il numero naturale formula_9 di termini che costituiscono la parte principale della serie di Laurent. Analogamente, formula_2 è un polo se per qualche formula_13 il limite:
esiste, è finito ed è diverso da zero. In questo caso la funzione ha nel punto formula_2 un polo di ordine formula_16.
Una funzione
dove formula_18 e formula_19 sono polinomi senza radici in comune (quindi la funzione è ridotta ai minimi termini), è definita su 
dove formula_21 sono le radici di formula_22. Ciascuno di questi punti è un polo, il cui ordine è pari alla molteplicità della radice. Ad esempio,
ha un polo di ordine formula_24 in formula_25 ed un polo di ordine formula_26 in formula_24.
La funzione 
è definita su
ed ha un polo di ordine uno su ogni punto formula_30. Ha quindi infiniti poli.
Una funzione olomorfa formula_31 avente poli nei punti formula_32 può essere considerata come una funzione il cui dominio comprende anche questi punti, il cui codominio è la sfera di Riemann formula_33: è sufficiente imporre formula_34. Il risultato di questa operazione è una funzione meromorfa.
</text>
</doc>
<doc id="42369" url="https://it.wikipedia.org/wiki?curid=42369">
<title>Quadrato (algebra)</title>
<text>
In algebra, viene definito quadrato di un numero formula_1 l'elevamento dello stesso alla seconda potenza, ossia la sua moltiplicazione per sé stesso eseguita una volta:
Il termine "quadrato" viene dalla geometria, poiché l'area di un quadrato si ottiene appunto moltiplicando il lato per sé stesso.
Il quadrato di un numero immaginario è un numero reale minore o uguale a zero, mentre per i numeri complessi si calcola
Ad esempio il quadrato di formula_6 è formula_7, ma anche il quadrato di formula_8 è uguale a formula_7.
Da una formula siffatta si identificano un sottoinsieme infinito numerabile delle terne pitagoriche, per ogni formula_21 intero. Ad es. : (3, 4, 5) con formula_22; (5, 12 ,13); (7, 24 ,25); (9, 40, 41); (11, 60, 61); (13, 84, 85); (15, 112, 113); (17, 144, 145); (19, 180, 181); (21, 220, 221); (23, 264, 265).
Il quadrato di un numero intero diverso da zero è sempre un numero naturale. I numeri naturali che sono quadrati di numeri interi si definiscono quadrati perfetti.
Di seguito alcune proprietà:
Ad esempio
indicabile attraverso la formula
indicabile attraverso la formula
</text>
</doc>
<doc id="42767" url="https://it.wikipedia.org/wiki?curid=42767">
<title>Immagine (matematica)</title>
<text>
, l'immagine di un sottoinsieme del dominio di una funzione è l'insieme degli elementi ottenuti applicando la funzione a tale sottoinsieme. Si tratta quindi di un sottoinsieme del codominio della funzione. L'immagine degli elementi dell'intero dominio è anche detta immagine della funzione, e se la funzione è suriettiva essa coincide col codominio.
Sia formula_1 una funzione. Si definisce immagine di formula_2 tramite formula_3, o immagine di formula_3, il sottoinsieme di formula_5 così definito: 
ove l'uguaglianza con formula_5 sussiste se e solo se la funzione formula_3 è suriettiva.
Si tratta, quindi, di quegli elementi formula_9 di formula_5 per i quali esiste un elemento di formula_2 che venga portato in formula_5 da formula_3.
Notare che nello scrivere formula_14 si è attuato un leggero abuso di notazione, in quanto formula_3 è una trasformazione che agisce sugli elementi di formula_2, non su formula_2 stesso. Tale uso è però talmente diffuso che sarebbe inutile provare a combatterlo. Altre notazioni, che non provocano alcun imbarazzo formale e che trovano comunque un certo seguito, sono: formula_18 e formula_19
Più in generale, se formula_20 è un sottoinsieme del dominio formula_2 si chiama "immagine di formula_22 tramite formula_3" l'insieme:
Se formula_25, si chiama immagine di formula_26 tramite formula_3 l'unico elemento formula_28 associato ad formula_26 da formula_3.
Considerata una funzione formula_1, valgono le seguenti proprietà:
È un esercizio utile e proposto regolarmente nelle scuole quello, data una funzione, di identificare la sua immagine. Per fare questo, se non si è in grado di farlo "a priori" (ad esempio, è noto senza fare alcun calcolo che la funzione formula_42 ha come immagine tutta la semiretta positiva delle ordinate formula_43, compreso lo zero), ci sono due metodi: o, con gli strumenti dell'analisi matematica, si identificano gli intervalli di monotonia e i massimi e i minimi, o, con calcoli puramente algebrici, si esplicita la formula_44 in funzione della formula_43, trovando in pratica la funzione inversa; ad esempio, se
allora la sua inversa si ottiene mediante:
Visto che nei vari passaggi si è applicato prima un logaritmo e poi una radice quadrata, si ottengono delle restrizioni, le uniche, per la formula_43, precisamente formula_49   e   formula_50
L'intersezione di queste due condizioni dà l'immagine, poiché i valori di formula_43 risultanti possiedono, per costruzione, un valore di partenza (dato dall'espressione trovata); in questo caso, dunque, l'immagine è formula_52
</text>
</doc>
<doc id="42953" url="https://it.wikipedia.org/wiki?curid=42953">
<title>Commutatività</title>
<text>
In matematica, un'operazione binaria formula_1 definita su un insieme formula_2 è commutativa se 
per ogni coppia di elementi formula_4 e formula_5 in formula_2. Se questa proprietà non è valida per ogni coppia di elementi, l'operazione è quindi detta non commutativa.
Due elementi formula_4 e formula_5 commutano se formula_9. Quindi l'operazione formula_1 è commutativa se e solo se due elementi di formula_2 commutano sempre.
I più comuni esempi di operazioni binarie commutative sono l'addizione (formula_12) e la moltiplicazione (formula_13), considerate sull'insieme di tutti i numeri reali, o solo sui numeri positivi, naturali o razionali, oppure estese ai numeri complessi; per esempio:
Altre operazioni binarie commutative sono:
Tra le operazioni binarie non commutative tra numeri vi sono la sottrazione (formula_16), la divisione (formula_17) e l'elevamento a potenza (formula_18), definite su insiemi opportuni di numeri reali.
Anche la composizione di funzioni (formula_19) in molti contesti non è commutativa: ad esempio le funzioni reali formula_20 e formula_21 non commutano, in quanto 
Un'altra importante operazione non commutativa è la moltiplicazione fra matrici quadrate. Ad esempio,
Un gruppo è abeliano, o anche "commutativo", se l'operazione che vi è definita è commutativa.
Un anello ha definite due operazioni, chiamate generalmente "somma" e "prodotto" in analogia con i numeri interi. L'operazione di "somma" è sempre commutativa, ma l'operazione "prodotto" no. Un anello è chiamato "abeliano" o "commutativo" se anche la moltiplicazione è commutativa.
Generalmente, le strutture algebriche abeliane sono molto più semplici delle analoghe non abeliane.
Un'operazione è commutativa se e solo se la sua tavola di composizione è simmetrica. Per esempio le tavole di composizione delle operazioni minimo comune multiplo e massimo comun divisore per l'insieme dei numeri interi da 1 a 6 sono
formula_26
e
formula_27
</text>
</doc>
<doc id="40950" url="https://it.wikipedia.org/wiki?curid=40950">
<title>Trigonometria</title>
<text>
La trigonometria (dal greco "trígonon" (τρίγωνον, triangolo) e "métron" (μέτρον, misura): risoluzione del triangolo) è la parte della matematica che studia i triangoli a partire dai loro angoli. Il compito principale della trigonometria, così come rivela l'etimologia del nome, consiste nel calcolare le misure che caratterizzano gli elementi di un triangolo (lati, angoli, mediane, etc.) partendo da altre misure già note (almeno tre, di cui almeno una lunghezza), per mezzo di speciali funzioni.
Tale compito è indicato come "risoluzione del triangolo". È anche possibile servirsi di calcoli trigonometrici nella risoluzione di problemi correlati a figure geometriche più complesse, come poligoni o figure geometriche solide, ed in molti altri rami della matematica.
Le funzioni trigonometriche (le più importanti delle quali sono il seno e il coseno), introdotte in questo ambito, vengono anche usate in maniera indipendente dalla geometria, comparendo anche in altri campi della matematica e delle sue applicazioni, ad esempio in connessione con la funzione esponenziale o con le operazioni vettoriali.
Per molti secoli, la trigonometria dovette i suoi progressi quasi esclusivamente all'opera di grandi astronomi e geografi. Infatti, la fondazione di questa scienza si deve a Ipparco di Nicea e a Claudio Tolomeo, entrambi più astronomi e geografi che matematici.
Contributi notevoli furono apportati a questa scienza dagli arabi, dal francese Levi ben Gershon e, successivamente, da Niccolò Copernico e Tycho Brahe, intenti a descrivere e a prevedere con sempre maggior precisione i fenomeni celesti, anche per un più esatto e comodo calcolo di longitudini e latitudini.
Strumento indispensabile della trigonometria sono le funzioni trigonometriche. Sono queste funzioni che associano lunghezze ad angoli, e viceversa.
Le tabelle in questa sezione mostrano le funzioni trigonometriche insieme alle loro principali proprietà; per ulteriori caratteristiche, consultare la voce relativa alla particolare funzione.
Sono dette funzioni trigonometriche "dirette" quelle che ad un angolo, solitamente espresso in radianti, associano una lunghezza o un rapporto fra lunghezze. A causa dell'equivalenza circolare degli angoli, tutte le funzioni trigonometriche dirette sono anche funzioni periodiche con periodo formula_1 o formula_2.
Ad ogni funzione trigonometrica diretta è associata una funzione inversa. Il dominio di ciascuna funzione trigonometrica inversa corrisponde, com'è prevedibile, al codominio della rispettiva funzione diretta. Poiché le funzioni dirette sono, tuttavia, "periodiche", e perciò non iniettive, per poterle invertire è necessario restringerne il dominio rendendole biiettive. La scelta della restrizione è teoricamente irrilevante e le possibilità sono infinite. La convenzione (rigida, in questo campo) vuole però che i domini vengano ristretti agli intervalli formula_3 oppure formula_4, in cui le funzioni — e dunque anche le loro inverse — siano monotone. Anche le funzioni arcosecante ed arcocosecante vengono definite dall'inversione delle funzioni dirette ristrette ad uno di tali intervalli.
Da questa si ricavano
Ricordare di valutare la posizione di formula_8 per la scelta opportuna dei segni.
che vale solo per formula_10 con formula_11.
Dalle due precedenti relazioni si ricava che
che vale solo per formula_10 con formula_11.
Da questa si ricava
Ricordare di valutare la posizione di formula_8 per la scelta opportuna dei segni.
che vale solo per formula_18 con formula_11.
che vale solo per formula_10 con formula_11.
che vale solo per formula_18 con formula_11.
Nella circonferenza goniometrica chiamiamo angoli associati gli angoli formula_8, formula_27, formula_28 e formula_29. Tali angoli hanno in valore assoluto stesso seno e stesso coseno.
formula_30
formula_31
formula_32
formula_33
formula_34
formula_35
formula_36
formula_37
formula_38
formula_39
formula_40
formula_41
Si dice che formula_42 è una funzione pari, mentre formula_43 e formula_44 sono dispari.
formula_45
formula_46
formula_47
formula_48
formula_49
formula_50
In trigonometria, le formule di addizione e sottrazione permettono di trasformare le funzioni trigonometriche della somma o differenza di due angoli in un'espressione composta da funzioni trigonometriche dei due angoli.
La formula della tangente vale per formula_55 con formula_56
La formula della cotangente vale per formula_57 con formula_56
La formula della tangente vale per formula_63 con formula_56
La formula della cotangente vale per formula_65 con formula_56
L'ultima formula vale per formula_70 e formula_71 con formula_11
L'ultima formula vale per formula_76 con formula_11
Attenzione: è necessario valutare in quale quadrante cade formula_78 per poter scegliere i segni opportuni delle seguenti formule
L'ultima formula vale per formula_82.
dove formula_86 con formula_82.
Le formule di prostaferesi trasformano somme di funzioni goniometriche in prodotti.
Le formule di Werner trasformano prodotti di funzioni goniometriche in somme.
La seguente uguaglianza è verificata sotto le seguenti condizioni
formula_96
Fare attenzione al fatto che la tangente goniometrica è periodica di 180° e dunque bisogna valutare preventivamente la posizione di formula_99 dunque
Nel gergo matematico risolvere un triangolo rettangolo significa calcolare le misure dei lati e degli angoli del triangolo.
Per convenzione esiste una nomenclatura nei triangoli rettangoli che si può vedere in figura. 
Si ricorda che 
Ad esempio formula_103 è opposto al cateto formula_104 e adiacente al cateto formula_105.
Sotto queste convenzioni in un triangolo rettangolo valgono i seguenti teoremi
Teorema. In un triangolo rettangolo un cateto è uguale al prodotto dell'ipotenusa con il seno dell'angolo opposto al cateto
Teorema. In un triangolo rettangolo un cateto è uguale al prodotto dell'ipotenusa con il coseno dell'angolo acuto adiacente al cateto.
Teorema. In un triangolo rettangolo un cateto è uguale al prodotto dell'altro cateto con la tangente dell'angolo opposto al cateto da calcolare.
Teorema. In un triangolo rettangolo un cateto è uguale al prodotto dell'altro cateto con la cotangente dell'angolo acuto adiacente al cateto da calcolare.
Tali teoremi si traducono nelle seguenti formule per la risoluzione dei triangoli rettangoli
Si consideri un triangolo rettangolo formula_114 con angolo retto di vertice formula_115. Detto formula_116 l'asse formula_117, sul vertice formula_118 si costruisce una circonferenza di raggio formula_119. Le coordinate del punto formula_120 rappresentano il formula_121 e il formula_122, e poiché formula_123 è acuto indicano anche rispettivamente le lunghezze dei cateti formula_124 e formula_125.
Dalla figura si può osservare che i due triangoli rettangoli formula_114 e formula_127 sono simili in quanto hanno due angoli congruenti: formula_123 in comune e gli angoli retti di vertice formula_115 e formula_130. Quindi è possibile costruire la proporzione fra i lati omologhi dei due triangoli simili (lati opposti agli angoli congruenti):
Sostituendo le misure dei lati si ottiene
e quindi
da queste due si ricava anche
Questo ragionamento può essere chiaramente esteso anche al terzo angolo formula_103 in modo da ottenere formule analoghe
Si consideri il seguente problema: calcolare l'altezza di una torre formula_142, potendo stare solo alla base (piano orizzontale) della stessa. Si distinguono due casi
In questo caso basta misurare il cateto formula_143 (formula_104), e dal punto formula_118 misurare l'angolo acuto formula_146 (formula_123) sotto cui si vede la sommità della torre formula_142 (formula_105). Applicando opportunamente le formule si ottiene
In questo caso formula_143 (formula_152) è incognita (in quanto il piede formula_115 non è raggiungibile). Si fa dunque una misura orizzontale formula_154 (formula_155) (quindi il cateto formula_156 è formula_157). Dal punto formula_118 si misura l'angolo acuto formula_146 (formula_160) e da formula_161 si misura l'angolo acuto formula_162 (formula_163) sotto cui si vede la sommità della torre formula_142 (formula_105). Applicando opportunamente le formule si ottiene
Confrontando le due altezze si ottiene una equazione nell'incognita formula_117
questa equazione è facilmente risolvibile noti d, formula_160 e formula_171
Trovato formula_117 si ha formula_173 e quindi si può calcolare
Per calcolare l'area del triangolo formula_114, di base formula_176, serve l'altezza formula_177. Nel triangolo rettangolo formula_178, di ipotenusa formula_179, l'altezza formula_180 può essere vista come il cateto che si oppone all'angolo formula_123. Utilizzando in modo opportuno le formule dei triangoli rettangoli si ottiene
e quindi
Questa formula vale anche se formula_123 è ottuso.
Fissato su un piano un punto origine formula_185 e una semiretta formula_186, dato un punto formula_120 del piano esso è univocamente individuato da una coppia di numeri reali formula_188 con la condizione formula_189 e formula_190. La coppia di numeri reali rappresenta le coordinate polari di formula_120.
Geometricamente formula_192 rappresenta la distanza formula_193, mentre formula_194 rappresenta l'angolo formula_195 misurato in senso antiorario con primo lato formula_196.
È possibile trovare le relazioni esistenti tra le coordinate cartesiane formula_197 e le coordinate polari formula_198 del punto formula_120. Le seguenti considerazioni fatte per un punto formula_120 sul primo quadrante valgono anche per gli altri quadranti.
Utilizzando le formule dei triangoli rettangoli si trovano le formule per la trasformazione in coordinate cartesiane
Elevando al quadrato e sommando si ottiene formula_202 e quindi si possono ricavare le formule per la trasformazione in coordinate polari
Fare attenzione che la tangente goniometrica non esiste per formula_205 ed è periodica di 180° e dunque bisogna valutare preventivamente la posizione di formula_120 per calcolare correttamente formula_194
I teoremi trigonometrici permettono la risoluzione di problemi di varia natura legata alla figura di un triangolo qualsiasi, esprimendo rapporti tra i lati e gli angoli di questo.
Data una circonferenza e una corda formula_142, il rapporto tra tale corda e il seno di un qualsiasi angolo alla circonferenza che insiste su di essa è uguale al diametro della circonferenza:
Considerato un triangolo qualsiasi di lati formula_211, formula_104 e formula_105, il rapporto tra i lati e i seni dei rispettivi angoli opposti è costante ed è uguale al diametro della circonferenza circoscritta:
Il teorema del coseno (chiamato anche teorema di Carnot) afferma che in un qualsiasi triangolo, il quadrato di un lato è uguale alla differenza tra la somma dei quadrati degli altri due lati e il doppio prodotto di tali lati per il coseno dell'angolo compreso tra essi.
Ovvero, indicando con formula_216 la lunghezza dei lati e formula_217 gli angoli ad essi opposti, si ottiene
Può essere considerato una generalizzazione del Teorema di Pitagora.
 Nel gergo matematico risolvere un triangolo significa calcolare le misure dei lati e degli angoli del triangolo.
Per risolvere un triangolo qualsiasi devono essere noti tre elementi dei quali almeno uno deve essere un lato. Si possono presentare quattro casi:
La nomenclatura dei lati e degli angoli segue la convenzione in figura.
Il problema ha sempre una sola soluzione se sono rispettate le seguenti condizioni
in caso contrario il problema non ha soluzione.
Il problema ha sempre una sola soluzione se sono rispettate le disuguaglianze triangolari
in caso contrario il problema non ha soluzione.
Il problema ha sempre una sola soluzione
Il problema può avere nessuna soluzione, una soluzione o due soluzioni.
Come per il resto delle lingue europee, l'italiano eredita i nomi delle funzioni trigonometriche dalle corrispondenti voci latine.
Il termine "seno" proviene dalla traduzione latina "sinus" della parola araba "jaib" (letteralmente "baia", tradotto in latino "sinus" a causa di una lettura equivoca: dal momento che l'arabo non scrive le vocali, la sequenza "jb", che stava per "jiba" ricalcando una parola sanscrita, è stata interpretata erroneamente come "baia", in luogo del corretto "corda") usata per indicare la metà della corda; in questo senso, il seno denota la corda piegata su se stessa.
La parola "tangente" viene da latino "tangens", letteralmente «che tocca», in riferimento alle proprietà geometriche del segmento utilizzato per la definizione grafica di questa funzione.
Analogamente si spiega l'etimologia della "secante", in latino "secans", «che taglia».
Le parole "coseno", "cotangente" e "cosecante" derivano dalla contrazione delle rispettive voci latine "complementi sinus", "complementi tangens", "complementi secans", vale a dire «seno dell'angolo complementare», «tangente dell'angolo complementare», «secante dell'angolo complementare».
</text>
</doc>
<doc id="64440" url="https://it.wikipedia.org/wiki?curid=64440">
<title>Prodotto scalare</title>
<text>
In matematica, in particolare nel calcolo vettoriale, il prodotto scalare è un'operazione binaria che associa ad ogni coppia di vettori appartenenti ad uno spazio vettoriale definito sul campo reale un elemento del campo. Si tratta di un prodotto interno sul campo reale, ovvero una forma bilineare simmetrica definita positiva a valori reali. Essendo un prodotto puramente algebrico non può essere rappresentato graficamente come vettore unitario.
La nozione di prodotto scalare è generalizzata in algebra lineare dallo spazio euclideo ad uno spazio vettoriale qualsiasi: tale spazio può avere dimensione infinita ed essere definito su un campo arbitrario (non necessariamente quello reale). Questa generalizzazione è di fondamentale importanza ad esempio in geometria differenziale e in meccanica razionale. Aggiungendo un'ulteriore proprietà, la completezza, porta inoltre al concetto di spazio di Hilbert, per il quale la teoria si arricchisce di strumenti più sofisticati, basilari nella modellizzazione della meccanica quantistica e in molti campi dell'analisi funzionale.
Si definisce prodotto scalare sullo spazio vettoriale formula_1 una forma bilineare simmetrica che associa a due vettori formula_2 e formula_3 di formula_1 uno scalare nel campo reale formula_5, generalmente indicato con formula_6 o formula_7.
Si tratta di un operatore binario che verifica le seguenti condizioni per formula_2, formula_3, formula_10 vettori arbitrari e formula_11 elemento del campo:
Diversi autori richiedono anche che la forma sia "definita positiva", cioè che:
per ogni formula_2 diverso da zero.
Le precedenti richieste implicano anche le seguenti proprietà:
e dal momento che un vettore moltiplicato per 0 restituisce il vettore nullo, segue che:
Il prodotto scalare è "degenere" se esiste un vettore formula_2 non nullo ortogonale a tutti i vettori, cioè tale che:
per ogni vettore formula_3 dello spazio.
Un prodotto scalare su uno spazio vettoriale formula_1 è definito positivo se:
definito negativo se:
semi-definito positivo:
semi-definito negativo se:
Un prodotto scalare semi-definito positivo è (raramente) chiamato anche "prodotto pseudoscalare".
Il prodotto scalare di due vettori formula_28 e formula_29 del piano, applicati sullo stesso punto, è definito come:
dove formula_31 e formula_32 sono le lunghezze di formula_28 e formula_29, e formula_35 è l'angolo tra i due vettori. Il prodotto scalare si indica come formula_36, e soddisfa le proprietà algebriche di simmetria:
per ogni coppia di vettori formula_28 e formula_29, e di bilinearità:
per ogni tripletta di vettori formula_28, formula_29, formula_46 e per ogni numero reale formula_47. Le prime due relazioni esprimono la "linearità a destra" e le altre due "a sinistra".
Il prodotto scalare di un vettore con se stesso è sempre maggiore o uguale a zero:
Inoltre, questo è zero se e solo se il vettore è zero (proprietà di annullamento del prodotto scalare):
Questa proprietà può essere espressa affermando che il prodotto scalare è definito positivo.
Poiché formula_50 è la lunghezza della proiezione ortogonale di formula_28 su formula_29, si può interpretare geometricamente il prodotto scalare come il prodotto delle lunghezze di questa proiezione e di formula_29. Si possono inoltre scambiare i ruoli di formula_28 e formula_29, interpretare formula_56 come la lunghezza della proiezione di formula_29 su formula_28 ed il prodotto scalare come il prodotto delle lunghezze di questa proiezione e della lunghezza di formula_28.
Il coseno di un angolo formula_35 è positivo se formula_35 è un angolo acuto (cioè compreso fra -90° e 90°), nullo se formula_35 è un angolo retto e negativo se è un angolo ottuso. Ne segue che il prodotto scalare formula_36 è:
I casi in cui formula_35 è acuto ed ottuso sono mostrati in figura. In entrambi i casi il prodotto scalare è calcolato usando l'interpretazione geometrica, ma il segno è differente.
In particolare, valgono inoltre le proprietà seguenti:
Se formula_28 e formula_29 sono versori, cioè vettori di lunghezza 1, il loro prodotto scalare è semplicemente il coseno dell'angolo compreso.
Il prodotto scalare di un vettore formula_28 con se stesso formula_83 è il quadrato della lunghezza formula_31 del vettore.
Nella fisica classica, il prodotto scalare è usato nei contesti in cui si debba calcolare la proiezione di un vettore lungo una determinata componente. Ad esempio, il lavoro formula_85 compiuto da una forza costante formula_86 su un corpo che si sposti in direzione formula_10 è il prodotto scalare:
dei due vettori.
Il teorema del coseno può essere formulato agevolmente usando il prodotto scalare. Dati tre punti formula_89, formula_90, formula_91 qualsiasi del piano, vale la relazione seguente:
Il prodotto scalare è definito in geometria analitica in modo differente: si tratta della funzione che, in un qualsiasi spazio euclideo associa a due vettori formula_93 e formula_94 il numero:
dove formula_96 denota una sommatoria.
Ad esempio, il prodotto scalare di due vettori tridimensionali [1, 3, −2] e [4, −2, −1] è [1, 3, −2]·[4, −2, −1] = 1×4 + 3×(−2) + (−2)×(−1) = 0.
In questo modo è possibile "definire" l'angolo formula_35 compreso fra due vettori in un qualsiasi spazio euclideo, invertendo la formula data sopra, facendo cioè dipendere l'angolo dal prodotto scalare e non viceversa:
Spesso il prodotto scalare fra formula_28 e formula_29 si indica anche come formula_101 o come formula_102. Utilizzando il prodotto tra matrici e considerando i vettori come matrici formula_103, il prodotto scalare canonico si scrive anche come:
dove formula_105 è la trasposta di formula_106. L'esempio visto sopra si scrive quindi in notazione matriciale nel modo seguente:
L'equivalenza fra le due definizioni può essere verificata facendo uso del teorema del coseno. Nella forma descritta sopra, il teorema asserisce che il prodotto scalare di due vettori formula_28 e formula_29 nel piano, definito in modo geometrico, è pari a:
Ponendo formula_111 e formula_112 ed usando il teorema di Pitagora si ottiene:
L'equivalenza in uno spazio euclideo di dimensione arbitraria può essere verificata in modo analogo.
Il prodotto scalare riveste fondamentale importanza sia in fisica che in vari settori della matematica, ad esempio nella classificazione delle coniche, nello studio di una funzione differenziabile intorno ad un punto stazionario, delle trasformazioni del piano o nella risoluzione di alcune equazioni differenziali. Spesso in questi contesti viene fatto uso del teorema spettrale, un importante risultato connesso al prodotto scalare.
Nel piano cartesiano il prodotto scalare permette di definire e trattare la nozione geometrica di lunghezza di un vettore. Tale concetto può essere esteso ad uno spazio vettoriale di dimensione arbitraria introducendo un concetto analogo: la norma. Formalmente, se formula_114 ed il prodotto scalare è definito positivo, è possibile dotare lo spazio vettoriale di una norma. Più precisamente, la funzione:
soddisfa per ogni vettore formula_116, formula_117 e per ogni scalare formula_47 le proprietà:
e dunque rende lo spazio vettoriale uno spazio normato.
In modo analogo alla matrice associata ad una applicazione lineare, fissata una base formula_123, un prodotto scalare formula_124 è identificato dalla matrice simmetrica associata formula_125, definita nel modo seguente:
formula_126
D'altro canto, ogni matrice simmetrica dà luogo ad un prodotto scalare. Molte proprietà del prodotto scalare e della base possono essere lette sulla matrice associata.
Fissata la matrice formula_127, per ogni coppia di vettori colonna formula_128 il prodotto scalare è definito dalla legge 
formula_129
Dove formula_130 è il vettore riga formula_131 trasposto.
La matrice scritta rispetto ad una base ortonormale di un certo prodotto scalare è la matrice identità; se la base è ortogonale ma non normalizzata, invece, la matrice sarà semplicemente diagonale.
Il "radicale" di un prodotto scalare è l'insieme dei vettori formula_132 per cui:
per ogni formula_134. Il radicale è un sottospazio vettoriale di formula_1. Il prodotto scalare si dice "degenere" se il radicale ha dimensione maggiore di zero.
Se formula_1 ha dimensione finita e formula_125 è la matrice associata a formula_124 rispetto ad una qualsiasi base, applicando il teorema della dimensione si trova facilmente che:
dove formula_140 è il rango di formula_125 e formula_142 è il radicale. Quindi un prodotto scalare è non degenere se e solo se la matrice associata è invertibile. Si definisce il "rango" del prodotto scalare come formula_140.
Un prodotto scalare definito positivo o negativo è necessariamente non degenere. Non è vero il contrario, infatti il prodotto scalare associato rispetto alla base canonica alla matrice:
non è degenere, ma non è né definito positivo né definito negativo.
Un vettore formula_2 è "isotropo" se formula_146 = 0. Tutti i vettori del radicale sono isotropi, ma possono esistere vettori isotropi che non appartengono al radicale. Ad esempio, per il prodotto scalare associato alla matrice formula_89 descritta sopra il vettore formula_148 è isotropo ma non è contenuto nel radicale, che ha dimensione zero.
Due vettori formula_2 e formula_3 si dicono ortogonali se formula_151. Il sottospazio ortogonale a formula_152 (sottospazio vettoriale di formula_1 ) è definito come:
Il sottospazio ortogonale è appunto un sottospazio vettoriale di formula_1. Contrariamente a quanto accade con il prodotto canonico nello spazio euclideo, un sottospazio ed il suo ortogonale non si intersecano in generale in un punto solo (possono addirittura coincidere). Per quanto riguarda le loro dimensioni, vale la seguente disuguaglianza:
Se il prodotto scalare è non degenere, vale l'uguaglianza
Infine, se il prodotto scalare è definito positivo o negativo, effettivamente uno spazio e il suo ortogonale si intersecano solo nell'origine e sono in somma diretta. Si ottiene:
Una base ortogonale di vettori di formula_1 è una base di vettori a due a due ortogonali. Una base è ortogonale se e solo se la matrice associata al prodotto scalare rispetto a questa base è diagonale.
Una trasformazione ortogonale è una applicazione lineare invertibile formula_160 in sé che preserva il prodotto scalare, cioè tale che:
Se formula_114 è il campo dei numeri reali e formula_1 ha dimensione "n", il teorema di Sylvester reale afferma che, dato un prodotto scalare formula_124 su formula_1, si ha che:
Quindi la matrice associata è una matrice diagonale avente sulla diagonale solo i numeri 0, 1, e -1, in ordine sparso. Siano formula_170, formula_171 e formula_172 rispettivamente il numero di volte che compaiono i numeri 0, 1 e -1 sulla diagonale: la terna formula_173 è la segnatura del prodotto scalare.
La segnatura è un invariante completo per l'isometria: due spazi vettoriali con prodotto scalare sono isometrici se e solo se hanno la stessa segnatura.
Il teorema di Sylvester complesso dice invece che esiste sempre una base ortogonale formula_174 tale che per ogni formula_175 il numero formula_176 è uguale a 0 oppure 1. In questo caso, il rango è un invariante completo per l'isometria: due spazi vettoriali complessi con prodotto scalare sono isometrici se e solo se hanno lo stesso rango.
Un endomorfismo formula_160 è simmetrico o "autoaggiunto" rispetto al prodotto scalare se:
per ogni coppia di vettori formula_179. Un endomorfismo è simmetrico se e solo se la matrice associata rispetto ad una qualsiasi base ortonormale è simmetrica.
</text>
</doc>
<doc id="65248" url="https://it.wikipedia.org/wiki?curid=65248">
<title>Matrice identità</title>
<text>
In matematica, la matrice identità, anche detta matrice identica o matrice unità, è una matrice quadrata in cui tutti gli elementi della diagonale principale sono costituiti dal numero 1, mentre i restanti elementi sono 0. Viene indicata con formula_1 oppure con formula_2, dove formula_3 è il numero di righe della matrice.
Usando la notazione applicata talvolta per descrivere in modo conciso le matrici diagonali, si può scrivere:
Si può anche scrivere con la notazione delta di Kronecker:
Dalla proprietà fondamentale segue che la matrice identità è l'elemento neutro della moltiplicazione nell'anello di tutte le matrici formula_13 a valori in un campo fissato formula_14.
Analogamente, è l'elemento neutro nel gruppo generale lineare formula_15 formato da tutte le matrici invertibili formula_13 a valori in formula_14.
Sia formula_14 un campo. Ogni matrice quadrata formula_7 induce una trasformazione lineare dallo spazio vettoriale formula_20 in sé, definita nel modo seguente:
La matrice identità è così chiamata perché induce la funzione identità. Più in generale, la matrice identità è la matrice associata alla funzione identità da uno spazio vettoriale in sé, rispetto ad una qualsiasi base.
</text>
</doc>
<doc id="48017" url="https://it.wikipedia.org/wiki?curid=48017">
<title>Area</title>
<text>
L'area è la misura dell'estensione di una regione bidimensionale di uno spazio, ovvero la misura dell'estensione di una superficie.
Come per le altre misure di natura geometrica, per la precisione si dovrebbe distinguere fra la regione bidimensionale (insieme di punti) e la sua area (valore numerico associato alla precedente). Spesso però, nel parlare comune ma anche in esposizioni scientifiche, il termine area e il termine superficie vengono usati indifferentemente.
Per l'area sono state e sono utilizzate tuttora varie unità di misura. Nel passato si sceglievano unità sulla base di esigenze locali e, in particolare nel mondo rurale, si avevano misure diverse anche in regioni limitrofe. Successivamente, a partire dalle spinte illuministiche, si sono date definizioni razionali e unificanti. Qui presentiamo le unità più importanti.
Unità di misura anglosassoni (sistema imperiale britannico e sistema consuetudinario statunitense):
</text>
</doc>
<doc id="76562" url="https://it.wikipedia.org/wiki?curid=76562">
<title>Funzione iniettiva</title>
<text>
In matematica, una funzione iniettiva (detta anche funzione ingettiva oppure iniezione) è una funzione che associa, a elementi distinti del dominio, elementi distinti del codominio.
In altre parole: una funzione da un insieme formula_1 a un insieme formula_2 è iniettiva se ogni elemento di formula_2 non può essere ottenuto in più modi diversi partendo dagli elementi di formula_1. 
Una funzione formula_5 si dice iniettiva se due elementi distinti del dominio hanno immagini distinte, ossia formula_6 implica formula_7; equivalentemente, se due elementi del dominio hanno la stessa immagine allora coincidono necessariamente, ossia formula_8 implica formula_9.
Simbolicamente:
oppure, nella forma contronominale:
Se formula_5 è una funzione iniettiva, allora ogni elemento dell'immagine formula_13 è immagine di esattamente un elemento del dominio, e la proiezione del grafico formula_14 sulla seconda coordinata è una funzione iniettiva.
In particolare, se formula_15 è una funzione reale di una variabile reale iniettiva, qualunque retta parallela all'asse delle formula_16 intersecherà il grafico della funzione in "al massimo" un punto.
Se inoltre la funzione iniettiva è definita e continua su un intervallo, allora è strettamente monotòna (strettamente crescente o strettamente decrescente).
Viceversa, se formula_15 è una funzione reale di variabile reale non iniettiva, allora esistono due elementi del dominio che hanno la stessa immagine, formula_18. Dunque la retta formula_19 interseca il grafico formula_20 in almeno due punti: formula_21 e formula_22.
Un omomorfismo di gruppi è iniettivo ("monomorfismo") se e solo se il suo nucleo è costituito dal solo elemento neutro.
In particolare, un'applicazione lineare tra spazi vettoriali è iniettiva se e solo se il suo nucleo è composto solo dal vettore nullo.
Equivalentemente in spazi di dimensione finita, un'applicazione lineare è iniettiva se e solo se la dimensione dell'immagine è uguale alla dimensione del dominio: non esistono quindi applicazioni lineari iniettive da uno spazio ad un altro di dimensione minore.
L'iniettività è una condizione necessaria ma non sufficiente per l'invertibilità.
Una funzione iniettiva formula_5 non è in generale invertibile, perché dovrebbe essere anche suriettiva. Restringendo però il codominio all'immagine si ottiene una diversa funzione formula_24, invertibile.
Una funzione invertibile formula_15 è iniettiva, ed anche la sua inversa formula_26, essendo invertibile, è iniettiva.
La composizione di due (o più) funzioni iniettive è iniettiva:
Se la funzione composta formula_28 è iniettiva, allora formula_15 è iniettiva, ma non è detto che formula_30 lo sia.
Ad esempio, la funzione iniettiva formula_31 è composizione di una funzione iniettiva formula_32 e di una funzione non iniettiva formula_33.
Se esistono due funzioni "distinte" formula_34 tali che formula_35, allora formula_15 non è iniettiva:
infatti esiste un formula_37 con formula_38, ma formula_39.
Una funzione il cui dominio abbia cardinalità superiore al codominio non può essere iniettiva. Dunque una funzione iniettiva tra due insiemi ha un codominio di cardinalità maggiore o uguale al dominio.
Questa proprietà è vera, oltre che per insiemi di cardinalità finita anche per insiemi di cardinalità infinita: per esempio, non esistono funzioni iniettive da un insieme con la cardinalità del continuo a un insieme numerabile.
Il numero di funzioni iniettive da un insieme finito formula_40 con formula_41 elementi ad un insieme finito formula_42 con formula_43 elementi è pari al numero di disposizioni semplici di formula_43 elementi, presi formula_41 a formula_41:
Quelle che seguono sono formulazioni equivalenti alla definizione dell'iniettività di una funzione formula_58 e, pertanto, sono interpretabili come ulteriori caratterizzazioni della stessa proprietà.
</text>
</doc>
<doc id="4582860" url="https://it.wikipedia.org/wiki?curid=4582860">
<title>Decomposizione in fratti semplici</title>
<text>
In algebra, la decomposizione in fratti semplici di una funzione razionale, anche detta decomposizione in frazioni semplici o espansione in fratti semplici, è la scrittura della frazione tramite un polinomio (che può essere nullo) sommato ad una o più frazioni con un denominatore più semplice. Tale metodo fornisce un algoritmo che consente di valutare le primitive di una funzione razionale.
Per illustrare l'idea del procedimento, sia data una funzione razionale formula_1, in cui formula_2 e formula_3 sono polinomi, e si consideri la fattorizzazione formula_4 del denominatore. Per ogni fattore che ha la forma formula_5 si considerano le frazioni formula_6, mentre per ogni fattore che ha la forma formula_7 si considerano le frazioni:
Si ottiene così la scrittura:
e calcolando i coefficienti formula_10 e formula_11 si trova una decomposizione che consente, analizzandone ogni singolo termine, di integrare la frazione di partenza. Essa conduce quindi formula_12 ad un'espressione del tipo:
dove formula_14 e formula_15 sono polinomi di grado inferiore rispetto a formula_2 e formula_3.
Se si applica la decomposizione fin dove è possibile si ottiene che il denominatore di ogni termine è una potenza di un polinomio non fattorizzabile e il numeratore è un polinomio di grado inferiore di quello del polinomio non fattorizzabile.
Si consideri una funzione razionale formula_18 nella variabile formula_19 il cui denominatore si può fattorizzare come:
sul campo formula_21, che può essere ad esempio formula_22 o formula_23. Se formula_24 e formula_25 non hanno nessun fattore comune, allora formula_12 si può scrivere come:
per qualche coppia di polinomi formula_28 e formula_29 su formula_21. L'esistenza di tale decomposizione è una conseguenza del fatto che l'anello dei polinomi su formula_21 è un dominio ad ideali principali, sicché:
per qualche coppia di polinomi formula_33 e formula_34 (si veda l'identità di Bézout).
Con tale approccio si può induttivamente scrivere formula_35 come una somma di frazioni i cui denominatori sono potenze di polinomi irriducibili.
In modo più rigoroso, siano formula_2 e formula_3 polinomi non nulli su formula_21. Si scriva formula_3 come prodotto di potenze di polinomi non fattorizzabili:
Allora esistono unici i polinomi formula_41 e formula_42, di cui formula_42 hanno grado inferiore a quello di formula_44, tali che:
e se il grado di formula_2 è minore di quello di formula_3 allora formula_48.
Si può verificare tale teorema scrivendo formula_49 come una somma in cui i denominatori sono potenze di formula_50 ed i numeratori sono polinomi di grado inferiore a quello di formula_50, più un eventuale polinomio aggiuntivo. Per fare ciò si può utilizzare l'algoritmo di Euclide applicato ai polinomi.
Se formula_21 è il campo dei numeri complessi formula_23 allora per il teorema fondamentale dell'algebra si può assumere che ogni formula_44 ha grado 1.
Siano formula_2 e formula_3 polinomi non nulli sul campo formula_21. Si scriva formula_3 come prodotto di potenze di polinomi vicendevolmente primi che non hanno radici multiple in un campo algebricamente chiuso:
Allora esistono unici i polinomi formula_41 e formula_61, di cui formula_61 hanno grado inferiore a quello di formula_44, tali che:
dove l'apice denota la derivata. Questo risultato consente di ridurre il calcolo della primitiva di una funzione razionale all'integrazione della somma al secondo membro, detta "parte logaritmica" a causa del fatto che la sua primitiva è una combinazione lineare di logaritmi. Infatti, si ha:
Vi sono diversi metodi per calcolare tale decomposizione, il più semplice dei quali è il metodo di Hermite: si basa sul fatto che formula_61 ha grado inferiore a quello di formula_44, e che il grado di formula_68 è la differenza (positiva) tra i gradi di formula_2 e formula_3: questo consente di scrivere tali polinomi incogniti come polinomi noti con coefficienti ignoti. Riducendo i due termini della formula precedente in un'unica frazione si ottiene un sistema di equazioni lineari che consente di trovare tali coefficienti.
Si vuole decomporre l'espressione:
ovvero scriverla nella forma:
in cui i parametri formula_73, formula_74 e formula_75 sono ignoti. Moltiplicando queste due espressioni per formula_76 ed uguagliandole si ottiene:
Raccogliendo i termini che moltiplicano le potenze di formula_19 si ha:
Il polinomio al secondo membro ha solo il coefficiente di grado zero non nullo, e si possono uguagliare i coefficienti che moltiplicano le potenze di formula_19 di entrambi i membri. In questo modo si ottiene il sistema di equazioni lineari:
che fornisce:
</text>
</doc>
<doc id="1357716" url="https://it.wikipedia.org/wiki?curid=1357716">
<title>Edge</title>
<text>
</text>
</doc>
<doc id="1369296" url="https://it.wikipedia.org/wiki?curid=1369296">
<title>Numero primo</title>
<text>
In matematica, un numero primo (in breve anche primo) è un numero intero positivo che abbia esattamente due divisori distinti. In modo equivalente si può definire come un numero naturale maggiore di 1 che sia divisibile solamente per 1 e per sé stesso; al contrario, un numero maggiore di 1 che abbia più di due divisori è detto composto. Ad esempio 2, 3 e 5 sono primi mentre 4 e 6 non lo sono perché sono divisibili rispettivamente anche per 2 e per 2 e 3. L'unico numero primo pari è 2, in quanto tutti gli altri numeri pari sono divisibili per 2.
La successione dei numeri primi comincia con 2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37…
Quello di numero primo è uno dei concetti basilari della teoria dei numeri, la parte della matematica che studia i numeri interi: l'importanza sta nella possibilità di costruire con essi, attraverso la moltiplicazione, tutti gli altri numeri interi, nonché l'unicità di tale fattorizzazione. I primi sono inoltre infiniti e la loro distribuzione è tuttora oggetto di molte ricerche.
I numeri primi sono oggetto di studio fin dall'antichità: i primi risultati risalgono infatti agli antichi Greci, e in particolare agli "Elementi" di Euclide, scritti attorno al 300 a.C. Ciononostante, numerose congetture che li riguardano non sono state ancora dimostrate; tra le più note vi sono l'ipotesi di Riemann, la congettura di Goldbach e quella dei primi gemelli, indimostrate a più di un secolo dalla loro formulazione.
Essi sono rilevanti anche in molti altri ambiti della matematica pura, come ad esempio l'algebra o la geometria; recentemente hanno assunto un'importanza cruciale anche nella matematica applicata, e in particolare nella crittografia.
Non è noto quando sia stato definito il concetto di numero primo, tuttavia un segnale che fa supporre una qualche consapevolezza della diversità di tali numeri è testimoniato dall'Osso d'Ishango, un reperto osseo datato al Paleolitico superiore, in cui compaiono dei segni rappresentanti i numeri primi compresi tra 10 e 20. Per trovare un altro segno di questa consapevolezza bisogna recarsi in Mesopotamia e aspettare il secondo millennio a.C.; a tale periodo appartengono infatti alcune tavolette contenenti le soluzioni di alcuni problemi aritmetici che, per essere svolti, richiedono una buona conoscenza della fattorizzazione in primi. Allo stesso millennio appartiene anche il papiro di Rhind (trascritto intorno al 1650 a.C.), che contiene alcune espansioni in frazioni egizie dei numeri nella forma ⁄. Le espansioni dei numeri che hanno in comune il più piccolo dei loro fattori sono simili, suggerendo che gli Egizi fossero almeno consapevoli della differenza tra i numeri primi e i composti.
La prima traccia incontestabile di un vero studio dei numeri primi è costituita dagli "Elementi" di Euclide, un libro composto tra il IV e il III secolo a.C., che fornisce un quadro completo delle conoscenze matematiche del tempo. Quest'opera contiene alcuni risultati fondamentali, tra cui il teorema dell'infinità dei primi e il lemma di Euclide, che prova un'importante caratterizzazione dei numeri primi. Euclide dimostra anche la possibilità di fattorizzare ogni intero positivo come prodotto di primi. All'antica Grecia dobbiamo anche il crivello di Eratostene, un semplice algoritmo per determinare quali sono i numeri primi.
I secoli seguenti registrarono un certo disinteresse per lo studio dei numeri primi e per diverso tempo non furono dimostrati risultati di particolare rilevanza su questo argomento. L'interesse verso di essi riprese vigore nel diciassettesimo secolo, con le dimostrazioni di nuovi e importanti risultati, alcuni dei quali dovuti a Pierre de Fermat: in particolare egli provò un teorema sulle congruenze modulo un primo, noto come "piccolo teorema di Fermat", e il teorema sulle somme di due quadrati che afferma che tutti i primi di una certa forma si possono scrivere come somma di due quadrati. Congetturò inoltre che tutti i numeri nella forma 2 + 1 (oggi chiamati in suo onore numeri di Fermat) fossero primi; Fermat stesso aveva verificato la sua congettura fino a "n" = 4, ma Eulero mostrò che per "n" = 5 si otteneva un numero composto. A oggi non sono noti altri numeri di questo tipo che siano primi. Nello stesso periodo, il monaco francese Marin Mersenne pose l'attenzione sui primi nella forma 2 − 1, con "p" primo, che oggi sono chiamati in suo onore primi di Mersenne.
Altri risultati vennero ottenuti da Eulero nel corso del diciottesimo secolo: tra di essi vi sono la divergenza della serie infinita ⁄ + ⁄ + ⁄ + ⁄ + ⁄ + ..., in cui gli addendi sono gli inversi dei numeri primi, e il cosiddetto prodotto di Eulero, una formula che evidenzia il legame dei primi con la serie armonica. Nella corrispondenza di Eulero con Christian Goldbach, quest'ultimo formulò inoltre la famosa congettura di Goldbach, ancora oggi non dimostrata, che riguarda la rappresentazione dei numeri naturali pari come somma di numeri primi.
Dall'inizio dell'Ottocento, l'attenzione di molti matematici si rivolse allo studio della distribuzione asintotica dei primi, ossia allo studio dell'andamento della funzione che conta i primi minori o uguali a "x". Legendre e Gauss congetturarono indipendentemente che tale funzione tende, al crescere di "x", a "x" / ln("x"), dove ln("x") indica il logaritmo naturale di "x". Nel 1859 Bernhard Riemann collegò questo problema con il posizionamento degli zeri della funzione zeta di Riemann, una funzione di variabile complessa; questo approccio portò alla dimostrazione della congettura, compiuta in modo indipendente da Hadamard e de la Vallée Poussin nel 1896. Tale risultato è oggi noto col nome di teorema dei numeri primi.
I numeri primi restarono confinati nell'ambito della matematica pura fino agli anni settanta, quando venne sviluppato il concetto di crittografia a chiave pubblica; il primo algoritmo di questo tipo, l'RSA, sfrutta infatti la difficoltà di fattorizzare numeri grandi formati da due soli fattori primi. Per questo motivo, ha assunto una notevole importanza anche la ricerca di numeri primi sempre più grandi. A partire dal 1951, tale ricerca viene effettuata attraverso l'uso di computer.
Il più piccolo numero primo è 2; tutti gli altri sono dispari, in quanto ogni numero pari è divisibile per 2. Nel passato 1 era a volte considerato un numero primo: ad esempio Derrick Norman Lehmer lo incluse nella sua tavola dei numeri primi pubblicata nel 1914. Oggi tuttavia si preferisce escluderlo, in quanto il suo inserimento tra i primi costringerebbe a riformulare in maniera più complessa diversi teoremi (come il teorema fondamentale dell'aritmetica) per tenere conto di questo caso speciale.
Un metodo per verificare se un numero "n" è primo si definisce "test di primalità". Un metodo che discende direttamente dalla definizione è controllare che non sia diviso da nessun numero minore di "n" o, in modo più efficiente, da nessun primo minore di "n". Ad esempio, per provare che 11 è primo, basta osservare che non è diviso da 2, 3, 5 e 7 (che sono i primi minori di 11).
Un antico algoritmo che evita le divisioni è il crivello (ossia "setaccio") di Eratostene che, più precisamente, determina l'insieme dei primi minori o uguali a "X". Per far ciò, l'algoritmo parte dall'insieme dei numeri naturali compresi tra 2 e "X", ed elimina i multipli dei numeri primi individuati in precedenza (perché non sono multipli di numeri più piccoli). In effetti, è possibile migliorare questo algoritmo fermandosi a eliminare i multipli dei primi minori o uguali alla parte intera della radice di "X": se infatti un numero composto "c" ha tutti i fattori maggiori della radice di "X", allora è maggiore di "X", in quanto, dovendo avere almeno due fattori,
La figura a destra mostra il funzionamento dell'algoritmo per "X" = 120. Analogamente, se si utilizza il metodo delle divisioni per dimostrare la primalità di un numero "X" si può evitare di controllare la divisibilità di "X" per numeri maggiori della radice quadrata di "X".
In una semplice interpretazione geometrica del concetto di numero primo, i numeri "n" che non sono primi sono esattamente quei numeri che possono essere rappresentati come rettangoli composti da "n" quadratini i cui lati sono maggiori di 1. Ad esempio 12 non è primo, perché può essere rappresentato come un rettangolo di lati 3 e 4, mentre 11 è primo, perché non ammette nessuna rappresentazione di questo tipo. Ogni rappresentazione di un numero composto tuttavia ne ammette una simmetrica a seconda che il lato lungo sia orizzontale o verticale; arrestare il crivello (o le divisioni) una volta raggiunta la radice di "X" significa considerare solo un rettangolo per ciascuna coppia di rettangoli simmetrici.
L'importanza dei numeri primi in matematica è enorme e deriva essenzialmente dal teorema fondamentale dell'aritmetica, il quale asserisce che qualsiasi numero intero positivo diverso da 1 può essere scomposto in fattori primi, e tale scomposizione è unica a meno dell'ordine dei fattori.
Ad esempio, 23244 si fattorizza come
e ogni altra sua fattorizzazione in numeri primi è ottenuta da questa permutando i fattori. Ad esempio, l'ulteriore fattorizzazione
non è altro che quella precedente con i fattori scritti in un ordine diverso. A causa di questa proprietà, ci si riferisce a volte ai numeri primi come agli "atomi dell'aritmetica".
Questa è tra l'altro la ragione principale per cui 1 è escluso dall'insieme dei primi. Infatti, se si moltiplica una fattorizzazione di un numero per uno, un numero di volte a piacere, si ottiene sempre il numero di partenza, creando così fattorizzazioni distinte.
Una proprietà strettamente collegata alla fattorizzazione unica è il lemma di Euclide: se un primo "p" divide il prodotto "ab", allora divide "a" o "b". Questa è considerata la definizione stessa di "elemento primo" in un dominio d'integrità, ed è ovvia a partire dal teorema fondamentale dell'aritmetica: la fattorizzazione di "ab" dovrà infatti contenere il primo "p", e visto che "p" non può essere "spezzato" in due fattori, deve necessariamente essere nella fattorizzazione di almeno uno dei due numeri.
I numeri primi sono infiniti. La più antica dimostrazione pervenutaci è quella di Euclide, che la presenta nel IX libro degli "Elementi", come proposizione 20, con le parole:
La dimostrazione procede per assurdo. Supponendo infatti che esista solo un numero finito di numeri primi "p", "p", ..., "p", si può considerare il numero "q" = "p""p" ··· "p" + 1: questo numero è ovviamente maggiore di 1 e diverso da tutti i numeri primi "p". Ora, vi sono due possibilità per "q": può essere primo o composto. Se fosse primo avremmo però una contraddizione, perché abbiamo assunto che i "p" siano tutti i numeri primi; se fosse invece composto, dovrebbe avere un fattore primo "d", che deve essere uno dei numeri primi "p". Ma allora "d" divide sia "q" sia il prodotto "p""p" ··· "p" (essendo uno dei numeri primi), e quindi deve dividere la loro differenza "q" − "p""p" ··· "p" = 1, il che è impossibile. Quindi "q" non può essere né primo né composto: ma questo è assurdo, e i numeri primi sono infiniti.
Una questione che sorge dalla dimostrazione è se i numeri nella forma "p""p" ··· "p" + 1, cioè il prodotto dei primi "n" primi più 1 (detti numeri di Euclide), siano o meno primi. Questo avviene nei primi casi (2·3 + 1 = 7 è primo, così come 2·3·5 + 1 = 31), ma è falso in generale: il più piccolo di tali numeri a essere composto è
Non è noto se in questa successione esistano infiniti numeri primi, anche se è stato congetturato che sia così.
Molte altre dimostrazioni sono state create nel corso dei secoli: Eulero dimostrò questo teorema a partire dalla divergenza della serie armonica, Goldbach attraverso i numeri di Fermat, mentre Harry Furstenberg ne ideò una usando metodi della topologia.
Un teorema più forte, da cui si ricava facilmente l'infinità dei numeri primi, è quello che stabilisce che la serie / + / + / + / + / + ..., formata dalla somma degli inversi dei numeri primi, diverge, e in particolare, usando la notazione O-grande:
Questo teorema è dovuto a Eulero, che lo dimostrò nel diciottesimo secolo.
Dalla dimostrazione di Euclide segue anche che
Tale disuguaglianza può essere migliorata: H. Bonse dimostrò nel 1907 (disuguaglianza di Bonse) che
per "n" &gt; 3. Su questa strada, è stato dimostrato che la disuguaglianza
è verificata per ogni "n" &gt; 2"k".
Una volta dimostrato che i numeri primi sono infiniti, sorge spontaneo chiedersi come si distribuiscono all'interno della sequenza dei numeri naturali, cioè quanto sono frequenti e quando ci si può aspettare di trovare l'"n"-esimo numero primo. Questo studio fu incominciato verso la fine del XVIII secolo indipendentemente da Gauss e da Legendre, che introdussero la funzione formula_9 (detta funzione enumerativa dei primi) e congetturarono che essa fosse approssimativamente
Il tentativo di dimostrare questa congettura attraversò tutto l'Ottocento; i primi risultati furono ottenuti tra il 1848 e il 1859 da Chebyshev, che dimostrò usando metodi puramente aritmetici che esistevano due costanti "A" e "B" tali che
per "x" sufficientemente grande. Riuscì anche a provare che, se il limite del rapporto esiste, allora esso deve essere 1.
Una dimostrazione fu invece trovata nel 1896 da Hadamard e da de la Vallée-Poussin, che, pur lavorando indipendentemente l'uno dall'altro, usarono metodi simili, basati sull'uso della funzione zeta di Riemann, la quale era stata introdotta da Bernhard Riemann nel 1859. Per una dimostrazione che usasse soltanto metodi elementari (cioè senza usare metodi di analisi complessa) si dovette attendere invece fino al 1949, quando essa fu ideata da Selberg e Erdős. Il teorema è oggi noto come teorema dei numeri primi.
Gauss aveva introdotto anche una stima più precisa, utilizzando la funzione logaritmo integrale:
Nel 1899 de la Vallée-Poussin dimostrò che l'errore che si commette approssimando formula_9 in questo modo è
per una costante positiva "a" e ogni intero "m"; tale risultato è stato leggermente migliorato nel corso degli anni. Inoltre, nel 1901 von Koch mostrò che se l'ipotesi di Riemann è vera, allora si ha la stima molto più precisa:
Una forma equivalente al teorema dei numeri primi è che "p", l'"n"-esimo numero primo, è ben approssimato da "n" ln("n"). In effetti, "p" è strettamente maggiore di questo valore, come è stato dimostrato da J. Barkley Rosser nel 1938; questa disuguaglianza è stata migliorata fino ad arrivare, nel 1995, a
per "n" ≥ 2.
Legato alla distribuzione dei numeri primi è lo studio degli intervalli tra due primi consecutivi. Questo, a parte la coppia formata da 2 e 3, deve essere necessariamente un numero pari maggiore o uguale a 2, perché tra due numeri consecutivi almeno uno è pari e quindi non primo. Se due numeri primi hanno come differenza 2, sono detti "gemelli": con l'eccezione della "tripletta" formata da 3, 5 e 7, i numeri primi gemelli si presentano a coppie, ed è semplice verificare che, tranne nel caso 3 e 5, il numero posto tra di loro è sempre un multiplo di 6. Le più piccole coppie di primi gemelli sono (3, 5), (5, 7), (11, 13), (17, 19) e (29, 31). È stato congetturato che esistano infinite coppie di numeri primi gemelli, sebbene nessuno sia ancora riuscito a dimostrarlo; un'estensione di questa idea è chiedersi se, dato un numero pari "k", la differenza tra due primi consecutivi sia pari a "k" infinite volte. Quest'ultimo problema prende il nome di congettura di Polignac.
È facile invece mostrare che questa differenza può essere grande a piacere: dato un intero "N", e indicando con "N!" il suo fattoriale (cioè il prodotto di tutti i numeri compresi tra 1 e "N"), i numeri
sono tutti composti: infatti, se "m" è minore di "N", allora ("N" + 1)! + "m" è divisibile per "m", e quindi non è primo. La sequenza, che comprende "N" numeri consecutivi, è quindi priva di numeri primi. Ad esempio, se "N" = 5, questi valori corrispondono a
mentre il valore successivo, 6!+7=727, è primo. Si noti comunque che esistono modi più "efficienti" per costruire intervalli senza numeri primi; ad esempio invece di ("N" + 1)! + 1 si può considerare il prodotto dei numeri primi minori di "N" + 2.
Dal teorema dei numeri primi discende facilmente che l'intervallo atteso tra due numeri primi consecutivi "p" e "p" ha lunghezza ln("p"); tuttavia questi intervalli sono talvolta molto più grandi e talvolta molto più piccoli. Sugli intervalli corti, la congettura dei primi gemelli afferma esattamente che l'intervallo è il minimo possibile infinite volte. Questa congettura è tuttora aperta, ma grazie al lavoro di Zhang Yitang (annunciato nel 2013, e basato sull'approccio di Goldston, Pintz e Yıldırım) e ai successivi contributi di James Maynard e di un progetto Polymath, è noto che esistono infiniti numeri primi consecutivi la cui differenza è minore di 246.
Sul problema opposto, degli intervalli lunghi, ci si aspetta che tali intervalli siano di ordine ln "p", o, più precisamente, che
mentre i migliori risultati dimostrati sono
e
dovuti rispettivamente a Ford, Green, Konyagin, Maynard e Tao e a Pintz.
Un altro risultato classico, seppur più debole di quelli appena riportati, è il postulato di Bertrand (che in realtà è un teorema, essendo stato dimostrato da Chebyshev nel 1850). Esso afferma che per ogni "n" esiste sempre un primo tra "n" e 2"n". Un'interessante conseguenza di questo risultato è che "p" &lt; 2"p"; considerando inoltre che "p" = 2 si deduce facilmente che per ogni "n" vale la disuguaglianza
Nel corso dei secoli, sono state proposte molte congetture sugli intervalli tra primi consecutivi. Le più famose sono la congettura di Legendre, che afferma che tra due quadrati consecutivi vi è sempre un primo, la congettura di Brocard che asserisce che tra i quadrati di due primi dispari consecutivi esistono sempre quattro numeri primi, e la congettura di Andrica che ipotizza che
Queste congetture sono tutte molto più deboli di quanto ritenuto comunemente vero, ma sono tuttora indimostrate. I migliori risultati in questa direzione sono la dimostrazione che tra "n" e ("n" + 1) giace sempre almeno un primo o un semiprimo, dovuta a Chen Jingrun, e il risultato di Baker, Harman e Pintz riportato sopra.
Essendo alle basi dell'aritmetica, i numeri primi sono ingredienti fondamentali in un gran numero di settori della matematica.
Le funzioni aritmetiche, ossia le funzione definite sugli interi e a valori nei numeri complessi, rivestono un ruolo cruciale nella teoria dei numeri. In modo particolare, tra queste le più importanti sono le funzioni moltiplicative, ovvero quelle funzioni "f" in cui, per ogni coppia ("a","b") di numeri coprimi, si ha
Esempi di funzioni moltiplicative sono la funzione φ di Eulero, che a "n" associa il numero degli interi che sono al contempo minori e coprimi con "n", e le funzioni divisore e sigma, che a "n" associano rispettivamente il numero dei suoi divisori e la loro somma. Il valore di tali funzioni nelle potenze dei primi è
Grazie alla proprietà che le definisce, le funzioni aritmetiche si possono facilmente calcolare conoscendo il valore che esse assumono nelle potenze dei primi. Infatti, dato un intero "n" di fattorizzazione
si ha che
e dunque si è ricondotto il problema di calcolare "f"("n") a quello di calcolare "f" sulle potenze dei primi che dividono "n", valori che sono in genere più semplici da ricavare rispetto a una formula generale. Ad esempio, per conoscere il valore della funzione φ di Eulero su "n" = 450 = 2×3×5 è sufficiente calcolare
Il fatto che una funzione moltiplicativa sia individuata dai valori assunti in corrispondenza delle potenze dei numeri primi è all'origine dell'uso delle serie di Bell, che sono delle particolari serie formali di potenze. Data una funzione moltiplicativa "f" e un primo "p", la serie di Bell di "f" rispetto a "p" è:
In particolare, se "f" è "completamente" moltiplicativa (cioè se "f"("ab") = "f"("a") "f"("b") per ogni "a" e "b"), allora "f" è individuata dai valori di "f"("p"), per "p" primo, e la sua serie di Bell è:
Nell'aritmetica modulare i numeri primi svolgono un ruolo molto importante: l'anello formula_37 delle classi di resto è infatti un campo se e solo se "n" è primo. In questo caso lo studio delle classi di resto è più semplice del caso generale, e fornisce un'utile base di partenza per l'analisi delle classi di resto con "n" qualunque.
Anche l'esistenza di una radice primitiva dell'anello formula_37 è legata ai numeri primi: questa infatti esiste solamente se "n" è un numero primo, 1, 2, 4 oppure un numero nella forma formula_39 o formula_40, dove "p" è un primo dispari.
Uno dei teoremi più importanti dell'aritmetica modulare è costituito dal piccolo teorema di Fermat. Tale teorema afferma che, per ogni primo "p" e ogni numero naturale "a" si ha
Equivalentemente, per ogni primo "p" e ogni intero "a" coprimo con "p", si ha
Questa proprietà può essere usata per verificare se un numero "non" è primo, infatti se "n" è tale che
per qualche intero "a", allora "n" non può essere primo. Tuttavia questa proprietà non può essere usata per controllare se un numero è primo: esistono infatti alcuni numeri, detti numeri di Carmichael (il più piccolo dei quali è 561), che verificano questa proprietà per ogni "a" pur non essendo primi. Nel 1994, William Robert Alford, Andrew Granville e Carl Pomerance hanno dimostrato che vi sono infiniti numeri di tale tipo.
Un altro degli argomenti principali della teoria dei numeri è costituito dallo studio dei numeri "p"-adici e delle loro proprietà. Tali numeri sono definiti nel modo seguente: per ogni primo "p" si considera una norma sui numeri razionali formula_44 che, valutata su un numero razionale "q", assume valori che si avvicinano allo 0 al crescere della massima potenza di "p" che divide "q". Tale norma è detta "norma "p"-adica". Completando il campo dei numeri razionali rispetto alla metrica indotta da tale norma, si ottiene un campo, indicato con formula_45, che "estende" i numeri razionali in un modo diverso dai numeri reali. Gli elementi di tale campo sono detti numeri "p"-adici. Tali numeri si possono anche costruire come limite proiettivo degli anelli formula_46.
I numeri primi hanno un ruolo centrale anche nell'algebra. Nella teoria dei gruppi, un gruppo in cui ogni elemento ha ordine la potenza di un primo "p" è detto "p-gruppo" o "gruppo primario". Tra i gruppi finiti, i "p"-gruppi sono tutti e soli i gruppi la cui cardinalità è la potenza di un primo; un esempio di "p"-gruppo infinito è il "p"-gruppo di Prüfer.
È noto che i "p"-gruppi hanno un centro non banale, e di conseguenza non possono essere semplici (a parte il gruppo con "p" elementi); se il gruppo è finito, inoltre, tutti i sottogruppi normali intersecano il centro in modo non banale.
Tutti i gruppi con un numero primo di elementi sono ciclici e dunque abeliani; anche ogni gruppo di ordine "p" è abeliano. Inoltre, ogni gruppo abeliano finito è isomorfo al prodotto diretto di un numero finito di "p"-gruppi ciclici.
Il teorema di Cauchy afferma che, dato un gruppo di ordine "n" e un primo "p" che lo divide, esiste un elemento di ordine "p", e quindi un sottogruppo con "p" elementi. Tale teorema è generalizzato dai teoremi di Sylow, che garantiscono che in ogni gruppo di ordine "n" esiste almeno un sottogruppo di ordine "p", per ogni "p" che divide "n".
Nella teoria degli anelli, la caratteristica di un dominio d'integrità "D" è 0 oppure un numero primo. Per un campo "F", che è un particolare tipo di dominio di integrità, la caratteristica determina il sottocampo fondamentale di "F": se essa è diversa da 0, e dunque è un numero primo, allora tale sottocampo è isomorfo al campo delle classi di resto formula_47.
Si mostra poi che tutti i campi finiti formano uno spazio vettoriale sul campo formula_47, e di conseguenza hanno un numero di elementi che è primo o è una potenza di un primo. Inoltre, due campi con lo stesso numero di elementi sono isomorfi; in particolare, ogni campo con un numero primo "p" di elementi coincide con formula_47, mentre ogni campo con "p" elementi è un'estensione di Galois di un campo con "p" elementi.
Tra le estensioni dei numeri razionali, un ruolo importante è svolto dalle estensioni ciclotomiche, ossia da quei campi che si possono ottenere aggiungendo a formula_44 le radici "n"-esime dell'unità, per un qualche numero naturale "n". Il grado di queste estensioni è strettamente legato alla primalità di "n". Infatti esso è "n" − 1 se e solo se "n" è primo: tale proprietà è equivalente al fatto che il polinomio
è irriducibile tra i polinomi a coefficienti razionali se e solo se "n" è primo. Per una dimostrazione si può procedere come segue: se "n" è composto (ad esempio "n" = "ab", con "a" e "b" interi maggiori di 1), lo si può dividere in "a" gruppi di "b" addendi, arrivando a una scomposizione. Ad esempio, se "n" = 10, prendendo "a" = 2 e "b" = 5, "P"("x") si può scomporre come
Per dimostrare l'inverso, si può usare l'invece il criterio di Eisenstein. Grazie a questa proprietà risulta inoltre che se "n" è primo, allora questo polinomio coincide con l'"n"-esimo polinomio ciclotomico.
È stato dimostrato da Legendre alla fine del Settecento che nessun polinomio a coefficienti interi può assumere valori soltanto primi: infatti, se esistesse un polinomio "P"("n") di questo tipo, si avrebbe "P"(1) = p per qualche primo "p" e quindi "P"(1) ≡ 0 mod "p". Ma "P"(1) ≡ "P"(1+"kp") mod "p" per ogni intero "k", e quindi "P"(1+"kp") dovrebbe assumere infinite volte il valore "p" (perché i multipli di "p" non possono essere primi). Tuttavia questo è assurdo, perché nessun polinomio può assumere uno stesso valore un numero di volte maggiore del proprio grado.
Alcuni polinomi sembrano assumere valori primi "più spesso" degli altri: ad esempio Eulero notò che il polinomio di secondo grado formula_53 produce numeri primi per ogni valore di "n" compreso tra 0 e 39; tuttavia, sebbene circa un terzo dei valori che questa funzione assume nei primi 10 milioni siano primi, non è stato ancora dimostrato che ne esistano infiniti. Più in generale, non c'è alcun polinomio in una sola variabile e di grado maggiore di uno di cui sia stato dimostrato che assume infiniti valori primi. Diversa è la situazione per i polinomi in due variabili: Dirichlet dimostrò che questo avviene per ogni forma quadratica formula_54 (a patto che "a", "b" e "c" siano coprimi e che la forma non sia il quadrato di un polinomio di primo grado), mentre nel 1998 John Friedlander e Henryk Iwaniec lo provarono per il polinomio di quarto grado formula_55.
A differenza di quanto accade per i polinomi di grado più alto, Dirichlet dimostrò nel 1837 che ogni polinomio di primo grado "ax"+"b" assume infiniti valori primi se e solo se "a" e "b" sono numeri naturali coprimi. Equivalentemente, una progressione aritmetica contiene infiniti numeri primi se e solo se la sua "ragione" e il suo primo valore sono coprimi. La prima dimostrazione di questo teorema, detto teorema di Dirichlet, viene considerata la nascita della Teoria dei numeri analitica.
È noto inoltre che, se "n" e "k" sono coprimi, il rapporto tra "M" e i primi minori di "M" che sono congrui a "k" modulo "n" tende a formula_56 per "M" che tende all'infinito, ovvero i primi tendono a dividersi equamente tra le formula_57 progressioni di ragione "n" che contengono più di un primo.
Sebbene non esistano progressioni aritmetiche i cui valori siano soltanto numeri primi, nel 2004 è stato dimostrato che esistono progressioni che contengono un numero arbitrariamente grande di termini consecutivi che sono primi (teorema di Green-Tao). Tale risultato è stato migliorato nel 2006 per includere anche le progressioni polinomiali; più precisamente è stato dimostrato che, dati dei polinomi "P", ...,"P" a coefficienti interi, esistono infiniti interi "a" e "m" tali che "a"+"P"("n"), ..., "a"+"P"("n") sono contemporaneamente primi per 1 ≤ "n" ≤ "m".
Tali teoremi non sono tuttavia "costruttivi", ovvero non permettono di determinare esplicitamente delle progressioni arbitrariamente lunghe; la più lunga sequenza di primi (attualmente conosciuta) che sono termini consecutivi di una progressione aritmetica è composta da 26 numeri. È stato anche congetturato che esistano sequenze arbitrariamente lunghe di questo tipo tali che tra due termini della progressione non ci siano altri numeri primi, e la più lunga sequenza di primi di questo tipo finora trovata comprende 10 termini.
Una progressione aritmetica di interesse particolare per la teoria dei numeri primi è quella di ragione 4: si possono infatti separare i primi (a parte 2) in due gruppi, quelli nella forma 4"k"+1 e quelli nella forma 4"k"+3. Il teorema di Fermat sulle somme di due quadrati asserisce che i primi che possono essere scritti come somma di due quadrati sono tutti e soli quelli del primo gruppo. Un'importante riformulazione di questo teorema è che un primo è scomponibile nell'anello degli interi di Gauss se e solo se è della forma 4"k"+1.
Per la loro definizione, i numeri primi sono intrinsecamente legati all'operazione di moltiplicazione. Tuttavia, sono di grande interesse anche alcuni problemi riguardanti loro proprietà additive.
Il più famoso di questi è senza dubbio la congettura proposta da Christian Goldbach nel Settecento, che afferma che ogni numero pari maggiore di 2 può essere espresso come somma di due primi. La congettura è tuttora indimostrata, ma è facilmente verificabile per gli interi “piccoli”, come ad esempio
e tramite l'uso di computer è stata controllata anche per tutti gli "n" minori di 2×10.
Alla congettura di Goldbach ne è legata un'altra, più debole e ora dimostrata, che afferma che ogni numero dispari è la somma di tre numeri primi. Questa "ex"-congettura è comunemente nota con il nome di congettura debole di Goldbach.
Mentre la congettura di Goldbach sembra molto lontana dall'essere risolta, la seconda ha conosciuto diversi progressi nel corso degli anni, culminati nella dimostrazione completa data da Harald Helfgott nel 2013. In precedenza, risultati significativi erano stati ottenuti da Hardy e Littlewood, che nel 1923 provarono che l'ipotesi di Riemann generalizzata implica che ogni numero dispari "sufficientemente grande" è la somma di tre primi, e da Ivan Vinogradov che nel 1937 dimostrò che l'assunzione dell'ipotesi di Riemann non è necessaria. Per completare la dimostrazione mancavano quindi solo un numero finito di numeri dispari da controllare, ma tale numero era ben al di là delle capacità computazionali dei moderni computer. Nel 2013, Helfgott introdusse diverse innovazioni all'interno della dimostrazione di Vinogradov, riuscendo ad abbassare notevolmente il numero di potenziali eccezioni a un numero effettivamente controllabile da un computer e quindi a completare la dimostrazione.
Sono noti anche altri risultati, sebbene molto più deboli. Usando il postulato di Bertrand si può dimostrare che ogni intero maggiore di 6 può essere scritto come somma di primi distinti. Inoltre, se "p" è l'"n"-esimo numero primo, allora almeno uno tra "p", "p" − 1 e "p" + 1 può essere scritto come
scegliendo opportunamente i segni "più" e "meno".
Problemi additivi sono considerati anche i già citati teorema di Green-Tao sulle progressioni aritmetiche, la congettura dei primi gemelli e la congettura di Levy, che afferma che ogni intero dispari è la somma di un primo e di un semiprimo pari.
Molte congetture riguardanti i numeri primi non sono ancora state dimostrate. La più importante tra queste è senza dubbio l'ipotesi di Riemann, uno dei problemi aperti più importanti di tutta la matematica: era uno dei ventitré problemi di Hilbert, enunciati nel 1900, ed è stato inserito tra i problemi per il millennio nel 2000. Nella sua formulazione originale, tale ipotesi riguarda il posizionamento degli zeri complessi della funzione zeta di Riemann: nonostante il suo legame con i numeri primi non sia immediatamente chiaro, è stato provato che la sua dimostrazione avrebbe come conseguenza un notevole miglioramento della comprensione dei numeri primi. In particolare, se l'ipotesi di Riemann fosse vera, i primi sarebbero distribuiti nel modo più regolare possibile.
Altri problemi aperti molto famosi sono le già citate congetture di Goldbach, dei primi gemelli e di Legendre.
Altre congetture riguardano l'esistenza o meno di infiniti numeri primi in una certa forma. Ad esempio si pensa che esistano infiniti numeri primi nelle sequenze "n" + 1, 2 - 1 (primi di Mersenne, ), "n"! + 1 e "n"! - 1 (primi fattoriali, sequenze e ), o che esistano infiniti primi nella successione di Fibonacci. Si congettura invece che vi siano solo un numero finito di primi di Fermat, i numeri primi nella forma 2 + 1. Al momento, gli unici primi di Fermat noti sono in corrispondenza di "n" = 0, 1, 2, 3 e 4.
Una formula per i numeri primi è un'espressione che genera solamente numeri primi. Non sono note formule chiuse (che cioè non fanno ricorso né a limiti né a serie né a sommatorie la cui lunghezza dipenda dal dato iniziale) per trovare tutti i numeri primi fino a "n", o anche solo l'"n"-esimo primo; sono state invece trovate alcune formule che generano solo numeri primi, seppure fondamentalmente inutili dal punto di vista pratico. Un esempio è dato dal teorema di Mills che afferma che esiste una costante θ tale che
è sempre un numero primo. Tuttavia non si conosce nessuna formula chiusa per calcolare la costante di Mills: le approssimazioni attualmente utilizzate si basano sulla sequenza dei cosiddetti primi di Mills (i numeri primi generati tramite questa formula), che non possono essere ricavati rigorosamente, ma solamente in maniera probabilistica, assumendo per vera l'ipotesi di Riemann.
A seguito della dimostrazione del teorema di Matiyasevich, sono stati trovati vari polinomi i cui valori positivi sono sempre numeri primi. Matijasevič dimostrò l'esistenza di un polinomio di 37º grado in 24 incognite, ma senza esplicitarlo; in seguito alcuni di questi sono stati determinati, ma rimangono poco utili per la ricerca di nuovi primi perché hanno diverse variabili e un grado molto elevato, e inoltre assumono spesso valori negativi.
Altre formule si possono costruire attraverso il teorema di Wilson con l'uso della funzione parte intera, ma anche queste sono sostanzialmente inutilizzabili a causa della loro elevata complessità computazionale.
Un "test di primalità" è un algoritmo che permette di stabilire se un dato numero è primo oppure no. Nella teoria della complessità computazionale, questo problema è a volte denotato come PRIMES, ed è stato recentemente dimostrato appartenere alla classe di complessità P.
Il più antico e semplice test di primalità è quello di "divisione per tentativi", che consiste nell'applicare direttamente la definizione di numero primo: si prova a dividere il numero "N" per tutti i numeri minori di "N": se nessuno di questi lo divide, allora il numero è primo. Un semplice miglioramento di questo metodo si ottiene limitando i tentativi di divisione ai numeri primi minori di formula_60. Sebbene molto semplice da descrivere e da implementare su un calcolatore, tale metodo è poco usato nella pratica, perché richiede tempi di calcolo che aumentano esponenzialmente rispetto al numero delle cifre di "N". Esso tuttavia fornisce anche i suoi fattori primi (ed è quindi un algoritmo di fattorizzazione): questo non succede nel caso di algoritmi più sofisticati, che riescono a stabilire se un numero non è primo anche senza determinare alcun divisore non banale.
Altri algoritmi di primalità piuttosto semplici, ma poco utili dal punto di vista pratico, sono il test che si può ricavare dal crivello di Eratostene e i test di Fermat e di Wilson, che si basano rispettivamente sul piccolo teorema di Fermat e sul teorema di Wilson.
Diversi altri algoritmi sono stati sviluppati nel corso del tempo: alcuni di essi si applicano solo a classi particolari di numeri, come ad esempio i test di Lucas-Lehmer e di Proth, che si applicano solo ai numeri di Mersenne e di Proth rispettivamente. Altri, come il test di Miller-Rabin, sono probabilistici, ovvero danno una risposta certa solo se affermano che il numero "non" è primo, mentre se si ottiene come risultato che il numero è primo, allora c'è solo un'alta probabilità che il numero effettivamente lo sia. I numeri che passano uno di questi test, pur senza essere primi, sono detti "pseudoprimi". La classe più famosa di pseudoprimi è quella dei numeri di Carmichael, che verificano il piccolo teorema di Fermat pur essendo composti.
Tra i test di primalità di uso generale il più usato attualmente è l'ECPP, basato sulle curve ellittiche; sebbene la sua complessità computazionale non sia nota, sperimentalmente si osserva che esso è un algoritmo polinomiale nel numero delle cifre di "n". Nel 2002, i tre matematici indiani Manindra Agrawal, Neeraj Kayal e Nitin Saxena hanno sviluppato l'algoritmo AKS, il primo test di primalità deterministico con complessità polinomiale, provando dunque che il problema di stabilire se un numero è primo o no sta nella classe di complessità P.
Un programma che ha lo scopo di individuare i fattori primi di un numero è detto "algoritmo di fattorizzazione"; gli algoritmi di questo tipo possono funzionare anche da test di primalità, ma sono quasi sempre più lenti da eseguire rispetto a programmi ideati solo per quest'ultimo scopo. Dopo il metodo di divisione per tentativi, i più antichi algoritmi di questo tipo sono il metodo di Fermat, che si basa sulle differenze tra il numero da fattorizzare "N" e alcuni quadrati, efficace in particolare quando "N" è il prodotto di due numeri primi vicini tra loro, e il metodo di Eulero, che si basa invece sulla rappresentazione di "N" come somma di due quadrati in due modi diversi.
Più recentemente, gli algoritmi per la fattorizzazione sono stati basati su una gran varietà di tecniche diverse, come le frazioni continue o le curve ellittiche, mentre altri, come ad esempio il crivello quadratico, sono basati su miglioramenti del metodo di Fermat. Altri ancora, come il metodo rho di Pollard, sono probabilistici, e non offrono la garanzia che, dato un numero non primo, ne trovino i divisori.
A oggi il più veloce algoritmo deterministico di impiego generale, ovvero senza necessità di numeri in forma particolare, è il "general number field sieve", che ha complessità esponenziale sul numero di cifre di "N"; è stato proposto un algoritmo che ha tempo di esecuzione polinomiale nel numero di cifre di "N" (algoritmo di Shor), ma esso richiede di essere eseguito su un computer quantistico, la cui simulazione su un normale calcolatore richiede un tempo esponenziale.
Proprio la difficoltà di fattorizzare grandi numeri ha portato allo sviluppo del primo metodo efficace di crittografia a chiave pubblica, l'RSA. In questo sistema crittografico, la persona che deve ricevere un messaggio cifrato genera una chiave formata da tre numeri: uno ("n") è il prodotto di due numeri primi di grandi dimensioni (generalmente si usano numeri di 1024 o 2048 bit), mentre gli altri due ("e" ed "f") sono l'uno l'inverso dell'altro modulo φ("n") (dove φ indica la funzione di Eulero). Uno tra questi ultimi due numeri deve essere tenuto segreto (e dunque prende il nome di "chiave privata"), mentre l'altro deve essere reso noto insieme al numero "n" (andando a formare la "chiave pubblica").
Dopo aver trasformato il messaggio in un numero "m" (secondo un codice stabilito in precedenza), la procedura di criptazione e decriptazione consiste nell'elevamento a potenza di "m" per il numero tra "e" ed "f" reso pubblico, prendendone poi il resto nella divisione per "n"; il teorema di Eulero garantisce che dopo quest'operazione si possa ritornare allo stesso numero di partenza conoscendo sia "e" sia "f".
È possibile, in teoria, ricavare la chiave privata dalle informazioni pubbliche: attualmente questo richiede la fattorizzazione del numero "n", rendendo quindi la trasmissione del messaggio sicura se i due primi scelti soddisfano alcune condizioni e sono "sufficientemente" grandi. Non è ancora noto se vi siano metodi efficienti per decriptare il messaggio che non prevedano l'attacco diretto alla fattorizzazione di "n", ma è stato mostrato che una cattiva scelta della chiave pubblica potrebbe rendere il sistema più vulnerabile ad attacchi di questo tipo.
Nel 1991 la RSA Security (l'azienda che ha sfruttato commercialmente l'RSA) ha pubblicato una lista di semiprimi, offrendo dei premi in denaro per la fattorizzazione di alcuni di essi, con lo scopo di provare la sicurezza del metodo e di incoraggiare la ricerca in questo ambito: l'iniziativa è stata chiamata RSA Factoring Challenge. Nel corso degli anni, diversi di questi numeri sono stati fattorizzati, mentre per altri il problema è ancora aperto; il concorso si è comunque concluso nel 2007.
Già da molti secoli la ricerca di numeri primi "grandi" ha destato l'interesse dei matematici; tuttavia questa ricerca ha assunto una particolare importanza negli ultimi decenni, a causa del bisogno di tali numeri che caratterizza algoritmi quali l'RSA.
Il metodo più efficace per ottenere numeri primi grandi risale al diciassettesimo secolo, quando Marin Mersenne congetturò che formula_61 sarebbe stato primo (quando "n" ≤ 257) solo per "n" uguale a 2, 3, 5, 7, 13, 19, 31, 67, 127 e 257. La verifica della primalità di tali numeri era molto al di sopra delle possibilità dell'epoca, e infatti soltanto nel Novecento si scoprì che la congettura era falsa e probabilmente fatta "alla cieca", in quanto Mersenne tralasciò tre casi (per "n" = 61, 89 e 107) e non si accorse che i numeri corrispondenti a "n" = 67 e "n" = 257 erano in realtà composti.
"M" (un numero di 39 cifre) fu dimostrato essere primo da Édouard Lucas nel 1876, e rimase il numero primo più grande conosciuto fino al 1951, quando vennero trovati (2+1)/17 (di 44 cifre) e, poco più tardi, 180 · (2 − 1) + 1 (di 79 cifre), quest'ultimo tramite un calcolatore elettronico. Da allora tutti i successivi primi più grandi sono stati scoperti con l'aiuto del computer: dal 1952 (quando lo SWAC dimostrò che "M" è primo) al 1996 essi sono stati trovati da supercomputer, e furono tutti primi di Mersenne (trovati usando il test di Lucas-Lehmer, un algoritmo specifico per questi numeri) con l'eccezione di 391581 · 2 − 1, che detenne il record tra il 1989 e il 1992.
In seguito, i quattordici nuovi numeri primi più grandi sono stati scoperti attraverso il GIMPS, un progetto di calcolo distribuito basato anch'esso sul test di Lucas-Lehmer. A oggi (dicembre 2018) il più grande numero primo, scoperto nel dicembre del 2018, è 2 − 1, composto da oltre 24 milioni di cifre decimali. I numeri primi noti più grandi sono numeri primi di Mersenne o altri numeri primi particolari, per i quali si dispone di un test molto efficiente in termini computazionali.
La Electronic Frontier Foundation ha offerto dei premi in denaro ai primi che riusciranno a trovare numeri primi di oltre un certo numero di cifre. I primi due di questi premi, di e dollari, sono stati assegnati nel 2000 e nel 2008 per il raggiungimento, rispettivamente, di un milione e di dieci milioni di cifre; il più alto premio attualmente in palio è di dollari, per l'arrivo al miliardo di cifre.
Il concetto di numero primo viene esteso anche in altri campi della matematica.
La definizione di numero primo può essere estesa a qualunque dominio d'integrità; vi sono due modi di estendere la definizione, in generale non equivalenti fra loro:
Un elemento primo è sempre irriducibile, ma non viceversa: tuttavia nell'anello degli interi le due definizioni sono equivalenti (come garantito dal lemma di Euclide), e più in generale sono equivalenti in tutti gli anelli a fattorizzazione unica.
Inoltre, dato un anello "A", un ideale "I" di "A" è detto "primo" se per ogni coppia "a","b" di elementi di A tali che "a"·"b" ∈ "I" almeno uno tra "a" e "b" appartiene a "I".
Questa definizione è molto vicina a quella degli ordinari numeri primi, tanto che nell'anello formula_62 gli ideali primi non nulli sono esattamente (2), (3), (5), ..., ovvero quelli generati dai numeri primi (più in generale, ciò avviene in ogni dominio ad ideali principali). Lo studio degli ideali primi è un punto centrale nella geometria algebrica e nella teoria algebrica dei numeri. Un'importante analogia tra numeri primi e ideali primi è dato dal fatto che nei domini di Dedekind per gli ideali vale l'analogo del teorema fondamentale dell'aritmetica.
Nella teoria dei gruppi, un ruolo simile a quello dei numeri primi è rivestito dai gruppi semplici. Si può dimostrare infatti che ogni gruppo finito "G" ammette una serie di composizione, cioè una serie del tipo
ove ogni "H" è un sottogruppo normale di "H" tale che il gruppo "H" / "H" (detto gruppo fattore della serie) sia un gruppo semplice. Il teorema di Jordan-Hölder assicura che tutte le serie di composizione per "G" hanno la stessa lunghezza "m" e gli stessi fattori di composizione, a meno di permutazioni e isomorfismi. È tuttavia da notare che gruppi diversi possono avere la stessa serie di composizione: ad esempio il gruppo ciclico formula_64 e il gruppo diedrale "D", per ogni primo "p", hanno entrambi la serie di composizione
corrispondente ai fattori formula_66 e formula_67.
In teoria dei nodi, un nodo primo è un nodo non banale che non può essere "scomposto" in due nodi più piccoli. In maniera più precisa, è un nodo che non può essere scritto come somma connessa di due nodi non banali.
Nel 1949 Horst Schubert dimostrò un teorema di fattorizzazione analogo al teorema fondamentale dell'aritmetica, che asserisce che ogni nodo è ottenibile in modo unico come somma connessa di alcuni nodi primi. Per questo motivo, i nodi primi hanno un ruolo centrale nella teoria dei nodi: una loro classificazione è stato da sempre il tema centrale della teoria fin dalla fine del XIX secolo.
In natura compaiono molti numeri, ed è quindi inevitabile che alcuni di essi siano primi. Sono tuttavia relativamente pochi gli esempi di numeri la cui presenza in natura si spieghi con la loro primalità.
Per la maggior parte, le stelle marine hanno 5 braccia, e 5 è un numero primo; tuttavia non è nota alcuna connessione tra questo numero di braccia e la primalità di 5. Il motivo della simmetria a 5 braccia che caratterizza la maggior parte delle stelle marine e molti altri echinodermi rimane un mistero.
In entomologia si trova uno dei casi in cui si suppone che un numero compaia proprio in quanto primo. Si è infatti notato che alcune specie di cicale del genere "Magicicada", che trascorrono la maggior parte delle loro vite come larve, emergono come pupe solo a intervalli di 13 o 17 anni, dopo di che si riproducono e infine muoiono dopo poche settimane. Si pensa che il motivo per cui l'intervallo di tempo è un numero primo di anni sia la difficoltà per un predatore di evolversi specializzandosi nella predazione delle "Magicicada": se infatti questi insetti apparissero dopo un numero non primo di anni, allora tutti i predatori il cui ciclo vitale fosse un divisore di quel numero avrebbero una elevata probabilità di trovare le "Magicicada". Sebbene esile, questo vantaggio evolutivo sembra essere stato sufficiente a selezionare cicale il cui periodo è di 13 o 17 anni.
I numeri primi hanno influenzato molti artisti e scrittori. Il compositore francese Olivier Messiaen era ossessionato da tali numeri e li utilizzò per creare musica non metrica: in opere come "La Nativité du Seigneur" (1935) o "Quatre études de rythme" (1949-50) impiegò simultaneamente motivi la cui lunghezza è un numero primo per creare ritmi imprevedibili. Secondo Messiaen questo modo di comporre era "ispirato dai movimenti dalla natura, movimenti di durate libere e disuguali". Anche nel movimento di apertura di un'altra composizione, "Quatuor pour la fin du temps", Messiaen utilizzò i numeri primi. Con l'obiettivo di dare l'idea dell'eternità, accostò infatti un tema di 17 note a un tema di 29 note. Essendo primi entrambi i numeri, i temi si ripetono insieme solo dopo 17 · 29 = 493 note. La stessa idea è stata utilizzata da Jem Finer che ha ideato un'installazione sonora che sino al 31 dicembre 2999 suonerà motivi sempre diversi.
I numeri primi svolgono un ruolo anche in alcuni libri. Ad esempio, nel romanzo di fantascienza "Contact" di Carl Sagan (così come nella sua versione cinematografica), i numeri primi vengono utilizzati dagli alieni per comunicare; un caso reale di uso dei primi come mezzo di comunicazione è presente nel saggio "L'uomo che scambiò sua moglie per un cappello", del neurologo Oliver Sacks, dove sono descritti due gemelli autistici che per parlarsi si scambiano primi molto elevati. Vi sono riferimenti ai numeri primi anche nel romanzo di Mark Haddon "Lo strano caso del cane ucciso a mezzanotte", in cui la numerazione dei capitoli segue la successione dei primi, e nel romanzo di Paolo Giordano "La solitudine dei numeri primi", vincitore del premio Strega nel 2008. Il romanzo "Lo zio Petros e la congettura di Goldbach" di Apostolos Doxiadis (pubblicato in italiano nel 2001) è stato trasposto per le scene da Angelo Savelli.
Molti film riflettono la fascinazione popolare verso i misteri dei numeri primi e della crittografia, come ad esempio "Cube - Il cubo", "I signori della truffa", "L'amore ha due facce", "A Beautiful Mind" e "La solitudine dei numeri primi".
</text>
</doc>
<doc id="2218360" url="https://it.wikipedia.org/wiki?curid=2218360">
<title>Onda sinusoidale</title>
<text>
In fisica, un'onda sinusoidale è un'onda descritta matematicamente dalla funzione seno.
Una sinusoide o curva sinusoidale è la curva rappresentata dal grafico del seno. Una sinusoide è analoga alla curva relativa alla funzione coseno, detta cosinusoide, sfasata di formula_1.
Un'onda sinusoidale è un'onda dove la variabile formula_2 è una funzione della forma:
dove formula_4 è l'ampiezza, mentre:
è la pulsazione (o velocità angolare, indica quanti periodi ci sono in un intervallo di formula_6). Inoltre:
è la frequenza, che indica quante volte in un'unità di tempo la funzione si ripete, e:
è il periodo, con formula_9 oppure formula_10 la fase.
Il grafico di una tale classe di funzioni è compreso tra le rette formula_11 e formula_12. 
Poiché si tratta di una funzione periodica, detto formula_13 il periodo si ha:
Usando la formula di Eulero, un'onda sinusoidale può essere rappresentata come la parte reale della funzione: 
dove formula_16 è il vettore d'onda, che identifica la direzione di propagazione dell'onda al posto della velocità di propagazione. Il suo modulo è chiamato "pulsazione spaziale", ed è legato alla lunghezza d'onda dalla relazione:
Lo scalare formula_18 è l'ampiezza dell'onda, e rappresenta il massimo valore della grandezza rappresentativa dell'onda in un periodo. Il termine formula_19 rappresenta la fase iniziale dell'onda.
Le onde sinusoidali sono una soluzione particolare dell'equazione delle onde. L'onda è una funzione dello spazio e tempo, per cui un'onda monodimensionale associa ad ogni posizione spaziale formula_2 e ad ogni tempo formula_21 un'ampiezza di oscillazione formula_22 attorno alla posizione di equilibrio:
Sono possibili perciò due punti di vista: 
In entrambi i casi si può partire dalla dipendenza co-sinusoidale delle variabili nel moto armonico, ricavate considerando quest'ultimo come una opportuna proiezione di un moto circolare uniforme: 
dove formula_33 è l'ampiezza dell'oscillazione e formula_19 è la fase iniziale. Attribuendo a formula_19 un valore di 90 gradi si può passare da una forma in coseno ad una in seno, quindi le espressioni sono equivalenti. L'espressione è in formula_22 per attuare la "visualizzazione" dell'oscillazione lungo l'asse verticale del sistema coordinato.
Fissando la variabile formula_2 si ha:
dove formula_39 è il periodo dell'onda. La fase iniziale è nulla, e se la perturbazione sul mezzo si propaga dall'inizio muovendosi con velocità di fase formula_40 allora essa raggiungerà un altro punto (a destra dell'origine) ad una certa distanza formula_41 dopo un tempo:
Ciò significa che il punto alla coordinata formula_41 avrà, al tempo formula_21, uno spostamento verticale uguale a quello che aveva il punto iniziale t1 secondi prima. La propagazione è quindi descritta dall'espressione:
Raccogliendo formula_6 si può passare ad una forma più comune che talvolta si trova sui testi:
Se si chiama numero d'onda formula_48 la quantità formula_49, e se la pulsazione è formula_50, il rapporto già noto dallo studio del moto circolare formula_51 consente di pervenire formalmente all"'equazione delle onde armoniche":
Se all'espressione in coseno iniziale si fosse aggiunta una fase di 90° si sarebbe ottenuta un'espressione in seno negativo poiché formula_53, e questo avrebbe portato a un'espressione sinusoidale con i segni interni invertiti, cioè formula_54, che talvolta viene presentata sui testi.
Considerando il secondo caso dell'elenco sopra, ad un tempo fissato:
Si è espresso il tempo come formula_56, sostituendo ed usando la relazione fondamentale delle onde formula_57 (la lunghezza d'onda è lo spazio percorso da un'onda con velocità di fase formula_58 in un periodo formula_39): in ogni caso, quel che conta è che si ottiene una cosinusoide di periodo spaziale formula_60 dipendente solo dalla posizione formula_2. Se l'impulso si sta muovendo lungo l'asse delle ascisse, inducendo una oscillazione sulle ordinate, ad un certo istante successivo a quello fissato il punto alla certa coordinata formula_2 avrà una elevazione uguale a quella del punto formula_63 da cui l'impulso è partito formula_21 secondi prima. L'onda si propaga quindi (verso destra) con un profilo dato da:
mentre si sarebbe dovuta considerare una espressione in parentesi tonda del tipo formula_66 se si fosse voluta descrivere la propagazione verso sinistra. Esprimendo formula_67 e sostituendo, si ha l'espressione:
che considerando la relazione goniometrica formula_69 è analoga a quella ottenuta in precedenza (perché si cambiano i segni dell'argomento).
 
</text>
</doc>
<doc id="848371" url="https://it.wikipedia.org/wiki?curid=848371">
<title>Sistema di coordinate polari</title>
<text>
In matematica, il sistema di coordinate polari è un sistema di coordinate bidimensionale nel quale ogni punto del piano è identificato da un angolo e da una distanza da un punto fisso detto polo.
Il sistema di coordinate polari è utile specialmente nei casi in cui le relazioni tra due punti possono essere espresse più facilmente in termini di angoli e di distanza; nel più familiare sistema di coordinate cartesiane, o sistema di coordinate rettangolari, tale relazione può essere espressa solamente tramite le funzioni trigonometriche.
Siccome il sistema di coordinate è bidimensionale, ogni punto è determinato da due coordinate polari: la coordinata radiale e quella angolare. La prima, di solito identificata con la lettera formula_1, denota la distanza del punto da un punto fisso detto polo (equivalente all'origine del sistema cartesiano). La coordinata angolare, solitamente denotata con la lettera greca θ, è anche detta "angolo azimutale" ed identifica l'angolo che la semiretta a 0° deve spazzare in senso antiorario per andare a sovrapporsi a quella che congiunge il punto al polo.
I concetti di angolo e raggio erano già utilizzati dai popoli antichi del I millennio a.C. L'astronomo greco Ipparco di Nicea (190-120 a.C.) costruì una tabella delle funzioni delle corde, che fornivano la lunghezza della corda sottesa da ogni angolo; esistono anche riferimenti all'utilizzo delle coordinate polari per stabilire le posizioni delle stelle.
Nel saggio "Sulle Spirali", Archimede descrive la sua famosa spirale, una funzione il cui raggio dipende dall'angolo. L'opera dei greci, comunque, non si estese a un sistema di coordinate polari universalmente accettato.
Verso la metà del XVII secolo, Gregorio di San Vincenzo e Bonaventura Cavalieri introdussero, indipendentemente l'uno dall'altro, il concetto di coordinate polari. Il fiammingo Gregorio di San Vincenzo espose questo concetto nell'opera "Opus geometricus" del 1647, ma si ritiene che ne fosse a conoscenza dal 1625. Cavalieri pubblicò il suo lavoro nel 1635, ma nel 1653 fu stampata un'edizione più corretta. Cavalieri per primo utilizzò le coordinate polari per risolvere i problemi relativi al calcolo dell'area sottesa da una spirale di Archimede. Pascal, in seguito, utilizzò le coordinate polari per calcolare la lunghezza degli archi parabolici.
Nel "Metodo per il calcolo differenziale" (scritto nel 1671 e pubblicato nel 1736), Isaac Newton esaminò le trasformazioni che avvenivano tra le coordinate polari e quelle che sussistevano tra altri nove sistemi di coordinate.
Nel giornale "Acta Eruditorum" del 1691, Jacob Bernoulli utilizzò il sistema con un punto e una linea, che chiamò rispettivamente "polo" e "asse polare". Le coordinate erano specificate dalla distanza dal polo e dall'angolo formato con l'asse polare; l'opera di Bernoulli si estese al calcolo del raggio di curvatura delle curve, espresse in queste coordinate.
Il termine "coordinate polari" è stato attribuito a Gregorio Fontana, e fu utilizzato dagli scrittori italiani del XVIII secolo. Alexis Clairaut fu il primo a pensare le coordinate polari in tre dimensioni, ed Eulero fu il primo a svilupparle effettivamente.
Ogni punto del sistema di coordinate polari può essere descritto con le due coordinate polari, di solito chiamate formula_1 (coordinata radiale) e θ (coordinata angolare). La coordinata formula_1 rappresenta la distanza radiale dal polo, mentre la θ è l'angolo in senso antiorario da percorrere partendo da 0° (asse di riferimento).
Ad esempio, le coordinate polari (3, 60°) devono essere disegnate con un punto posto a tre unità di distanza dal polo e in modo tale che la semiretta congiungente il punto al polo formi un angolo di 60° con l'asse di riferimento. È ovvio che il punto (3, -300°) coinciderà con il punto precedente, perché l'angolo -300° corrisponde esattamente all'angolo 60°; per la stessa proprietà degli angoli, anche tutti i punti (3, 60° + "K"×360°), con K numero intero, coincideranno con il primo punto, perché sommando o sottraendo angoli giri dallo stesso angolo, il risultato non cambia. In generale, tutti i punti identificati dalle coordinate ("r", θ + "K"×360°) corrispondono allo stesso punto disegnato nel piano polare.
Ciò illustra un importante aspetto del sistema di coordinate polari, che non è presente in quello a coordinate cartesiane: ogni singolo punto può essere espresso con un numero infinito di coordinate differenti, ognuna delle quali è sfasata rispetto alle altre di un numero intero di angoli giri.
Le coordinate arbitrarie (0, θ) sono convenzionalmente utilizzate per rappresentare il polo, senza particolare interesse per il valore di θ, infatti ogni punto distante 0 dal polo coincide con il polo qualunque sia il suo angolo.
In notazione polare, gli angoli sono generalmente definiti in gradi o radianti, utilizzando la convenzione per cui 2π rad = 360°. Le applicazioni per la navigazione utilizzano maggiormente i gradi, mentre le applicazioni fisiche (specialmente in meccanica rotazionale) e quasi tutta la letteratura matematica sul calcolo utilizzano le misure in radianti.
Le due coordinate polari formula_1 e formula_5 possono essere convertite nelle coordinate cartesiane formula_6 e formula_7 utilizzando le formule delle funzioni trigonometriche seno e coseno:
mentre le due coordinate cartesiane formula_6 e formula_7 possono essere convertite nella coordinata polare formula_1 applicando il teorema di Pitagora:
Per determinare invece la coordinata angolare formula_5, bisogna considerare i due seguenti casi.
Per ottenere formula_5 nell'intervallo formula_21, si usano invece le seguenti:
Un metodo alternativo di ricavare formula_5 in termini di formula_6 e formula_7 è la seguente. Sfruttando un'identità trigonometrica
quindi
Il vantaggio di questo approccio è che vale questa unica formula per formula_5 su tutto formula_34.
Moltissimi software (tra cui Microsoft Excel) e linguaggi di programmazione (tra cui Java) dispongono della funzione "atan2" per passare dalle coordinate cartesiane a quelle polari.
L'equazione che definisce una curva algebrica espressa in coordinate polari è conosciuta come "equazione polare". In molti casi, tale equazione può essere semplicemente espressa definendo formula_1 come funzione di formula_5. La curva risultante consiste quindi dei punti della funzione formula_37 e può essere considerata come il grafico della funzione polare di formula_1.
Dall'equazione della funzione polare di formula_1 si possono dedurre diverse forme di simmetria. Se formula_40, la curva sarà simmetrica rispetto all'asse orizzontale, mentre se formula_41 sarà simmetrica rispetto a quello verticale, e se formula_42, costituirà una simmetria rotazionale di formula_43 in senso antiorario.
A causa della natura circolare del sistema di coordinate polari, molte curve possono essere descritte da una equazione polare piuttosto semplice, mentre la loro espressione cartesiana sarebbe più complicata. Tra le curve più conosciute di questa specie ci sono la rodonea, la spirale di Archimede, la lemniscata, il limaçon, e la cardioide.
L'equazione generale della circonferenza con centro in (formula_1, φ) e raggio formula_45 è
Questa formula può essere semplificata in vari modi, per adattarla a casi più specifici, come l'equazione 
per una circonferenza con centro nel polo e raggio formula_45.
Le rette radiali (che attraversano il polo) sono rappresentate dall'equazione 
in cui φ è l'angolo formato dalla retta; cioè, φ = arctan formula_50 dove formula_50 è l'inclinazione della retta nel sistema di coordinate cartesiane. La linea non radiale che attraversa la retta radiale θ = φ perpendicolarmente nel punto (formula_1, φ) ha equazione 
Una rodonea è una celebre curva matematica che appare come un fiore con petali, e che si può esprimere semplicemente con un'equazione polare, data da
o
Posto formula_56 come numero intero, l'equazione produrrà con formula_56 dispari una rodonea di tipo formula_56, con formula_56 pari di tipo formula_60. Se formula_56 è invece un numero irrazionale, la curva formerà un disco. È da notare che queste equazioni non definiscono il numero dei petali della rosa; la variabile formula_45 rappresenta solamente la lunghezza dei petali.
La spirale di Archimede è una famosa spirale che fu scoperta da Archimede, e che può essere espressa semplicemente con una equazione polare, della forma
Modificando il parametro formula_45, la spirale ruoterà, mentre formula_65 controlla la distanza tra i bracci, che per una data spirale è sempre costante. La spirale di Archimede presenta due bracci, uno per θ &gt; 0 e l'altro per θ &lt; 0, ed entrambi si congiungono nel polo. Prendendo l'immagine speculare di un braccio lungo la retta 90°/270°, i due bracci si sovrappongono. Questa curva è notevole anche per essere stata una delle prime curve, dopo le sezioni coniche, ad essere descritta in un trattato matematico, e per essere stata il primo esempio di una curva che è rappresentata dalle coordinate polari meglio che dalle cartesiane.
Una sezione conica con un fuoco sul polo e l'altro coincidente con un altro punto dell'asse a 0° (in modo che l'asse maggiore della conica possa giacere sull'asse polare) è data dall'equazione
in cui "e" è l'eccentricità della curva e formula_67 è la perpendicolare al semiasse maggiore della curva. Se "e" &gt; 1, questa equazione definisce un'iperbole, se "e" = 1, definisce una parabola e se "e" &lt; 1, definisce un'ellisse. Il caso particolare in cui "e" = 0, riduce l'ellisse a una circonferenza di raggio formula_67.
Ogni numero complesso può essere rappresentato come un punto del piano complesso, e può quindi essere espresso sia in coordinate cartesiane (chiamata "forma rettangolare") o nelle coordinate polari del punto (chiamata "forma polare"). Il numero complesso formula_69 si rappresenta in forma rettangolare come 
in cui formula_71 è l'unità immaginaria, o può alternativamente essere scritto in forma polare come 
e da qui come
in cui formula_74 è il numero di Nepero. Le due formule sono equivalenti per quanto stabilito dalla formula di Eulero. Per convertire la forma rettangolare a polare e viceversa, si possono applicare le formule sopra citate.
Per le operazioni di moltiplicazione, divisione e esponenziale di numeri complessi, è in generale più semplice operare con i numeri complessi espressi in forma polare, piuttosto che in forma rettangolare. Infatti, per le regole degli esponenziali:
Sfruttando la forma polare di un numero complesso si può arrivare alla formula della traslazione in coordinate polari. Presi due punti nelle coordinate polari formula_78 e formula_79, espressi con la formula di Eulero sono
Se consideriamo il punto formula_81 come punto traslato e formula_82 come origine del nuovo asse, sia poi formula_83 il punto non traslato, allora la traslazione è
Il modulo del numero complesso formula_85 è un numero reale positivo definito come
Considerando ora un'equazione polare
Per applicare una traslazione alla funzione possiamo sostituire formula_88 con la formula sopra, quindi
E l'equazione diventa
Se formula_91 è una funzione costante allora la traslazione rappresenta una circonferenza in quanto l'equazione
rappresenta una circonferenza con centro l'origine è raggio il valore della costante. Nota che in termini geometrici la traslazione di un punto è la distanza tra il punto e l'origine del nuovo asse, come è ben visibile dalla formula stessa. Per convertire la curva traslata da coordinate polari a cartesiane dobbiamo prelevare il punto sulla curva in coordinate polari e trasformarlo in coordinate cartesiane, si ottiene semplicemente lo stesso sistema visto sopra, basta infatti prelevare il punto formula_81 che è componente del punto formula_83.
Il calcolo infinitesimale può essere applicato alle equazioni espresse in coordinate polari. La coordinata angolare formula_5, in questa sezione, viene espressa in radianti, la scelta convenzionale nel calcolo infinitesimale.
Esistono le formule seguenti:
Oppure le inverse:
Per trovare la pendenza cartesiana della tangente alla curva polare formula_100 in un dato punto, la curva viene per prima cosa espressa con un sistema di equazioni parametriche.
Derivando entrambe le equazioni rispetto a formula_5
Effettuando il rapporto tra la seconda e la prima equazione, si ottiene la pendenza della retta tangente alla curva nel punto ("θ", "r"(θ)):
Se "R" indica la regione dello spazio racchiusa dalla curva "r"(θ) e dalle rette θ = "a" e θ = "b", in cui 0 &lt; "b" - "a" &lt; 2π, allora l'area di "R" è espressa da
Si può giungere a questo risultato come segue: per prima cosa, l'intervallo ["a", "b"] viene diviso in "n" sottointervalli, in cui "n" è un numero intero positivo arbitrario. Chiamata Δθ l'ampiezza di ogni sottointervallo, sussiste la relazione che Δθ è pari a "b" − "a" (l'ampiezza totale dell'intervallo), divisa per "n", numero dei subintervalli. Per ogni sottointervallo, "i" = 1, 2, …, "n", si chiama θ il suo punto medio, e si costruisce un settore circolare con centro nel polo, raggio "r"(θ), e angolo Δθ. L'area di ogni settore costruito è perciò pari a formula_108. L'area totale di tutti i settori sarà pertanto uguale a
Quando cresce il numero dei sottointervallo formula_110, l'approssimazione dell'area continua a migliorare. Al limite, per formula_111, la somma giunge ad essere la somma di Riemann per l'integrale precedente.
La lunghezza della linea espressa da una funzione polare, viene ottenuta per integrazione di segmenti infinitesimi lungo la curva "r"("θ") stessa. Usiamo "L" per denotare la lunghezza della linea a partire dal punto iniziale "A" fino al punto finale "B", dove questi punti corrispondono a: "formula_112 = a" , "formula_113 = b".
La lunghezza "L" della linea è allora data dal seguente integrale:
Utilizzando le coordinate cartesiane, un elemento infinitesimo di area può essere calcolato come formula_116. La regola della sostituzione per integrali multipli stabilisce che, passando ad altre coordinate, bisogna considerare il valore assoluto del determinante della matrice jacobiana:
formula_117
Pertanto, un elemento di area in coordinate polari può essere scritto come
Ora, una funzione data in coordinate polari, può essere integrata come segue:
Qui formula_120 è la stessa regione ricavata sopra, e corrisponde all'area racchiusa dalla curva formula_100 e dalle rette formula_122 e formula_123.
La formula per il calcolo di formula_120 è stata ricavata assumendo formula_91 identicamente uguale a formula_126. Un'applicazione sorprendente di questo risultato riguarda la possibilità di calcolare l'integrale di Gauss
Le coordinate polari si possono applicare anche al calcolo vettoriale e, in particolare, allo studio della cinematica dei moti piani. 
Sia formula_128 la posizione del vettore 
espresso in un sistema di coordinate cartesiane di versori di base formula_130 e formula_131,
Il modulo formula_1 e l'angolo polare formula_5 dipendono dal tempo formula_134 e descrivono il movimento di un corpo nel piano.
Siano formula_135 il vettore unità (versore) nella direzione di formula_128 e 
formula_137 il versore dell'angolo di formula_128.
Le derivate prima e seconda del vettore posizione
sono, rispettivamente, il vettore velocità
ed il vettore accelerazione 
del moto. Si ricorda che le derivate dei versori sono:
Il sistema di coordinate polari si può estendere anche alle tre dimensioni, con due differenti sistemi di coordinate, quelle cilindriche e quelle sferiche, delle quali entrambe richiedono coordinate polari planari o bidimensionali come base. In sintesi, il sistema di coordinate cilindriche estende le coordinate polari aggiungendo un'altra coordinata della distanza, mentre le coordinate sferiche aggiungono un'altra coordinata angolare.
Il "sistema di coordinate cilindriche" è un sistema di coordinate che estende il sistema bidimensionale polare aggiungendo una terza coordinata, che misura l'altezza di un punto dal piano base, in modo simile a quello in cui si introduce la terza dimensione nel piano cartesiano. La terza coordinata è spesso chiamata formula_144, e l'intera terna è quindi formula_145.
Le tre coordinate cilindriche possono essere convertite in coordinate cartesiane con le formule
Le coordinate polari possono essere estese in tre dimensioni anche utilizzando le coordinate formula_147, in cui formula_148 è la distanza dal polo, formula_5 è l'angolo formato con l'asse formula_69 e formula_151 è l'angolo formato dalla proiezione sul piano formula_152, con l'asse formula_6. Questo sistema di coordinate, chiamato "sistema di coordinate sferiche", è simile al sistema della latitudine e longitudine utilizzato per la Terra, con la latitudine δ che è il complementare di formula_5, se formula_69 è l'asse di rotazione terrestre, determinato dalla relazione δ = 90° − θ, e la longitudine est "l" = φ (se φ compreso fra 0° e 180°) ovvero la longitudine ovest "l" = - φ (se φ compreso fra -180° e 0°), se il semipiano formula_156 con formula_157 contiene il meridiano di Greenwich. 
Le tre coordinate cartesiane di un punto si ottengono dalle tre coordinate sferiche di quel punto con le formule:
Le coordinate polari sono bidimensionali, e pertanto possono essere utilizzate soltanto ove le posizioni dei punti giacciano su un singolo piano bidimensionale. Il loro utilizzo è appropriato in ogni contesto in cui il fenomeno considerato sia legato alla direzione e alla distanza da un certo punto; gli esempi sopra mostrati mostrano come elementari equazioni polari siano sufficienti a definire curvecome la spirale di Archimedela cui equazione in coordinate cartesiane sarebbe molto più intricata. Inoltre, molti sistemi fisicicome quelli riguardanti i corpi che si muovono intorno a un punto centrale e con fenomeni originanti da un punto centralepossono essere trattati in modo più semplice e più intuitivo utilizzando le coordinate polari. La motivazione iniziale per l'introduzione del sistema polare fu lo studio del moto circolare e del moto orbitale.
Le coordinate polari sono spesso utilizzate nella navigazione, quando la destinazione o la direzione del viaggio possono essere forniti attraverso un angolo e una distanza dal luogo di arrivo. Ad esempio, gli aeroplani utilizzano una versione leggermente modificata delle coordinate polari per la navigazione. In questo sistema, gli angoli sono considerati in senso orario. L'angolo a 360° (0°) corrisponde al nord magnetico, mentre gli angoli a 90°, 180° e 270° corrispondono all'est magnetico, sud magnetico e ovest magnetico rispettivamente. Pertanto, un aeroplano che viaggia a 5 miglia nautiche a est starà viaggiando a 5 unità verso l'angolo di 90°.
I sistemi che mostrano una simmetria radiale sono l'ambiente ideale per l'applicazione delle coordinate polari, con il punto centrale che agisce da polo. Sistemi con una forza centrale sono anche potenziali candidati all'utilizzo delle coordinate polari: questi sistemi includono i campi gravitazionali, che obbediscono alla legge dell'inverso del quadrato, come anche i sistemi con una sorgente puntiforme, come le radio antenne.
</text>
</doc>
<doc id="1012521" url="https://it.wikipedia.org/wiki?curid=1012521">
<title>Quadrante (geometria analitica)</title>
<text>
Un quadrante è la porzione di piano cartesiano individuata da ciascuna coppia di semiassi. 
A seconda dei segni delle coordinate, il punto "P"=("x","y") è posto:
I quadranti quindi si contano in senso antiorario, a partire dal quadrante in alto a destra, per cui:
Determinare per quali valori a il punto formula_1 appartiene al 1° quadrante.
Il punto formula_2 appartiene al 1° quadrante se ha entrambe le coordinate positive, pertanto deve essere soddisfatto il sistema:
Quindi il punto formula_2 appartiene al 1° quadrante se formula_5
</text>
</doc>
<doc id="520796" url="https://it.wikipedia.org/wiki?curid=520796">
<title>Numero</title>
<text>
In matematica, un numero è un modo di esprimere una quantità, oppure la posizione in un elenco di elementi, oppure il rapporto tra grandezze dello stesso tipo. Il concetto di numero nasce per la necessità del conteggio, come astrazione del concetto di quantità, realizzato attraverso una corrispondenza biunivoca tra elementi di due insiemi distinti.
Si definisce operazione numerica una procedura che, a partire da uno o più numeri, genera un altro numero. Le operazioni numeriche fondamentali (dette anche "operazioni aritmetiche") sono: l'addizione, la sottrazione, la moltiplicazione e la divisione. Lo studio delle proprietà di queste operazioni è parte dell'algebra elementare.
Un insieme di numeri è frequentemente espresso attraverso il concetto di campo.
Un numero che esprime la dimensione di un insieme di elementi, così come un numero che identifica la posizione in una successione di oggetti, è detto numero naturale. La necessità di esprimere una grandezza in relazione ad un'altra grandezza ha reso necessaria l'introduzione di altre classi di numeri, come i numeri razionali ed i numeri reali. L'esigenza di rappresentare il numero ottenuto attraverso un'operazione matematica, infine, ha giustificato l'utilizzo di ulteriori classi di numeri come, ad esempio, i numeri algebrici.
Durante la storia della matematica sono stati definiti diversi insiemi numerici. Tra questi i numeri naturali, che sono:
I numeri naturali (il cui insieme è indicato per convenzione con il simbolo formula_1) sono usati per contare e per ordinare. La presenza dello zero fra i numeri naturali dipende dalla convenzione scelta. Lo zero è previsto dagli assiomi di Peano.
L'insieme dei numeri naturali costituisce una successione ordinata. Ogni numero è descritto tramite una o più cifre.
Se a partire dall'insieme dei numeri naturali si introduce il segno (e lo zero se non incluso), distinguendo tra "numeri positivi" e "numeri negativi", si ottengono i numeri interi relativi (o semplicemente "interi"), il cui insieme è indicato per convenzione con il simbolo formula_4. I numeri interi relativi sono:
Se a partire dai numeri interi si costruiscono numeri dati dal rapporto tra di loro, si ottengono i numeri razionali, i quali sono quindi esprimibili tramite una frazione ("ratio" in latino, da cui il nome di numeri "razionali"). Ad esempio:
L'insieme di tutti i numeri razionali è per convenzione indicato col simbolo formula_7.
I numeri algebrici sono numeri ottenibili come radici di equazioni algebriche a coefficienti interi. I numeri razionali sono algebrici. Il viceversa non è in generale vero: ad esempio:
sono numeri algebrici che non possono essere descritti tramite una frazione, non sono razionali.
Un numero non algebrico è detto "trascendente". Ad esempio, formula_11 (pi greco) ed formula_12 sono trascendenti: non è possibile ottenere formula_13 come radice di un'equazione polinomiale a coefficienti interi.
L'insieme dei numeri reali comprende i numeri esprimibili, con o senza la virgola, tramite il sistema numerico decimale. I numeri reali comprendono i numeri elencati precedentemente. In particolare i numeri reali si dividono in razionali e irrazionali, oppure in algebrici e trascendenti.
L'insieme dei numeri reali è simboleggiato per convenzione con formula_14.
L'insieme dei numeri reali non fornisce tutte le soluzioni delle equazioni algebriche. Per esempio, l'equazione:
non ha soluzioni nel campo dei numeri reali, perché il quadrato di un numero reale è sempre positivo o nullo. Per risolvere questo problema, è stata introdotta l'unità immaginaria formula_18 . Essa è così definita:
Tale numero non appartiene all'insieme dei numeri reali, esso appartiene all'insieme dei numeri complessi. In generale, un numero complesso è un'espressione del tipo:
dove formula_21 è l'unità immaginaria e formula_22 sono numeri reali. L'insieme dei numeri complessi è indicato per convenzione con il simbolo formula_16.
Gli insiemi numerici sono ciascuno sottoinsieme dell'altro, secondo quest'ordine (dove il simbolo formula_24 indica l'inclusione stretta):
Per particolari scopi, formula_16, può essere ulteriormente esteso, al prezzo, però, di perdere alcune proprietà e, di conseguenza, subire un declassamento come struttura algebrica.
I numeri complessi sono stati estesi e hanno dato luogo ai quaternioni. L'operazione di moltiplicazione dei quaternioni non gode della proprietà commutativa.
Gli ottonioni estendono i quaternioni. Questa volta si perde la proprietà associativa. Gli unici sistemi associativi con dimensione finita, oltre ai complessi, sono i quaternioni.
Estendendo gli ottonioni si ottengono i sedenioni, che perdono la proprietà dell'algebra alternativa, ma mantengono comunque la proprietà associativa della potenza.
I numeri vanno distinti per il tramite dei nomi, dato che i numeri sono dei concetti e anche se i nomi utilizzati nelle varie lingue variano i concetti rimangono gli stessi. La notazione di numero come serie di cifre è definita dai sistemi di numerazione. I popoli spesso associano a dei numeri utilizzati di frequente dei nomi particolari, oltre a quelli che vengono assegnati dal sistema di numerazione, spesso questi nomi sono utilizzati in contesti specifici, un classico esempio è la dozzina.
Gli ultimi sviluppi della teoria dei numeri hanno condotto ai numeri iperreali ed ai numeri surreali, che estendono i numeri reali dai numeri infinitesimi ai numeri infinitamente grandi attraverso degli inserimenti. Mentre i numeri reali sono infinitamente prolungabili alla destra del punto decimale, si può anche provare a espandere i numeri a sinistra in modo infinito, ciò conduce ai numeri p-adici. Per gestire insiemi infiniti, i numeri naturali sono stati generalizzati nei numeri ordinali e nei numeri cardinali. Il primo insieme viene utilizzato per definire l'ordine di inserimento degli insieme il secondo definisce il formato di inserimento. Nel caso di insiemi finiti si equivalgono.
Le operazioni aritmetiche sui numeri sono addizione, sottrazione, moltiplicazione e divisione. Queste operazioni sono state generalizzate in una branca dell'algebra chiamata algebra astratta. Essa contiene i concetti di gruppo, anello e campo.
In molte culture la rappresentazione grafica dei numeri è assai simile. I numeri "uno", "due" e "tre" degli antichi romani erano espressi come I, II, III (numeri romani). I cinesi usavano una notazione analoga, con le cifre in orizzontale, o in verticale, ma al contrario dei romani utilizzavano un sistema posizionale, simile al nostro attuale, con le cifre da 0 a 9. I numeri, detti "tsu" o "hêng", cambiavano orientamento a seconda della posizione: | = | era 121, - || - ◦ era 1210. Gli tsu erano verticali, gli hêng orizzontali, i numeri sopra al cinque avevano una bacchetta disposta perpendicolarmente alle altre. Il sistema era impiegato con le "bacchette da calcolo", che i cinesi manovravano a velocità tali da stupire i primi missionari nestoriani.
Tuttavia, non c'era un segno univoco per definire il quattro tra i romani, mentre per i cinesi era ||||. I romani usavano una notazione a sottrazione: esprimevano il quattro con una V preceduta da una I. La V indicava il numero cinque, il simbolo I anteposto indicava che andava sottratto (cinque meno uno = quattro). Nell'assegnare un simbolo particolare al cinque c'era un motivo antropomorfico - la mano ha cinque dita - ma vi era anche una motivazione che coinvolge il cervello umano. Gli psicologi hanno dimostrato che il nostro cervello ha difficoltà a distinguere più di cinque simboli simili vicini: si provi a dire se è più grande ||||||||| o ||||||||||; più semplice se sono scritti come IX e X.
Il sistema adottato in Europa è il sistema di numerazione decimale, detto anche numerazione araba. In realtà proviene dall'India, e molto probabilmente deriva dai numeri corsivi egiziani, i numeri copti. La cifra 1 è molto simile al simbolo romano, 2 e 3 sono varianti dello stesso simbolo che consentono di scrivere i numeri senza alzare la penna e quindi consentono una scrittura rapida, ma conservano l'idea della linea doppia o tripla orizzontale. Col simbolo 4 la corrispondenza si perde.
</text>
</doc>
<doc id="3085527" url="https://it.wikipedia.org/wiki?curid=3085527">
<title>Risoluzione di un'equazione</title>
<text>
In matematica, per risolvere un'equazione si intende la ricerca degli elementi (numeri, funzioni, insieme, ecc.) che soddisfino la rispettiva equazione (due espressioni unite da un'uguaglianza). Queste espressioni contengono una o più incognite, che sono variabili libere per le quali sono cercati i valori che fanno sì che la condizione espressa dall'equazione sia soddisfatta. Per essere precisi, di solito si intende che questi valori non sono necessariamente valori reali, ma, in realtà, spesso sono espressioni matematiche. Una soluzione dell'equazione è un'assegnazione di espressioni alle incognite che soddisfi l'equazione, in altre parole, quando questi risultati vengono sostituiti alle incognite, l'equazione diventa una tautologia (un'affermazione dimostrabilmente vera).
Per esempio, l'equazione 
è risolvibile nell'incognita formula_2 da 
in quanto sostituendo formula_2 con formula_5 l'equazione sarà formula_6, un'affermazione vera. È anche possibile prendere in considerazione la variabile formula_7, e quindi la soluzione questa volta sarà formula_8. Oppure formula_2 and formula_7 possono essere trattate entrambe come incognite, e in questo caso ci sono più soluzioni dell'equazione, tra cui, ad esempio, formula_11 (cioè formula_12 e formula_13), formula_14, ed in generale formula_15 per ogni valore possibile formula_16.
A seconda del problema, il compito potrebbe essere quello di trovare una soluzione, o qualche soluzione, o tutte le soluzioni. L'insieme di tutte le soluzioni è detto insieme delle soluzioni. È anche possibile che l'obiettivo sia quello di trovare, tra le possibili, la soluzione migliore sotto qualche aspetto. Problemi di questo tipo sono chiamati problemi di ottimizzazione; risolvere un problema di ottimizzazione non è in genere chiamato "risoluzione di un'equazione".
Un'affermazione come" "un'equazione in formula_2 e formula_7"", o ""risolvere per formula_2 e formula_7"", significa che le incognite sono quelle indicate: in questo caso formula_2 e formula_7.
In un caso generale, abbiamo una situazione come:
dove formula_38 è una costante, che ha un insieme delle soluzioni formula_39 della forma:
dove formula_41 è il dominio della funzione. Si noti che l'insieme delle soluzioni può avere cardinalità arbitraria. Ad esempio:
Ad esempio, un'espressione come:
può essere risolto, per prima cosa si cerca di riscriverla in modo più leggibile matematica senza però modificare l'uguaglianza ad esempio possiamo portare tutto al primo membro sottraendo formula_46 da entrambi i lati dell'equazione ottenendo:
In questo caso particolare non vi è solo uno soluzione dell'equazione, ma un insieme infinito di soluzioni, che può essere scritto:
Una particolare soluzione è formula_49, formula_50, formula_51. In realtà, questo particolare insieme di soluzioni descrive un piano in tre dimensioni, che passa per il punto formula_52
se l'insieme delle soluzioni è vuoto, allora non ci sono formula_53 tali che:
diventa vera per un determinato formula_38.
Per esempio, esaminiamo il classico caso di una variabile, data una funzione:
Consideriamo l'equazione:
L'insieme soluzione è formula_43, in quanto nessun numero reale positivo risolve l'equazione. Tuttavia nel tentativo di trovare la soluzioni per l'equazione, se si modifica la definizione della funzione, più specificamente, della funzione del "dominio", siamo in grado di trovare la soluzioni a questa equazione. Quindi, se definiamo:
ha una serie di soluzioni formula_61, dove formula_62 è l'unità immaginaria. Questa equazione ha esattamente due soluzioni.
Abbiamo già visto che alcuni insiemi di soluzioni sono in grado di descrivere le superfici. Per esempio, nello studio della matematica elementare, si sa che l'insieme delle soluzioni di un'equazione in forma formula_63 con formula_16, formula_65, e formula_38 numeri reali a valori costanti, rappresenta una linea in uno spazio vettoriale formula_67 (cioè bidimensionale). Tuttavia, non sempre è facile rappresentare graficamente l'insiemi delle soluzioni.
Per esempio, la soluzione di un'equazione della forma formula_68 (con formula_16, formula_65, formula_38, formula_72, e formula_73 valori reali costanti) è un iperpiano formula_74.
I metodi per risolvere le equazioni, in generale, dipendono dal tipo di equazione, sia il tipo di espressioni nell'equazione che il tipo di valori che possono assumere le incognite. La varietà di tipi di equazioni è vasta, tanto quando i corrispondenti metodi risolutivi. Di seguito saranno trattati solo alcuni tipi specifici, un completo esame non è possibile.
In generale, data una classe di equazioni, può accadere che non esista un metodo sistematico (algoritmo) che garantisce la soluzione. Ciò può essere dovuto ad una mancanza di conoscenze matematiche, infatti alcuni problemi sono stati risolti solo dopo secoli di sforzo. Ma questo fa riflettere anche che, in generale, tale metodo non può esistere: alcuni problemi sono noti per essere irrisolvibili da un algoritmo, come il decimo problema di Hilbert, che è stato dimostrato irrisolvibile nel 1970.
Per le diverse classi di equazioni sono stati trovati algoritmi per risolverli, alcuni dei quali sono stati attuati e inseriti in sistemi di algebra computazionale, ma spesso richiedono solo "carta e penna". In altri casi, i metodi euristici conosciuti hanno spesso successo, ma non è garantito che portino al successo.
Se la soluzione di un'equazione è limitata, cioè è un insieme finito (come nel caso delle equazioni in aritmetica modulare, per esempio), o può essere limitata a un numero finito di possibilità (come nel caso di alcune equazioni diofantee), l'insieme delle soluzioni può essere trovato con la forza bruta, cioè verificando tutti i possibili valori e controllando se essi risolvono l'equazione. Può succedere, però, che il numero di possibilità da considerare, anche se finito, sia così grande che una ricerca con questo metodo è praticamente impossibile; su questa difficoltà si basano alcuni metodi di crittografia.
Come con tutti i tipi di problemi, procedere per tentativi ed errori può talvolta produrre una soluzione, in particolare quando la forma di un'equazione, o la sua somiglianza ad un'altra equazione nota (già risolta), possa portare ad un"'intuizione ispirata" della soluzione. Se un'intuizione, quando venga testata, non porta una soluzione, lo studio del modo in cui essa non funziona può portare ad una modifica e quindi alla soluzione.
Equazioni che coinvolgono semplici funzioni razionali o lineare, con una sola incognita appartenente al insieme dei reali, diciamo formula_2, come:
può essere risolta con metodi di algebra elementare, e applicando i principi di equivalenza.
I piccoli sistemi di equazioni lineari possono essere risolti con i metodi dell'algebra elementare. Per la risoluzione di sistemi di grandi dimensioni numeriche, gli algoritmi utilizzati si basano sull'algebra lineare.
I polinomi con grado minore al quinto possono essere risolte con metodi algebrici, tra cui ad esempio la formula quadratica è la più semplice. Equazioni polinomiali con un grado superiore al quinto richiedono metodi numerici (vedi sotto) o funzioni speciali come il trasporto di radicali.
Nelle equazioni diofantee le soluzioni devono appartenere ai numeri interi. In alcuni casi per risolverle può essere utilizzato un approccio mediante forza bruta, come indicato sopra. In altri casi, in particolare se l'equazione è ad una incognita, è possibile risolvere l'equazione per i valori razionali dell'incognita (vedi Teorema delle radici razionali), e quindi per trovare le soluzioni dell'equazione diofantea si limita la soluzione solo ai valori interi dell'insieme delle soluzioni. Ad esempio, l'equazione polinomiale
ha come soluzioni razionali formula_78 e formula_79, ma essendo un'equazione diofantea l'unica soluzione è formula_80.
Nel semplice caso di una funzione ad una variabile, per esempio, formula_81, possiamo risolvere un'equazione con la forma:
considerando che è nota la "funzione inversa" di formula_84.
Infatti, data una funzione formula_85, la funzione inversa (indicata con formula_86) e determinata da formula_87 è funzione tale che:
Ora, se applichiamo la funzione inversa a entrambi i membri della funzione:
otteniamo
e abbiamo trovato la soluzione dell'equazione. Tuttavia, a seconda della funzione, la funzione inversa può essere difficile da definire, o non può essere una funzione inversa se tutti i valori dell'insieme di formula_93 (o un sottoinsieme) non hanno un solo valore in formula_94.
Esempi di funzioni inverse includono la radice n-esima (l'inverso di formula_95), il logaritmo (l'inverso di formula_96), le funzioni trigonometriche inverse, e la funzione W di Lambert (l'inversa di formula_97).
Se il membro sinistro, cioè l'espressione formula_98, di un'equazione formula_99 può essere fattorizzato in formula_100, l'insieme delle soluzioni originale è costituito dall'unione della soluzione dei due insiemi di equazioni formula_101 e formula_102.Ad esempio, l'equazione goniometrica:
può essere riscritta in:
che può essere fattorizzata, utilizzando l'identità formula_105 (a condizione che i componenti siano definiti), diventando:
Le due equazioni formula_107 e formula_108 hanno l'identico insieme delle soluzioni
che è, quindi, la soluzione dell'equazione originale.
Con equazioni più complesse in reali o numeri complessi, i metodi semplici per risolvere l'equazioni possono fallire. Spesso, si può calcolare uno zero della funzione con metodi numerici come il metodo di Newton-Raphson che, per alcune applicazioni, può essere completamente sufficiente per risolvere certi problemi.
Un importante studio della matematica è volto ad esaminare se sia possibile generare qualche semplice funzione per approssimare un'equazione molto complessa ad punto dato. In effetti, polinomi in una o più variabili possono essere utilizzati per approssimare in questo modo le funzioni: queste sono note come serie di Taylor.
Le equazioni che coinvolgono matrici e vettori di numeri reali possono spesso essere risolte utilizzando metodi di algebra lineare.
C'è un vasto elenco di metodi per risolvere vari tipi di equazioni differenziali, sia numericamente che analiticamente. Un metodo per la ricerca di soluzioni analitiche a tempo indeterminato è l'algoritmo di Risch - che purtroppo è troppo complicato per l'uso con carta e penna.
</text>
</doc>
<doc id="3106888" url="https://it.wikipedia.org/wiki?curid=3106888">
<title>Operazione aritmetica</title>
<text>
Un'operazione aritmetica, in matematica, è un'operazione binaria tra numeri: partendo da almeno due numeri, detti «"operandi"», si ottiene un unico risultato (che è anch'esso un numero), dipendente dal tipo di operazione od «"operatore"» utilizzato.
Ogni operazione è identificata da un simbolo. Oltre all'operazione diretta esiste l'operazione inversa, che permette di risalire dal risultato ai numeri iniziali.
L'addizione (o somma) è l'operazione aritmetica più conosciuta: consiste nel sommare i singoli elementi, detti «addendi».
formula_1
Per esempio, con formula_2 e formula_3, si ha che: formula_4.
La sottrazione (o differenza) è l'operazione inversa all'addizione.
Essa consiste nella differenza tra due elementi: il primo è detto «minuendo», mentre il secondo «sottraendo».
formula_5
Per esempio, se formula_6 e formula_7 si ha che: formula_8.
Nell'insieme dei numeri interi formula_9 la sottrazione può essere considerata identica all'addizione, poiché: formula_10.
Le altre operazioni aritmetiche principali sono la moltiplicazione e la divisione (la sua operazione inversa). Le rispettive formule sono:
formula_11
formula_12
Nella risoluzione di un'espressione aritmetica, i calcoli relativi a moltiplicazioni e divisioni hanno la priorità rispetto alle addizioni e alle sottrazioni. 
Mentre i multipli di un numero sono infiniti, la divisione presenta dei limiti. In particolare:
formula_13
formula_14
formula_15
L'elevamento a potenza consiste nel moltiplicare il primo termine (la base) per se stesso, per il numero di volte indicato dal secondo termine (l'esponente). 
Per esempio, in formula_16 la base è formula_17 e l'esponente è formula_18 e si ha: formula_19.
La tetrazione è una ripetizione di elevamenti a potenza:
formula_20
</text>
</doc>
</document>
